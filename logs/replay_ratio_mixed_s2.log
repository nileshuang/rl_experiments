starting log


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.747517
Reward: 1.000000
Mean value of predictions: 0.0012588513
Proportion of valid SMILES: 0.7956181533646323
Sample trajectories:
BP(=O)(O)COP(=O)(O)O
Brc1ccc(-n2cccc2-c2ccccn2)cc1
Brc1cnc(NCC2CCN(Cc3ncccn3)CC2)s1
C#CC(=O)C(C)=NNC(=O)C1C=C(S(=O)(=O)c2ccc3ccccc3c2)CC=CC1
C#CC1C(OCC)OC(c2ccncc2)=CC1c1ccc(C(=O)c2cccc(Cl)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.020521894
Proportion of valid SMILES: 0.7751114158381899
Sample trajectories:
Brc1ccc(C2=Nc3ncnn3CCCc3ccccc32)o1
Brc1ccc(Nc2ncnc3ccncc23)cc1
Brc1ccc2[nH]c3c(c2c1)CCCC3
Brc1ccc2c(c1)NCc1cccnc1N2
Brc1ccc2nc(NN=Cc3ccccc3SCCCCCCCCCCCCCN3CCOCC3)sc2c1
Fine tuning...
Mean value of predictions: 0.03316537
Proportion of valid SMILES: 0.6856155778894473
Sample trajectories:
BrCCN=C(Nc1nc(-c2ccccc2)c2ncnc(Nc3ccccc3)c2n1)c1ccccc1
Brc1ccc(-c2cncnc2)c(CNc2ccccc2)c1
Brc1ccc(Nc2ncnc(Nc3ccc(Br)cc3)n2)cc1
Brc1ccc(Nc2ncnc3cccc(Br)c23)cc1
Brc1ccc(Nc2nnc(NCCN3CCOCC3)c3ccccc23)cc1

  2 Training on 397 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.318637
Reward: 1.111429
Trajectories with max counts:
14	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.032444645
Proportion of valid SMILES: 0.6972274732199117
Sample trajectories:
Brc1ccc(Br)c(Br)c1
Brc1ccc(N=Nc2nccs2)cc1
Brc1ccc(Nc2ccnc(-c3nc4ccccc4s3)n2)cc1
Brc1ccc2ccc(Br)c(CCCN3CCCCCC3)c2c1
Brc1ccc2nc(-c3ccccc3)ccc2c1
Policy gradient replay...
Mean value of predictions: 0.072287664
Proportion of valid SMILES: 0.5097977243994943
Sample trajectories:
Bc1ccc(Nc2ncnc3c(Nc4ccc(Br)cc4)ncnc23)cc1
Brc1ccc(-c2cnc(Nc3c4ccccc4cc(Br)ccc3n3cnnn3)nc2)cc1
Brc1ccc(Nc2cnc3cccc(Br)c3c2)cc1
Brc1ccc(Nc2nc(NC3CCCCC3)oc3ncnc23)cc1
Brc1ccc(Nc2nc(Nc3ccsc3)ncc2-c2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.1053209
Proportion of valid SMILES: 0.5707576706324358
Sample trajectories:
Brc1cc(Nc2ncnc3ccccc23)ncn1
Brc1cc2c(Br)c(Br)ccc2o1
Brc1ccc(C=NNc2ncnc3sccc23)cc1
Brc1ccc(Nc2ccnc3cc(Br)nc(Nc4ccccc4)c23)cc1
Brc1ccc(Nc2nc(-c3ccccc3)nc3ccccc23)cc1

  3 Training on 955 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.181208
Reward: 1.076488
Trajectories with max counts:
16	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.12014134
Proportion of valid SMILES: 0.5321215919774366
Sample trajectories:
BP(=O)(CC(F)F)C(F)(F)F
Brc1cC(Nc2cccs2)=Nc2nc3ccccc3nc2c2c(Br)c(cc1)Nc1ncnc2c2ccc(Br)cc12
Brc1ccc(Br)c(Br)c1
Brc1ccc(NN=Cc2cccnc2)cc1
Brc1ccc(Nc2ccnc(N3CCCC3)c2CNc2cnc3cccc(Br)c3n2)nc1
Policy gradient replay...
Mean value of predictions: 0.14801179
Proportion of valid SMILES: 0.43207126948775054
Sample trajectories:
Brc1cc(Br)c2c(Nc3nc(Br)nc(Nc4ccnc5cc(Br)c(Br)c(Br)c45)n3)ccnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1ccc(Br)c(Br)c1
Brc1ccc(CCNc2nc3c(Nc4ccc(Br)cc4)ncnc3s2)cc1
Brc1ccc(Nc2ncnc(Nc3cncc(Br)c3)n2)cc1
Fine tuning...
Mean value of predictions: 0.15877192
Proportion of valid SMILES: 0.5721455457967378
Sample trajectories:
BrCCCCCCCCCNc1ccc2nc3ccccc3nc2c1
BrCc1cccc(Nc2nc(Nc3ccsc3)nc3ccccc23)n1
Brc1ccc(-c2nc3ccccc3nc2-c2occc2-c2cc3ccccc3cc2-c2ccccc2)cc1
Brc1ccc(-c2ncnc3ccc(-c4ccc5nc(N6CCCCC6)c(-c6cccc(-c7ccccc7)c6)nc5c4)cc23)cc1
Brc1ccc(-c2nsc(-c3ccc(Br)cc3)n2)cc1

  4 Training on 1905 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 20.481602
Reward: 1.413937
Trajectories with max counts:
31	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.17705788
Proportion of valid SMILES: 0.5891739674593242
Sample trajectories:
BP(=O)(SCC#C)c1ccc(Br)cc1
Brc1cc(Br)c(Nc2cc(-c3ccccc3)ccn2)c(Br)c1
Brc1ccc(-c2ccccc2)o1
Brc1ccc(-n2c(Nc3ccc4c(c3)CCCC4)nc3ccccc32)cc1
Brc1ccc(Br)c(Nc2nccc(-c3ccc(Nc4ncnc5ccc(Br)cc45)cc3)n2)c1
Policy gradient replay...
Mean value of predictions: 0.23037282
Proportion of valid SMILES: 0.5712496085186345
Sample trajectories:
BP(=O)(Nc1ccc(F)cc1)N(=O)=O
BP(=O)(OC(C)C)C(F)F
Brc1cc(-c2ccccn2)c(Br)c(-c2ncnc3cccnc23)n1
Brc1cc2ncnc(N3CCOCC3)c2nc1NCc1cccnc1
Brc1ccc(-c2cc(Nc3cccs3)ncn2)cc1
Fine tuning...
Mean value of predictions: 0.21861497
Proportion of valid SMILES: 0.5654761904761905
Sample trajectories:
BP(=O)(CCl)NO
BP(=O)(NCCCCCO)n1cc(-c2ccc3ccccc3c2)nc1SCC(=O)N(CC)CC
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BrC1=C2C3=C(Br)C2CCC3C1=CNc1cccc2cnc3ccccc3c12

  5 Training on 3272 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 21.629597
Reward: 1.572696
Trajectories with max counts:
34	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.24429066
Proportion of valid SMILES: 0.5430629502035703
Sample trajectories:
BP(=O)(OCC(=O)CN(C#N)CC)c1ccc(Br)cc1
BP1(=O)OCC(OC(=O)C(Cl)Cl)C(c2nc(Br)[nH]c2Br)c2c(Nc3cc(Br)c(Br)c(Br)c3)cc(Br)cc2O1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.31559733
Proportion of valid SMILES: 0.566593544343466
Sample trajectories:
Brc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1
Brc1cc(Br)c2ncnc(Nc3cccnc3)c2c1
Brc1ccc(-c2c(Br)ccc3ncsc23)c(Br)c1Br
Brc1ccc(-c2c(Cc3ncnc4c3ncn4N3CCCCO3)cc3ncnc(Nc4ccccn4)nc23)cc1
Brc1ccc(-c2cc(Br)cnc2Br)cc1
Fine tuning...
Mean value of predictions: 0.29777086
Proportion of valid SMILES: 0.6031894934333959
Sample trajectories:
BrCCNc1ncnc2sc3c(c12)CCCCC3
Brc1cc(Br)c2ccc(NNc3ccnc4ccccc34)cc2c1
Brc1cc(Br)cc(Nc2nc3cc(Br)ccc3s2)c1
Brc1cc2ncnc(Nc3ccc(I)cc3)c2c(Br)c1Br
Brc1ccc(-c2cc(Br)ccc2Br)cc1

  6 Training on 4962 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 20.745762
Reward: 1.567394
Trajectories with max counts:
39	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.287402
Proportion of valid SMILES: 0.5978125
Sample trajectories:
BP(=O)(OCC)OC1CC(O)C(O)C(OP(=O)(O)O)C1
BrC1=NSC(=Nc2ccc(Br)cc2)N1
BrCCCCC=CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCBr
BrCCCCCBr
Brc1ccc(-c2cccc(Br)c2Br)cc1
Policy gradient replay...
Mean value of predictions: 0.32653537
Proportion of valid SMILES: 0.5422557335846685
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCCC(=O)NCCCCN
BrC(Br)=Nc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCCC(Br)(CBr)CBr
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC=CCNC1CCCCCCCCCCCCCCCCCCCCC1
BrCCCN1CC(Cc2cc(Br)c(Br)cc2Br)CCO1
Fine tuning...
Mean value of predictions: 0.34917128
Proportion of valid SMILES: 0.5675760426465977
Sample trajectories:
Brc1cc(Br)c(-c2cccc(I)c2)cc1Nc1ncnc2cncnc12
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(I)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1

  7 Training on 6740 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 24.547747
Reward: 2.050365
Trajectories with max counts:
30	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.3360544
Proportion of valid SMILES: 0.5983093299937383
Sample trajectories:
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3scnc23)o1
Brc1cc(Br)c2ncnc(Nc3ccc(Nc4cc(Br)c(Br)cn4)cc3)c2c1
Brc1cc(Br)cc(Nc2ncnc(Nc3cccc(NCc4ccncc4)c3)c2Nc2ncnc3[nH]cnc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.36132994
Proportion of valid SMILES: 0.6117021276595744
Sample trajectories:
BP(=O)(CCCO)Nc1nccc(Nc2ccc(Br)cc2)n1
BP(=O)(NC(=O)COc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(Nc1cnc(Nc2ccc(Br)cc2)c2c(F)c(F)c(F)cc12)c1ccc(F)c(F)c1
BP(=O)(OCC1OC(N2C=CC(=O)NS(=O)(=O)CCN2C)C(O)C1O)C(=O)OCCc1cccc2ccc(Br)cc12
BrC#Cc1ncnc2snc(Nc3ccc(Br)cc3)c12
Fine tuning...
Mean value of predictions: 0.35719556
Proportion of valid SMILES: 0.5928125
Sample trajectories:
BP(=O)(OCC)c1ccc(Nc2ncnc3cc(Br)ccc23)cc1F
BrCCN(c1ccccc1)c1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BrCCSc1cccc2ncnc(Nc3cccc(Br)c3)c12
Brc1cc(Br)c2cc(Br)c(Nc3ncnc4cncnc34)nc2c1
Brc1cc(Br)c2ncnc(-c3cc(Br)c(Br)cn3)c2n1

  8 Training on 8661 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 24.425086
Reward: 2.075824
Trajectories with max counts:
35	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.36117256
Proportion of valid SMILES: 0.5660613650594866
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)(O)C1O)N(=O)=O
B[PH](=O)(O)(CCCl)CCCN
BrC=CC=CCC=CCCCC=CBr
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.40824527
Proportion of valid SMILES: 0.5923606762680025
Sample trajectories:
BP(=O)(CCCCCCCCCC)Nc1cc(Br)c(Br)cc1Br
BP(=O)(CCl)Nc1ccc(Br)cc1
BP(=O)(Cl)OCOC(=O)C(Cl)(Cl)Cl
Bc1cc(Br)cc(Br)c1OC1CN(Cc2ccc(Br)cn2)CCN1
BrSSc1sc2ccc(Nc3ncnc4nc(Br)sc34)cc2c1Br
Fine tuning...
Mean value of predictions: 0.38739318
Proportion of valid SMILES: 0.5859154929577465
Sample trajectories:
BP(=O)(SCCS)C(O)(Br)Br
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BrC=CC=Cc1sccc1Br
BrC=CCN1CCN(c2cc(Br)ccc2Br)CC1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)cc(Br)c23)c1

  9 Training on 10298 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 23.345244
Reward: 2.074601
Trajectories with max counts:
36	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.39778027
Proportion of valid SMILES: 0.5641828428303068
Sample trajectories:
BP(=O)(Nc1ccnc2c(Br)c(Br)c(Br)cc12)OCCOCCn1cnc2c(N)ncnc21
BP1(=O)CCCN(C(=O)c2cccc(Br)c2)C(=O)C(Cl)OC1=O
Bc1ccc(Br)c(Nc2ncnc3ccc(Br)c(Br)c23)c1
BrC1=CCN(c2cc(Br)ccc2Nc2ncnc3cccc(Br)c23)CCC1
BrCc1nc2c(Nc3ccc(Br)cc3Br)ncnc2cc(Br)cc1Nc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.42183727
Proportion of valid SMILES: 0.5956848030018762
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ccnc3ccc(I)cc23)c(Br)s1
Brc1cc2nc(Br)c(Br)c(Br)c2cc1Br
Fine tuning...
Mean value of predictions: 0.41538462
Proportion of valid SMILES: 0.5983093299937383
Sample trajectories:
BP(=O)(NC(=O)OCC)c1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CSc1nc2c(Br)c(Br)c(Br)c(Br)c2s1
BP(=O)(c1ccc(Br)cc1)N(CC(=O)Nc1ccc(Br)cc1)c1ccc(Br)c(F)c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1

 10 Training on 11901 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.585389
Reward: 2.148845
Trajectories with max counts:
26	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.38676402
Proportion of valid SMILES: 0.6421875
Sample trajectories:
BrCCCN1CCc2ncnc(Nc3ccccc3)c2C1
Brc1cc(Br)c2c(Nc3ccccc3)ncnc2c1
Brc1cc2nc3ccncc(c4cccsc(ncn4)N2)c3n1
Brc1cc2ncnc(Nc3ccncn3)c2cc1Br
Brc1ccc(-c2ccc(-c3ncn[nH]3)c(Br)c2)cc1
Policy gradient replay...
Mean value of predictions: 0.4374502
Proportion of valid SMILES: 0.6292698213726104
Sample trajectories:
BP(=O)(OCC)c1cc(Br)c(Br)c(Br)c1Br
Bc1cc(Br)cc(CO)c1
BrCCC=CC=C=NOc1ccc(Br)cc1Br
Brc1cc(Br)c(Nc2ncnc3ccc(-c4ccccc4)cc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.42621797
Proportion of valid SMILES: 0.6223819943732416
Sample trajectories:
BrCCBr
BrCCCNc1ccc2c(c1)Nc1ccccc1N2
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(-c4ccc5ccncc5c4)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

 11 Training on 13762 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.618315
Reward: 2.116805
Trajectories with max counts:
60	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.37451613
Proportion of valid SMILES: 0.58125
Sample trajectories:
BP(=O)(CCCCCC=CCC(=O)O)OCC
BP(=O)(NP(=O)(O)c1ccc(Nc2ncnc3ccccc23)cc1)OCC
BP(=O)(OCC(=O)c1ccc(Br)c(Br)c1)c1ccccc1
BP(=O)(OCC1C=CC(=O)C=CC=C1O)Oc1ccc(Br)cc1
BP(=O)(Oc1ccccc1)Oc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.44430313
Proportion of valid SMILES: 0.5920928773140884
Sample trajectories:
BP(=O)(O)CSCCCCCCCCC(N)=O
BP(=O)(OCC)OC(=O)CSc1nc(Br)c(N)s1
BrCCCCCCCCCCCN1CCN(CC2CCCCC2)CC1
Brc1cc(Br)c(Br)[nH]1
Brc1cc(Br)c(Br)c(Br)c1Br
Fine tuning...
Mean value of predictions: 0.42421916
Proportion of valid SMILES: 0.6511264080100125
Sample trajectories:
B[PH](=O)(Cl)(NP(=O)(O)Oc1ccc2nc(Br)sc2c1)OCC(=O)OCCl
Bc1cc(Br)cc(NC(=O)C2CCCN2)c1
BrCCCc1ccc(Nc2ncnc3ccccc23)cc1
BrCc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Brc1cc(Br)c(Br)c(Br)c1

 12 Training on 15503 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.606070
Reward: 2.657065
Trajectories with max counts:
78	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.44931972
Proportion of valid SMILES: 0.551594746716698
Sample trajectories:
BP(=O)(COC(=O)CN)C(=O)N(C)C(=O)Nc1cc(Cl)c(Br)c(Br)c1
BP(=O)(OCC)OC(=O)CCCCCCCCCCCN
Bc1ccc(Nc2ncnc3ccnc(Br)c23)cc1
BrCCN(c1ccc(Br)cc1)c1ncnc2ccccc12
BrCN1CCN(Cc2cncnc2)CC1
Policy gradient replay...
Mean value of predictions: 0.52874535
Proportion of valid SMILES: 0.5910513141426783
Sample trajectories:
BP(=O)(O)CCCCCCCCCCCCCC(=O)Nc1ccc(I)cc1
Bc1cc(Br)cnc1Nc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)c(Cl)c1
Brc1cc(-c2cnc3c(Br)cnc(Br)c3n2)c2cncnc2c1Br
Brc1cc(Br)c(-c2ccc(Nc3ncnc4ccsc34)c(Br)c2)cc1Br
Fine tuning...
Mean value of predictions: 0.50458354
Proportion of valid SMILES: 0.6344590368980613
Sample trajectories:
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3cnc(Br)cc3Br)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

 13 Training on 17674 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.990690
Reward: 2.438090
Trajectories with max counts:
15	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.48806965
Proportion of valid SMILES: 0.6124176857949201
Sample trajectories:
BP(=O)(C(=O)Nc1c(Cl)cc(Br)cc1Br)N(O)C=O
BrCc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Nc2cccc(Nc3ncnc4ccc(Br)cc34)c2)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3c(Br)c(Br)sc23)s1
Policy gradient replay...
Mean value of predictions: 0.4938053
Proportion of valid SMILES: 0.6360225140712945
Sample trajectories:
BP(=O)(Br)C=CBr
BP(=O)(C=CS(=O)(=O)NCCO)OCCO
BP(=O)(c1ccc(Nc2ncnc3c4c(Br)ncnc4c23)cc1Cl)C(F)(F)P(F)(F)(F)F
BrCCCCCCCNC1=Nc2cc(Br)ccc2NCCCN1
Brc1cc(Br)c(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.4887424
Proportion of valid SMILES: 0.616635397123202
Sample trajectories:
Bc1cc(Nc2ccc(Br)cc2Br)c(Br)c(Br)c1Br
Bc1ccc(Br)cc1
BrCCNc1ccc2ncnc(Nc3ccnc4ccc(Br)cc34)c2c1
Brc1cc(Br)c(Br)[nH]1
Brc1cc(Br)c(Nc2ncnc3c(Br)csc23)cc1Br

 14 Training on 19913 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.718365
Reward: 2.467552
Trajectories with max counts:
60	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.41953003
Proportion of valid SMILES: 0.5986245701781807
Sample trajectories:
BP(=O)(CC=C)CNc1ccc(Br)cc1
BP(=O)(N(O)C(Cl)(Br)Br)S(=O)(=O)c1ccc(Br)cc1
BP(=O)(NC(=O)OCC(Br)CBr)NC(c1cccc(Br)c1)P(=O)(O)O
BP(=O)(NS(=O)(=O)c1ccc(Nc2ncnc3ccc(Br)cc23)cc1)c1ccccc1Br
BP(=O)(O)OCC(=O)Nc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.47974497
Proportion of valid SMILES: 0.6377854238348452
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3cccc(-c4ccccc4Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Nc2ccc(Br)c(Br)c2)c2sc(-c3ccccc3Br)nc2c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.5162393
Proportion of valid SMILES: 0.6219512195121951
Sample trajectories:
Bc1ccnc(Nc2ncnc3c(Nc4ccccc4)cc(Br)cc23)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)cnc23)c1
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(I)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ccncc2c1

 15 Training on 21984 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.764492
Reward: 2.834515
Trajectories with max counts:
43	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5175502
Proportion of valid SMILES: 0.607567229518449
Sample trajectories:
BP(=O)(OCCBr)N(=O)=O
Brc1cc(-c2ccc(Nc3ncnc4ccccc34)cc2)c2cncnc2c1Nc1ncnc2scc(Br)c12
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)nc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.5792577
Proportion of valid SMILES: 0.616614420062696
Sample trajectories:
BIc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCCCc1cc2nncnc2c2ncnc(Nc3ccc(Br)cc3)c12
BrCN1CCN(CCOc2cc(Br)c3ccncc3c2)CC1
Brc1cc(Br)c(Nc2ncnc3c(Br)sc(Br)c23)s1
Brc1cc(Br)c(Nc2ncnc3cc(Br)cc(Br)c23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.5394737
Proportion of valid SMILES: 0.6420525657071339
Sample trajectories:
BP(=O)(O)CNC(=O)CCCCCCCCCCCCCCCCC
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(-c2nc3ccccc3s2)sc1Nc1ncccn1
Brc1cc(Br)c(Nc2ncnc3c(Nc4cccc(I)c4)ncnc23)c(Br)c1Br
Brc1cc(Br)c2c(Nc3cc(Br)c4ccccc4c3)ncnc2c1

 16 Training on 24532 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.598599
Reward: 2.645353
Trajectories with max counts:
42	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.55499494
Proportion of valid SMILES: 0.6203442879499218
Sample trajectories:
BP(=O)(CCCl)NP(=O)(CCCl)N(C)[PH](Cl)=N(=O)O
BP(=O)(N(O)C(=O)C(Cl)(C(=O)Nc1cc(F)cc(F)c1F)P(=O)(Oc1ccc(Br)cc1)P(=O)(O)Oc1ccc(F)cc1F)N(O)C(C)C
BP(=O)(Nc1c(Br)cc(Br)cc1I)C(=O)Nc1ccc(Br)c(Br)c1
BP(=O)(OCC)OC(=O)CCC=CCCCCCCCCCC
BP(=O)(c1ccc(Cl)c(F)c1)N(CC(=O)Nc1cc(Br)cc(Br)c1)P(=O)(O)N(C=O)Oc1cc(Br)c(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.514183
Proportion of valid SMILES: 0.6086956521739131
Sample trajectories:
Bc1ncncc1-c1c(Br)ccc2c(Br)c(Br)ccc12
BrCCc1ccc(-c2ccccc2)c2c1-c1ccccc12
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Nc2ncnc3sc(Br)cc23)c(Br)s1
Fine tuning...
Mean value of predictions: 0.5615269
Proportion of valid SMILES: 0.6221875
Sample trajectories:
BP(=O)(CC(=O)Nc1ccc(Br)cn1)OCCOc1nc(Br)sc1Br
BrIc1cncc2nc(N3CCCCCCC3)c(Br)nc12
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)n3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1

 17 Training on 27057 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.437523
Reward: 2.990085
Trajectories with max counts:
42	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.53606313
Proportion of valid SMILES: 0.6346274264245461
Sample trajectories:
BP(=O)(OCC)N(Br)C(=O)OC(Cl)(Br)Br
Bc1ccc(Br)c(Br)c1S(=O)(=O)Nc1cc2ncnc(Nc3ccc(Br)s3)c2c2c(Br)c(Br)cc(Br)c12
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Bc1nc2cc(Nc3ccc(I)cn3)ccc2[nH]1
BrCCCCCCC=CCCCCCCCCCCCCCCCBr
Policy gradient replay...
Mean value of predictions: 0.5530693
Proportion of valid SMILES: 0.63125
Sample trajectories:
Bc1cc(Br)ccc1Nc1ncnc2c(I)c(Br)ccc12
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrC(=NN1CCN(c2cccc(Br)c2)CC1)c1ncc2ncnc(Nc3ccc(Br)cc3)c2n1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccsc12
Brc1cc(Br)c(-c2ncnc3ccsc23)cc1Br
Fine tuning...
Mean value of predictions: 0.5771541
Proportion of valid SMILES: 0.6352830778855176
Sample trajectories:
BP(=O)(O)C(CCCCCBr)P(B)(=O)N[PH](=O)(=O)N(O)C=O
BP(=O)(OCC)C(=O)c1cc(Nc2ncnc3cccc(F)c23)sc1-c1cc(Br)c(O)c(Br)c1
BrC=CCC=CCC=CC=CC=CC=CC=CC=CCC=CC=CC=CCCBr
Brc1cc(Br)c(Br)c(Nc2ncnc3sc(Br)cc23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1

 18 Training on 29765 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.155589
Reward: 3.206161
Trajectories with max counts:
57	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5426068
Proportion of valid SMILES: 0.570625
Sample trajectories:
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Nc2ncnc3ccsc23)c(Br)c1
Brc1cc(Br)c2c(Br)cc(Br)c(Br)c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccccc4)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ccnc2c1
Policy gradient replay...
Mean value of predictions: 0.5661505
Proportion of valid SMILES: 0.5856785490931833
Sample trajectories:
BP(=O)(OCC)Oc1c(Br)cc(Br)c(Br)c1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(-c4ccccc4Br)cc3)c2c1
Brc1cc(Br)c2ncnc(Nc3ccc(OCCN4CCOCC4)cc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.5684639
Proportion of valid SMILES: 0.6149515170472318
Sample trajectories:
BP(=O)(Nc1ccc2c(Nc3cc(Br)c(Br)cc3F)c(N3CCCC(F)(F)C3)nc(F)c2c1)OCC1CCCO1
BP(=O)(OCC)C(F)(F)F
BrC=C(Br)Br
BrCCBr
BrCCN1CCN(Cc2cncn2-c2ccc(Br)cc2)CC1

 19 Training on 32262 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.855937
Reward: 3.133710
Trajectories with max counts:
45	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5402
Proportion of valid SMILES: 0.625
Sample trajectories:
BP(=O)(NN(=O)=O)OCC
B[PH](=O)(Nc1ccc(Br)cc1)(c1ccc(Br)cc1)c1ccc(Br)cc1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccccc12
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.6360902
Proportion of valid SMILES: 0.5827856025039123
Sample trajectories:
BP(=O)(CC(O)CCl)NCCCN
BP(=O)(Nc1cc(Br)cc(Br)c1)P(=O)(O)C(F)(F)F
BP(=O)(O)c1ccc(Nc2ncnc3c(Br)cc(Br)c(Br)c23)o1
BrCCNc1ncc(Br)c2sc(Nc3cc(Br)ccc3Br)cc12
Brc1cc(Br)c(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.6087956
Proportion of valid SMILES: 0.6253125
Sample trajectories:
BP(=O)(OCC)OCCCCCCCCCCCCCCCCCNS(=O)(=O)NCCO
BrCCCCCCCCCCCCCCCCCCCCNCCCCNc1ccc2sccc2c1
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1

 20 Training on 35067 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.021241
Reward: 3.224095
Trajectories with max counts:
41	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5969136
Proportion of valid SMILES: 0.6075
Sample trajectories:
Bc1cc(Br)cc(Nc2ncnc3cc(Br)cc(Br)c23)c1
BrCCCCCCCCCCCCCNc1ccc(Nc2ncc(Br)cn2)cc1
BrCCCCCCCCCCCNc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrSc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.6334356
Proportion of valid SMILES: 0.61125
Sample trajectories:
BP(=O)(OCC(=O)Nc1cc(Br)c(Br)c(Br)c1)Oc1ccc(Br)cc1
BP(=O)(Oc1ccc(Br)cc1)Oc1ccc(Br)cc1
Bc1cnc(Nc2ncnc3sc(Br)nc23)cc1Br
BrNc1cccc(Br)c1Br
Brc1cc(Br)c(Br)cc1Br
Fine tuning...
Mean value of predictions: 0.62049776
Proportion of valid SMILES: 0.6409133562715046
Sample trajectories:
Bc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3scc(Br)c23)cc1
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)c(Nc2ncnc3ccc(NCC4CCCC4)cc23)cc1Br

Trajectories with max counts:
172	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5876874
Proportion of valid SMILES: 0.5384663497623218
Mean Internal Similarity: 0.46491209434540126
Std Internal Similarity: 0.095139739130603
Mean External Similarity: 0.41498881037765084
Std External Similarity: 0.07156773370863047
Mean MolWt: 426.8021279572106
Std MolWt: 107.89518386133116
Effect MolWt: -0.6948952113340253
Mean MolLogP: 4.884183225673731
Std MolLogP: 1.486838295485168
Effect MolLogP: 0.11711562083108851
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.407698% (1240 / 1273)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5658.854799985886, 'valid_fraction': 0.5384663497623218, 'active_fraction': 0.5646416540829364, 'max_counts': 172, 'mean_internal_similarity': 0.46491209434540126, 'std_internal_similarity': 0.095139739130603, 'mean_external_similarity': 0.41498881037765084, 'std_external_similarity': 0.07156773370863047, 'mean_MolWt': 426.8021279572106, 'std_MolWt': 107.89518386133116, 'effect_MolWt': -0.6948952113340253, 'mean_MolLogP': 4.884183225673731, 'std_MolLogP': 1.486838295485168, 'effect_MolLogP': 0.11711562083108851, 'generated_scaffolds': 1273, 'novel_scaffolds': 1240, 'novel_fraction': 0.974076983503535, 'save_path': '../logs/replay_ratio_mixed_s2-1.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.664134
Reward: 1.000000
Mean value of predictions: 0.0
Proportion of valid SMILES: 0.7974286610222641
Sample trajectories:
BP(=O)(OCCC(Cl)Cl)P(=O)(OC(Cl)Cl)C(=O)O
Brc1cc(C[P+](CCCCc2c[nH]c3ccccc23)C(Cc2ccccc2)=Nc2ccccc2)c2ccccc2n1
Brc1ccc(CN2CCN(Cc3ccc4c(c3)OCO4)CC2)cc1
C#CC1(O)CCC2C(O)C(O)C3CC(C)C1(C)CC23
C#CCC(=O)c1ccc(C)cc1O
Policy gradient replay...
Mean value of predictions: 0.017065557
Proportion of valid SMILES: 0.6038328620797989
Sample trajectories:
Brc1ccc(Cn2cnc(Nc3c4cc(Nc5ccccc5)c4nc4cccnc34)n2)c(Br)c1
Brc1ccc(Nc2ncnc3nc(-c4ccncn4)oc23)cc1
Brc1ccc(Nc2ncnc3nccnc23)cc1
Brc1ccc2nonc2c1-c1ncn[nH]1
C#CC=CCN(CC=C)CC(=O)Nc1ccc(F)c(CCC(=O)O)c1
Fine tuning...
Mean value of predictions: 0.025586968
Proportion of valid SMILES: 0.6540269507991225
Sample trajectories:
Brc1cc(I)c2sc3ncccc3c2c1
Brc1ccc(Nc2cc(-c3ccncc3)c3ccccc3n2)cc1
Brc1ccc(Nc2ncnc3sc(CCNCC4COC4)cc23)cc1
Brc1ccc2c(c1)N=C(Cc1ccccc1)N1CCCC(c3ccccc3)C1=N2
Brc1cccc(Nc2ncnc3nc(-c4ccncn4)sc23)c1

  2 Training on 342 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.652162
Reward: 1.011979
Trajectories with max counts:
2	Cc1ccc(Nc2ncnc3ccc(Cl)cc23)cc1
2	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.020505618
Proportion of valid SMILES: 0.6712759270898806
Sample trajectories:
Brc1cc(Br)c(Nn2cnc3c(I)cccc32)s1
Brc1ccc(-c2c(Br)cc(-c3ccccc3)n2CCN2CCNCC2)cc1
Brc1ccc(CN2CCN(C3CCCO3)CC2)cc1
C#CC(O)C(=O)C=CCSCCCc1cccnc1
C#CC1(O)CC2(c3ccccc3)C=C(C(=O)OC(C)(C)C)C(=O)C(CC=C(C)C)C(=C)C12
Policy gradient replay...
Mean value of predictions: 0.06899458
Proportion of valid SMILES: 0.5213433772755807
Sample trajectories:
BP(=O)(CCCl)N=[PH](=O)(O)O
Brc1cc2ncnc(Nc3ccc4c(c3)CC=CC=N4)c2s1
Brc1ccc(-c2cc(-c3c(-c4ccc(Br)cc4)nnc4ccccc34)c3ccccc3n2)cc1
Brc1ccc(-c2cccc(Nc3ncnc4sccc34)c2)c2cncnc12
Brc1ccc(-c2cccnc2)c2sc(Nc3ncnc4ccccc34)c(-c3ccccn3)c12
Fine tuning...
Mean value of predictions: 0.07958116
Proportion of valid SMILES: 0.5385338345864662
Sample trajectories:
Brc1ccc(-c2ccc(CN3CCOCC3)c(-c3cn4ccncc4n3)c2)c(Cc2ccccc2)c1
Brc1ccc(-c2ccnc3ccccc23)c(-c2ccncn2)c1
Brc1ccc(-c2nc3cc(I)ccc3nc2CCCCCCCCCN2CCOCC2)cc1
Brc1ccc(-c2ncnc3ccncc23)[nH]1
Brc1ccc(-n2ccc(-c3ccc4[nH]cnc4c3)n2)cc1

  3 Training on 782 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 20.851149
Reward: 1.228338
Trajectories with max counts:
6	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.07850255
Proportion of valid SMILES: 0.5531848133040477
Sample trajectories:
BrC1(c2cc3cnccc3o2)CCCC2=C1C=CC=CC=NN2
BrCCN1CCN(Cc2ccccc2)CC1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2c3cncnc3nc3c(Br)cccc23)cc1
Brc1ccc(Nc2nc(-c3ccccn3)c3cc(Br)ccc3n2)cc1
Policy gradient replay...
Mean value of predictions: 0.12126984
Proportion of valid SMILES: 0.5924764890282131
Sample trajectories:
BP(=O)(OCC)OC(=O)C(Cl)(Cl)P(=O)(OCC)OC(=O)CS
BrCC(Br)(Br)Br
Brc1c2ccccc2ncnc2ncnc(Nc3cccnc3)c12
Brc1cc2c(Nc3cccnc3)ncnc2cn1
Brc1ccc(Nc2cc3nccc(ncn2)nc2ncc(Nc4cnccn4)ccc2-3)cc1
Fine tuning...
Mean value of predictions: 0.13115264
Proportion of valid SMILES: 0.603194487942374
Sample trajectories:
BP(=O)(OCC)c1ccc(Nc2cc(Br)cc(Br)c2)cc1
Brc1ccc(-c2ccc3cnccc3c2)c2cncnc12
Brc1ccc(-c2cnc3cccc(-c4ccccc4)c3c2)nc1
Brc1ccc(COc2ccc(Br)cc2Nc2ncnc3[nH]cc(Br)c23)cc1
Brc1ccc(Nc2cc(Br)cc3ncnc(Nc4ccc(Br)cc4)c23)cc1

  4 Training on 1652 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 21.094029
Reward: 1.302711
Trajectories with max counts:
10	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.1363145
Proportion of valid SMILES: 0.6385315343583308
Sample trajectories:
Br
BrCCCCCCCC=CCC=CCC=CCC=CI
BrCCCCCCN1CCCCCCC1
BrCCN(c1ccccc1)c1ccncc1
Brc1cc2ncnc(Nc3ccc(Br)s3)c2cn1
Policy gradient replay...
Mean value of predictions: 0.16140172
Proportion of valid SMILES: 0.6155048452641451
Sample trajectories:
BrCCCCCCCC[n+]1ccc2ncsc2c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Nc1ncccn1
Brc1ccc(NN=Cc2cccnc2)cc1
Brc1ccc(Nc2nc3ccccc3s2)cc1
Brc1ccc(Nc2ncnc3cccnc23)cc1
Fine tuning...
Mean value of predictions: 0.1776053
Proportion of valid SMILES: 0.5660495764041418
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3nc(Br)cn23)c1
Brc1ccc(-c2ccc3c2c(Br)ccc2nc4cc([nH]c4Nc4ccccc4)n23)cc1
Brc1ccc(-c2nc(Br)cc(Nc3ncnc4ccccc3[nH]4)n2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2cc3nc4cc(Br)ccc4nc23)cc1

  5 Training on 2829 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 20.426964
Reward: 1.361535
Trajectories with max counts:
5	Clc1ccc(Nc2ncnc3scnc23)cc1
Mean value of predictions: 0.18697974
Proportion of valid SMILES: 0.5900503778337531
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCBr
Brc1[nH]c2c(Br)cccc2c1Br
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4nc(Br)c34)s2)c(Br)c1
Brc1cc(Br)c2c(Nc3ncc(Br)s3)ncnc2c1
Brc1cc(Br)c2sc(Nc3ncnc4cc(Br)sc34)nc2c1
Policy gradient replay...
Mean value of predictions: 0.2523322
Proportion of valid SMILES: 0.5516159397552557
Sample trajectories:
BrCCN1c2ncccc2c2c(-c3cnc(Br)nc3)c[nH]c21
BrCc1ccc2c(Nc3ccc(Nc4ccc(Br)cc4)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3cc(Br)c4ccccc4n3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccccc23)ncn1
Fine tuning...
Mean value of predictions: 0.23531894
Proportion of valid SMILES: 0.5946708463949844
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc2ncnc(Nc3ccc4c(c3)OCCO4)c2cc1OC1CCOC1
Brc1cc2sc(Br)c(Br)c2c2c1-c1c(Nc3ccccc3)ncnc1N2
Brc1ccc(-c2c[nH]c3ccc(Br)cc23)cc1
Brc1ccc(-c2ccsc2)c2nncn2C1c1ccc(Br)cc1

  6 Training on 4225 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 22.537796
Reward: 1.886198
Trajectories with max counts:
30	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.26348227
Proportion of valid SMILES: 0.6088180112570356
Sample trajectories:
BrCc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc2c(cc1CSc1ncccn1)OCC2
Brc1cc2c(nc1-c1ncc[nH]1)c1ccsc12
Brc1cc2ncnc(Br)c2s1
Brc1ccc(-c2nccs2)c2c1Sc1ccccc1O2
Policy gradient replay...
Mean value of predictions: 0.25129032
Proportion of valid SMILES: 0.5814316974054392
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(Br)c(Br)c1
Brc1ccc(CN(Cc2ccccc2)Cc2ccncc2)cc1
Brc1ccc(Nc2c(Br)ccc3ncc(Br)cc23)cc1
Brc1ccc(Nc2cc3cncnc3cn2)cc1
Fine tuning...
Mean value of predictions: 0.2918296
Proportion of valid SMILES: 0.6244131455399061
Sample trajectories:
BrSc1ccc(Br)cc1Br
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(CCNCc2cncnc2Br)c2cc3ccccc3nc2n1
Brc1cc2c(nc3nc4ccccc4oc13)-c1ccccc1N2
Brc1cc2ncnc(Nc3cccs3)c2s1

  7 Training on 5816 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 23.890251
Reward: 2.062913
Trajectories with max counts:
21	Clc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Mean value of predictions: 0.3808026
Proportion of valid SMILES: 0.5769712140175219
Sample trajectories:
Bc1ccc(Nc2ncnc3scnc23)cc1
Brc1cc(Cc2cccc3ccccc23)cc2c1OCO2
Brc1cc(Nc2cnccn2)cc2cncnc12
Brc1cc2ncnc(Nc3ccc(Br)s3)c2cn1
Brc1ccc(-c2ccc3ncnc(Nc4ccccc4)c3n2)cc1
Policy gradient replay...
Mean value of predictions: 0.34393004
Proportion of valid SMILES: 0.6080700656865812
Sample trajectories:
BP(=O)(OCCc1ccc(Nc2cccc(Br)c2)cc1)C(O)CP(=O)(O)O
BP(=O)(Oc1ccc(Br)c(Nc2ncnc3c(Br)c(Br)c(Br)cc23)c1)N(CCCl)N(=O)=O
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2cc(Br)c(Br)nc2-c2ccsc2)cc1
Fine tuning...
Mean value of predictions: 0.33771166
Proportion of valid SMILES: 0.6098247809762203
Sample trajectories:
BP(=O)(OC)OCC
Br
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1ccc(COc2nc(-c3ccccc3)c3ccccc3n2)cc1
Brc1ccc(Nc2ccc(Nc3ncnc4ccc(I)cc34)cc2)cc1

  8 Training on 7773 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 24.919722
Reward: 2.515009
Trajectories with max counts:
77	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.35713455
Proportion of valid SMILES: 0.5372340425531915
Sample trajectories:
BP(=O)(OCC)N(O)S(=O)(=O)N(C)C(=O)C(Cl)(P(=O)(O)O)P(=O)(O)O
BrCc1ncc(Nc2cccnc2)c2c(Nc3cccc(Br)c3)ncnc12
Brc1cc(Br)c2c(Nc3ccc(Br)c4cc(Br)ccc34)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.37377566
Proportion of valid SMILES: 0.593994369721614
Sample trajectories:
BP(=O)(OCC)OC(=O)C(Br)(Br)I
Brc1cc(Br)cc(Nc2ccc(Nc3ncnc4ccc(Br)cc34)cc2)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)nn23)c1
Brc1cc2c(Nc3cccc(I)c3)ncnc2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Fine tuning...
Mean value of predictions: 0.36160335
Proportion of valid SMILES: 0.5926852141294154
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Nc4ccccc4Br)cc3Br)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)nc(Nc4ccccc4Br)c23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(-c2ncnc3[nH]cnc23)c2cncnc12
Brc1ccc(Br)c(Nc2sc3nc4sccc4c(Br)nc23)c1

  9 Training on 9492 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 24.248815
Reward: 2.307959
Trajectories with max counts:
14	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
14	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
14	Fc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.40257648
Proportion of valid SMILES: 0.5823694904657706
Sample trajectories:
BP(=O)(Oc1ccc(Cl)cc1)P(=O)(O)O
B[PH](=O)(=O)Oc1cccc(Nc2ncnc3cccc(Br)c3s2)c1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCc1cc2ncnc(Nc3cc(Br)c(Br)c(Br)c3Br)nc2c1Nc1ncccn1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.44000003
Proportion of valid SMILES: 0.6238273921200751
Sample trajectories:
BrCCNc1ccnc2c(Nc3ccc(Br)cc3)ncnc12
BrCCNc1ncnc2ncnc(Nc3cccc(Br)c3)c12
Brc1cc(Br)cc(Nc2ncnc3ncc(Br)nc23)c1
Brc1cc(Br)cc(Nc2ncnc3ncnc(Nc4ccccc4Br)c23)c1
Brc1cc2ncnc(Nc3cc(Br)c(Br)[nH]3)c2s1
Fine tuning...
Mean value of predictions: 0.38242304
Proportion of valid SMILES: 0.6297686053783614
Sample trajectories:
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc2ncnc(Nc3ncnc4ccccc34)c2cc1Br
Brc1cc2ncnc(c1Br)Nc1ccccc1C2
Brc1ccc(-c2ccc3ccccc3c2)c2c(Br)cccc12
Brc1ccc(Br)c(Br)c1

 10 Training on 11190 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.120249
Reward: 2.802889
Trajectories with max counts:
17	Clc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Mean value of predictions: 0.44360754
Proportion of valid SMILES: 0.6314142678347935
Sample trajectories:
BP(=O)(NCCCCC[N+](C)(C)C)Oc1cc(Br)cc(Br)c1
BrCCOc1cc2c(cc1Br)[nH]c1c(Br)cc(Br)cc12
Brc1cc(Br)c2c(Nc3cc(Br)ncn3)ncnc2c1
Brc1cc(Br)c2c(Nc3ncnc4ccc(Br)cc34)cccc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Policy gradient replay...
Mean value of predictions: 0.4623016
Proportion of valid SMILES: 0.6313811462574381
Sample trajectories:
BrCCCCCCCNc1ccc(Nc2ncn[nH]2)cc1
BrCCCNc1ncnc2c1-c1cc(Br)ccc12
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)c2c(Nc3cc(Br)c4c(n3)Nc3ccccc3-4)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccsc3)c2c1
Fine tuning...
Mean value of predictions: 0.40334287
Proportion of valid SMILES: 0.6547842401500938
Sample trajectories:
BP(=O)(OCC)Oc1cc(Br)c(Br)c(Br)c1OC
BrC1=Nc2ccc(Br)cc2Nc2ccccc21
Brc1cc(Br)c2c(Nc3cc(Br)c(Br)cc3Br)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(c1)Oc1cc(Br)cn1-2

 11 Training on 13174 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.405080
Reward: 2.648412
Trajectories with max counts:
28	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.38388944
Proportion of valid SMILES: 0.6595410248349576
Sample trajectories:
BP(=O)(OCC)OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O
BP(=O)(OCC1CC(F)(F)C(F)C1F)c1ccccc1
BrCCNc1nc2ncnc(Nc3ccccc3)c2s1
Brc1cc(Nc2ncnc3ccc(Br)cc23)c2ccccc2c1
Brc1cc2c(Nc3ccc(Br)c(I)c3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.43439308
Proportion of valid SMILES: 0.649155722326454
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Nc2ncnc3ccc(Cl)cc23)c(Br)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cn1
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1ccc(-c2ncnc3cc[nH]c23)c2ncnc(Nc3ccc4ccccc4c3)c12
Brc1ccc(-c2ncnc3cccc(Br)c23)cc1
Fine tuning...
Mean value of predictions: 0.4463495
Proportion of valid SMILES: 0.6636448890278211
Sample trajectories:
BP(=O)(COc1ccc(Br)cc1)NP(=O)(OCC)Oc1cncc(Br)c1
BP(=O)(NO)c1ccc(Cl)cc1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1ccc(-c2cc(Br)ccc2Br)cc1
Brc1ccc(-c2cc(Nc3ccc(I)cc3)cc(Br)n2)cc1

 12 Training on 15169 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.461077
Reward: 3.221059
Trajectories with max counts:
31	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4456533
Proportion of valid SMILES: 0.6152643102908977
Sample trajectories:
BP(=O)(C=C(Br)Br)OCC
BP(=O)(OCC)c1ccc(Nc2ncnc3cccc(Br)c23)cc1
Bc1ccncc1Nc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BrCCNc1cccc(Nc2ncnc3ccccc23)c1
Brc1cc(Br)c(Oc2cccc(Nc3ncnc4ccc(Br)cc34)c2)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.4686712
Proportion of valid SMILES: 0.644576430134417
Sample trajectories:
BP(=O)(COc1ccc(Br)cc1)[PH](c1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O
BP(=O)(NC(=O)OCCC)OP(=O)(O)OP(=O)(O)OP(=O)(O)OCCCl
B[PH](=O)(NO)(Nc1ccc(Br)c2ncnc(Nc3ccc(Br)cc3)c12)c1ccc(Br)cc1
BrBr
BrC(Br)=C(Br)Br
Fine tuning...
Mean value of predictions: 0.43054372
Proportion of valid SMILES: 0.6613508442776735
Sample trajectories:
BP(=O)(OCC)C(CNC(=O)c1ccc(Br)cc1)NCCCCCCOP(=O)(O)Oc1ccc(F)cc1F
Bc1cc2ccccc2nc1-c1ccc(I)cc1F
BrCC#CC=C(Br)Br
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2n1
Brc1cc(Nc2ncnc3ccccc23)ncn1

 13 Training on 17282 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.053505
Reward: 2.966026
Trajectories with max counts:
60	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.43065327
Proportion of valid SMILES: 0.6220693966864645
Sample trajectories:
BP(=O)(NCCCCCCCCCCC)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCCS)C(=O)NO
BP(=O)(Oc1ccccc1F)C(=O)Oc1ccccc1
Bc1ccc(Nc2ncnc3cccnc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.49889332
Proportion of valid SMILES: 0.7094849246231156
Sample trajectories:
BrCCCCCCNc1c2ccccc2nc2cccc(Nc3ccc(Br)cc3)c12
Brc1cc2ncnc(N3CCCCC3)c2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCCCC1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ccnc3cc(Br)sc23)cc1
Fine tuning...
Mean value of predictions: 0.5075606
Proportion of valid SMILES: 0.6590410529614541
Sample trajectories:
Bc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cc1Br
BrC=C(Br)c1ccccc1-c1cccc2ncnc(Nc3ccc(Br)cc3)c12
BrCCNc1ccc(Nc2c(Br)cc(Br)cc2Br)cc1
Brc1cc(-n2cnc3c(cc2Br)Nc2ccccc2-3)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Br)c1

 14 Training on 19654 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.694892
Reward: 3.092401
Trajectories with max counts:
28	Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Mean value of predictions: 0.52557653
Proportion of valid SMILES: 0.5964363863707408
Sample trajectories:
BP(=O)(NO)c1ccc2ncnc(Nc3ccc(Br)cc3)Oc2c1
Bc1ccc(Br)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c4ccsc34)ncnc2c1
Brc1cc(Br)c2c(c1)sc1ccc(Br)cc12
Policy gradient replay...
Mean value of predictions: 0.34687367
Proportion of valid SMILES: 0.7253675320613074
Sample trajectories:
BP(=O)(OCC1(P(=O)(O)O)CCCCS(=O)(=O)O1)n1cnc2c(N)ncnc21
Brc1ccc(Br)c(Br)c1
Brc1ccc(Cc2ccccc2)s1
Brc1ccc(N=Cc2ccc3ccccc3c2)cc1
Brc1ccc(Nc2ccc3ccncc3c2)cc1
Fine tuning...
Mean value of predictions: 0.4850023
Proportion of valid SMILES: 0.6776110068792995
Sample trajectories:
BP(=O)(C(CCBr)P(=O)(O)O)P(=O)(O)O
Br
Brc1cc(Br)c2c(Nc3ccc(NN=Cc4ccc(Br)c(Br)c4)cc3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Nc2ncnc3sccc23)no1

 15 Training on 21861 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.969049
Reward: 3.363484
Trajectories with max counts:
52	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5487462
Proportion of valid SMILES: 0.623125
Sample trajectories:
BP(=O)(C=Cc1cc2cc(Br)c(Br)c(Br)c2nc1Br)OCC
BP(=O)(O)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCC)C(=O)Nc1ccc(Br)c(Br)c1
BP(=O)(Oc1ccncc1)N(O)C(Cl)(Cl)P(=O)(O)c1ccc(Br)cc1
BP(=O)(c1ncnc2c1c(C(=O)OCC(F)(F)F)nn2-c1ccccc1)N1CCOCC1
Policy gradient replay...
Mean value of predictions: 0.5653385
Proportion of valid SMILES: 0.6531828159297586
Sample trajectories:
BP(=O)(OCC)OC(=O)C=CC=CCC(=O)O
BP(=O)(OCC)S(=O)(=O)NCCCCCCCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCc1cn(CCCN2CCN(c3cc4c(Nc5ccc(Br)cc5)ncnc4cc3Br)CC2)cn1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)c[nH]c2n1
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.5268505
Proportion of valid SMILES: 0.6467459324155194
Sample trajectories:
BP(=O)(NO)C(=O)CS(=O)(=O)CC(F)(F)F
BP(=O)(OCC(=O)Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(OCCC)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
B[PH](=O)(NCCCCCCC)(C(=O)Nc1ccc(Br)cc1)c1ccc(Br)c(Br)c1
BrCc1csc2ncnc(Nc3ccc(Br)cc3)c12

 16 Training on 24509 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.319385
Reward: 3.844027
Trajectories with max counts:
35	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.59093034
Proportion of valid SMILES: 0.6707431796801505
Sample trajectories:
BP(=O)(NCCCCCCCCCCCCCCCCCCCCCn1cnc(Cl)c1)c1ccc(Br)cc1
BP(=O)(OCC)C(=O)C(F)(F)F
BP(Br)(Br)(Br)C(F)(F)F
Bc1ccc(NN=Cc2cccc(Br)c2)cc1
Brc1cc(Br)c(Nc2nc3ccc(Br)cc3o2)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.610149
Proportion of valid SMILES: 0.6727215784528656
Sample trajectories:
BP(=O)(O)C(F)(F)F
B[PH](=O)(C=CBr)(NO)C(=O)NO
Bc1cc2ncnc(Nc3cccc(Br)c3)c2s1
Bc1cc2ncnc(Nc3ccccc3F)c2cc1Cl
Brc1cN(c2ncnc3cc(Br)c(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.5585165
Proportion of valid SMILES: 0.6827133479212254
Sample trajectories:
Bc1cc(Br)ccc1Nc1cc2ncnc2c2ncnc(Nc3ccc(Br)cc3Br)c(Br)c12
BrCCCCCOc1cc2c(Nc3ccc(Br)s3)ncnc2cc1Br
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2nc1Nc1ccc(Br)cc1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1

 17 Training on 27675 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.899183
Reward: 4.127294
Trajectories with max counts:
28	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
28	Clc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6130287
Proportion of valid SMILES: 0.6446255092447508
Sample trajectories:
BP(=O)(OCC)OC(=O)c1ccc(Br)cc1Br
BrCc1cc(-c2ccccc2)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3ccsc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3Br)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.59310675
Proportion of valid SMILES: 0.6451612903225806
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Br)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2nc(Br)cc(Nc3ncnc4ccc(Br)cc34)n2)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2nc3sc(Br)cc3s2)ccn1
Fine tuning...
Mean value of predictions: 0.5736022
Proportion of valid SMILES: 0.6823014383989994
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)cc(-c2cccnc2Nc2ccc(Br)c3ncncc23)c1
Brc1cc(Br)cc(Nc2ccc(Br)c(Br)c2)c1

 18 Training on 30765 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 38.285064
Reward: 4.350919
Trajectories with max counts:
20	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5472796
Proportion of valid SMILES: 0.6664582682088153
Sample trajectories:
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
Brc1cc2ncnc(Nc3ccccc3)c2nc1Nc1ccccn1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2cc3ncnc(Nc4ccc(Br)s4)c23)c1
Brc1ccc(I)cc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Policy gradient replay...
Mean value of predictions: 0.61849445
Proportion of valid SMILES: 0.6725
Sample trajectories:
BrC=CC=CC=C(Br)Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3Br)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(OCc4ccncc4)cc23)c1
Brc1ccc(NCc2cc3ncnc(Nc4cccc(Br)c4)c3s2)cc1
Fine tuning...
Mean value of predictions: 0.53150064
Proportion of valid SMILES: 0.6940882076947138
Sample trajectories:
Brc1cc2nccc(Nc3ccc(Br)c(-c4ccccc4)c3)c2cc1Br
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1ccc(-c2cc3c(Br)cc4cc(cccc2Br)Nc(ncn4)c2cc(Br)ccc2N3)cc1
Brc1ccc(-c2nc3ccc(Br)cc3s2)cc1
Brc1ccc(-c2ncnc3ncncc23)c2cncnc12

 19 Training on 33747 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.211145
Reward: 3.987330
Trajectories with max counts:
33	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.567091
Proportion of valid SMILES: 0.6648902821316615
Sample trajectories:
BrCc1cc(Nc2ncnc3ccsc23)nc2ccccc12
Brc1cc(Br)c(Br)c(Nc2ccnc3cc(Br)c(Br)c(Br)c23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccsc3)ncnc2c1Br
Brc1cc(Br)c2ncnc(Nc3cc(Br)c(Br)c(Br)c3)c2c1
Policy gradient replay...
Mean value of predictions: 0.6130035
Proportion of valid SMILES: 0.6446668751954958
Sample trajectories:
Brc1cc(I)ccc1Nc1ncnc2ccsc12
Brc1cc2c(Nc3c(Br)cccc3Br)ncnc2s1
Brc1ccc(-c2ncnc3ccc(CCN4CCCCC4)cc23)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ncnc3cc(Br)nn23)c1
Fine tuning...
Mean value of predictions: 0.5864498
Proportion of valid SMILES: 0.6923076923076923
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Brc1cc(Br)c2ncc(Br)c(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2ncnc(Nc3cccc(I)c3)c2cc1Br
Brc1cc2ncnc(Nc3ccccc3Br)c2cn1

 20 Training on 36807 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 38.201707
Reward: 4.502828
Trajectories with max counts:
40	CC(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6898256
Proportion of valid SMILES: 0.6454033771106942
Sample trajectories:
Brc1cc(Br)c2ncnc(Nc3cc(Br)c(Br)c(CN4CCCCCCCCCCCCCCCC4)c3)c2n1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(I)sc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Brc1cc(Nc2ncnc3ccccc23)cnc1Br
Policy gradient replay...
Mean value of predictions: 0.6735211
Proportion of valid SMILES: 0.6660412757973734
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Nc2ncnc3c(Br)cc(Br)c(Cl)c23)cc1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Nc2cc(-c3cccc4c(Nc5cccs5)ncnc34)ncn2)ccn1
Brc1cc2ncnc(Nc3cc(Br)c(Br)cn3)c2s1
Brc1ccc(-c2cc(Nc3ncnc4ccc(Br)cc34)ncc2Br)cc1
Fine tuning...
Mean value of predictions: 0.61405
Proportion of valid SMILES: 0.6630196936542669
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(CSc4ccncc4)cc3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(Nc2ccc(Nc3ncnc4ccc(Br)cc34)cc2)cc1
Brc1ccc(Nc2ccc3c(c2)nc(NCc2ccccc2)n3Cc2cc3ncnc(Nc4ccccc4Br)c3cc2Br)nc1
Brc1ccc(Nc2ccnc(Nc3ccc(Br)cn3)n2)cc1

Trajectories with max counts:
118	Clc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.59782726
Proportion of valid SMILES: 0.5697761660622733
Mean Internal Similarity: 0.46168348684227484
Std Internal Similarity: 0.08887755288497633
Mean External Similarity: 0.4172703010620954
Std External Similarity: 0.07125908674159974
Mean MolWt: 430.73070510360714
Std MolWt: 108.7206226236676
Effect MolWt: -0.6577835646337845
Mean MolLogP: 4.816962250575597
Std MolLogP: 1.6898850618201535
Effect MolLogP: 0.0640741520047773
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.405190% (976 / 1002)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 10, 'n_policy_replay': 15, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5682.029993057251, 'valid_fraction': 0.5697761660622733, 'active_fraction': 0.5719302095906946, 'max_counts': 118, 'mean_internal_similarity': 0.46168348684227484, 'std_internal_similarity': 0.08887755288497633, 'mean_external_similarity': 0.4172703010620954, 'std_external_similarity': 0.07125908674159974, 'mean_MolWt': 430.73070510360714, 'std_MolWt': 108.7206226236676, 'effect_MolWt': -0.6577835646337845, 'mean_MolLogP': 4.816962250575597, 'std_MolLogP': 1.6898850618201535, 'effect_MolLogP': 0.0640741520047773, 'generated_scaffolds': 1002, 'novel_scaffolds': 976, 'novel_fraction': 0.9740518962075848, 'save_path': '../logs/replay_ratio_mixed_s2-2.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.698529
Reward: 1.000000
Mean value of predictions: 0.0011881188
Proportion of valid SMILES: 0.7910401002506265
Sample trajectories:
BP(=O)(OC(C)C)C1CCCC=CC1OC(=O)CCC(N)=O
Brc1cc(Br)c2c(c1Br)C1CC=CC(CO2)O1
Brc1ccc(-c2ccc3oc(N4CCCC4)nc3n2)cc1
Brc1ccco1
C#CCC#Cc1ccn(C(=O)N2CCN(c3ccccc3)CC2)c1
Policy gradient replay...
Mean value of predictions: 0.0028673834
Proportion of valid SMILES: 0.7846875
Sample trajectories:
Brc1ccc2c(c1)Sc1ccccc1O2
Brc1ccc2ccccc2c1
Brc1ccccc1
Brc1ccccc1-c1cc2ccccc2cc1-c1ccccc1
Brc1ccccc1-c1cc2ccccc2nc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.018753069
Proportion of valid SMILES: 0.6377582968065122
Sample trajectories:
Brc1ccc(-c2[nH]ncc2-c2cccc(-c3cccc(Br)c3)c2)cc1
Brc1ccc(Nc2ncnc3cnccc23)nc1
C#CCCC12CC1(CO)CC1C2N1CCCc1nsc(CN2CCC(N3CCN(CC)CC3)O2)n1
C#CCNC(=O)C1=NN(Cc2cn(CC)cn2)Nc2ccccc21
C#Cc1c(-c2ccccc2)ccc(Nc2cc(-c3ccccc3)c(C)nn2)c1C#N

  2 Training on 287 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.377350
Reward: 1.040467
Trajectories with max counts:
6	Oc1ccc2ccccc2c1
Mean value of predictions: 0.022521008
Proportion of valid SMILES: 0.7439824945295405
Sample trajectories:
Brc1ccc(-c2ccccc2)o1
Brc1ccc(Nc2nc(Nc3ccccc3)nc3ccccc23)cc1
Brc1ccc(Nc2ncc3sc(-c4ccc(Br)o4)nc3n2)cc1
Brc1ccc(Nc2ncnc3cnc(Nc4cccc(Br)c4)nc3-2)cc1
Brc1ccc2ncc(Nc3ccccc3Br)nc2c1
Policy gradient replay...
Mean value of predictions: 0.005430211
Proportion of valid SMILES: 0.8176985616010006
Sample trajectories:
Brc1cc2ccccc2cc1-c1ccc2ccccc2c1
Brc1ccc(Nc2ccccc2Nc2ccccc2Br)cc1
Brc1ccc(Nc2cccnc2)cc1
Brc1cccc(Nc2ncccc2-c2ccccc2)c1
Brc1ccccc1-c1cccc2ccccc12
Fine tuning...
Mean value of predictions: 0.04758621
Proportion of valid SMILES: 0.6359649122807017
Sample trajectories:
BrCCOc1ccc(-c2ccccn2)c2ccccc12
Brc1ccc(-c2nc3cccnc3s2)o1
Brc1ccc(-c2ncc[nH]2)s1
Brc1ccc(Br)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1

  3 Training on 531 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.292418
Reward: 1.334564
Trajectories with max counts:
11	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.055350196
Proportion of valid SMILES: 0.6425
Sample trajectories:
BP(=O)(CCC)C(=O)Nc1ccc(Br)cc1
Brc1ccc(-c2ccncc2)o1
Brc1ccc(-c2nc(-c3cncnc3)c3ccccc3n2)cc1
Brc1ccc(Nc2ccncn2)cc1
Brc1ccc(Nc2nc3ccccc3[nH]2)cc1
Policy gradient replay...
Mean value of predictions: 0.064326465
Proportion of valid SMILES: 0.7126328955597249
Sample trajectories:
BP(=O)(c1ccccc1)N1CCOCC1
Brc1cc2c(Br)cccc2c2ccccc12
Brc1ccc(N2C=C(Nc3nccs3)c3ccccc3C23COC(N2CCN(c4ccccc4)CC2)C3)cc1
Brc1ccc(N=Nc2ccncc2)cc1
Brc1ccc(Nc2nc3ccccc3nc2c2ncnc3ccccc32)cc1
Fine tuning...
Mean value of predictions: 0.10121774
Proportion of valid SMILES: 0.641963727329581
Sample trajectories:
BP(=O)(CCN1CCCC(F)C(F)C1)OCC
BP(=O)(N(O)COP(=O)(O)OP(=O)(O)O)P(=O)(Nc1cccc(F)c1)OCOC(=O)N(O)C(CC(=O)O)NS(=O)(=O)CCl
BP(=O)(NO)c1cccc(Cl)c1
BrCc1ccc2c(Nc3ccccc3-c3cccnc3-c3cncnc3-c3ccccc3Br)ncnc2c1
Brc1ccc(-c2ccc3[nH]ccc3c2)c2ccccc12

  4 Training on 1203 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 20.076134
Reward: 1.582864
Trajectories with max counts:
90	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.17416807
Proportion of valid SMILES: 0.554582421019706
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(cc1CSc1nc3ccccc3nc1Nc1ncnc3sc4ccccc4c13)Sc1ccccc1N2
Brc1ccc(-c2cnc(Nc3ccccc3)nc2)cc1
Brc1ccc(Br)c(-c2ccc(Nc3ccc(Br)s3)cc2)c1
Brc1ccc(C=C2NC(c3c[nH]c4ccccc34)=NSS2)cc1
Policy gradient replay...
Mean value of predictions: 0.16084905
Proportion of valid SMILES: 0.5304973412574289
Sample trajectories:
Bc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc2ncnc(Nc3ccccc3)c2nc1CN1CCCCC1
Brc1ccc(-c2ccccc2Br)cc1
Brc1ccc(-c2ncnc3c(Nc4ccccc4Br)ncnc23)s1
Brc1ccc(Br)c(Nc2ncc3ncnc(-c4ccccc4)c3n2)c1
Fine tuning...
Mean value of predictions: 0.17446353
Proportion of valid SMILES: 0.5828642901813633
Sample trajectories:
BP(=O)(C=O)OCC
BrCCOc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(-c2ccccc2)cc1Nc1ncnc2ccccc12
Brc1ccc(-c2ccccc2)nc1Nc1ncc2ccccc2n1
Brc1ccc(-c2nc(-c3ccccc3)c3c(-c4ccccc4Br)ncnc3n2)s1

  5 Training on 2438 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 19.652863
Reward: 2.065749
Trajectories with max counts:
422	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.09520295
Proportion of valid SMILES: 0.508125
Sample trajectories:
Bc1cnccc1N=C(Nc1ccccc1N)C(N)=O
Brc1cc2ccccc2cc1-c1cccc(Nc2nccs2)c1
Brc1ccc(-c2ccc(Nc3cccs3)cc2)cc1
Brc1ccc(-c2ccccc2)c2ccccc12
Brc1ccc(NCCCN2CCN(c3ccccc3Br)CC2)s1
Policy gradient replay...
Mean value of predictions: 0.17282984
Proportion of valid SMILES: 0.6373866833385433
Sample trajectories:
BrCc1ccccc1-n1ccc2ccccc21
Brc1ccc(Br)c(Nc2cccc(Nc3ccccc3N=Cc3ccccc3Br)n2)c1
Brc1ccc(Br)c(Nc2ncnc3ncnc(Nc4ccccc4Br)nc3n2)c1
Brc1ccc(Nc2ncnc(Nc3cccc(Br)c3)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.18297435
Proportion of valid SMILES: 0.609375
Sample trajectories:
BP(=O)(OCC)OC(=O)CCC(=O)OC(C)C=CC=CBr
Bc1ccccc1-c1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(-c2cccnc2)c2ncnc(Nc3ccccc3)c2c1Nc1ccccc1
Brc1cc(Br)c(Br)c(-c2ccccc2-c2ccccc2)c1
Brc1ccc(-c2[nH]ccc2Br)cc1

  6 Training on 3542 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 19.323853
Reward: 2.696473
Trajectories with max counts:
1084	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.18702596
Proportion of valid SMILES: 0.313125
Sample trajectories:
Brc1cccc(Nc2ncnc3ccccc23)c1
Brc1ccccc1-c1ccccc1-c1ccccc1Nc1ccccc1Nc1ncnc2ccccc12
Brc1ccccc1-c1ccccc1Nc1ncccc1Nc1ccccc1
Brc1ccccc1-c1ccccc1Nc1ncnc2ccccc12
Brc1ccccc1-c1cnccc1Nc1ncnc2ccccc12
Policy gradient replay...
Mean value of predictions: 0.26840797
Proportion of valid SMILES: 0.5028142589118199
Sample trajectories:
BP(=O)(NC(C)(COC(=O)CCCl)OC)OCCF
BP(=O)(OCC)OC(=O)CCCCCCC(=O)CI
BP(=O)(OCC1OC(OP(=O)(O)O)C(O)C1O)n1cnc2c(N)ncnc21
BrC1=NNc2ncc(Br)c3cccc1c23
BrC1=Nc2sc3c(c2C=C1)CCCCC3
Fine tuning...
Mean value of predictions: 0.26655114
Proportion of valid SMILES: 0.5417840375586854
Sample trajectories:
BP(=O)(C(=O)Oc1ccc(F)cc1)N1CCOCC1
Brc1cc(Nc2ncnc3ncnc(Nc4ccncc4Br)c23)ccn1
Brc1ccc(-c2ccccc2)cc1Nc1ncnc2ccccc12
Brc1ccc(-c2nc3ncccc3s2)cc1
Brc1ccc(Br)c(Nc2ncnc3cc(Br)ccc23)c1

  7 Training on 4760 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 21.623724
Reward: 2.868256
Trajectories with max counts:
439	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.22054055
Proportion of valid SMILES: 0.4625
Sample trajectories:
BP(=O)(OCCCC)OCOP(=O)(O)OP(=O)(O)O
BP(=O)(Oc1ccccc1P(=O)(Oc1ccccc1)c1ccccc1)C(O)CCC
BP1(=O)CCN1CCS(=O)(=O)Nc1ccc(Br)cc1
BrC1=Nc2ccccc2Nc2ccccc2S1
BrSc1ccccc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Policy gradient replay...
Mean value of predictions: 0.3390625
Proportion of valid SMILES: 0.5209768315591734
Sample trajectories:
BP(=O)(N=Nc1ccc(I)cc1)N(O)CP(=O)(O)O
BrCCOc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(N2CCCC2CN2CCC(N3CCOCC3)CC2)c2c(Nc3cccc(-c4ccccc4Br)c3)ncnc2c1
Brc1ccc(-c2cnc3sc(Nc4ccccc4)nc3c2)cn1
Brc1ccc(Br)c(Nc2cc(Br)c(Br)cn2)c1
Fine tuning...
Mean value of predictions: 0.27404994
Proportion of valid SMILES: 0.5758049390434511
Sample trajectories:
BP(=O)(Nc1ccc(F)cc1)c1ccc(F)cc1F
Bc1cccc(Nc2ncnc3cc(Cl)ccc23)c1
BrCCc1ccncc1Nc1ncnc2ccccc12
Brc1cc2c(nc1Nc1ncnc3sc(-c4ccccc4)cc13)Sc1ccccc1N2
Brc1ccc(-c2ccccc2Nc2ccccc2Br)o1

  8 Training on 6190 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 22.429478
Reward: 3.021695
Trajectories with max counts:
82	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.33915246
Proportion of valid SMILES: 0.4945261182358461
Sample trajectories:
BP(=O)(OCC)OC(=O)C1OP(F)(=S)OC1(F)F
Brc1cc2ccc1Oc1cccc3nc[nH]c3c(ncn1)N2
Brc1ccc(-c2nc(Nc3cccs3)[nH]c2-c2ccccc2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ncnc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.25866947
Proportion of valid SMILES: 0.5965625
Sample trajectories:
BP(=O)(Nc1cccc(Nc2ccccc2Cl)c1)NS(=O)(=O)c1ccc(Br)cc1
BP(=O)(Oc1ccccc1Br)P(=O)(Oc1ccccc1)Oc1ccc(Br)cc1
Bc1ccc(Nc2ccccc2Br)c(Br)c1Nc1ccccc1Br
BrC1=Nc2ccccc2-c2ccc(Br)c(Br)c2S1
BrCCCCC1=Nc2sc3ccccc3c2OC1=Nc1ccccc1
Fine tuning...
Mean value of predictions: 0.2955665
Proportion of valid SMILES: 0.5712945590994372
Sample trajectories:
BP(=O)(NC(=O)OCC)c1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CSCCP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(c1ccccc1Nc1c(N)cc(Cl)cc1O)N(O)CC=O
Bc1[nH]c2ccccc2c1-c1nc2cc(Nc3ccc(Br)cc3)cc(Br)cn12
BrC=C1N=Nc2ccc(Br)cc2Nc2cc(Br)ccc21

  9 Training on 7707 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 23.830911
Reward: 3.064039
Trajectories with max counts:
62	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.2935501
Proportion of valid SMILES: 0.5961237886839638
Sample trajectories:
BP(=O)(OCC)C1Oc2sc(Br)cc2N1C(=O)c1cc(Br)ccc1Br
BrCBr
Brc1c(-c2ccccc2)cccc1-c1cnc2ccccn12
Brc1cc(-c2ccccc2)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2sccc12
Policy gradient replay...
Mean value of predictions: 0.39168197
Proportion of valid SMILES: 0.5112570356472795
Sample trajectories:
BP(=O)(N1CCN(C(=O)Oc2ccc(Cl)c(F)c2)CC1)C(F)(F)F
BP(=O)(OCC1OC(n2cnc3c(N)cc(Br)cc32)C(O)(O)C1O)C(=O)NS(=O)(=O)Oc1ccccc1
BP(=O)(OCCC)OCOP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BrC1(c2ccccc2)CC1=NNc1ncnc2sc3cccs3c2n1
Brc1cc(Br)c2ncnc(Nc3ccc(I)cc3)c2c1
Fine tuning...
Mean value of predictions: 0.38305473
Proportion of valid SMILES: 0.5423569865582994
Sample trajectories:
BP(=O)(NCc1ccccc1)P(=O)(O)O
BP(=O)(OCC)OP(=O)(O)OP(=O)(O)Nc1ccc(F)c(F)c1
BrC1=Nc2scnc2Nc2ccccc21
BrCc1nc2ncnc(Nc3cccc(-c4ccccc4Br)c3)c2s1
Brc1cc(-c2cccs2)c2c(Br)c(Br)cnc2c1-c1ccccc1

 10 Training on 9282 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 22.537999
Reward: 3.240157
Trajectories with max counts:
176	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.5040068
Proportion of valid SMILES: 0.36679174484052535
Sample trajectories:
BP(=O)(CC(F)(F)F)NO
BrBr
Brc1cc(Br)c2ncsc2c1
Brc1cc(Nc2nc3ccccc3s2)sc1Br
Brc1cc(Nc2ncnc3sccc23)sc1Nc1ncnc2sc3ccccc3c12
Policy gradient replay...
Mean value of predictions: 0.3842627
Proportion of valid SMILES: 0.504375
Sample trajectories:
BP(=O)(NC(C1CCC1)P(=O)(O)O)n1cnc2c(N)ncnc21
BP(=O)(NCc1ccc(F)cc1)OP(=O)(Nc1cc(Br)cc(Br)c1)Oc1ccccc1Br
BP(=O)(OCC)Oc1nc2c(Br)cc(Br)cc2s1
BP(=O)(OCC)c1nc(-c2ccccc2)c2nc(-c3ccccc3)c(-c3ccccc3)c(Br)c(Br)c2[nH]1
BP(=O)(OCCCC)Oc1ccc(Nc2ncnc3sc4scnc4c3s2)cc1
Fine tuning...
Mean value of predictions: 0.38331428
Proportion of valid SMILES: 0.546875
Sample trajectories:
BP(=O)(OCC(=O)Nc1ccc(F)cc1F)c1cccc(F)c1
BP(=O)(OCC)C(=O)NNc1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CCCCOP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)CC(F)(F)F
B[PH](=O)(Br)(Br)C(Br)=C(Br)Br
Bc1ccc(Nc2ncnc3ccsc23)cc1

 11 Training on 10634 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.040359
Reward: 3.436797
Trajectories with max counts:
323	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4588517
Proportion of valid SMILES: 0.391875
Sample trajectories:
BP(=O)(CCC=C(Br)Br)OCC
BP(=O)(NCc1ccc(Br)cc1)[PH](=O)(Nc1cncs1)(Oc1ccccc1Nc1ccc(N)cc1)C(F)(F)F
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1O
Bc1cccc(Br)c1Nc1ncnc2ccccc12
Bc1cccc(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.440882
Proportion of valid SMILES: 0.524867062871442
Sample trajectories:
BP(=O)(COP(=O)(O)O)N(CCN)P(=O)(O)O
BP(=O)(OCBr)OC(=O)Cc1cnc(Br)[nH]1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Br
Fine tuning...
Mean value of predictions: 0.41147733
Proportion of valid SMILES: 0.5501719287277275
Sample trajectories:
BP1(=O)OCC2OC(NC(=O)OCC(COc3c(Br)cc(Br)cc3Br)O1)C(O)(n1cnc3c(Br)cc(Br)cc31)C(O)C2O
B[PH](=O)(=NO)Nc1ccc(Br)cc1
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc2c(N=Cc3ccccc3Br)ncnc2s1

 12 Training on 12073 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.384958
Reward: 3.817497
Trajectories with max counts:
291	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.49982083
Proportion of valid SMILES: 0.3488590184432635
Sample trajectories:
BP(=O)(CCC=CBr)OCC
BP(=O)(Cl)OP(=O)(O)OP(=O)(O)OP(O)(F)(F)Br
BP(=O)(N(O)C=O)n1cnc2c(N)ncnc21
BP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC)OC(=O)Nc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.47692308
Proportion of valid SMILES: 0.463849765258216
Sample trajectories:
BP(=O)(OCC1OC(O)C(O)C1O)OP(=O)(O)OP(=O)(O)Oc1ccc(Nc2ncc(Br)c(Br)c2Br)nc1
Bc1ccc(Nc2ncnc3c2c(Br)sc3c2nc3ccc(Br)cc3s2)cc1
Bc1ccc(Nc2ncnc3sc(Nc4ccccc4)cc23)cc1
BrC1=Nc2ncnc(Br)[n+]2s1
BrCc1nc2c(Nc3nc(-c4ccc(Br)cc4)c(Br)cc3Br)ncnc2s1
Fine tuning...
Mean value of predictions: 0.4195923
Proportion of valid SMILES: 0.5520475148483901
Sample trajectories:
BP(=O)(CC(=O)ON=C(Nc1ccc(Br)nc1)Oc1ccccc1)NO
Bc1ccccc1Nc1ncnc2sc(Nc3ccc(Br)c(Br)c3)nc12
BrC(=NNc1ncnc(Nc2ccc(Br)cc2)n1)c1ncnc2sc(Br)cc12
BrCCI
BrCc1ccccc1-c1nc(N2CCOCC2)c2ncnc(Nc3ccccc3)c2n1

 13 Training on 13568 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.813995
Reward: 3.502899
Trajectories with max counts:
66	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4378855
Proportion of valid SMILES: 0.567677399187246
Sample trajectories:
BP(=O)(C(=O)O)N1CCC(=O)Nc2sc(C(=O)Oc3ccccc3)cc21
Bc1cccc(Nc2ncnc3ccsc23)c1
BrCCNc1ccc(Nc2ncnc3ncnc(Nc4ccc(Br)cc4)c23)cc1
BrCc1ccccc1-c1ccccc1Nc1ncnc2sc(Nc3ccccc3Br)nc12
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2n1
Policy gradient replay...
Mean value of predictions: 0.40839326
Proportion of valid SMILES: 0.5214129415442326
Sample trajectories:
BP(=O)(NC(c1ccccc1)c1ccccc1)c1ccccc1
BP(=O)(OCCS)C(=O)N1CCC(Br)(CBr)CC1
BrC(Br)=Nc1ccccc1Nc1ncnc2nc(Br)ccc12
Brc1cc(Br)c2c(c1)Oc1cc(Br)sc1-2
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Fine tuning...
Mean value of predictions: 0.39943945
Proportion of valid SMILES: 0.5575
Sample trajectories:
Brc1cc(Br)c2sc3ncnc(Nc4ccccc4Oc4ccccc4Br)c3c2c1
Brc1cc(Nc2ccsc2)ccc1I
Brc1cc(Nc2ncnc3ccccc23)nc2ccccc12
Brc1cc2c(Nc3cccc(-c4ccccc4Br)c3)ncnc2s1
Brc1cc2c(Nc3cccc(I)c3)ncnc2s1

 14 Training on 15102 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.593015
Reward: 3.773487
Trajectories with max counts:
317	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.5105079
Proportion of valid SMILES: 0.356875
Sample trajectories:
BP(=O)(O)Oc1ccccc1Nc1ncnc2ccccc12
BP(=O)(O)c1ccccc1Nc1ncnc2c(Br)c(Br)ccc12
BP(=O)(OC)OC(=O)CBr
BP(=O)(OCC)C1(Cc2ccc(Br)cc2)OC(=O)C(C)(C)Oc2c1ccc(Br)c2Br
BP(=O)(OCC)OCC1NC(O)=CC(=N)O1
Policy gradient replay...
Mean value of predictions: 0.27874362
Proportion of valid SMILES: 0.4278125
Sample trajectories:
BP(=O)(CCOCCOCCC(=O)N1CCCN(Cc2ccc(Br)s2)O1)OCC
Bc1ccccc1-c1ccccc1Nc1ncnc2ccccc12
Bc1ccccc1N1cc2c3cccnc3Nc3ccccc3Nc3ccccc3N=CC=C2N(c2ccccc2)CC1
BrC=CC=CC=CCNc1ncnc2sc3ccccc3c12
BrCCOc1ccccc1Nc1ncnc2ccccc12
Fine tuning...
Mean value of predictions: 0.4305263
Proportion of valid SMILES: 0.534375
Sample trajectories:
BP(=O)(OCC)c1cc(Br)cc(Br)c1Br
BrC1=C(c2ccccc2)c2nc(-c3ccccc3)sc2-c2cc(Br)ccc2N=C1CN1CCOCC1
BrCCBr
BrCCCCCCCCBr
Brc1c(Br)n(-c2ccccc2)c2ccc(Nc3ncnc4ccccc34)cc12

 15 Training on 16316 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.354739
Reward: 3.694452
Trajectories with max counts:
167	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4548632
Proportion of valid SMILES: 0.41125
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCC=CCC=C)OCC
BP(=O)(OCC)OCCCCCC
BP(=O)(Oc1cc(Br)ccc1Br)OP(=O)(O)O
Bc1ccc(Nc2ccsc2)cc1Cl
BrBr
Policy gradient replay...
Mean value of predictions: 0.4954717
Proportion of valid SMILES: 0.49703032197561736
Sample trajectories:
BP(=O)(CCC(=O)Oc1ccccc1Nc1ncnc2sccc12)COCC
BP(=O)(CCC(=O)c1cc(Br)c(OP(=O)(O)O)c(Br)c1)OCC
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC)OC1CCC[P+](=O)([O-])Oc2ccc(Br)cc21
BP(=O)(OCC)Oc1ccc(Br)cc1S(=O)(=O)Nc1nccs1
Fine tuning...
Mean value of predictions: 0.45218405
Proportion of valid SMILES: 0.5367302281963113
Sample trajectories:
BP(=O)(N(O)C(F)F)P(=O)(Oc1ccccc1)Oc1ccc(Br)c(Br)c1
BP(=O)(OCC)OC(=O)Nc1ccc(Br)c(Cl)c1
BP(=O)(OCC)c1ccc(Br)cc1
BrC=Cc1ccccc1Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1Br
Brc1cc(Br)c2c(Br)n(-c3ccccc3Nc3ncnc4sccc34)c(-c3cccs3)c2c1

 16 Training on 17898 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.409496
Reward: 4.006271
Trajectories with max counts:
406	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5596014
Proportion of valid SMILES: 0.34510784620193813
Sample trajectories:
BP(=O)(NC1CCCC(C(=O)N2CCCCC2)C1)OS(=O)(=O)O
BP(=O)(Nc1cc(Br)c(Br)cc1Br)C(P(=O)(O)O)P(=O)(O)P(=O)(O)O
BP(=O)(Nc1cc(Br)cc(Nc2ccc(Br)c(Br)c2)c1)C(F)(F)P(=O)(O)O
BP(=O)(Nc1ccc(Br)c(Br)c1)c1ccc(Br)c(Br)c1
BP(=O)(OCC)c1ccc(Nc2ncnc3c(Br)c(Br)c(Br)c(Br)c(Br)c(Br)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.4288961
Proportion of valid SMILES: 0.5775
Sample trajectories:
Bc1ccccc1Nc1ncnc2cc(Br)ccc12
BrCCNc1ccc2ncnc(Nc3ccccc3-c3ccccc3Br)c2n1
BrCc1nc2c(-c3ccccc3Br)ncnc2s1
BrCc1sc2ncnc(Nc3ccc(Br)cc3)c2c1Br
BrSc1ccccc1-c1ccccc1Nc1ccccc1
Fine tuning...
Mean value of predictions: 0.43252948
Proportion of valid SMILES: 0.5571875
Sample trajectories:
BP(=O)(CCCO)c1cc2ncnc(Nc3ccc(Br)cc3F)c2s1
BP(=O)(OCCCC)Oc1ccc(Nc2ncnc3ccccc23)cc1
Bc1ccc(Nc2nc(Nc3cc4ccccc34)nc(-c3cccs3)c2-c2cncnc2)cc1
Bc1ccccc1Nc1ncnc2sc(Nc3ccccc3Br)nc12
BrC(=Nc1ccsc1)c1ccccc1Nc1cccc2c(Nc3ccccc3Br)ncnc12

 17 Training on 19549 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.230745
Reward: 3.590635
Trajectories with max counts:
40	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49202454
Proportion of valid SMILES: 0.5095342294467021
Sample trajectories:
BP(=O)(CCCCCl)NO
BP(=O)(Nc1ccc(Nc2ncnc3ccccc23)cc1)c1ccc(Br)cc1
BP(=O)(OCC)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCC1CCCN1C(=O)OP(=O)(O)CCC1CC1)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)OP(=O)(O)OP(=O)(OCC)OCCCCCC
Policy gradient replay...
Mean value of predictions: 0.49682155
Proportion of valid SMILES: 0.5114098155673648
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Br)cc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)c1cccc(Br)c1
Bc1cnc(Nc2cc3ccc(Br)cc3nc3c(Br)c(Br)nc23)[nH]1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccsc12
Fine tuning...
Mean value of predictions: 0.49479166
Proportion of valid SMILES: 0.5403377110694184
Sample trajectories:
BP(=O)(C=COc1ccc(Br)cc1)OC
BP(=O)(CCC=CC(C)=O)OCCC
BP(=O)(NOP(=O)(O)OP(=O)(O)O)P(=O)(O)OP(=O)(O)Oc1ccccc1Br
Bc1cccc(Nc2ncnc3ccsc23)c1
BrC=CBr

 18 Training on 21412 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.229218
Reward: 4.113505
Trajectories with max counts:
115	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.54767823
Proportion of valid SMILES: 0.4778125
Sample trajectories:
BIc1ccccc1Nc1ncnc2nc(-c3ccccc3)sc12
BP(=O)(OCC1CCCCC1)Oc1ccccc1Nc1ncnc(Nc2ccc(Br)c(Br)c2O)n1
Bc1ccccc1-c1cc(Nc2cccc(Br)c2)ncn1
Bc1ccccc1Nc1ncnc2sc(Br)cc12
Bc1ccccc1Nc1ncnc2sccc12
Policy gradient replay...
Mean value of predictions: 0.49370462
Proportion of valid SMILES: 0.51625
Sample trajectories:
Bc1ccc2c(Nc3ccccc3)ccnc2c1
Bc1ccccc1-c1ccccc1-c1ncnc2sc3ccccc3c12
Bc1ccccc1-c1ccccc1Nc1ncnc2sc(Nc3ccc(Br)s3)cc12
Bc1ccccc1Nc1ncnc2sccc12
BrCC=CCC=CC=NNc1ncnc2sc(Br)cc12
Fine tuning...
Mean value of predictions: 0.48607877
Proportion of valid SMILES: 0.5636136292591435
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)OCC1OC(c2cnc(Br)cc2Br)C(O)(CC)O1
Brc1c(-c2ccc(Br)c3c(Nc4ccccc4)ncnc23)ccc2c1CCC2
Brc1cc(Br)c2sc3ncnc(Nc4ccccc4Br)c3c2c1
Brc1cc(CN2CCCC2)c2c(Nc3ccc(Nc4ccccc4Br)cc3)ncnc2c1
Brc1cc(Nc2ncnc3sc(-c4ccsc4)cc23)cc2ccccc12

 19 Training on 23340 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.129191
Reward: 4.499421
Trajectories with max counts:
326	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.6182163
Proportion of valid SMILES: 0.329375
Sample trajectories:
BP(=O)(NCc1ccccc1)c1ccc(Nc2ccc(Br)cc2)s1
BP(=O)(Nc1ccc(Br)cc1Br)OCC=C
BP(=O)(Nc1nc(N)cs1)Nc1ccc(Br)cc1F
BP(=O)(OCC)OC(=O)C(Br)Br
BP(=O)(OCC)OC(=O)CN(c1ccccc1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.44122905
Proportion of valid SMILES: 0.559375
Sample trajectories:
BP(=O)(CCCl)NO
BP(=O)(Nc1ccccc1)c1ccc(Br)cc1
Bc1ccccc1Nc1ncnc2sc(Nc3cccc(Br)c3)nc12
BrC12Cc3ccccc3C1CNc1ccncc12
BrCc1nc2c(-c3ccccc3Br)ncnc2s1
Fine tuning...
Mean value of predictions: 0.5047934
Proportion of valid SMILES: 0.5671875
Sample trajectories:
BP(=O)(Nc1cc(Br)cc(Br)c1Br)c1nc2c(Br)c(Br)c(Br)c(Br)c2s1
B[PH](=O)(Cl)(Cl)OCCCl
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1

 20 Training on 25129 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.623426
Reward: 4.858248
Trajectories with max counts:
274	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.5507274
Proportion of valid SMILES: 0.3221875
Sample trajectories:
BP(=O)(NCCCCCCl)c1ccccc1Nc1cccc(Nc2ncnc3sccc23)c1
BP(=O)(c1ccccc1)N1CCC(F)(F)C1
BP(=O)(c1ccccc1Nc1ccccc1)N1CCCCC1
BP1(=O)OCC(O)C(Oc2ccccc2)N1Cc1cncnc1Cl
B[PH](=O)(OCC)=C(Br)Br
Policy gradient replay...
Mean value of predictions: 0.4937616
Proportion of valid SMILES: 0.5072055137844611
Sample trajectories:
BP(=O)(OCC)Oc1cccc(Br)c1Nc1c2c(F)c(Br)c(Br)cc2nc2c(F)c(F)c(F)c(F)c12
BP(=O)(c1cc(Br)cc(Nc2cncnc2)c1)S(=O)(=O)Nc1cc(F)cc(Cl)c1
BP1(=O)OCC2OC(=O)OCC2c2cc(N)c(F)c(F)c21
Bc1ccc(Nc2cc(-c3ccc4ccc(Br)cc4n3)nc(Br)c2F)cc1
BrCCC=CC=CC=CC=CC=C=NNc1ncnc2sccc12
Fine tuning...
Mean value of predictions: 0.51342136
Proportion of valid SMILES: 0.5684803001876173
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCl
Bc1ccccc1Nc1ncnc2sccc12
BrC(=Nc1ccc(Br)cc1)c1ccc(Nc2ncnc3sc(Br)cc23)cc1
BrC=CC(Br)Br
Brc1cc(-c2cccs2)sc1-c1ccsc1-c1cncc(Nc2ccc3ncnc(Br)c3c2)c1

Trajectories with max counts:
231	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.46509284
Proportion of valid SMILES: 0.4713383759454898
Mean Internal Similarity: 0.4655441047686505
Std Internal Similarity: 0.10073750341490033
Mean External Similarity: 0.4167015223187768
Std External Similarity: 0.06696205085652614
Mean MolWt: 414.1099825878116
Std MolWt: 93.67718164568349
Effect MolWt: -0.8567850736331483
Mean MolLogP: 5.700259867907537
Std MolLogP: 1.60665689782518
Effect MolLogP: 0.6390134862536615
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.936371% (1139 / 1163)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5560.3574295043945, 'valid_fraction': 0.4713383759454898, 'active_fraction': 0.44177718832891244, 'max_counts': 231, 'mean_internal_similarity': 0.4655441047686505, 'std_internal_similarity': 0.10073750341490033, 'mean_external_similarity': 0.4167015223187768, 'std_external_similarity': 0.06696205085652614, 'mean_MolWt': 414.1099825878116, 'std_MolWt': 93.67718164568349, 'effect_MolWt': -0.8567850736331483, 'mean_MolLogP': 5.700259867907537, 'std_MolLogP': 1.60665689782518, 'effect_MolLogP': 0.6390134862536615, 'generated_scaffolds': 1163, 'novel_scaffolds': 1139, 'novel_fraction': 0.9793637145313844, 'save_path': '../logs/replay_ratio_mixed_s2-3.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.767311
Reward: 1.000000
Mean value of predictions: 0.0005568815
Proportion of valid SMILES: 0.7885821831869511
Sample trajectories:
Brc1ccc(-n2nnc3ccncc32)s1
Brc1ccc(CCCN2CCC3(CCCC3)CC2)cc1
Brc1ccc(COc2cccnc2Oc2ccccc2)cc1
Brc1ccc(CSc2nnc(C3CCCC3)n2C2CC2)cc1
C#CCC(Cc1ccc(O)cc1)NC(=O)NCCCNCCCCCCN
Policy gradient replay...
Mean value of predictions: 0.011343611
Proportion of valid SMILES: 0.568922305764411
Sample trajectories:
BP(=O)(Cc1ccccc1)P(=O)(O)O
Brc1ccc2cc1OC1(CC1)c1ncnc(c3ncncc3n1)N2
Brc1cnc2c(Br)cc(N3CCCC3)nc2c1
Brc1nccc2ncnc(NC3CC4COCCN4C3)c12
C#CC(CC)C1CCC2C3CC=C4CC(CO)C(O)CCC45CC(O)CCC5(C)C3CCC12C
Fine tuning...
Mean value of predictions: 0.012219571
Proportion of valid SMILES: 0.656328320802005
Sample trajectories:
BrCN1CCC(ON(c2nc3ccccc3nc2Nc2ccc3ccccc3n2)C2CCCCC2)CC1
Brc1cc2c(N=Nc3ccccc3)ccc3c2c(Br)ccc2n(c3c1)CCC2
Brc1ccc(-c2nc(CNCc3ccco3)no2)o1
Brc1ccc(Br)c(Nc2nccc3ccccc23)c1
Brc1ccc2[nH]c(-c3ccc4nc(-c5ccc6ncncc6c5)[nH]c4c3)nc2c1

  2 Training on 292 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.614019
Reward: 1.030747
Trajectories with max counts:
2	Cn1cnc2cc(Cl)ccc21
Mean value of predictions: 0.014993059
Proportion of valid SMILES: 0.6774294670846395
Sample trajectories:
BP(=O)(C(=O)O)N1CCCC(Nc2ccc(Br)cc2)C1
Brc1cc2nc(-c3cc4ccccn4c3)[nH]c2cc1CN1CCCCC1
Brc1ccc(OCc2ccc(CN3CCc4ccccc4C3)cc2)cc1
Brc1cccc(CN2CCN(C3CCCCC3)CC2)c1
Brc1ccccc1SC1=CC=C2C=C1C=Nc1ccccc1N2
Policy gradient replay...
Mean value of predictions: 0.011792453
Proportion of valid SMILES: 0.663121676571786
Sample trajectories:
Brc1ccc(-c2cc(Br)ccc2Br)cc1
Brc1ccc(Nc2ccncc2)cc1
Brc1ccc2c(Nc3ccccc3)ncnc2c1
Brc1ccc2c(Nc3ccccc3-c3ccccc3)cc(-c3ccccc3)nc2c1
Brc1ccc2c(c1)Sc1ccccc1N2
Fine tuning...
Mean value of predictions: 0.033967793
Proportion of valid SMILES: 0.6415153412648716
Sample trajectories:
BrC1=CCCCCC1
Brc1ccc(Br)c(CSc2nnc(-c3ccccn3)n2-c2ccccc2)c1
Brc1ccc(Nc2ccnc3cc(Br)c(Br)cc23)cc1
Brc1ccc(Nc2ncnc(NCN3CCCCCC3)n2)cc1
Brc1ccc2oc(-c3ccncc3)nc2c1

  3 Training on 479 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.930290
Reward: 1.286686
Trajectories with max counts:
17	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.070866995
Proportion of valid SMILES: 0.5806754221388368
Sample trajectories:
Brc1ccc(Nc2nccc3ncc(Br)nc23)cc1
Brc1ccc(Nc2ncnc3c4ccccc4c23)cc1
Brc1ccc2c(-c3cncnc3)ncnc2c1
Brc1ccc2ccccc2n1
Brc1ccc2ncnc(Nc3ccccc3)c2c1
Policy gradient replay...
Mean value of predictions: 0.08770883
Proportion of valid SMILES: 0.5240775484677924
Sample trajectories:
B=C(N(C)C1CS(=O)(=O)c2cc(Br)ccc2NC1=O)n1ccnc1
Brc1ccc(Nc2nccnc2-c2nnc(N3CCCC3)s2)cc1
Brc1ccc(Nc2ncnc3cc(Br)cnc23)cc1
Brc1ccc2ncnc(Nc3cccc(Br)n3)c2c1
Brc1cccc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.0707542
Proportion of valid SMILES: 0.5781053952321205
Sample trajectories:
Brc1cc(-c2ccncc2)nc2ncnc(N3CCC(=Cc4cccs4)n3)nc2c1
Brc1cc(Nc2cccnc2)c2sc3ccccc3c2n1
Brc1ccc(Br)c(NCCNc2ncnc3ccccc23)c1
Brc1ccc(Nc2ncnc3c2nc(Nc2cccc(Br)c2)N3)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)nc1

  4 Training on 1077 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 21.064306
Reward: 1.645984
Trajectories with max counts:
62	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.08477103
Proportion of valid SMILES: 0.586875
Sample trajectories:
BrCn1cnc2nncnc21
Brc1cc2ccccc2nc1SCc1cccnc1
Brc1ccc(Br)c(-c2ccc3ncnc(Nc4cccc(C5CCN(c6ccccn6)CC5)c4)c3n2)c1
Brc1ccc(C=NNc2ncnc3sc(Nc4ccccc4)ncnc23)cc1
Brc1ccc(NN=Cc2cccnc2)cc1
Policy gradient replay...
Mean value of predictions: 0.12248552
Proportion of valid SMILES: 0.594180225281602
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(Nc2ccccc2)cc1
Brc1ccc(Nc2ccnc3cccnc23)cc1
Brc1ccc2nc(Nc3cc4ncnc(Nc5ccccc5)c4nc3-c3ccccc3)sc2c1
Brc1ccc2ncnc(Nc3ccc4c(c3)OCCO4)c2c1
Fine tuning...
Mean value of predictions: 0.13213031
Proportion of valid SMILES: 0.6238273921200751
Sample trajectories:
Brc1ccc(Nc2c3ccccc3nc3cccnc23)cc1
Brc1ccc(Nc2ccnc3c(Nc4ccc(Br)cn4)nc23)cc1
Brc1ccc(Nc2ncn3cc(Br)cnc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1

  5 Training on 2018 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 20.740068
Reward: 2.005010
Trajectories with max counts:
118	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.18182905
Proportion of valid SMILES: 0.5229759299781181
Sample trajectories:
BP1(=S)OC(=O)Oc2ccc(Br)cc21
Brc1cc(Br)c2ncnc(Nc3cccc4ccccc34)c2c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCCCC1
Brc1cc2ncnc(Nc3ccccc3)c2cn1
Policy gradient replay...
Mean value of predictions: 0.22134891
Proportion of valid SMILES: 0.5895428929242329
Sample trajectories:
Brc1cc(Cc2cccc3ncncc23)nc2[nH]cnc12
Brc1cc2c(Nc3cncnc3)ncnc2cn1
Brc1ccc(-c2ccc(Br)nc2Nc2ncnc3cc(Br)c(Br)cc23)cc1
Brc1ccc(COc2cncc(Nc3ncnc4ccsc34)n2)cc1
Brc1ccc(Nc2ncc(Br)s2)nc1
Fine tuning...
Mean value of predictions: 0.23034854
Proportion of valid SMILES: 0.5833593994369721
Sample trajectories:
BP(=O)(OCC(=O)N(C)C)c1ccc(Br)cc1
BrCc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Brc1cc(Br)c2nncnc2n1
Brc1cc2ccc(Nc3c(Br)cc(Br)c4ncnn34)nc2cc1Br
Brc1ccc(-c2cc3nccn3cn2)cc1

  6 Training on 3496 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 23.039802
Reward: 2.501696
Trajectories with max counts:
56	COc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1OC
Mean value of predictions: 0.33272618
Proportion of valid SMILES: 0.5150093808630394
Sample trajectories:
BrCC1=COc2cc(Br)ccc2O1
Brc1cc(Nc2ncnc3cc(Br)ncc23)ccn1
Brc1cc2ncnc(Nc3ccccc3)c2cn1
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1cc2ncncn2c1CN(Cc1ccc2c(c1)OCO2)Cn1c(Br)c2ccccc2nc2ccccc21
Policy gradient replay...
Mean value of predictions: 0.29587406
Proportion of valid SMILES: 0.5774294670846395
Sample trajectories:
Brc1cc2ncnc(Nc3ccccn3)n2n1
Brc1ccc(NNc2nc3ccc(Br)cc3s2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Brc1ccc2ccc(CSc3nc4ccc(s3)Nc3ccc(cn4)Nc4ccnc(n3)C4)nc2c1
Brc1ccc2nc(Nc3ncnc4ccsc34)ccc2n1
Fine tuning...
Mean value of predictions: 0.2845171
Proportion of valid SMILES: 0.6115625
Sample trajectories:
Brc1cc2c(Nc3ccccc3)ncnc2cc1CCn1ccnc1
Brc1cc2cc(I)c3ncncc3c2cc1Br
Brc1ccc(CNc2ccncc2Nc2ccc3ncnc3ncn2-c2ccc(Br)cc2)cc1
Brc1ccc(Nc2ccnc3ccsc23)cc1
Brc1ccc(Nc2nc3cc(Br)cnc3cc2Br)cc1

  7 Training on 5328 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 25.208624
Reward: 3.209978
Trajectories with max counts:
48	Clc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Mean value of predictions: 0.41415423
Proportion of valid SMILES: 0.543125
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1cc2ncncn2c2ncnn2c1Nc1nccc2ccccc12
Brc1ccc(NCCNc2cc3ncnc(Nc4ccc(Br)cc4)c3c3cncnc23)cc1
Brc1ccc(Nc2cc(CN3CCN(c4cc(Br)ccc4Br)CC3)ccn2)nc1
Brc1ccc(Nc2ncnc3c4cc[nH]c4c23)cc1
Policy gradient replay...
Mean value of predictions: 0.25360933
Proportion of valid SMILES: 0.58875
Sample trajectories:
B[PH](=O)(NO)(NC(=O)c1ccccc1)Oc1cc2ccccc2nc1Nc1ccnc2cc(Cl)ccc12
BrCCOc1ccc2ncncc2c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1cccc2ccccc12
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1Br
Brc1ccc(C2=NNC(c3sccc3Br)C2)cc1
Fine tuning...
Mean value of predictions: 0.32844603
Proportion of valid SMILES: 0.6056910569105691
Sample trajectories:
Brc1cc2c(Nc3ccccc3)ncnc2s1
Brc1cc2ncnc(Nc3ccncc3)c2cn1
Brc1ccc(CCOc2ccc(Nc3nc(Nc4ccccc4)nc4scnc34)nc2)cc1
Brc1ccc(Cc2cncnc2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(N4CCCC4)cc23)cc1

  8 Training on 7196 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 25.648810
Reward: 3.323667
Trajectories with max counts:
45	COc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cc1Br
Mean value of predictions: 0.50030214
Proportion of valid SMILES: 0.5175109443402126
Sample trajectories:
Brc1cc(Br)c2c(Nc3cccnc3)ncnc2c1
Brc1cc(Br)c2cc(Br)sc2c1
Brc1cc(Nc2ncnc3cc(Br)nn23)ccn1
Brc1cc2c(Nc3cc(Br)c(Br)s3)ncnc2cc1NC1CCN(c2nc3nncnc3cc2Br)CC1
Brc1cc2nc3cc(Br)c4ncncc4c3c2[nH]c1Br
Policy gradient replay...
Mean value of predictions: 0.4427163
Proportion of valid SMILES: 0.5718759787034137
Sample trajectories:
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1cc(Nc2ccc(Br)c(Br)c2)cc(Nc2ncncc2Br)c1
Brc1cc(Nc2ncn3c(Br)cnc3n2)ccc1I
Brc1cc(Nc2ncnc3cc(Br)sc23)c(Br)s1
Fine tuning...
Mean value of predictions: 0.3663202
Proportion of valid SMILES: 0.6014379493591747
Sample trajectories:
BP(=O)(OCC)C(=O)NO
Brc1cc2c(Nc3cccnc3)ncnc2s1
Brc1cc2cccnc2nc1N1CCCCCCC1
Brc1cc2nccnc2nc1Nc1ccc2ncnc(-c3ccccc3)c2c1
Brc1cc2ncnc(Nc3ccccc3)c2nc1C#Cc1ccccc1

  9 Training on 9447 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 27.399568
Reward: 4.208213
Trajectories with max counts:
121	COc1cc2ncnc(Nc3cccc(Br)c3)c2s1
Mean value of predictions: 0.50709414
Proportion of valid SMILES: 0.45826820881525476
Sample trajectories:
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc2ncnc(Nc3ccccc3Br)n2n1
Brc1ccc(Br)c(-c2sc3ncnc(Nc4ccccc4)c3c2OCCCNc2nc3ccccc3s2)c1
Brc1ccc(Br)cc1
Brc1ccc(CNc2ccc3ncnc(Nc4ccccc4)c3n2)cc1
Policy gradient replay...
Mean value of predictions: 0.5015029
Proportion of valid SMILES: 0.5407939981244139
Sample trajectories:
BrC=CBr
Brc1cc(Br)c2ncnc(Nc3cc[nH]n3)c2n1
Brc1cc(Br)cc(Nc2ncnc3c2CN(C=Cc2ccncc2)CCO3)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cn1
Brc1cc2ncnc(Nc3cccs3)c2s1
Fine tuning...
Mean value of predictions: 0.446723
Proportion of valid SMILES: 0.59125
Sample trajectories:
BP(=O)(Oc1ccccc1-c1cccc2ncnc(Nc3ccc(O)cc3)c12)OP(=O)(O)OP(=O)(O)COP(=O)(O)O
Brc1cc2c(Nc3ccccc3)ncnc2cc1I
Brc1ccc(Nc2ncnc(Nc3cccc(Br)c3)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccccc4)nc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Nc4ncnc5ncnc(Nc6ccccc6)c45)nc23)cc1Br

 10 Training on 11614 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 28.467528
Reward: 4.005080
Trajectories with max counts:
78	COc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Mean value of predictions: 0.54732877
Proportion of valid SMILES: 0.4973429196623945
Sample trajectories:
BP(=O)(OCC=C(C)c1ccccc1)OP(=O)(O)Oc1ccc(N)cc1
Brc1cc2c(Nc3cc4cc(Br)sc4s3)ncnc2cn1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.4958599
Proportion of valid SMILES: 0.58875
Sample trajectories:
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ccnc(Br)c2)cc1
Brc1ccc(Nc2ccnc3ccccc3cc2)cc1
Brc1ccc(Nc2csc3ccc(Br)nc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Nc4ccccc4)cc23)cc1
Fine tuning...
Mean value of predictions: 0.4631922
Proportion of valid SMILES: 0.575625
Sample trajectories:
BP1(=O)OCC(OC(=O)CCCCCCCCNC2=NC(=O)O2)C1NC(=O)c1ccc(Br)c(Br)c1
BrCCNc1nc2ncnc(Nc3cccc(Br)c3)c2s1
Brc1cc2cncnc2c(Nc2ncnc3sccc23)n1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1cc2ncnc(Nc3ccccc3)c2s1

 11 Training on 13747 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.580783
Reward: 4.417707
Trajectories with max counts:
264	COc1cc2ncnc(Nc3cccc(Br)c3)c2s1
Mean value of predictions: 0.5418515
Proportion of valid SMILES: 0.4084375
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(cc1Nc1ccncc1)OCOC=N2
Brc1cc2cc(Br)c(NCc3ccccc3)cc2s1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc2ncnc(Nc3ccc4ccccc4c3)sc2s1
Policy gradient replay...
Mean value of predictions: 0.5198867
Proportion of valid SMILES: 0.5515625
Sample trajectories:
BrC=NNC=Nn1cc2c(Br)ccnc2c2ncnc21
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Nc2ncnc3ccc(Br)nc23)ccn1
Brc1cc2c(N3CCOCC3)Ncnc2cnc1Nc1ccccc1
Brc1cc2c(s1)-c1ncccc12
Fine tuning...
Mean value of predictions: 0.50639594
Proportion of valid SMILES: 0.6158174429509221
Sample trajectories:
BP(=O)(OCC)OP(=O)(O)OCCCCNS(=O)(=O)c1cc2cnc(Nc3ccc(Br)cc3)cc2n1C
BP(=O)(OCCOCOC(=O)Cl)N(=O)=O
BP(=O)(OCCOc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1)C(N)=O
BrCCc1ccc(Nc2ncnc3ccc(Nc4ncc(Br)s4)ncnc23)cc1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1

 12 Training on 15883 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.704009
Reward: 4.781612
Trajectories with max counts:
252	Oc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Mean value of predictions: 0.6192668
Proportion of valid SMILES: 0.3666770865895592
Sample trajectories:
BP(=O)(OCC)Oc1cc(Br)c2ncnc(Nc3cc(Br)c(Br)s3)c2c1
Bc1cc2ncnc(Br)c2o1
BrNc1ncnc2c1sc1ncc(Br)cc12
Brc1cc(-c2csc3cc(Br)sc23)ccn1
Brc1cc(Br)c2c(Nc3cc(Br)sc3Br)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.45800823
Proportion of valid SMILES: 0.60875
Sample trajectories:
BP(=O)(Nc1ccc2cc(Br)cnc2c1)C(F)F
BP(=O)(Nc1ccccc1-c1ccccc1)c1ccc(Br)cc1
Brc1cc(Br)c2nc(Nc3cccc4ccccc34)ncc2c1
Brc1cc(Br)cc(Nc2ccnc3ccc(Br)cc23)c1
Brc1cc2c(s1)-c1cccnc1N2
Fine tuning...
Mean value of predictions: 0.5716282
Proportion of valid SMILES: 0.5815625
Sample trajectories:
BP(=O)(Nc1ccc(Br)c(Br)c1)Nc1ccc2ncnc(Nc3cc(Br)cc(Br)c3)c2c1
BrCN(Br)Nc1cccc(Br)c1
Brc1cc(Br)c2cc(Br)[nH]c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Brc1cc(Nc2ccc(Br)c(-c3nc4ccccc4s3)c2)c(Br)cn1

 13 Training on 18130 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.284736
Reward: 5.615771
Trajectories with max counts:
112	Oc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Mean value of predictions: 0.6797527
Proportion of valid SMILES: 0.4045014066895905
Sample trajectories:
BP(=O)(CP(=O)(c1ccco1)c1cc(Br)c(Br)cc1O)Nc1cc(Br)nc(Br)c1
BP(=O)(Nc1cc2nc(Br)nc2nc2ccsc12)P(=O)(O)O
BP(=O)(OCC)Oc1ccc(I)cc1
B[PH](=O)(Nc1cc(Br)c(Br)c(Br)c1)(P(=O)(O)O)P(=O)(O)O
Bc1cc2ncnc(Nc3cccc(Br)c3)c2nc1Br
Policy gradient replay...
Mean value of predictions: 0.5679809
Proportion of valid SMILES: 0.523125
Sample trajectories:
BrC1=C2C=C(C3=NN(c4ccncc4)C(Nc4ccc(Br)cn4)C=C3)C2C1
Brc1cc2ncnc(-c3ccsc3)c2cn1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Brc1cc2ncnc(Nc3ccccc3)c2s1
Fine tuning...
Mean value of predictions: 0.5806897
Proportion of valid SMILES: 0.5890625
Sample trajectories:
BP(=O)(O)COCC(=O)Oc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2c(Nc2ccc(Br)cc2)n1
Brc1cc(Br)c2ncncc2c1Nc1ncnc2ccc(Br)c(Br)c12
Brc1cc(Br)c2sc3c(Br)cc(Br)c(Br)c3c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2[nH]c(N3CCOCC3)nc2c(Nc2cccnc2)c2ccncc2n1

 14 Training on 20602 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.947141
Reward: 5.650540
Trajectories with max counts:
171	COc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Mean value of predictions: 0.68235296
Proportion of valid SMILES: 0.40375
Sample trajectories:
BP(=O)(Nc1ccnc(Nc2ncnc3ccccc23)c1)C(F)(F)F
BP(=O)(OCC(=O)CN)C(=O)O
BP(=O)(OCC1OC(=O)C(O)C1O)C(O)C(O)CO
BP(=O)(OCCOc1cc2ncnc(N)c2s1)Oc1ccc2ncnc(N)c2n1
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.6423227
Proportion of valid SMILES: 0.5598624570178181
Sample trajectories:
BP(=O)(COP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O)OCCO
BP(=O)(OCC)OC(=O)CCCCCCCCOc1cc2ncnc(Nc3cc(Br)cnc3F)c2s1
BP(=O)(OCCCCCOCCOCCO)P(=O)(O)O
Brc1c[nH]c(Nc2ncnc3cc(Br)c(Nc4cccnc4)nc23)c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Fine tuning...
Mean value of predictions: 0.61913615
Proportion of valid SMILES: 0.5715625
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCC(=O)OCCOCCOP(=O)(O)OP(=O)(O)O
Bc1ccc(Br)cc1Nc1ncnc2ncnc(Nc3cccs3)c12
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Nc2ncnc3c(Br)cc(Nc4nc5c(Nc6ccc(Br)c(Br)c6)ncnc5cc4Br)nc23)c(Br)s1
Brc1cc(Nc2ncnc3ccc(Nc4nc5c(Nc6ccc(Br)s6)ncnc5cc4Br)cc23)ccn1

 15 Training on 23292 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.710471
Reward: 5.536261
Trajectories with max counts:
68	Oc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Mean value of predictions: 0.614321
Proportion of valid SMILES: 0.506408252578931
Sample trajectories:
BP(=O)(OCC)OCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2n1
BrCCN(c1ccc(Br)cc1)c1cccc(Br)c1
Brc1cc(Br)c2cc(Br)sc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Brc1cc2ncnc(Nc3ccc4ccccc4c3)c2cc1Nc1cccc2ncncc12
Policy gradient replay...
Mean value of predictions: 0.68028796
Proportion of valid SMILES: 0.5652582159624413
Sample trajectories:
BP(=O)(CCNc1cc2ncnc(Nc3cc(Br)c(Br)cc3F)c2nc1N(=O)=O)C(=O)OCCO
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Fine tuning...
Mean value of predictions: 0.57985306
Proportion of valid SMILES: 0.5958111909971866
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3cc(-c4cc5cncnc5cc4I)sc23)c1
Brc1cc(I)cc(Nc2ccc(I)cc2)c1
Brc1cc(Nc2ncnc3ccc(Br)nc23)ccn1
Brc1cc2c(nc1N1CCOCC1)-c1ccccc1N2
Brc1cc2ncnc(Nc3cc(Br)c(Br)s3)c2nc1Br

 16 Training on 26087 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.686472
Reward: 5.749573
Trajectories with max counts:
190	Oc1cc2ncnc(Nc3cccc(Br)c3)c2s1
Mean value of predictions: 0.6531685
Proportion of valid SMILES: 0.404375
Sample trajectories:
BP(=O)(NO)c1cccc(Br)c1
BP(=O)(Nc1cc2ncnc(Nc3ccc(Br)cc3)c2s1)OCC
BP(=O)(O)CN(O)P(=O)(O)OC
BP(=O)(OCC)OC(=O)COP(=O)(O)Oc1cc2ncnc(Nc3ccccc3)c2s1
BP(=O)(OCC)Oc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.64876175
Proportion of valid SMILES: 0.5679899968740232
Sample trajectories:
BP(=O)(OCC)c1nc2cc(Cl)c(Br)cc2s1
BrCCCSc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Nc4ccccc4)nc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.6231961
Proportion of valid SMILES: 0.5848702719599875
Sample trajectories:
BrCc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2nc1Nc1ccccc1
Brc1cc(Br)c(N2CCCCC2)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1

 17 Training on 28781 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 33.210945
Reward: 6.093404
Trajectories with max counts:
357	Oc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Mean value of predictions: 0.77563787
Proportion of valid SMILES: 0.3796875
Sample trajectories:
BP(=O)(OC(CO)COP(=O)(O)OP(=O)(O)O)P(=O)(O)O
Bc1cc(Br)c(Br)c(Br)c1Nc1cc2ncnc(Nc3cc(Br)sc3Br)c2nc1Cl
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Nc2ncnc3cc(Br)c23)oc1Br
Policy gradient replay...
Mean value of predictions: 0.5787928
Proportion of valid SMILES: 0.5748671459831197
Sample trajectories:
Brc1c(Br)c(Br)c2sccc2c1Br
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cn1
Brc1ccc(-c2cc3c(Nc4ccccc4I)ncnc3cc2Br)cc1
Brc1ccc(-c2sccc2-c2ncccc2I)cc1
Brc1ccc(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.63022125
Proportion of valid SMILES: 0.5797872340425532
Sample trajectories:
Bc1ccc(Br)cc1-c1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2nc1Nc1ccc(Br)cn1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc2ncnc(Nc3ccc4c(c3)OCO4)c2s1
Brc1cc2ncnc(Nc3ccccc3)c2nc1Nc1ccccc1

 18 Training on 31500 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.109484
Reward: 6.220120
Trajectories with max counts:
60	Oc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
60	Oc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Mean value of predictions: 0.7846868
Proportion of valid SMILES: 0.5389184120037511
Sample trajectories:
Bc1ccc(Nc2ncnc3scnc23)cc1
Br
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(-c4ccnc(Nc5ncnc6cc(Br)cnc56)c4)nc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(-c4ccncc4)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.68382573
Proportion of valid SMILES: 0.6242575804939043
Sample trajectories:
BP(=O)(OCCCCCCCl)OCCCCCOCCOC
BrCN1CCN(CC=CCCNc2ccnc3ccc(Br)cc23)CC1
BrCN1c2ncnc(-c3ccc(Br)c(Br)c3)c2CN1c1ncc(Br)s1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Nc4cccnc4)nc23)cc1Br
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Fine tuning...
Mean value of predictions: 0.6512794
Proportion of valid SMILES: 0.6108158799624883
Sample trajectories:
Brc1cc(Br)c2ncnc(Nc3ccsc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Nc4ccc5ncnc(Nc6ccccc6)c5c4)nc23)c1
Brc1cc2ccccc2nc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Brc1cc2ncnc(Nc3ccc(Br)c4ccccc34)c2s1
Brc1cc2ncnc(Nc3ccc(Nc4ccc(Br)c(Br)c4)cc3)c2s1

 19 Training on 34952 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.697505
Reward: 6.271696
Trajectories with max counts:
132	Oc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Mean value of predictions: 0.7823305
Proportion of valid SMILES: 0.4105691056910569
Sample trajectories:
Bc1cc2c(cc1Br)C(=O)c1cc(Br)ccc1N2
Bc1ccc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)cc1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(-c4cc5c(Nc6ccc(Br)c(Br)c6)ncnc5cc4Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.6144259
Proportion of valid SMILES: 0.5417317911847452
Sample trajectories:
BP(=O)(Oc1cc2ncncc2c2ccccc12)c1cccc2cccc(Br)c12
Bc1ccccc1CNc1cc2ncnc(Nc3ccc(Br)cc3)c2nc1Nc1ccccc1
BrCc1cc2ncnc(Nc3cccc(Br)c3)c2s1
Brc1cc(Br)c2nc3sc(-c4cc5cncnc5cc4Br)cc3cc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.6549626
Proportion of valid SMILES: 0.5858080650203189
Sample trajectories:
BP(=O)(Nc1cccc(Br)c1)c1cc(Br)cc(Br)c1
Bc1cc(Br)ccc1-c1cc2ncnc(Nc3ccccc3)c2cc1O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Br)c(Nc2nc(-c3c(Br)cccc3Br)cs2)c1
Brc1cc(Br)c(Br)s1

 20 Training on 37837 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.369309
Reward: 6.516937
Trajectories with max counts:
278	Oc1cc2ncnc(Nc3cccc(Br)c3)c2s1
Mean value of predictions: 0.5924256
Proportion of valid SMILES: 0.3465625
Sample trajectories:
Bc1ccc2c(c1)N2Nc1ccccc1
Brc1cc2ncnc(Nc3ccccc3Br)c2s1
Brc1ccc(-c2c(Br)ccc3ncc(Br)cc23)cc1
Brc1ccc(Br)cc1
Brc1ccc(Nc2ncnc3cc(-c4ccccc4Br)sc23)cc1
Policy gradient replay...
Mean value of predictions: 0.6912447
Proportion of valid SMILES: 0.5926852141294154
Sample trajectories:
BP1(=O)O[PH](O)(N=O)C(=O)N2c3c(sc(Br)c3Br)C(=O)C=C(O)N21
BrCCNc1nc2ccccc2nc1-c1cc2cc(Br)ccc2nc1Nc1cccnc1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Brc1cc(Br)cc(Nc2ncnc3cnc(Br)nc23)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cn1
Fine tuning...
Mean value of predictions: 0.6918153
Proportion of valid SMILES: 0.595625
Sample trajectories:
BrCc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1-c1cccc2cccnc12
Brc1cc2ncnc(Nc3cc(Br)c(Br)c(Br)c3)c2s1
Brc1cc2ncnc(Nc3cc(Br)c(Br)s3)c2s1
Brc1cc2ncnc(Nc3cc[nH]c3)c2s1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cn1

Trajectories with max counts:
187	Oc1cc2ncnc(Nc3cccc(Br)c3)c2s1
Mean value of predictions: 0.650564
Proportion of valid SMILES: 0.4932183261453841
Mean Internal Similarity: 0.49488324024384517
Std Internal Similarity: 0.09697120387687591
Mean External Similarity: 0.43446455143785323
Std External Similarity: 0.0746574674320509
Mean MolWt: 499.05840853905306
Std MolWt: 134.64789386987388
Effect MolWt: -0.016696102138676384
Mean MolLogP: 5.8351797490894395
Std MolLogP: 1.8494803470641308
Effect MolLogP: 0.647720626016596
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 98.225256% (1439 / 1465)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 20, 'n_policy_replay': 5, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5671.3313636779785, 'valid_fraction': 0.4932183261453841, 'active_fraction': 0.626283107337473, 'max_counts': 187, 'mean_internal_similarity': 0.49488324024384517, 'std_internal_similarity': 0.09697120387687591, 'mean_external_similarity': 0.43446455143785323, 'std_external_similarity': 0.0746574674320509, 'mean_MolWt': 499.05840853905306, 'std_MolWt': 134.64789386987388, 'effect_MolWt': -0.016696102138676384, 'mean_MolLogP': 5.8351797490894395, 'std_MolLogP': 1.8494803470641308, 'effect_MolLogP': 0.647720626016596, 'generated_scaffolds': 1465, 'novel_scaffolds': 1439, 'novel_fraction': 0.9822525597269625, 'save_path': '../logs/replay_ratio_mixed_s2-4.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.912507
Reward: 1.000000
Mean value of predictions: 0.0012485369
Proportion of valid SMILES: 0.8029448621553885
Sample trajectories:
BP(=O)(NCc1ccc2c(c1)C=CC(=O)O2)P(=O)(O)O
BrCC(ON=C(c1ccccc1)c1ccc(CNc2nc(I)cs2)cc1)N1CCCCC1
Brc1ccc(CNc2ncccn2)nc1
C#CCC#Cc1cc(C(C#CCCCCCC)=CC(=O)OC)ccc1O
C#CCCC(NCc1ccccc1)C(OC(=O)NC(NC1CCCCC1)C(O)COC(=O)Cc1ccc(C(=O)NCCO)nc1)C(=O)NCC1CCCO1
Policy gradient replay...
Mean value of predictions: 0.00015791552
Proportion of valid SMILES: 0.7960402262727844
Sample trajectories:
Brc1ccc(-c2nn3ccccc3c2Br)cc1
Brc1cccc(Nc2nc3ccccc3nc2-c2ccccc2)c1
C#CC1OC(=O)C(CC)c2c(C(C)(C)C)cc(c3ccccc3)nc2SCC(=O)N1C
C#CC1c2ccccc2C2=CC(c3ccccc3)CC(=O)NC1O2
C#CCC(=O)OCP(=O)(NC(C)C(=O)OCC)c1ccc2c(c1)OCO2
Fine tuning...
Mean value of predictions: 0.010570627
Proportion of valid SMILES: 0.6693800876643707
Sample trajectories:
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2nc(c3cccnc3)cnc2Nc2ccc3c(n2)-c2ccccc2N3c2cccnc2)cc1
Brc1cccc(C=NNc2ncnc3[nH]cnc23)c1
C#CCC1CC(CC(=O)C2=C(N3COCC3(C)C)CCCC2)C(C)(C)C1(C)O
C#CCCc1ccc(C(=O)NC(C(=O)OCC)c2ccc(Cl)c(S(N)(=O)=O)c2)cc1

  2 Training on 260 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.757332
Reward: 1.023917
Trajectories with max counts:
2	Cc1cccc(Nc2ncnc3cc(Cl)ccc23)c1
2	Fc1ccc(Nc2nc3ccccc3[nH]2)cc1
2	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.008456845
Proportion of valid SMILES: 0.7191222570532916
Sample trajectories:
BrCc1ccc(C(=Nc2ccc3ccccc3n2)c2cnc3ccccc3c2)cc1
Brc1ccc(-c2csc3ncnc(Nc4ccc5ccccc5c4)c23)cc1
Brc1ccc(Nc2csc(-c3cnn[nH]3)c2)cc1
Brc1ccc(Oc2ccc3c(Br)cccc3c2)cc1
Brc1ccc2ncn(-c3ccccc3Br)c2c1
Policy gradient replay...
Mean value of predictions: 0.009006623
Proportion of valid SMILES: 0.7086983729662077
Sample trajectories:
Brc1ccc(N2CCN(c3ncccn3)C2)c(NC2CCCC2)c1
Brc1ccc(Sc2ncnc3[nH]c(N4CCOCC4)nc23)c2ncccc12
Brc1ccccc1-c1cccc(-n2ccnc2)n1
Brc1cccs1
Brc1cnc(N2CCNc3cnnn3O2)nc1
Fine tuning...
Mean value of predictions: 0.016788322
Proportion of valid SMILES: 0.6873628096582001
Sample trajectories:
Brc1cc(Nc2cnc3ccccc3n2)on1
Brc1ccc(C(=NNc2nccs2)Nc2ccc3c(c2)OCO3)cc1
Brc1ccc(C=NN2CCN(Cc3ccccc3)CC2=Nc2ccccc2)cc1
Brc1ccc(Nc2cc(N3CCCC3)ccc2Br)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1

  3 Training on 382 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.323023
Reward: 1.049076
Trajectories with max counts:
2	O=C1c2cc(O)ccc2C(=O)c2c(O)cc(O)cc21
Mean value of predictions: 0.011538463
Proportion of valid SMILES: 0.6375353662370323
Sample trajectories:
Brc1ccc(Br)c(Nc2ncnc3cc(Br)c(Br)c(-c4cccnc4)c23)c1
Brc1ccc(Nc2nc3ccccc3nc2-n2cncn2)cc1
C#CC1C(=O)N(C)S1(=O)=O
C#CCNC(=O)S(=O)(=O)c1nc(NC(=O)c2ccccc2)c(N2CCOCC2)s1
C#Cc1ccc(N2nc3cccc(F)c3c(-c3nn(C)c4ncncc34)nc2c2cc(C)ccc2Cl)cc1
Policy gradient replay...
Mean value of predictions: 0.018033573
Proportion of valid SMILES: 0.6558666247247562
Sample trajectories:
BP(=O)(OCC(=O)Nc1ccc(F)cn1)P(=O)(Oc1ccccc1)Oc1ccccc1
BrC1=Nc2ccccc2SC(=Nc2ccncn2)C1
Brc1cc(Br)cc(CCNc2cc(-c3cncnc3)ncn2)c1
Brc1ccc(-c2cc(COc3cccc(Br)c3)nc(-c3ccccn3)n2)cc1
Brc1ccc(-c2ccn3cncc3n2)cc1
Fine tuning...
Mean value of predictions: 0.03131783
Proportion of valid SMILES: 0.6058234189104571
Sample trajectories:
BrC1=CN2C(N=C3c4ccccc4-c4ccccc43)=CSC2=C(CN2CCCC2)C=C1
BrCCNc1ncnc2ncnc(N3CCN(c4ccc5ncccc5c4)CC3)c12
Brc1cc2c(cc1CCN1CCCCC1)OCO2
Brc1ccc(-n2cc(-c3ccc(Br)cc3Br)nc2-c2cccnc2)cc1
Brc1ccc(-n2ccnc2CN=C(Nc2nnc(-c3cc4cnccc4nc3-c3cccc(Br)c3)s2)N2CCCCCC2)cc1

  4 Training on 573 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.822212
Reward: 1.145593
Trajectories with max counts:
15	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.038660713
Proportion of valid SMILES: 0.701095461658842
Sample trajectories:
Brc1ccc(-c2ccc3ccsc3c2)cc1
Brc1ccc(-c2nc3c(s2)-c2ccccc2-3)cc1
Brc1ccc(-c2nnc3cc(-c4cccnc4)ccn23)cc1
Brc1ccc(Br)cc1
Brc1ccc(N=Nc2cccc(Oc3ccccc3)c2)cc1-c1cccs1
Policy gradient replay...
Mean value of predictions: 0.035549525
Proportion of valid SMILES: 0.692018779342723
Sample trajectories:
BrC1(CCNc2ncnc3ccccc23)CCCCN1
Brc1ccc(-c2noc(-c3ccccc3Br)n2)o1
Brc1ccc(C=NNc2ccc(Nc3ccncc3)cc2)cc1
Brc1ccc(N=C(c2ccncc2)c2ccccc2Br)cc1
Brc1ccc(NN=C(c2ccc(Br)cc2)c2ccccc2Br)cc1
Fine tuning...
Mean value of predictions: 0.045386534
Proportion of valid SMILES: 0.6275430359937402
Sample trajectories:
BP(=O)(OCC1OC(Oc2ccccc2)SC(CO)C(O)C(O)C1O)OC(=O)Nc1cc(Br)cc(Br)c1
Brc1cc(Nc2cccnc2)nc2ccccc12
Brc1ccc(-n2cnc3c(Nc4ncncc4Br)ncnc32)cc1
Brc1ccc(NN=Cc2ccc3ccccc3c2)cc1
Brc1ccc(Nc2ccc3ncsc3n2)cc1

  5 Training on 950 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 21.498862
Reward: 1.735846
Trajectories with max counts:
25	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.10866389
Proportion of valid SMILES: 0.5991244527829893
Sample trajectories:
BP1(=O)CCN1CCN(=O)(O)CC(F)(F)F
Brc1cc(-c2ccccc2)c2ccccc2n1
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1ccc(-c2c[nH]c3ccccc23)c2ccccc12
Brc1ccc(-c2ncc(-c3ccccc3)c(Nc3cccc(-c4cc(Br)cc(Br)c4Br)n3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.101484895
Proportion of valid SMILES: 0.6105032822757112
Sample trajectories:
BP(=O)(NCCCCSS(N)(=O)=O)N(Cc1cccc(Cl)c1)c1nc(C(N)=O)c(N)c2cc(Br)ccc12
Brc1cc(Br)c2ncncc2nc2c1Oc1ccccc1-2
Brc1ccc(-c2ncc3ccccc3n2)cc1
Brc1ccc(-c2ncnc3sc(-c4cnco4)nc23)cc1
Brc1ccc(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.10425532
Proportion of valid SMILES: 0.588603631809643
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1OP(=O)(O)O)S(=O)(=O)c1ccc(Cl)cc1
Brc1cc(-c2ccccc2)c2ccccc2n1
Brc1cc(Br)c2c(Nc3ccc(I)cc3)ncnc2c1
Brc1cc(Nc2ncnc3ccccc23)cnc1-c1ccccc1
Brc1ccc(-c2cnc(Nc3ccc4c(n3)-c3ccccc3N4)cn2)cc1

  6 Training on 1805 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 21.405947
Reward: 2.054332
Trajectories with max counts:
51	Fc1ccc(Nc2ncnc3cc(F)ccc23)cc1
Mean value of predictions: 0.16686009
Proportion of valid SMILES: 0.4849906191369606
Sample trajectories:
Brc1c(I)cc(I)cc1-c1cnc2ncnc(Nc3ccccn3)c2c1
Brc1ccc(-c2ccccn2)c2cccnc12
Brc1ccc(-c2nc3ccccc3nc2Nc2nc(NCC3CCCCC3)c(Br)cc2Br)nc1
Brc1ccc(-c2ncnc3[nH]c(Br)cc23)c(Br)c1
Brc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.17050183
Proportion of valid SMILES: 0.5109443402126329
Sample trajectories:
BP(=O)(c1ccnc(N2CCOCC2)c1)C(O)C(F)(F)F
Brc1cc2ncnc(Sc3ccc(Br)c(Br)c3)n2n1
Brc1ccc(Br)c(Oc2c(Br)cc(Br)cc2Oc2ccc(Oc3cccnc3)nc2)c1
Brc1ccc(CSc2cnc3ncnc(Nc4ccc(Br)cc4)c3n2)cc1
Brc1ccc(N2CCC(Oc3ncncc3Nc3ccccc3)C2)s1
Fine tuning...
Mean value of predictions: 0.16747968
Proportion of valid SMILES: 0.5765625
Sample trajectories:
BP(=O)(Oc1ccc(Nc2cncc(Br)c2)nc1)OC(C)C
Br
BrC1=CC(c2ccccc2)=NC(c2ccsc2)=CN1
BrC=C1CCC2=CNC(Br)=C2CC1
Brc1c2ccccc2n2c1sc1ccccc12

  7 Training on 2945 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 21.205140
Reward: 2.659594
Trajectories with max counts:
169	Fc1ccc(Nc2ncnc3ccc(F)cc23)cc1
Mean value of predictions: 0.22447845
Proportion of valid SMILES: 0.449375
Sample trajectories:
BP(=O)(OC)N(CC(F)[PH](F)(F)F)C(=O)OC(Cl)CC
Br
BrCc1ccc(Nc2ncnc3c4ccccc4c23)cc1
Brc1cc(-c2ncnc3[nH]ccc23)n2ccncc12
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Policy gradient replay...
Mean value of predictions: 0.24040997
Proportion of valid SMILES: 0.42700844013754297
Sample trajectories:
Brc1cc(Br)c(Nc2nccnc2Nc2ccnc(Br)c2)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3cccc(Br)c23)c1
Brc1ccc(-c2c(Br)c(Br)c3c(-c4ccsc4)ncnn23)cc1
Brc1ccc(-c2cnc3c(n2)SCC2(CCN(c4ccc(Br)cc4)C2)c2ccccc2-3)cc1
Fine tuning...
Mean value of predictions: 0.2133989
Proportion of valid SMILES: 0.5085964363863708
Sample trajectories:
BP(=O)(NC(Cc1cccc(Br)c1)c1ccc(Br)cc1)C(=O)Oc1cc(Br)cc(Br)c1F
BP(=O)(NCc1cnc(Br)s1)OCCC(F)(F)F
BP(=O)(NO)n1cnc2c(N)ncnc21
BP(=O)(OCC)OC(=O)CCCCCCCCP(=O)(O)O
BrCc1cc(-c2ccc(Nc3ncnc4ccsc34)cc2)nc2c(Br)cncc12

  8 Training on 4084 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 21.170357
Reward: 4.064193
Trajectories with max counts:
485	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.24783656
Proportion of valid SMILES: 0.26
Sample trajectories:
BP(=O)(OCC)OC(=O)C=CC
Brc1[nH]cc(Nc2ncnc3cccnc23)c1Br
Brc1ccc(Nc2cncnc2)cc1
Brc1ccc(Nc2ncccc2n2ncnc2-c2ccc(Br)cc2)cc1
Brc1ccc(Nc2nccnc2-c2nc3ccccc3[nH]2)cc1
Policy gradient replay...
Mean value of predictions: 0.23674911
Proportion of valid SMILES: 0.2653125
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Brc1ccc(Nc2c(-c3cncs3)cnc3ccccc23)cc1
Brc1ccc(Nc2cc(Nc3cc4ccccc34)ncn2)cc1
Brc1ccc(Nc2nc3ccccc3s2)c(Br)c1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.2529321
Proportion of valid SMILES: 0.4052532833020638
Sample trajectories:
BP(=O)(NP(=O)(OP(=O)(O)O)c1ccc(Nc2nc3ccc(Br)cc3s2)cc1)OCC=CC
Brc1cc(-c2nc[nH]n2)c2ncnn2c1
Brc1ccc(-c2ncnc3occc23)c2cccnc12
Brc1ccc(Br)c(Nc2ncnc3cnc(Nc4cnc(Br)s4)cc23)c1
Brc1ccc(CSc2ncnc3cccnc23)cc1

  9 Training on 4911 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 23.159089
Reward: 5.251230
Trajectories with max counts:
201	Fc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.3553379
Proportion of valid SMILES: 0.3190625
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(N(c2ccccc2)c2ncnc3ccccc23)c(Br)c1
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)sc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.36592448
Proportion of valid SMILES: 0.3228125
Sample trajectories:
BP(=O)(CCCO)OCCC(=O)N(O)COP(=O)(O)OP(=O)(O)n1cnc2c(Nc3ccc(Br)cc3)ncnc21
Brc1cc(Br)cc(CNc2ccnc(-c3cncnc3)c2Nc2ncnc3ncnc(N4CCCC4)c23)c1
Brc1ccc(N2CCN(c3ncnc(Nc4cc(Br)c(Br)cc4Br)n3)CC2)cc1Br
Brc1ccc(Nc2cncc(Br)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)cnc23)cc1
Fine tuning...
Mean value of predictions: 0.3278459
Proportion of valid SMILES: 0.356875
Sample trajectories:
Brc1cc(Br)c(Br)s1
Brc1ccc(Br)c(-c2nccnc2SC2CCCC2)c1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2cnc3ccc(Br)nc3n2)cc1
Brc1ccc(Nc2nc3ccccc3s2)cc1

 10 Training on 6116 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 19.413221
Reward: 5.674921
Trajectories with max counts:
1211	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.2914956
Proportion of valid SMILES: 0.213125
Sample trajectories:
B[PH](=O)(Nc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O
Brc1ccc(NN=C2c3ccccc3CCc3ccccc32)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.3102041
Proportion of valid SMILES: 0.214375
Sample trajectories:
BrC(=Nc1cccc(Br)c1)c1ccc(Br)cc1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1ccc(-c2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Br)s1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.36444795
Proportion of valid SMILES: 0.3990625
Sample trajectories:
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c2c(Nc3csc(Br)n3)ncnc2c1
Brc1cc2ncnc(Nc3ncc(Br)c(Nc4ncnc5ncnc6ncnc4c56)n3)c2cc1Br
Brc1ccc(-c2ncnc3ccc(Br)cc23)c(Br)c1
Brc1ccc(-n2cnnc2Nc2cc(Br)ncn2)cc1

 11 Training on 7028 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 18.387916
Reward: 4.420758
Trajectories with max counts:
151	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.34653324
Proportion of valid SMILES: 0.4373241638011879
Sample trajectories:
BP(=O)(Br)OC
BP(=O)(OCC1NC(=N)O1)C(=O)OCCC
Brc1cc(-c2c(Br)cc(Br)c(Br)c2Br)[nH]c1Br
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Nc2ncnc3cccnc23)c(I)c1
Policy gradient replay...
Mean value of predictions: 0.3702683
Proportion of valid SMILES: 0.4309375
Sample trajectories:
BP1(=O)OCC2OC(=N)C(Cl)C(O2)C(N)=Nn2cnc3ncnc(cc(Br)cc(Br)c32)c2cc1ccc2Nc1cc(Br)c(Br)c(Br)c1O
BrC1=CC2c3cc(Br)ccc3OCC2N1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c(I)cc1Br
Brc1cc(Br)c(Nc2ncnc3ncnc(Nc4ccnc5cc(Br)sc45)c23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.34401223
Proportion of valid SMILES: 0.4098155673648015
Sample trajectories:
Brc1cc(Br)c(Br)s1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(I)cc23)c1
Brc1cc(Br)nc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc2c(s1)c(Br)cc1scnc12

 12 Training on 7822 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 16.469823
Reward: 3.992788
Trajectories with max counts:
615	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.35680473
Proportion of valid SMILES: 0.316875
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)N(O)C=O
BP(=O)(OCCS(=O)(=O)OC(Cl)(Cl)Cl)P(=O)(O)Oc1ccc(Br)cc1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Nc2ccc(Br)nc2)c1
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4cc(Br)cnc34)cc2)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.3349544
Proportion of valid SMILES: 0.3084375
Sample trajectories:
BrC1=Nc2cc(Br)ccc2O1
BrCCCCCCCn1nnnc1Nc1ccc(Br)cc1
BrSc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1cc(Br)c2c(c1)Nc1ncnn1CCC2
Brc1cc(Br)cc(CNc2ccc(Nc3ncnc4ccccc34)cc2)c1
Fine tuning...
Mean value of predictions: 0.36666667
Proportion of valid SMILES: 0.399375
Sample trajectories:
BP1(=O)NC(=O)OCC(Oc2ccc(Nc3cnc(Br)c(Br)n3)cc2)C(O)C(O)C1O
Bc1cc(Nc2ncnc3cc(Br)cc(Br)c23)cc(Br)c1Br
BrSc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(I)cc3)ncnc2c1

 13 Training on 8551 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 18.710470
Reward: 4.676478
Trajectories with max counts:
352	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.4223865
Proportion of valid SMILES: 0.2959375
Sample trajectories:
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Brc1c(Br)c(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(I)c1
Policy gradient replay...
Mean value of predictions: 0.43608248
Proportion of valid SMILES: 0.303125
Sample trajectories:
BrCc1nc2c(c(Nc3ccc(Br)cc3)n1)Nc1ncnc(Nc3ccc(Br)cc3)c1C(c1ccc(Br)o1)=N2
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4cc(Br)cc(Br)c34)cc2)c(Br)c1
Brc1cc(Br)c(Nc2nc(Br)cnc2Br)cn1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.41021895
Proportion of valid SMILES: 0.38555347091932457
Sample trajectories:
BP1(=O)OCC(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OC1=O
Bc1ccc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)cc1
Br
Brc1c(-c2ccc(Nc3ncnc4ccccc34)cc2)sc2ncncc12
Brc1cc(Br)c(Br)c(Br)c1Br

 14 Training on 9474 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 19.326524
Reward: 5.142766
Trajectories with max counts:
541	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.4526841
Proportion of valid SMILES: 0.25046904315197
Sample trajectories:
BP(=O)(NC(=O)CBr)C(F)(F)F
BP(=O)(O)OP(=O)(O)OP(=O)(O)P(=O)(O)O
BP(=O)(OCC1NC=C(OC[PH](=S)OCP(=O)(O)O)C(=O)O1)C(=O)O
BP(=O)(OCC1OC(OP(=O)(O)O)C(O)C1O)n1cnc(N)c1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)OP(=O)(O)O
Policy gradient replay...
Mean value of predictions: 0.4733656
Proportion of valid SMILES: 0.25820568927789933
Sample trajectories:
BP(=O)(Cc1ccc(Br)s1)P(=O)(OP(=O)(Oc1ccccc1)Oc1ccc(Br)cc1)c1cc(Br)c(Br)cc1Nc1ccc(Br)cc1
BP(=O)(OC(C)C)C(Cl)=CCl
BP(=O)(OCC1OC(=O)N(O)OCC=CN1C(=O)CF)c1ccc(F)cc1
BP(=O)(OCC1OC(Nc2ccc(Br)cc2)C(O)C(O)C1O)C(=O)OCC=C
BP(=O)(OCCCl)c1cccc(Nc2ncnc3c(Br)cnc(Br)c23)c1
Fine tuning...
Mean value of predictions: 0.44474447
Proportion of valid SMILES: 0.3240625
Sample trajectories:
BP(=O)(COP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O)N(O)COP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(NC(=O)C(CC(=O)O)NC(=O)Cc1cc(F)c(F)c(F)c1)OCC=CC(=O)OP(=O)(O)OP(=O)(O)O
BP(=O)(NC(=O)c1ccc(F)c(F)c1)Nc1cc(F)cc(F)c1F
BP(=O)(OC(F)(F)F)c1cc(Br)c(O)c(Br)c1
BrCc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Br

 15 Training on 10327 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 19.988956
Reward: 5.471231
Trajectories with max counts:
450	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.45328555
Proportion of valid SMILES: 0.26172607879924953
Sample trajectories:
BP(=O)(Nc1ccc(Nc2cc(Br)cnc2F)cc1)C(F)(F)F
BP(=O)(Nc1ccc(Nc2ncnc3cc(Br)cc(F)c23)c(Br)c1)c1ccc(F)cc1
BP(=O)(Nc1nc(Cl)c(Br)cc1F)C(=O)OCCl
BP(=O)(OC(=O)Cl)C(=O)OCC
BP1(=O)OCC(CC(O)COc2cc(F)cc(Br)c2F)c2cc(F)c(F)cc21
Policy gradient replay...
Mean value of predictions: 0.46469802
Proportion of valid SMILES: 0.2328125
Sample trajectories:
BC(=O)OCCS(=O)(=O)O
BP(=N)(N=O)c1ccc(Br)cc1
BP(=O)(N=[P+]([O-])Oc1ccc(Br)cc1)NCCCl
BP(=O)(OCC)OC(=O)CCCl
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)C(F)(F)P(=O)(O)CCl
Fine tuning...
Mean value of predictions: 0.4204757
Proportion of valid SMILES: 0.30228196311347294
Sample trajectories:
BP(=O)(COP(=O)(O)OCOP(=O)(O)O)OCCO
BP(=O)(OCCOS(=O)(=O)c1ccc(Br)cc1)Oc1ccc(Br)cc1
B[PH](=O)(Cl)(Cl)OCCl
Bc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Br
Bc1ccc(Br)cc1Nc1ncc(Br)s1

 16 Training on 11154 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.903712
Reward: 5.932201
Trajectories with max counts:
592	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.47097704
Proportion of valid SMILES: 0.21756798999687402
Sample trajectories:
BP(=O)(CBr)CC(=O)O
BP(=O)(CCl)NO
BP(=O)(N(CCCl)OP(=O)([O-])OP(=O)([O-])OCC[N+](C)(Br)P(=O)(O)O)n1cnc2c(N)ncnc21
BP(=O)(NO)c1cc(Br)cc(Br)c1Br
BP(=O)(Nc1cccc(F)c1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.47810814
Proportion of valid SMILES: 0.23125
Sample trajectories:
BP(=O)(Cl)CCC(F)F
BP(=O)(Cl)N(CC(=O)OC(COP(=O)(O)O)C(C)(F)F)C(=O)NC(C(F)F)C(F)(F)F
BP(=O)(NO)c1ccc(Br)cc1Br
BP(=O)(Nc1ccc(Br)cc1)P(=O)(Oc1ccc(Br)cc1)Oc1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cc1)[PH](Br)(Br)OP(=O)(O)O
Fine tuning...
Mean value of predictions: 0.45964915
Proportion of valid SMILES: 0.285
Sample trajectories:
BP(=O)(N1CCCC1)N1C(=O)Oc2ccccc21
BP(=O)(OC(C)Cl)P(=O)(O)O
BP(=O)(OCC1OC(=O)Nc2cc(Br)ccc21)c1cccc(Br)c1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)O
BP(=O)(OCCS)C(Br)C(Br)Br

 17 Training on 11976 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.025646
Reward: 6.516703
Trajectories with max counts:
484	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.5107652
Proportion of valid SMILES: 0.24108818011257035
Sample trajectories:
Bc1cc(Br)cc(Nc2ncnc3cc(F)c(Br)c(Br)c23)c1
Bc1cc(Nc2ncnc3cc(Br)ccc23)ccc1Br
BrCC=CC=NNc1cc(Br)cs1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Policy gradient replay...
Mean value of predictions: 0.5307895
Proportion of valid SMILES: 0.23764853033145716
Sample trajectories:
BP(=O)(NOC(=O)CBr)Oc1ccc(Br)cc1
BP(=O)(Nc1ccc(Nc2cc(Br)cc(Br)c2)cc1)C(=O)OCCl
BP(=O)(OCC)Oc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BP(=O)(c1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1F)N(O)C(F)(F)F
B[PH](=O)(=O)c1ccc(Nc2nc(Br)nc(NP(=O)(OCBr)P(=O)(O)OP(=O)(O)O)c2Cl)cc1
Fine tuning...
Mean value of predictions: 0.45902336
Proportion of valid SMILES: 0.294375
Sample trajectories:
BIc1ccc(Nc2ncnc3ccc(Nc4ccc(Br)cc4)cc23)cc1
BP(=O)(CCl)N(O)C(=O)OC(C)(C)C
BP(=O)(N1CCN(C(=O)O[PH](N)(=O)=O)CC1)N(=O)=O
BP(=O)(NO)c1cccc(F)c1F
BP(=O)(OC(=O)C[n+]1ccc(Br)cc1)P(=O)(OC(C)(C)O)C(F)(F)F

 18 Training on 12890 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.332298
Reward: 6.703996
Trajectories with max counts:
726	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.47389942
Proportion of valid SMILES: 0.19875
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c1
Brc1ccc(-c2ncnc3cnc(Br)cc23)c(Br)c1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ccc(Nc3ncnc4cc(Br)ccc34)cc2)c1
Policy gradient replay...
Mean value of predictions: 0.47238693
Proportion of valid SMILES: 0.2003125
Sample trajectories:
BP(=O)(OCC1OC(N2C=CC(N)=NC2=O)C(O)C1O)Oc1ccc(F)c(F)c1
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
BrCCNc1c2cnccc2cc2ncnc(Nc3ccc(Br)cc3)c12
BrCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Nc2ccnc3ccc(Br)cc23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.50242954
Proportion of valid SMILES: 0.32166301969365424
Sample trajectories:
BP(=O)(NCC(=O)OCCl)Oc1ccc(Nc2cc(Br)c(Br)c(Br)c2Br)cc1Br
BP(=O)(Nc1cc(Br)c(Br)cc1F)C(=O)Nc1cc(F)c(F)c(F)c1F
Bc1cc(Br)cc(Br)c1Br
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Brc1cc(Br)c(Br)c(Br)c1

 19 Training on 13710 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.363876
Reward: 6.381353
Trajectories with max counts:
638	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.33534744
Proportion of valid SMILES: 0.20693966864645202
Sample trajectories:
BP(=O)(Br)OP(=O)(Br)OP(=O)(O)OP(=O)(O)OP(B)(=O)Oc1ccc(Nc2cc(Br)ccn2)cc1
BP(=O)(CC(=O)O)Nc1ccc(Nc2cncc(Br)c2F)cc1
BP(=O)(CC(O)(C(F)(F)F)[PH](F)(F)P(=O)(O)O[PH](O)(F)F)NO
BP(=O)(N(CC(N)=O)Cc1ccc(Br)cc1)P(=O)(Oc1ccccc1)Oc1ccccc1
BP(=O)(NN=Cc1ccc(Br)cc1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.3267974
Proportion of valid SMILES: 0.19125
Sample trajectories:
BP(=O)(N=C(N)Oc1ccc(Br)cc1Br)OCC
BP(=O)(O)CCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BP(=O)(OCC)n1c(Nc2ncnc3c(Br)cc(Br)cc23)nc2ccccc21
BP(=O)(OCC1NC(=N)N=C(N)O1)c1ccccc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(F)C1O)OC(C(=O)c1cc(F)cc(Br)c1)C(F)(F)F
Fine tuning...
Mean value of predictions: 0.54524314
Proportion of valid SMILES: 0.2958098811757348
Sample trajectories:
BP(=O)(OC(C)(F)F)c1cc(Br)c2c(c1)NC=C(N1C=CC(=O)N1)C(=O)O2
BP(=O)(OCC)OC(=O)CCS
BP(=O)(OCC)Oc1ccc(Nc2ncnc3sc(Br)cc23)cc1
BP1(=O)NP(=O)(OCC)OC(CCP(=O)(O)O)C(n2cnc3c(N)ncnc32)O1
Bc1cc(Br)ccc1Br

 20 Training on 14396 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.887766
Reward: 6.972285
Trajectories with max counts:
999	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.5436567
Proportion of valid SMILES: 0.1675
Sample trajectories:
BP(=O)(Br)Br
BP(=O)(NO)c1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)c(Br)c1)c1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cc1)c1c(N)ncnc1Cl
BP(=O)(OC(Br)CBr)OP(=O)(O)OP(=O)(O)O
Policy gradient replay...
Mean value of predictions: 0.52404577
Proportion of valid SMILES: 0.16375
Sample trajectories:
BP(=O)(NO)P(=O)(NO)c1ccc(Br)c(Br)c1
BP(=O)(Nc1ccc(Cl)c(Br)c1)Oc1ccc(Br)cc1
BP(=O)(Nc1ccc(Nc2nc3ccc(Br)cc3s2)cc1)c1ccc(Br)cc1
BP(=O)(OCC)Oc1cc(N)nc(Br)c1
BP(=O)(OCC1Nc2c(Br)cc(Br)cc2OC(=O)O1)C(Cl)=NO
Fine tuning...
Mean value of predictions: 0.5084706
Proportion of valid SMILES: 0.265625
Sample trajectories:
BP(=O)(N(O)Cc1ccc(Br)c(Br)c1)P(=O)(Oc1ccc(F)cc1)Oc1ccc(Br)cc1
BP(=O)(OC(F)Cl)c1ccc(OP(=O)(O)O)cc1
BP(=O)(OCC(=O)Nc1ccc(Br)cc1)Oc1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1
B[PH](=O)(=NO)N(O)CSc1nc2c(F)c(F)cc(F)c2s1
B[PH](=O)(Nc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O

Trajectories with max counts:
1988	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.4224138
Proportion of valid SMILES: 0.19575
Mean Internal Similarity: 0.5176349676064322
Std Internal Similarity: 0.11773832938319559
Mean External Similarity: 0.4046284102103635
Std External Similarity: 0.08102665271586021
Mean MolWt: 413.43195104895113
Std MolWt: 96.88483736085078
Effect MolWt: -0.8834653856160407
Mean MolLogP: 5.1741681196581215
Std MolLogP: 1.3091747425570577
Effect MolLogP: 0.34186753123957814
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 93.922652% (340 / 362)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5620.662403821945, 'valid_fraction': 0.19575, 'active_fraction': 0.4109195402298851, 'max_counts': 1988, 'mean_internal_similarity': 0.5176349676064322, 'std_internal_similarity': 0.11773832938319559, 'mean_external_similarity': 0.4046284102103635, 'std_external_similarity': 0.08102665271586021, 'mean_MolWt': 413.43195104895113, 'std_MolWt': 96.88483736085078, 'effect_MolWt': -0.8834653856160407, 'mean_MolLogP': 5.1741681196581215, 'std_MolLogP': 1.3091747425570577, 'effect_MolLogP': 0.34186753123957814, 'generated_scaffolds': 362, 'novel_scaffolds': 340, 'novel_fraction': 0.9392265193370166, 'save_path': '../logs/replay_ratio_mixed_s2-5.smi'}
