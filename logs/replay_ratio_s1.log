starting log


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.548579
Reward: 1.000000
Mean value of predictions: 0.0012553942
Proportion of valid SMILES: 0.7983088005010961
Sample trajectories:
Brc1ccccc1-c1nc2cccnc2[nH]1
Brc1ccccc1C=Nc1nc(-c2ccccc2)cs1
Brc1cnc(-c2cnc3ncnc(Nc4ccncc4)n23)nc1
C#CC(=CCC=CCCC)CCCC(=O)O
C#CCCN(Cc1ccccc1OCCO)c1cccc(COC(Cn2cncn2)c2ccc3ccccn23)c1
Policy gradient replay...
Mean value of predictions: 0.010656754
Proportion of valid SMILES: 0.7565625
Sample trajectories:
BP(=O)(NCCCCOc1ccccc1Cl)S(=O)(=O)Nc1nccs1
BrCC1Cc2cccn2C1
Brc1ccc(Nc2ccccc2)cc1
Brc1ccc2c(Nc3ccccc3)ncnc2c1
Brc1ccc2ccc(CN(Cc3ccncc3)Cc3ccccn3)cc2c1
Fine tuning...
Mean value of predictions: 0.011087779
Proportion of valid SMILES: 0.7445278298936836
Sample trajectories:
BP(=O)(OCCC#N)c1ccccc1
Brc1cc(Br)n(Cc2ccccc2Br)c1
Brc1ccc(-c2nc3ccccc3nc2N=Cc2ccccc2Br)cc1
Brc1ccc(Nc2nc3ccccc3s2)s1
Brc1ccc(Nc2ncnc3ccccc23)cc1

  2 Training on 308 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 15.575635
Reward: 1.000000
Trajectories with max counts:
28	Cc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.010370995
Proportion of valid SMILES: 0.74125
Sample trajectories:
BP(=O)(OCC1(O)Oc2ccccc21)Oc1ccc(F)cc1
BrC1CC=C(Cc2ccccc2)N1CCc1ccccc1
Brc1cc2ccccc2nc1-c1ccccc1
Brc1ccc(C=NNc2ncnc3ccccc23)cc1
Brc1ccc2c(Nc3ccccc3OC(c3ccccc3)c3ccccc3)cccc2c1
Policy gradient replay...
Mean value of predictions: 0.09180709
Proportion of valid SMILES: 0.5379806189434199
Sample trajectories:
BP(=O)(CCCl)OCCCl
Bc1ccc(C=Nc2ncnc3[nH]c(-c4ccco4)nc23)cc1
BrC1=C(Br)OC2c3ccccc3N2C(c2cccnc2)=C1
Brc1cc2c(Nc3ccc(I)cc3)ncnc2cn1
Brc1ccc(-c2nc[nH]c2-c2ccc3ncnc(Nc4cccc(NCCN5CCOCC5)c4)c3n2)cc1
Fine tuning...
Mean value of predictions: 0.09842165
Proportion of valid SMILES: 0.5550688360450563
Sample trajectories:
BP(=O)(CCC)N(NC(=O)c1ccccc1)Nc1ccc(Br)cc1
Brc1cc(-c2cc3c(cc2-c2ccccc2)OCO3)c2ncnn2n1
Brc1cc2c(cc1Br)N=C(c1ccccc1)C=C2
Brc1ccc(-c2ccc3ncnc(Nc4cccc(Nc5cccc(Br)c5)c4)c3c2)cc1
Brc1ccc(-c2ccc3ncnc(Nc4ccccc4Br)c3n2)nc1

  3 Training on 812 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.357710
Reward: 1.132669
Trajectories with max counts:
6	Brc1cccc(Nc2ncnc3ccccc23)c1
6	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
6	Oc1cccc2ccccc12
Mean value of predictions: 0.1
Proportion of valid SMILES: 0.5649045980606819
Sample trajectories:
BP(=O)(NP(=O)(OCC)OCC)OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O
BP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCCCC)OC(=O)CCCl
Brc1c(Nc2ncnc3c2Nc2ccccc23)sc2ccccc12
Brc1cc(I)ccc1Nc1ncnc2cccnc12
Policy gradient replay...
Mean value of predictions: 0.10815851
Proportion of valid SMILES: 0.6705220381369178
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCCCC)Oc1ccc(Nc2ccc3ncccc3c2)cc1
B[PH](=O)OCC1OC(C(O)C#N)C(F)C1Br
Bc1cccc(Nc2ncnc3c(Nc4ccccc4)cc23)c1
Bc1cccc2ccc(Br)cc12
Fine tuning...
Mean value of predictions: 0.11756168
Proportion of valid SMILES: 0.6459375
Sample trajectories:
BP(=O)(NC(=O)OCc1ccccc1)NC(=O)OCc1ccccc1
Bc1ccc(Nc2nccc3ccc(NC(=O)CBr)cc23)cc1Cl
BrCCOc1cccc2ncncc12
Brc1cc2cc(Nc3ccc4ccccc4c3)ccc2nc1Br
Brc1cc2nc(Nc3ccncc3)[nH]c2cc1Br

  4 Training on 1717 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 16.926292
Reward: 1.242465
Trajectories with max counts:
30	Brc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.11317393
Proportion of valid SMILES: 0.6669793621013134
Sample trajectories:
Bc1cc(Br)ccc1Nc1ncnc2ccccc12
Brc1cc(Br)c2c(Nc3cnc(CNC4CCN(c5ccccc5)CC4)nc3-c3ccccc3Br)ncnc2c1
Brc1ccc(-c2ccc3cncc(Nc4ccccc4)c3c2)s1
Brc1ccc(-c2nc(Nc3ccccc3)c3ccccc3n2)cc1
Brc1ccc(-c2nccs2)s1
Policy gradient replay...
Mean value of predictions: 0.17699745
Proportion of valid SMILES: 0.6152160300563556
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
B[PH](=O)(Br)(OCCCCCCCCBr)c1cccc(Br)c1
BrCC1OCC(c2nnc3cncnc3n2)O1
BrSc1ccc(Nc2nc3ccc(Br)cc3s2)cc1
Brc1cc(Br)cc(Nc2cccc(CSc3ccc4ncnc(-c5ccccc5)c4n3)c2)c1
Fine tuning...
Mean value of predictions: 0.18944697
Proportion of valid SMILES: 0.6172878170999061
Sample trajectories:
BP(=O)(NCCCCCCCCCCCCCC(=O)O)n1cnc2c(N)ncnc21
BP(=O)(NCCCCCNC(=O)C(Nc1ccc(Br)cc1)P(=O)(O)O)N(=O)=O
BrCCNc1ncc(Br)c2c1-c1ccccc1N2
BrCCc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cc(Nc2ccccc2)nc(-c2ccccc2)c1-c1cccnc1

  5 Training on 2933 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 17.454379
Reward: 1.363316
Trajectories with max counts:
48	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.22098434
Proportion of valid SMILES: 0.5792606516290727
Sample trajectories:
BrCCOc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cc(Nc2ncnc3ccccc23)cc(Br)c1-c1cccnc1
Brc1ccc(-c2cccc(Nc3ncnc4ccccc34)c2)cc1
Brc1ccc(-c2nc(Nc3ccc4c(c3)OCCO4)ncc2-c2ccccc2)cc1
Brc1ccc(C=NNc2ccc(Br)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.26950276
Proportion of valid SMILES: 0.5665101721439749
Sample trajectories:
BP(=O)(NCC(F)F)P(=O)(O)O
BP1(F)(F)NC(SCC(=O)Nc2ccc(F)cc2)=NCC1F
Bc1cccc(Nc2ncnc3cc(Br)c(Cl)cc23)c1
Bc1ncnc(Nc2ccc(Br)cc2)n1
BrC=CBr
Fine tuning...
Mean value of predictions: 0.28172645
Proportion of valid SMILES: 0.5583724569640063
Sample trajectories:
BP(=O)(OCC)OC(=O)OCCCCCCCCC
BrC=CBr
BrCc1ccc2c(Nc3ccc(Br)cc3)ncnc2c1
BrCc1nc2c(Nc3ccc(Br)o3)ncnc2s1
Brc1cc(Br)c(Nc2ccccc2)c(Nc2ccc3ncnc(Nc4ccc(I)cc4)c3c2)c1

  6 Training on 4472 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 18.142192
Reward: 1.549752
Trajectories with max counts:
35	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.2506893
Proportion of valid SMILES: 0.589375
Sample trajectories:
BP(=O)(C(=O)C=Cc1ccc(F)c(N)c1)N1CCN(c2ccc(F)cc2)CC1
BP(=O)(OCC)OCC1Oc2ccc(Br)cc2N(Cc2ccc(Br)c(Br)c2)C1O
BrCN1CCN(CCc2csc(Nc3ncnc4ccccc34)c2)CC1
BrCc1ccccc1Nc1ncc2ccc(Br)cc2n1
Brc1cc(-c2ccc(CN3CCCCC3)cc2)c2ncnc(Nc3c(Br)cccc3Br)c2c1
Policy gradient replay...
Mean value of predictions: 0.26289377
Proportion of valid SMILES: 0.5853125
Sample trajectories:
BP(=O)(OCC)OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Br
Brc1cc(-c2ccncc2)c2ccccc2n1
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.26510173
Proportion of valid SMILES: 0.5990625
Sample trajectories:
BP(=O)(I)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(O)CCCCCCCCCCCC(=O)NCCNP(=O)(O)OP(=O)(O)OP(=O)(O)O
B[PH](=O)(Nc1ccc(Br)cc1)=[PH](c1ccc(Br)cc1)c1cccc(Br)c1
Brc1c(OCCCCCCC[n+]2cccc(I)c2)ccc2ccccc12

  7 Training on 5990 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 21.196538
Reward: 2.110505
Trajectories with max counts:
66	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.39005458
Proportion of valid SMILES: 0.5159574468085106
Sample trajectories:
BP(=O)(NC(c1ccc(Br)cc1)c1cc(Br)cc(Br)c1O)C(=O)O
BP(=O)(NO)C(F)(F)F
BP(=O)(O)CN(CCc1ccccc1)[PH](=O)Oc1ccc(Br)nc1
BP(=O)(O)c1cccc(Nc2ncnc3c(Br)cccc23)c1
BP(=O)(OCC(=O)Nc1ccc(Br)cn1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.32857877
Proportion of valid SMILES: 0.609443402126329
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1ccc(Br)c(Br)c1
BP(=O)(OCC)OC1CCC(c2ccc3ncnc(Nc4ccc(Br)c(Br)c4)c3c2)N1C
BrCc1ccc(Nc2ncnc3ncnc(Nc4ccccc4Br)c3[nH]2)c(Br)c1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.33715454
Proportion of valid SMILES: 0.6108158799624883
Sample trajectories:
BP(=O)(OCC)OP(=O)(O)OP(=O)(O)OP(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC1C=CC(N2C=CC(=O)NC2=O)O1)OP(=O)(O)Oc1cc(NP(=O)(O)O)c(Nc2ccc(I)cc2)nn1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1cccc(-c2nc3ccccn3c2-c2ccccc2Cl)c1
Br

  8 Training on 7859 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 19.766548
Reward: 1.981567
Trajectories with max counts:
31	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.23892301
Proportion of valid SMILES: 0.6615625
Sample trajectories:
BP(=O)(Oc1ccccc1)Oc1ccccc1Cl
Bc1ccc(Nc2ccsc2)cc1
BrCCCCCCCCCCCCCCCCCCCCCCCBr
BrIc1ccccc1-c1ccccc1-c1ncccc1-c1ccccc1
Brc1cc2c(Nc3ccccc3Br)ncnc2cc1I
Policy gradient replay...
Mean value of predictions: 0.38560355
Proportion of valid SMILES: 0.5649045980606819
Sample trajectories:
BP(=O)(OCC)N1C=Nc2c(Br)cnc(Br)c2Nc2c1ccc(F)c2F
BP(=O)(c1cc(Nc2ccccc2Br)c2ccccc2n1)N(O)C=O
BrC1=CSc2ccccc2Nc2ccccc2N1
BrC=CCCBr
BrCC=C(Br)CBr
Fine tuning...
Mean value of predictions: 0.40178075
Proportion of valid SMILES: 0.5617380431384807
Sample trajectories:
BP(=O)(NC(CS(C)=O)c1ccc2c(c1)c1ccccc1O2)C(=O)O
BP(=O)(OCC)N(O)C(=O)Oc1ccc(Br)cc1
BP(=O)(OCC1OC(C[N+](C)(C)C)Oc2cc(Br)cc(Br)c21)C(=O)OCc1ccccc1
BrC#CC#Cc1ccc(Nc2ncnc3cc(Br)sc23)cc1
BrCCNc1nc(Br)c(Nc2c(Br)ccc3c(Br)c(Br)sc23)s1

  9 Training on 9500 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 19.696229
Reward: 1.896230
Trajectories with max counts:
30	Oc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.42630038
Proportion of valid SMILES: 0.5360275689223057
Sample trajectories:
BP1(=O)OCC(OC2(O)C(O)C(O)C(O)C(O)C2O)C(O)C(O)C(Br)C1=O
Bc1ccc(Nc2ncnc3cc(-c4cncnc4)sc23)nc1
BrC(Br)=NCc1nc2c(Nc3cc(Br)ccc3Br)ncnc2s1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
BrNc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.3524655
Proportion of valid SMILES: 0.6343446981545199
Sample trajectories:
BP(=O)(OCC)N1C(=O)c2cccc(Nc3ccsc3)c2C1=O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCc1ccc2ncnc(-c3ccccc3)c2c1
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(N2CCCCC2)c1
Brc1cc(Nc2ncnc3ncnc(Nc4ccccc4Br)c23)c(Br)s1
Fine tuning...
Mean value of predictions: 0.3398186
Proportion of valid SMILES: 0.6201938105658018
Sample trajectories:
BP(=O)(OCC)OCCC=CC1CCC(C2CCCCC2)O1
BrC(Br)(Br)Br
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(I)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2s1

 10 Training on 11011 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 19.557462
Reward: 1.975763
Trajectories with max counts:
31	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.40755874
Proportion of valid SMILES: 0.6120662707095967
Sample trajectories:
BP(=O)(OCC)Oc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BP(=O)(OCc1ccccc1)OCC1OC(n2cnc3c2NC=NC3=O)C(O)C1O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1cnc2nc(Nc3ccc(Br)cc3)cnc2c1Br
BrC(Br)=NNc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)s1
Policy gradient replay...
Mean value of predictions: 0.41618994
Proportion of valid SMILES: 0.5790625
Sample trajectories:
BP(=O)(OCC)OCC=C
BP(=O)(OCC=C)c1cccc(Br)c1
Bc1ccc(Nc2ncnc3sccc23)cc1
Bc1ccccc1I
BrC(Br)(Br)Br
Fine tuning...
Mean value of predictions: 0.4332786
Proportion of valid SMILES: 0.5709375
Sample trajectories:
B[PH]1(=O)(N(C)C)OC(=O)N1CCC(O)(Oc1c(F)cccc1F)P(=O)(O)O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Bc1ccc(Nc2ncnc3sccc23)cc1
BrC=CBr

 11 Training on 12734 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.594178
Reward: 2.520495
Trajectories with max counts:
84	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.44149378
Proportion of valid SMILES: 0.5278473091364205
Sample trajectories:
BP(=O)(NP(=O)(O)Oc1cccc(Nc2nc3c(Br)cc(Br)cc3nc2N=C(N)N)c1F)OCCCC
Bc1ccc(Nc2ncnc3ccsc23)cc1
Br
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(C=NNc2ccc(Br)c(Br)c2)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.45132336
Proportion of valid SMILES: 0.5439749608763693
Sample trajectories:
BP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC)C(=O)Oc1ccc(Br)cc1Br
BP(=O)(OCC)Oc1ccc(-c2c(Br)cc(Br)c(Br)c2Br)c(Br)c1Nc1ccc(Cl)c(Br)c1
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCCCCBr
Fine tuning...
Mean value of predictions: 0.44613665
Proportion of valid SMILES: 0.5588235294117647
Sample trajectories:
BP(=O)(NCc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(O)C(Nc1ccc(Br)cc1)P(=O)(O)OP(=O)(O)O
BP(=O)(O)c1ccc(Br)cc1
BP(=O)(OCC)OC(=O)C(CCCCNC(=N)N)NC(=O)CCCCCCCCCCCCCCCCCCCCCCCCCCCN
BP(=O)(OCC)OC(=O)CC(C)CCC=CCCC=C(Br)Br

 12 Training on 14480 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.488674
Reward: 2.571149
Trajectories with max counts:
128	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.44743836
Proportion of valid SMILES: 0.49437148217636023
Sample trajectories:
BP(=O)(CC(N)C(=O)NCC(=O)OP(=O)(O)OP(=O)(O)O)NO
BP(=O)(Nc1ccc(Br)cc1)OCOc1ccc(Br)cc1Br
BP(=O)(OCC)C(=O)O
BP(=O)(Oc1ccccc1Br)c1ccc(Br)cc1
Bc1ccc(Br)s1
Policy gradient replay...
Mean value of predictions: 0.477374
Proportion of valid SMILES: 0.5349639385387269
Sample trajectories:
BC=C1CCC(=O)O1
BP(=O)(CCCCCCCCCCCCCCCCCCCCCl)NO
BP(=O)(O)CCCCCCCCC#CCCCCCCCCCCCCCCCCCl
BP(=O)(OC)OCC
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCCCCC
Fine tuning...
Mean value of predictions: 0.4538507
Proportion of valid SMILES: 0.5328947368421053
Sample trajectories:
BP(=O)(OCC=C(Cl)Cl)N(Cl)Cl
BP(=O)(c1ccc(Br)cc1)c1ccc(Br)cc1
Bc1ccc(Nc2ncnc3scc(-c4cc(Br)cc(Br)c4)c23)cc1Br
BrCCCCCN1CCCCCCC1
BrCN1CCCCC1CCCN1CCCC1

 13 Training on 16268 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.753423
Reward: 2.647251
Trajectories with max counts:
78	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49421898
Proportion of valid SMILES: 0.5084427767354597
Sample trajectories:
BP(=O)(N=C=Cc1ccc2ccccc2c1)OCC
BP(=O)(Nc1ncnc2cc(Br)cc(F)c12)Oc1cc(Br)ncc1Br
BP(=O)(OCC)OCCCCCCC
BP(=O)(OCC)S(=O)(=O)Nc1ccc(Br)cc1
BP(=O)(Oc1ccccc1Nc1ccc(Br)cc1)N1CCCC1
Policy gradient replay...
Mean value of predictions: 0.472213
Proportion of valid SMILES: 0.5639662183296841
Sample trajectories:
BP(=O)(OCC)N1CCN(CC(=O)Nc2cc(Br)c(Br)cc2F)CC1
BP1(=O)OCC(OC(=O)C=Cc2ccc(Br)cc2)c2ccc(Br)cc21
B[PH](=O)(OCCCl)(c1ccc(Br)cc1)c1ccc(Br)cc1
Bc1ccc(Br)s1
Bc1ccc(Nc2ncnc3c(Br)c(Br)cc(Br)c23)cc1
Fine tuning...
Mean value of predictions: 0.4447123
Proportion of valid SMILES: 0.5712050078247262
Sample trajectories:
BP(=O)(CCSCCCNC(=O)C(CCC(=O)O)NC(=O)C(Cl)(Br)P(=O)(O)O)OCOC(=O)CN
BP(=O)(N(O)CCCl)P(=O)(O)O
BP1(=O)OCC(CCCCCP(=O)(O)O)C(O)C1O
BrC1CNc2ccccc2O1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2cn1

 14 Training on 18177 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.144277
Reward: 2.622367
Trajectories with max counts:
36	Brc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Mean value of predictions: 0.50486934
Proportion of valid SMILES: 0.5265791119449656
Sample trajectories:
BP(=O)(CC(=O)Nc1ccc(Br)c(Br)c1)NO
BP(=O)(O)CC(=NO)Nc1cc(Br)c(Br)c(Br)c1
BP(=O)(O)c1cc(Br)cc(Br)c1Br
BP(=O)(OCC)OC(=O)C(I)CCl
Bc1cc(Br)c(Br)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.482372
Proportion of valid SMILES: 0.5796875
Sample trajectories:
BP(=O)(NCCSP(=O)(O)O)P(=O)(O)O
BP(=O)(Nc1cc(Br)c(Br)cc1Br)c1ccc(Br)cc1
BP(=O)(OCC)Oc1ccc(Br)c(Br)c1
Bc1ccc(Nc2ncnc3sccc23)cc1Br
Bc1cccc(Br)c1Nc1ncnc2sccc12
Fine tuning...
Mean value of predictions: 0.48109588
Proportion of valid SMILES: 0.5708476696903347
Sample trajectories:
BP(=O)(OC)OCS
BP(=O)(OCC)C(=O)OCC
BP(=O)(OCC)OCCCCCCC=C
BP(=O)(Oc1ccc(Br)cc1)Oc1ccc(Br)cc1
B[PH](=O)Oc1cc2ncnc(c1Br)Nc1cc(Br)c(Br)c(Br)c12

 15 Training on 20209 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.625083
Reward: 2.851462
Trajectories with max counts:
107	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.46049544
Proportion of valid SMILES: 0.479375
Sample trajectories:
BP(=O)(C=O)NO
BP(=O)(OCC)OCCCC
BP(=O)(OCC)OCOc1ccccc1
BP(=O)(OCC)Oc1ccc(Br)cc1Nc1c(Br)cc(Br)c(Br)c1I
BP(=O)(Oc1ccc2ccccc2c1)Oc1ccc2cc(Br)c(Br)cc2c1
Policy gradient replay...
Mean value of predictions: 0.37715337
Proportion of valid SMILES: 0.6784375
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Nc2ncnc3[nH]cc(Br)c23)ccc1I
Brc1cc2ncnc(Nc3ccc(Nc4ccccc4)c(Br)c3)c2cc1Br
Brc1cc2ncnc(Nc3cccc4ccccc34)c2cn1
Brc1ccc(-c2cc3c(Nc4cnc5ccccc5c4)ncnc3cc2Br)cc1
Fine tuning...
Mean value of predictions: 0.3770353
Proportion of valid SMILES: 0.6640625
Sample trajectories:
BP(=O)(NO)c1ccccc1Br
BP(=O)(Nc1nc2c(Br)c(Br)c(Br)cc2s1)OCC
BrCCc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
BrNc1nc(Br)ccc1-c1ncnc2ccsc12
Brc1cc(Nc2ncnc3ccsc23)ccc1I

 16 Training on 21898 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.605175
Reward: 2.950300
Trajectories with max counts:
43	Oc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5169652
Proportion of valid SMILES: 0.5944253053554651
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Nc2ncnc3sc(C)nc23)cc1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Nc2ncnc3scnc23)sc1-c1ccccc1
Brc1cc2c(Nc3ccccc3I)ncnc2s1
Brc1cc2ncnc(Nc3ccc(C4CCCC4)cc3)c2cc1-c1cccc(Nc2ncnc3ccsc23)c1
Policy gradient replay...
Mean value of predictions: 0.42509425
Proportion of valid SMILES: 0.5803125
Sample trajectories:
BP(=O)(CCCl)NO
BP(=O)(NO)n1cc(Br)c2ncnc(N)c21
BP(=O)(OCC)ON(O)C[N+](C)(C)C
BP(=O)(Oc1ccccc1)Oc1ccc2c(N)ncnc2c1
BP(=O)(Oc1ccccc1)Oc1ccccc1
Fine tuning...
Mean value of predictions: 0.4387788
Proportion of valid SMILES: 0.5834375
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1cccc(Br)c1
BP(=O)(OCCC)Oc1ccccc1
Bc1ccc(Nc2nc3cccc(I)c3s2)cc1
BrSc1ccc(Nc2ncnc3scc(-c4ccccc4)c23)cc1
Brc1cc(-c2ccccc2Br)c2c(cnc3ccccc32)c1

 17 Training on 23829 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.536105
Reward: 3.186516
Trajectories with max counts:
90	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5434988
Proportion of valid SMILES: 0.52875
Sample trajectories:
BP(=O)(CCCCS(=O)(=O)O)NO
BP(=O)(NC(=O)C(Br)=CBr)OC(C)C
BP(=O)(O)c1ccc(Br)c(Br)c1Nc1ccc(Br)cc1
Bc1ccc(CNC(=S)Nc2ccc(Br)cc2)cc1Br
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cn1
Policy gradient replay...
Mean value of predictions: 0.5472928
Proportion of valid SMILES: 0.5670426065162907
Sample trajectories:
BP(=O)(Oc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1)Oc1cc(Br)c(Br)cc1Br
Bc1cc(I)c2c(Nc3cc(Br)cnc3Br)ncnc2n1
BrCc1cc2ncnc(Nc3ccc(Br)s3)c2cc1Nc1cccc(Br)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)c(Br)c(Br)c23)n1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)sc23)c1
Fine tuning...
Mean value of predictions: 0.5407982
Proportion of valid SMILES: 0.5662272441933459
Sample trajectories:
Bc1cc(Br)cc(Br)c1Br
BrC(=Nc1sccc1Br)c1ccc(Br)cc1
BrCC(Br)CBr
Brc1cc(Br)c(Br)c(Nc2ncnc3c(I)c(Br)cc(Nc4ccccc4)c23)c1Br
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)c(Br)nc23)c1

 18 Training on 26173 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.586809
Reward: 3.011490
Trajectories with max counts:
19	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5353933
Proportion of valid SMILES: 0.5579937304075235
Sample trajectories:
BP(=O)(CCCC(=O)Nc1cc(Br)c(Br)c(Br)c1Br)OCC
BP(=O)(CCCC)OCC
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCCCC[N+](C)(C)C)C(O)CCCCCCCCCCC
Bc1ccc(Nc2ncnc3c(Br)cncc23)cc1
Policy gradient replay...
Mean value of predictions: 0.5610376
Proportion of valid SMILES: 0.5912363067292645
Sample trajectories:
BP(=O)(CCCCCCCCCCCC(=O)O)N(O)CCCl
BP(=O)(OC)OCCCCCCCCCCCCCCCC1CCCCCC1
BP(=O)(OCC)OCCCCCCCCCCCCCCCCCCCCCCC
Bc1ccc(Br)cc1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1
Fine tuning...
Mean value of predictions: 0.56399995
Proportion of valid SMILES: 0.5784865540963102
Sample trajectories:
BP(=O)(OCC)OC(=O)c1cc2c(Br)c(Br)c(Br)c(Br)c2s1
Bc1cc(Br)ccc1Nc1ncnc2c(Br)cnc(Nc3ccc(Br)c(Br)c3Br)c12
Bc1ccc(Nc2ncnc3scnc23)cc1
BrC1CCN(Cc2ncnc3scnc23)CC1
BrCCNc1cc(Br)c(Nc2ncnc3cc(Br)cc(Br)c23)c(Br)c1Br

 19 Training on 28640 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.659580
Reward: 3.345370
Trajectories with max counts:
38	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.59402657
Proportion of valid SMILES: 0.565
Sample trajectories:
BP(=O)(CCCN)N(N)NCC(=O)Nc1ccc(Br)cc1
Bc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BrC=CC=CC=Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCC(Nc1cc(Br)c(Br)c(Br)c1)c1cc(Br)ccc1Br
BrCCCCCCCCCCCCCC=CC=CC=NNc1ncccn1
Policy gradient replay...
Mean value of predictions: 0.5279344
Proportion of valid SMILES: 0.609878086902157
Sample trajectories:
BP(=O)(C=CC(=O)OCC1CCCN1CC(=O)NCCP(=O)(O)O)OCC
BP(=O)(OCCCCCCCBr)ON(=O)=O
Bc1ccc(Br)cc1CNc1ccnc(Nc2ccc3ccncc3n2)c1
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(Br)c(Nc2cc(Nc3ncnc4cc(Br)ccc34)nc3sccc23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.5355135
Proportion of valid SMILES: 0.6121363778542384
Sample trajectories:
BP(=O)(OCCS)C(=O)NCc1ccc(Br)cc1
BrCC(Br)C(Br)Br
BrCCNc1ncnc2sc(Nc3ccccc3)nc12
BrCCNc1ncnc2sc3ccccc3c12
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1

 20 Training on 31161 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.102502
Reward: 3.456505
Trajectories with max counts:
32	Brc1ccc(Nc2ncnc3ccsc23)cc1
32	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.53033227
Proportion of valid SMILES: 0.583125
Sample trajectories:
BP(=O)(CCC(O)P(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O)NO
BP(=O)(O)C=C(Br)CBr
BP(=O)(OCC)c1cc(Nc2ncnc3c(F)c(Br)nc(F)c23)nc2c(N)ncnc12
BrCc1ccc(I)c(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(-c2ccncc2)c(Br)cn1
Policy gradient replay...
Mean value of predictions: 0.555726
Proportion of valid SMILES: 0.6114410753360425
Sample trajectories:
BP(=O)(O)CCCCC=C(Br)Br
BrCc1cccc(Nc2ncnc3ccsc23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(I)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c4sccc34)ncnc2c1
Brc1cc(Br)c2c(c1)sc1ncnc(Nc3ccccc3Br)c12
Fine tuning...
Mean value of predictions: 0.522007
Proportion of valid SMILES: 0.6292591434823382
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(I)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(Nc2ncnc3ccc(I)cc23)ccc1Nc1cccc(I)c1
Brc1cc(Nc2ncnc3ccc(Nc4ccccc4Br)cc23)ccn1

Trajectories with max counts:
200	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.50226307
Proportion of valid SMILES: 0.5305484334938403
Mean Internal Similarity: 0.4680128365214941
Std Internal Similarity: 0.09387211825148996
Mean External Similarity: 0.41253261056489443
Std External Similarity: 0.07353812305094874
Mean MolWt: 415.990804914005
Std MolWt: 101.53661957243735
Effect MolWt: -0.8097367481922252
Mean MolLogP: 5.315819329238331
Std MolLogP: 1.8789634322887223
Effect MolLogP: 0.3618073757973095
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.811448% (1162 / 1188)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 0, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5486.061059951782, 'valid_fraction': 0.5305484334938403, 'active_fraction': 0.4797265440829797, 'max_counts': 200, 'mean_internal_similarity': 0.4680128365214941, 'std_internal_similarity': 0.09387211825148996, 'mean_external_similarity': 0.41253261056489443, 'std_external_similarity': 0.07353812305094874, 'mean_MolWt': 415.990804914005, 'std_MolWt': 101.53661957243735, 'effect_MolWt': -0.8097367481922252, 'mean_MolLogP': 5.315819329238331, 'std_MolLogP': 1.8789634322887223, 'effect_MolLogP': 0.3618073757973095, 'generated_scaffolds': 1188, 'novel_scaffolds': 1162, 'novel_fraction': 0.9781144781144782, 'save_path': '../logs/replay_ratio_s1-1.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.688360
Reward: 1.000000
Mean value of predictions: 0.0011904762
Proportion of valid SMILES: 0.7912087912087912
Sample trajectories:
BrC1=Nc2ccccc2C(Nc2ccccc2)=N1
Brc1ccc(-n2cc3ccncc3c2)cc1
Brc1cccc(-c2ccc(-c3noc(-c4ccccc4)n3)cc2)c1
Brc1cccc(Br)c1Br
C#CC(C)(O)C1C=C(CO)C2CCC13CC(C)C(O)C23
Policy gradient replay...
Mean value of predictions: 0.02200865
Proportion of valid SMILES: 0.6515341264871635
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC)OCOC(=O)C=CC1OC(N2C=CC(=O)NC2=O)C(O)C1O
Brc1ccc(-c2cc(-c3ccccc3)nc3nccc(N4CCOCC4)c23)cc1
Brc1ccc(-c2cn[nH]n2)cc1
Brc1ccc(-c2ncc(Cc3ccc4[nH]cnc4c3)c3cccnc23)cc1
Fine tuning...
Mean value of predictions: 0.024772836
Proportion of valid SMILES: 0.6548700281866583
Sample trajectories:
Brc1cc(Br)c2ccccc2c1-c1ccccc1
Brc1ccc(-c2cccnc2)cn1
Brc1ccc(C=NNc2nnc(-c3ccc(Br)cc3)s2)cc1
Brc1ccc(CNc2ccnc3ccccc23)cc1
Brc1ccc(CNc2ncnc3ncnn23)cc1

  2 Training on 364 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.394007
Reward: 1.000000
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.019691471
Proportion of valid SMILES: 0.6898278560250392
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c2oc3ccccc3c2c1
Brc1ccc(-c2c(Cc3ccc4[nH]c(-c5ccncc5)nc4c3)cnn2-c2ccccc2)cc1
Brc1ccc(-c2cccc(Nc3ccsc3)c2CNc2ccc3c(c2)OCO3)cc1
Brc1ccc(-c2nc(-c3ccccc3)c3cc(-c4ccccc4)c(NC4CC4)nc3n2)cc1
Policy gradient replay...
Mean value of predictions: 0.078990534
Proportion of valid SMILES: 0.4976452119309262
Sample trajectories:
BP(=O)(CC(O)(Oc1cccc(Br)c1)c1ccc(Cl)cc1)OCC1OC(Oc2ccccc2)C(O)C1O
BP(=O)(OCCn1cnc2c(N)ncnc21)OP(=O)(OCc1ccccc1)Oc1ccc(Cl)cc1
Brc1c(Br)c(Br)c2c(Br)cc(N3CCN(c4ncnc5ccsc45)CC3)nc2c1I
Brc1c[nH]c(C2CCCN(Cc3cc4ccccc4[nH]3)CC2CC2CCCCC2)n1
Brc1cc(-c2ccco2)n2ncnc2n1
Fine tuning...
Mean value of predictions: 0.07304015
Proportion of valid SMILES: 0.4926216640502355
Sample trajectories:
Brc1cc(Br)c2nc(-c3ccncn3)sc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1ccc(-c2ccc3ncnn3c2)o1
Brc1ccc(-c2nc(-c3ncnc4[nH]cc(-c5ccccc5)c34)[nH]c2Br)cc1
Brc1ccc(-c2ncnc3cc(NCNc4ncncn4)nc4c(Nc5cccnc5)ncnc4nc23)cc1

  3 Training on 773 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.109193
Reward: 1.051858
Trajectories with max counts:
5	Fc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
Mean value of predictions: 0.07714102
Proportion of valid SMILES: 0.4895964691046658
Sample trajectories:
BP(=O)(OCC1CCC(Nc2ccc(Br)cc2)CC1)C(=O)O
Brc1ccc(-c2cc(N3CCOCC3)on2)c2cnc(Nc3cccc4ccncc34)cc12
Brc1ccc(-c2nc3c(Br)cc(Br)cc3c(Br)c2I)cc1
Brc1ccc(Br)cc1
Brc1ccc(C=Nc2ccc(Br)cn2)cc1
Policy gradient replay...
Mean value of predictions: 0.038838174
Proportion of valid SMILES: 0.753125
Sample trajectories:
Bc1ccccc1Nc1ncccn1
BrC1=C2Cc3ccccc3N12
BrC1=Nc2ccc2Nc2ccccc2Nc2cc(Br)ccc2Sc2ccccc21
Brc1ccc(-c2ccccc2)c2ccccc12
Brc1ccc(Br)c(CSc2ncnc3cccnc23)c1
Fine tuning...
Mean value of predictions: 0.037815474
Proportion of valid SMILES: 0.7555486089402939
Sample trajectories:
BP(=O)(Oc1ccccc1)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1ccccc1
Brc1ccc(-c2csc(Nc3cccc(Nc4ccccc4)n3)n2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ccc(-c3cccnc3-c3ccccc3Br)cc2)cc1

  4 Training on 1214 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.189283
Reward: 1.381272
Trajectories with max counts:
43	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.11055372
Proportion of valid SMILES: 0.6603125
Sample trajectories:
BP(=O)(OCC)OC(=O)C=CC(C)=CC=CC(C)=O
BrC1=Nc2ccccc2N=C(Nc2ncccn2)c2ccccc2Nc2ccccc21
Brc1cc(-c2ccccc2)c2cnccc2n1
Brc1cc(Br)cc(Nc2ncnc3cccnc23)c1
Brc1cc(c2ccccn2)ccc1Nc1ncnc2ncncc12
Policy gradient replay...
Mean value of predictions: 0.19526185
Proportion of valid SMILES: 0.5044025157232704
Sample trajectories:
BP(=O)(OCC1OC(O)C(N)C(O)(N(O)N(=O)=O)C1O)C(=O)O
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OC(CP(=O)(O)O)C(=O)O
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)NC(C)=O
Bc1nc2c(Nc3cc(Br)c(Br)c(Br)c3)ncnc2s1
Bn1cnc(Nc2ncnc3c(F)cccc23)c1F
Fine tuning...
Mean value of predictions: 0.17569527
Proportion of valid SMILES: 0.5206169342146679
Sample trajectories:
BP(=O)(OCC)OC(=O)OCC(F)(F)F
Brc1cc(Br)c(-c2ccc(I)cc2)c(I)c1Br
Brc1cc(Br)c(Nc2c(Br)c(Br)c3ncnc(Nc4cc(Br)c(Br)cn4)c3c2NCC2CCCCC2)c(Br)c1
Brc1cc(Br)c2c(Br)ccc(-c3ccc4c(c3)OCO4)c2c1
Brc1cc(Br)c2sc3c(Nc4ccccc4)c(-c4ccccc4)c3c2c1Br

  5 Training on 2335 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 19.683698
Reward: 1.587523
Trajectories with max counts:
17	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.22344829
Proportion of valid SMILES: 0.5015723270440252
Sample trajectories:
Bc1cc(Br)c2cc(Br)c(Br)c(Br)c2c1Nc1cc(Nc2cc(Br)c(Br)c(Br)c2)c2ncnc(Nc3ccc(Br)cc3)c2c1
BrCc1nc(-c2ccc(Br)c(Br)c2)c2cc(Br)cc(Nc3ccnc(Nc4ccc(Br)cc4)c3)c2n1
Brc1cc(Br)c(Br)cn1
Brc1cc(Br)c(Nc2cc(Br)c(Br)c(Br)c2I)cc1Br
Brc1cc(Br)c(Nc2ncnc3nc4cc(Br)c(Br)c(Br)c4nc23)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.27540323
Proportion of valid SMILES: 0.4677774284816096
Sample trajectories:
BP(=O)(C=O)OCC1OC(S(=O)(=O)CCC=CCCC)C(O)C1O
BP(=O)(OCC)OC(=O)C=CC=CCCC=CCCC
BP1(=O)OCC(OC(=O)C(=Cc2c(Br)[nH]c3c(N)ncnc23)C(F)(F)F)C(O)C(O)C1O
BrC#CCn1c(I)cnc1Nc1ncnc2ncnc(Nc3ccc(OCCCN4CCNCC4)cn3)c12
BrC(Cc1ccc(Br)cc1Br)=NNc1ccc(Br)nc1
Fine tuning...
Mean value of predictions: 0.2558904
Proportion of valid SMILES: 0.46027742749054223
Sample trajectories:
BrC1=Nc2ccc(Nc3cncnc3)cc2C2=CC=CC1=CC=CN2
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(-c2ccccn2)nc2ncnc(Nc3ccc(I)cn3)c12
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Nc2ncnc(-c3ccc(Br)c(Br)c3)n2)c1

  6 Training on 3703 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 19.423380
Reward: 1.692141
Trajectories with max counts:
86	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.2936743
Proportion of valid SMILES: 0.46641556811048335
Sample trajectories:
BP(=O)(Nc1cc(-c2ccc(Br)cn2)nc(N)n1)N1CC(=O)OCC2=Cc3cccc(Br)c3OC2=Nc2c1ncc(Br)c2Br
BP(=O)(Nc1ccc(Nc2ncnc3ccccc23)c(Cl)c1)N(=O)=O
BP(=O)(OCC)C(=O)N(O)C(=O)Nc1cc(Br)c(Br)c(Br)c1O
BP(=O)(OCC)OC(=O)C(Br)Br
BP(=O)(OCC)OC(=O)Nc1c(Br)c(Br)c(Br)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.23116615
Proportion of valid SMILES: 0.6060037523452158
Sample trajectories:
BP(=O)(COCCS(=O)(=O)Nc1ccccc1)N1CCC(F)(F)CC1
Brc1c-c(Nc2ncnc3ccc(Br)cc23)ccc1
Brc1cc(Br)c(Nc2cncc(Br)c2Nc2ccc(Br)nc2)c(Br)c1
Brc1cc(I)ccc1Nc1cnccn1
Brc1cc2c(Br)c3ccccc3cc2c2ccccc12
Fine tuning...
Mean value of predictions: 0.23432273
Proportion of valid SMILES: 0.5792435135979994
Sample trajectories:
Bc1ccc(Br)cc1Br
Br[n+]1c2ccccc2c(-c2ccccc2)c2ccccc21
Brc1cc(Nc2ccccc2-c2ccccc2Br)ncn1
Brc1cc(Nc2ccnc3cc(Br)ncc23)ncn1
Brc1cc(OCc2ccc3c(Br)cccc3n2)c2ccccc2c1

  7 Training on 5144 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 20.922008
Reward: 2.041432
Trajectories with max counts:
82	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.31724137
Proportion of valid SMILES: 0.48983421958085704
Sample trajectories:
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c2ncnc(Nc3cccc4ccccc34)c2c1
Brc1cc(Nc2ncnc3scc(-c4ccccc4)c23)cs1
Brc1cc2ncnc(Nc3cccnc3Br)c2s1
Brc1ccc(-c2coc(-c3ncnc4scnc34)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.35724813
Proportion of valid SMILES: 0.5100250626566416
Sample trajectories:
BP(=O)(Nc1ccc(F)c(F)c1)C(=O)O
BP(=O)(OCC)OC(=O)C=CC=CC=CC=CC=C(C)Br
BP(=O)(OCC)Oc1cc(Br)c(Br)c(Br)c1
BrC(=NNc1ncnc2ccc(Br)cc12)c1cccs1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2cc1Br
Fine tuning...
Mean value of predictions: 0.34654656
Proportion of valid SMILES: 0.5212899185973701
Sample trajectories:
Bc1cc(Br)c(Br)c(Br)c1Br
BrCc1cc2c(Br)c3ccc(Br)cc3nc2c(Nc2ccc(Br)cn2)ncc1Nc1ncnc2ccccc12
Brc1cc(Br)c(-c2c(Br)c(Br)cc3c2c2c(Br)cccc2N3)c(Br)c1Br
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4cc(Br)sc34)cc2)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c(Br)c1

  8 Training on 6844 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 21.859866
Reward: 2.412215
Trajectories with max counts:
116	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.39738053
Proportion of valid SMILES: 0.4774859287054409
Sample trajectories:
BP(=O)(COc1ccc(Nc2c(F)cc(F)c(F)c2F)cc1)P(=O)(O)O
BP(=O)(NC(Br)(CBr)CBr)C(=O)Nc1ccc(Br)cc1
BP(=O)(NC(Cl)(Br)Br)P(=O)(c1cc(Br)c(Br)c(Br)c1N(=O)=O)N(O)C(F)(F)F
BP(=O)(NO)C(=O)N(c1ccc(Br)cc1)c1ccc(Cl)c(Br)c1
BP(=O)(OCC)OC(=O)C(Nc1ccc(Br)cc1)N(=O)=O
Policy gradient replay...
Mean value of predictions: 0.38547418
Proportion of valid SMILES: 0.5207877461706784
Sample trajectories:
BP(=O)(Nc1ccc2c(c1)OCO2)Nc1ccc(Br)cn1
BP(=O)(OCC1OC(N(C)C)C(O)C1O)C(=O)N(c1ccccc1)c1ccccc1
BP(=O)(OP(=O)(O)OP(=O)(O)OP(=O)(O)O)P(=O)(O)O
BrC(Br)(Br)Br
BrC1=C(c2ccc(Br)c(Br)c2)CCN(c2ncnc3ccc(Br)cc23)C1
Fine tuning...
Mean value of predictions: 0.40365136
Proportion of valid SMILES: 0.5309568480300187
Sample trajectories:
BP(=O)(C=CC(=O)Nc1ccc(Br)c(Br)c1)OCC
BP(=O)(Nc1ccc(Br)o1)P(=O)(O)Oc1ccc(N)cc1
BP(=O)(OCC)N1CCN(C(=O)c2cccc3ncnc(Nc4cc(Cl)cc(Br)c4)c23)CC1
BP(=O)(OCC1CC(Oc2cccc(Nc3ncnc4nc(Br)cc(Br)c34)c2)CCC1F)P(=O)(O)OCc1ccc(Br)cc1
BP(=O)(OCCC)Oc1cccc(Nc2ncnc3cc(Br)ccc23)c1

  9 Training on 8710 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 23.164893
Reward: 2.839349
Trajectories with max counts:
205	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.30555958
Proportion of valid SMILES: 0.4328125
Sample trajectories:
BP(=O)(NC(=O)C(F)(F)F)c1ccc(NP(=O)(O)CP(=O)(O)O)cc1
BP(=O)(Nc1ccc(Br)cc1)N(O)C=O
BP(=O)(Nc1ccc(Br)cc1)Nc1cc(Br)c(Br)cc1Br
BP(=O)(Nc1ccccc1)c1ccccc1
BP(=O)(OC)OC(=O)C=CCCC=CC(=O)N(O)N(O)C(=O)C(F)(F)F
Policy gradient replay...
Mean value of predictions: 0.27814293
Proportion of valid SMILES: 0.5992497655517349
Sample trajectories:
BP(=O)(Br)Oc1ccccc1Br
BP(=O)(OCC1CC(P(=O)(O)O)N=C(N)N1)c1ccc2nc(Nc3ccc(Br)cc3)ncc2n1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCC=CC=CC=CC=CCC=CC=CC=CC=CC=CC=CC=C(Br)Br
Brc1cc(Br)c(Nc2ccc(-c3ccccc3Br)cc2)c(Br)c1
Fine tuning...
Mean value of predictions: 0.2691745
Proportion of valid SMILES: 0.5983119724914036
Sample trajectories:
Bc1ccccc1-c1ncccc1Nc1ncnc(Br)c1Oc1ccc(Br)cc1
Br
BrCCCCCCCBr
BrCc1cc2c(Nc3ccccc3)ncnc2cc1Nc1ccccc1Br
Brc1cc(Br)c(Nc2ncnc3ccc(Nc4ncccc4Br)cc23)c(Br)c1

 10 Training on 9819 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 25.190301
Reward: 3.366871
Trajectories with max counts:
94	Brc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Mean value of predictions: 0.4835406
Proportion of valid SMILES: 0.4279899812147777
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCNC(=O)c1c(Br)oc2c(Br)c(Br)c(Br)c(Br)c12
BP(=O)(OCC)OC(=O)Cn1c(Br)cc(Br)c1Nc1cc(Br)c(Br)c(Br)c1F
BP1(=O)CC(F)(F)CN1CC(F)(F)F
B[PH](=O)(=NO)N(CCCl)N(=O)=O
Br
Policy gradient replay...
Mean value of predictions: 0.23140496
Proportion of valid SMILES: 0.6430134417005314
Sample trajectories:
BP(=O)(Nc1ccccc1Br)C(F)F
Bc1ccccc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Br
Brc1cc(Br)c(Nc2ccc(Br)c(Br)c2)c(Br)c1
Brc1cc2c(Nc3ncnc(Nc4ccccc4Br)n3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.23320648
Proportion of valid SMILES: 0.6570803376055018
Sample trajectories:
BP(=O)(OCCCCCCCCCC)OC(=O)CN
BrCCCNc1ccccc1Nc1ccc(Br)cc1Br
Brc1cc(Br)c(Nc2ccc(-c3ccccc3Br)cn2)c(Br)c1
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1ccccc1
Brc1cc2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2s1

 11 Training on 10958 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.949640
Reward: 3.108091
Trajectories with max counts:
71	Brc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Mean value of predictions: 0.4344292
Proportion of valid SMILES: 0.5234668335419274
Sample trajectories:
B=S(=O)(Nc1ccc2ncn(-c3ccccc3)c2n1)c1ccccc1
BC(=O)Nc1cc2ncnc(Nc3csc(Br)c3)c2s1
BP(=O)(N(O)C(F)(F)F)C(F)(F)F
BP(=O)(N=C(Nc1ccc(Br)cc1Br)c1cccc(Br)c1)OCC
BP(=O)(OCC)C(CCC(=O)Nc1ccc(Br)cc1)P(=O)(O)Oc1ccccc1Cl
Policy gradient replay...
Mean value of predictions: 0.43643966
Proportion of valid SMILES: 0.5234521575984991
Sample trajectories:
BP(=O)(N(O)CCCl)P(=O)(Oc1ccccc1)c1ccc(Cl)cc1
BP(=O)(Nc1ccc(I)cc1)c1ccccc1-c1ccc(Br)cc1
BP(=O)(OCC)OC(=O)C(Br)Br
BP(=O)(OCCS)C(F)(F)F
BP(=O)(On1cnc2cnc(Nc3ccc(Br)cc3)nc21)S(=O)(=O)c1cc(Br)cc(Br)c1
Fine tuning...
Mean value of predictions: 0.45935565
Proportion of valid SMILES: 0.5050062578222778
Sample trajectories:
BP(=O)(C(=O)O)N1CCC(c2cc3cc(Br)c(Br)cc3nc2Br)CC1
BP(=O)(C=CC(=O)C(F)(F)F)NC(CP(=O)(O)O)NC(=O)OC(C)(C)C
BP(=O)(OCC)c1ccc(Nc2nc3cc(Br)ccc3s2)cc1
BP(=O)(c1ccc(Br)cc1)N(O)Cc1ccccc1
Bc1cc(Br)c2ncnc(Nc3cc(Br)cc(Br)c3)c2c1

 12 Training on 12604 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.775249
Reward: 3.407290
Trajectories with max counts:
207	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49295354
Proportion of valid SMILES: 0.416875
Sample trajectories:
BI[N-][N+]#N
BP(=O)(NC(Cl)(Cl)Br)P(N)(=O)O
BP(=O)(OCC)C(=O)O
BP(=O)(OCC)N1C=CC(Nc2ccc(Br)cc2)=C(O)C1=O
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCCCCCCCC=CCCCCC=CCCC=CC=CCCCC(Br)Br
Policy gradient replay...
Mean value of predictions: 0.45072815
Proportion of valid SMILES: 0.515322076297686
Sample trajectories:
B[PH](=O)(=Nc1cc(Br)c(Br)c(Br)c1)Nc1cc(Br)c(Br)c(Br)c1
Bc1ccc(F)c(Nc2ncnc3cc(Br)cc(Br)c23)c1
Br
BrCc1ccc(Nc2ncnc3cc(Br)sc23)cc1
BrCc1ccc2c(c1)sc1ncnc(Nc3ccc(Br)cc3)sc12
Fine tuning...
Mean value of predictions: 0.45030376
Proportion of valid SMILES: 0.514535792435136
Sample trajectories:
BP(=O)(Nc1ncnc2c(Br)nc(Br)c(Br)c12)c1ccc2c(Nc3cccc(Br)c3)ncnc2c1
BP(=O)(O)CN(c1ccccc1)c1cccc(Br)c1
BP(=O)(OCC)c1cc(O)cc(Br)c1
BrBr
BrC(Nc1ncnc2ncnc(nc3ccccc32)Nc2cccc1c2)n1cncn1

 13 Training on 14245 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.602212
Reward: 3.212852
Trajectories with max counts:
128	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4130693
Proportion of valid SMILES: 0.47358549546733353
Sample trajectories:
BP(=O)(OCC(N)P(=O)(O)O)P(=O)(O)O
BP(=O)(c1cc(F)c(O)cc1F)N1CCC(Br)(Br)CC(F)(F)C1
Bc1ccc(Nc2ncnc3ccccc23)cc1
BrC(=NNc1nccc(-c2ncncn2)c1Nc1ccccc1)c1ccc(Br)cc1
BrC(Br)(Br)Br
Policy gradient replay...
Mean value of predictions: 0.45406696
Proportion of valid SMILES: 0.5239736759636477
Sample trajectories:
BP(=O)(Nc1cc(Br)c(Br)cc1Br)OP(=O)(O)O
Bc1cc(Br)cc(Br)c1Br
Bc1ccc(Nc2ncnc3sccc23)cc1
BrCc1ccc(Nc2ncnc3sc(Br)nc23)cc1
Brc1c2scnc2cc(I)c2c(-c3cccs3)sc12
Fine tuning...
Mean value of predictions: 0.44970274
Proportion of valid SMILES: 0.5257893091591123
Sample trajectories:
BP(=O)(OCC(=O)Nc1ccc(Br)cc1)c1ccc(Br)o1
BP(=O)(c1ccc(Br)cc1)N1CCN(C(=O)c2cc(Br)cc(Br)c2)CC1
Br
BrBr
BrCCN(c1ccccc1)c1c[nH]nc1-c1ccc2ncnc(Nc3ccccc3)c2c1

 14 Training on 15847 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.615251
Reward: 3.318918
Trajectories with max counts:
99	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.43921334
Proportion of valid SMILES: 0.524375
Sample trajectories:
BP(=O)(Nc1cc(Br)cc(Br)c1)P(=O)(O)OP(F)(F)(F)F
BP(=O)(OCC)C(=O)Oc1ccc(Br)c(Br)c1
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC1OC(=O)N(C(N)=O)N(Nc2cccc(Br)c2)C1=O)C(F)(F)F
B[PH](=O)(Br)(OCC)Oc1cc(Br)c(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.50375146
Proportion of valid SMILES: 0.5356357927786499
Sample trajectories:
Bc1cnc(Nc2ncnc3c(Br)c(Br)sc23)c(Br)c1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1c(Br)c(Br)c(Br)c(Br)c1Br
Brc1c(Br)c(Br)c2c(Nc3cc4c(Br)sc(Br)c34)cccc2c1Br
Brc1cc(Br)c(-c2ccccc2)[nH]1
Fine tuning...
Mean value of predictions: 0.4838749
Proportion of valid SMILES: 0.5103254067584481
Sample trajectories:
BP(=O)(CC(F)(F)F)Nc1snc(Br)c1F
BP(=O)(OCC1CNC(=O)C(O)C1N(CCF)CCN1CCN(c2cc(Cl)cc(Br)c2F)CC1)C(=O)COC(=O)CCCl
Brc1cc(-c2cnc(Nc3ccc(Br)c(Br)c3)s2)c2sccc2n1
Brc1cc(Br)c(-c2cc(Br)c(Br)cc2Br)cc1Br
Brc1cc(Br)c(Br)c(Br)c1

 15 Training on 17632 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.220980
Reward: 3.137079
Trajectories with max counts:
101	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5001186
Proportion of valid SMILES: 0.5273694088207694
Sample trajectories:
BP(=O)(OCCCC)Oc1ccc(Br)cc1
BrCN(CCN1CCCCC1)c1ccccc1Nc1ccc(Br)nc1
BrCc1sc2ncnc(Nc3ccc(Br)cc3)c2c1Br
BrNc1nc(-c2ncnc3c(Br)cc(Br)cc23)cs1
BrNc1ncnc2nc(Nc3ccccc3Br)c(Br)cc12
Policy gradient replay...
Mean value of predictions: 0.48556212
Proportion of valid SMILES: 0.5297805642633229
Sample trajectories:
BBr
BP(=O)(CCCCCCCCCCCCNc1ccc(Br)cc1)P(=O)(OCC)OCC
BP(=O)(NS(=O)(=O)c1cc2c(Br)c(Br)c(Br)c(Br)c2[nH]1)N(CS(=O)(=O)N(C)C)NCCCCCCCCCBr
BrBr
BrCCCCCCCCCCCCCCNc1ccc(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.49375367
Proportion of valid SMILES: 0.538340666247643
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
BrC=CC=CC=CCC=CCCC=CC=CC=CCC=CC=CCCC=CCc1c[nH]c2c(NN=Cc3ccc(Br)cc3)ncnc12
BrCCCCn1nc(Nc2ccc(Br)c(Br)c2)c2c(Nc3ccc(Br)cc3)ncnc21
BrCCNc1nc(Br)cc2ncnc(Nc3ccc(Br)c(Br)c3)c12
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1

 16 Training on 19489 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.048909
Reward: 3.585075
Trajectories with max counts:
221	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49432376
Proportion of valid SMILES: 0.4459375
Sample trajectories:
BP(=O)(CCCCCCCC=CCCCCCC)OCC
BP(=O)(NC(Nc1ccccc1)C(=O)O)c1ccccc1
BP(=O)(c1cccc(Nc2ncnc3cc(F)ccc23)c1)N(O)C(F)(F)F
B[PH](n1cnc2c(Nc3cccc(Br)c3)c(Br)cnc21)=[PH](=O)(O)OCCCCCP(B)(=O)O
Bc1ccc(Nc2ncnc3ncnc(N4CCCC4)c3s2)cc1
Policy gradient replay...
Mean value of predictions: 0.505971
Proportion of valid SMILES: 0.5402442843720638
Sample trajectories:
BP(=O)(Oc1cc2c(Nc3ccc(F)c(F)c3)ncnc2c(N)n1)P(=O)(O)O
Br
BrCC(Br)CBr
BrCC1CCN(CCCOc2cc3c(Nc4ccc(Br)s4)ncnc3cc2Br)CC1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.5065909
Proportion of valid SMILES: 0.5522434891747725
Sample trajectories:
BP(=O)(OCC)C(=O)ON=C(CC=C)CC(=O)O
BP(=O)(OCC)c1ccc(Nc2ncnc3c(F)c(F)c(Br)c(Br)c23)cc1
Bc1cc(Br)c(Br)cc1Oc1ncnc2ccc(Br)cc12
BrC(=Nc1sccc1Br)c1ccc(Br)cc1
BrC=CC(Br)Br

 17 Training on 21363 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.300091
Reward: 3.878306
Trajectories with max counts:
51	Brc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Mean value of predictions: 0.5686298
Proportion of valid SMILES: 0.5216300940438872
Sample trajectories:
BP(=O)(Cl)OCCCl
BP(=O)(NCCCCCCCCCCl)S(=O)(=O)Nc1cc(Br)c2ncn(-c3ccc(Br)c(Br)c3)c2c1
BP(=O)(Nc1ccc(Br)cc1)c1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BP(=O)(OCC)C(Br)C(Br)Br
BP(=O)(OCC)Oc1ccc(Nc2ncnc3sc(Br)cc23)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.44742858
Proportion of valid SMILES: 0.5472170106316447
Sample trajectories:
BC=C1C(=O)C(Br)=C(Br)C1=O
BP(=O)(CC=CC(F)P(=O)(O)O)NO
BP(=O)(CCl)N(Br)CCBr
BP(=O)(Nc1ccc(Br)cc1F)Oc1cccc(F)c1
BP(=O)(OCCCCCC)C(=O)Nc1ccc2c(Br)c(Br)c(Br)c(Br)c2n1
Fine tuning...
Mean value of predictions: 0.4330286
Proportion of valid SMILES: 0.5473881764153894
Sample trajectories:
Br
BrBr
BrCCBr
BrCCCN(CCBr)Oc1ccc2ncnc(Nc3ccc(Br)cc3Br)c2c1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1

 18 Training on 23253 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.322325
Reward: 3.711406
Trajectories with max counts:
219	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.4152239
Proportion of valid SMILES: 0.41875
Sample trajectories:
BP(=O)(NCCCCCCCCCCC)C(=O)Nc1ccccc1Br
BP(=O)(Nc1cc(Br)c(Br)cc1Br)c1ccc2c(Nc3ccc(Br)c(Br)c3)cc(Br)cc2c1
BP(=O)(OCC)C(O)(P(=O)(O)O)P(=O)(O)O
BP1(=O)Nc2ccc(Br)cc2S(=O)(=O)NC(O)CO1
BrC(=NNc1ccc(Br)cn1)c1cccc2ncnc(Nc3ccc(Br)cc3)c12
Policy gradient replay...
Mean value of predictions: 0.54522437
Proportion of valid SMILES: 0.5448275862068965
Sample trajectories:
BP(=O)(NCCCO)N(=O)=O
BP(=O)(Nc1cc(Br)c(Br)c(Br)c1)P(=O)(O)C=CCl
B[PH](=O)(Nc1ccc(Br)c(Br)c1)(P(=O)(O)O)P(=O)(O)O
Bc1ccc(Nc2ncnc3ccccc23)cc1
Bc1ccc(Nc2ncnc3sc(Br)cc23)s1
Fine tuning...
Mean value of predictions: 0.5350694
Proportion of valid SMILES: 0.541692789968652
Sample trajectories:
Bc1cc(Br)cc(Nc2ncnc3sc(Br)c(Br)c23)c1
BrCN1CCN(CCCCc2ncc(Br)cc2Br)CC1
BrNc1ccc2ncnc(Nc3ccc(Br)cc3Br)c2c1
Brc1cc(Br)c(Nc2ncnc3c(Br)ncnc23)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br

 19 Training on 25187 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.685696
Reward: 3.892764
Trajectories with max counts:
177	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.53375614
Proportion of valid SMILES: 0.4437148217636023
Sample trajectories:
BP(=O)(CCl)Nc1cc(Br)c(Nc2cc(Br)c(Br)c(Br)c2Br)s1
BP(=O)(OCC(F)(F)F)C(F)(F)F
B[PH](=O)(=Nc1cc(Br)c(Br)c(Br)c1)Nc1ccc(Br)c(Br)c1
B[PH](=O)(=Nc1ccc(Br)cc1)Nc1cc(F)c(Br)c(Br)c1
B[PH](=O)(Nc1ccc(Br)c(Br)c1)=C(Br)c1cccc(Br)c1
Policy gradient replay...
Mean value of predictions: 0.43805972
Proportion of valid SMILES: 0.5864332603938731
Sample trajectories:
BP(=O)(CCCl)NO
BP(=O)(NCCCCCCCBr)N(O)C(=O)C(Br)Br
BP(=O)(O)OCCCCCCCCCCCCCCCCCF
Bc1ccc(Nc2ncnc3c(Cl)scc23)c2ccccc12
BrC(Br)(Br)Br
Fine tuning...
Mean value of predictions: 0.44412237
Proportion of valid SMILES: 0.5823694904657706
Sample trajectories:
BP(=O)(CC(=O)O)N(CCCF)P(=O)(O)CCS
BP(=O)(O)CC(F)(F)F
BP(=O)(O)OP(=O)(O)C(O)CCCCCCCCCCCP(=O)(O)O
BP(=O)(OCC)OCCF
BP(=O)(c1ccc(Br)cc1)N(O)CCl

 20 Training on 27012 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.049679
Reward: 4.094553
Trajectories with max counts:
238	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.50668675
Proportion of valid SMILES: 0.4161976235146967
Sample trajectories:
BP(=O)(=Nc1cc(Br)c(Br)c(Br)c1F)(NO)Sc1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cc1)P(=O)(Oc1ccc(Br)cc1)N(C)C
BP(=O)(Nc1nc(Nc2ccc(Br)cc2)cs1)c1ccc(F)cc1F
BP(=O)(OC(=O)c1cc(Br)c(Br)cc1Br)P(=O)(O)O
BP(=O)(OCC)C(F)(F)F
Policy gradient replay...
Mean value of predictions: 0.4428345
Proportion of valid SMILES: 0.5909375
Sample trajectories:
BP(=O)(Nc1ccc(F)c(F)c1)P(=O)(O)O
BrCc1ccc2c(Nc3ncnc4cc(-c5ccccc5)ccc34)cccc2c1
Brc1cN(c2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(Br)c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.43748015
Proportion of valid SMILES: 0.5903125
Sample trajectories:
BP(=O)(O)c1cccc(Br)c1
BP1(=O)OCC(Br)NC(=O)N(Nc2cc(Br)c(O)c(I)c2)C1=O
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1Br
BrCC(Br)CCCCCI
Brc1cc(Br)c2c(-c3cc4c(Nc5ccccc5)ncnc4s3)c(-c3ccccc3Br)sc2c1

Trajectories with max counts:
224	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4051724
Proportion of valid SMILES: 0.48575
Mean Internal Similarity: 0.47103112068183417
Std Internal Similarity: 0.10057467912329249
Mean External Similarity: 0.4079766428962088
Std External Similarity: 0.07396872279984355
Mean MolWt: 392.03623204237016
Std MolWt: 89.35324570145667
Effect MolWt: -1.0404890620293066
Mean MolLogP: 5.1074770208540246
Std MolLogP: 1.4627186727806958
Effect MolLogP: 0.2785686331443196
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 96.291013% (675 / 701)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 10, 'n_policy_replay': 15, 'n_fine_tune': 0, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5440.75750875473, 'valid_fraction': 0.48575, 'active_fraction': 0.3887030365414308, 'max_counts': 224, 'mean_internal_similarity': 0.47103112068183417, 'std_internal_similarity': 0.10057467912329249, 'mean_external_similarity': 0.4079766428962088, 'std_external_similarity': 0.07396872279984355, 'mean_MolWt': 392.03623204237016, 'std_MolWt': 89.35324570145667, 'effect_MolWt': -1.0404890620293066, 'mean_MolLogP': 5.1074770208540246, 'std_MolLogP': 1.4627186727806958, 'effect_MolLogP': 0.2785686331443196, 'generated_scaffolds': 701, 'novel_scaffolds': 675, 'novel_fraction': 0.9629101283880172, 'save_path': '../logs/replay_ratio_s1-2.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.761316
Reward: 1.000000
Trajectories with max counts:
2	Cc1cccc(O)c1
Mean value of predictions: 0.00081103
Proportion of valid SMILES: 0.7766929133858268
Sample trajectories:
Brc1ccc(Nc2ncnc3nc(Br)nn23)cc1
C#CC(=O)N1CCN(CCOC(=O)c2ccccc2)C(C)C1
C#CC(CCC(=C)CCCCC#N)c1cn(O)c2c1C(C)(C)NC2(C)C
C#CCCCCCC1C(C)CCCN1CCC#N
C#CCCCCCCCC(=O)N1CCC(P(=O)(O)COC)CC1
Policy gradient replay...
Mean value of predictions: 0.012785388
Proportion of valid SMILES: 0.6169014084507042
Sample trajectories:
Brc1cc2c(c3c1CCNC3)OCCO2
Brc1ccc(NN=C2CCCCCC2)cc1
Brc1ccc(Nc2ccnc3ncnc(Nc4cccc(Br)c4)c23)cc1
Brc1ccc(Nc2ncnc3ncncc23)nc1
Brc1ccc(Nc2onc(-c3ccccc3)c2-c2ccc3[nH]ccc3n2)cc1
Fine tuning...
Mean value of predictions: 0.019393336
Proportion of valid SMILES: 0.6294209702660407
Sample trajectories:
BP(=O)(NN=Cc1ccncn1)c1cccc(Cl)c1
BrC1CCCSc2cc3c(cc2C1)Cnc3-c1ccccc1
Brc1ccc(-c2cn[nH]c2)c2ccccc12
Brc1ccc(-c2nn(Cc3ccccc3)c3ccccc23)cc1
Brc1ccc(N2CCCCC2c2cc3c(cn2)CCCC3)c(-c2nc3ccccc3s2)c1

  2 Training on 325 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.808011
Reward: 1.111544
Trajectories with max counts:
6	Nc1ncnc2c1ncn2C1CCCCC1
Mean value of predictions: 0.020009954
Proportion of valid SMILES: 0.6282051282051282
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)O
Brc1cc2c(cn1)C(=Nc1ccccn1)N2c1cccnc1
Brc1ccc(C=NNc2[nH]nc3ccccc23)cc1
Brc1ccc(Nc2nc3cc(Br)ccc3[nH]2)cc1
Brc1ccc(Nc2ncnc3c2cnn3-c2ccncc2)cc1
Policy gradient replay...
Mean value of predictions: 0.036516577
Proportion of valid SMILES: 0.6337409846346818
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)O
BrCN1CCN(Cc2ccncc2)CC1
Brc1cc(I)ccc1-c1ncnc2[nH]ccc12
Brc1cc(Nc2c3ccccc3nc3ccccc23)nc2ccccc12
Brc1ccc(CNc2cccnc2)cc1
Fine tuning...
Mean value of predictions: 0.042073477
Proportion of valid SMILES: 0.6224937343358395
Sample trajectories:
BP(=O)(NCCCN)N(CC)CC
Brc1ccc(N2CCCCCC2)cc1-c1cccc(Nc2ccc3ccncc3c2)c1
Brc1ccc(N=Nc2ccc3ccncc3c2)cc1OCCCc1ccccc1
Brc1ccc(Nc2cc3c2CCCc2ncnnc2-3)cc1
Brc1ccc(Nc2ccnc3cc(Br)cnc23)cc1

  3 Training on 625 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.334131
Reward: 1.059753
Trajectories with max counts:
2	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
2	Brc1cccc(Nc2ncnc3ccccc23)c1
2	COc1ccc(Nc2ncnc3ccc(Cl)cc23)cc1
2	Cc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
2	Cc1ccc(Nc2ncnc3ccccc23)cc1
2	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
2	Clc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
2	Clc1ccc(Nc2ncnc3ccccc23)cc1
2	Fc1ccc(Nc2ncnc3ccccc23)cc1
2	Fc1ccccc1F
2	N#Cc1cccc(Nc2ncnc3ccccc23)c1
2	O=C(Nc1ccccc1)Nc1ccc(Nc2ncccn2)cc1
Mean value of predictions: 0.03496994
Proportion of valid SMILES: 0.6255092447508618
Sample trajectories:
Br
BrC1=CC(COc2ncccn2)O1
Brc1cc(Nc2cc(CNc3ccncc3)ncn2)ccn1
Brc1ccc(-c2cncnc2)cc1
Brc1ccc(Nc2cc(-c3ccccc3)nc3ccccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.006085526
Proportion of valid SMILES: 0.76
Sample trajectories:
BP(=O)(OCC)OC(=O)Nc1ccc2c(Nc3ccccc3Cl)cccc2c1
B[PH](=O)(OC(c1ccccc1)c1ccccc1)=P(N)(=O)(O)PO
Brc1c2ccccc2cc2ccccc12
Brc1ccc(Br)c(Nc2cccc3ccccc23)c1
Brc1ccc(NCc2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.004571429
Proportion of valid SMILES: 0.765625
Sample trajectories:
BP(=O)(OCC)Oc1ccccc1
Brc1cc2cccs2c1Nc1ccccc1
Brc1ccc(-c2ccccc2)cc1
Brc1ccc(N=Nc2cccc(Br)c2)cc1
Brc1ccc(Nc2ccccc2)cc1

  4 Training on 765 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 14.822560
Reward: 1.050288
Trajectories with max counts:
40	O=C(Nc1ccccc1)c1ccccc1
Mean value of predictions: 0.0055888225
Proportion of valid SMILES: 0.7828125
Sample trajectories:
BP(=O)(OC(=O)c1ccccc1)c1ccccc1
Bc1ccccc1-c1ccccc1Br
BrCc1ccccc1Nc1ccccc1Nc1ccccc1Nc1ccccc1-c1ccccc1
Brc1c(Nc2ccccc2I)ccc2ccccc12
Brc1ccc(Nc2ccccc2Oc2ccccc2)cc1
Policy gradient replay...
Mean value of predictions: 0.1358713
Proportion of valid SMILES: 0.5829946858393248
Sample trajectories:
Brc1ccc(-c2c(Br)ccc3nncn23)cc1
Brc1ccc(-c2nc(Nc3ccc4ccccc4n3)nc3ccccc23)cc1
Brc1ccc(-n2cnc(-c3cccnc3)n2)c2ccccc12
Brc1ccc(Nc2ccc3ncncc3n2)cc1
Brc1ccc(Nc2cnc3ncc(Br)cc3n2)cc1
Fine tuning...
Mean value of predictions: 0.128056
Proportion of valid SMILES: 0.5812206572769953
Sample trajectories:
BC(=O)Nc1c(I)cc(I)cc1Nc1ncnc2ncnc(Nc3cccc(I)c3F)c(Nc3ccc(I)cc3F)nc12
Brc1cc(Nc2ccncc2)cc2ncnn12
Brc1cc2c(Br)c(Br)c(Br)c(Br)c2s1
Brc1cc2c(Nc3ccccc3)ncnc2cc1CN1CCOCC1
Brc1ccc(-c2cc3cc(Br)oc3cn2)nc1

  5 Training on 1449 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 17.276234
Reward: 1.345734
Trajectories with max counts:
41	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.15474615
Proportion of valid SMILES: 0.567313713212273
Sample trajectories:
B[PH](=O)(OC(C)P(=O)(O)OP(=O)(O)O)=C(Br)Br
BrC1=C(Br)CNC(c2ccccc2)=C1
Brc1cc(Nc2ncnc3ccccc23)ccn1
Brc1ccc(-c2cc(Nc3ccccc3)on2)c2ncnc(Nc3ccc(N4CCCCC4)cc3)c12
Brc1ccc(C=C(c2ccccc2)c2cccnc2)cc1
Policy gradient replay...
Mean value of predictions: 0.17603487
Proportion of valid SMILES: 0.5746478873239437
Sample trajectories:
Brc1cc2c(s1)-c1ccccc1N2
Brc1ccc(-c2ccc(Br)s2)o1
Brc1ccc(-c2nccs2)c(NCCCN2CCCCC2)c1
Brc1ccc(Br)cc1
Brc1ccc(C=NNc2nnc(-c3ccoc3)o2)cc1
Fine tuning...
Mean value of predictions: 0.18378672
Proportion of valid SMILES: 0.5750938673341677
Sample trajectories:
BC(=O)c1cc(Br)cc(NP(=O)(O)Oc2ccc(N)c(Br)c2)c1
BP(=O)(OCC1OC(N2C=C(C)C(=O)NC2=O)C(O)C1O)OC(=O)CCCNC(=O)C(C)(Cl)P(=O)(O)O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ncnc(N4CCOCC4)c23)cc1
Brc1cc(Br)c(Br)c(Br)c1

  6 Training on 2638 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 19.049791
Reward: 1.628267
Trajectories with max counts:
26	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.23146474
Proportion of valid SMILES: 0.5200626959247648
Sample trajectories:
BrC(=NNc1cccc(Nc2ncnc3cccnc23)c1)c1cccc(Br)c1
BrC=C(Br)Br
BrC=CC=CC=CC1CCCOC1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3cccc(Br)c23)sc1-c1ccccc1
Policy gradient replay...
Mean value of predictions: 0.1941697
Proportion of valid SMILES: 0.6005001562988433
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)c1ccc(F)cc1
BP(=O)(c1ccc(OC(F)F)c(Br)c1)N(O)C(N)=O
Bc1ccc(Nc2ncnc3ccccc23)cc1
BrC(=NNc1cccc(-c2cccc(Br)c2)c1)N1CCCCC1
Brc1cc(Nc2ncnc3ccccc23)ccc1Oc1ccccc1
Fine tuning...
Mean value of predictions: 0.19569662
Proportion of valid SMILES: 0.5816645807259074
Sample trajectories:
BP(=O)(N(O)CCCCCl)[PH](=O)N(Cl)CCl
Bc1ccc(Nc2ncnc3ccc(Cl)cc23)cc1
BrCCCCCCCCc1ccc2ncnc(-c3ccccc3)c2c1
Brc1ccc(-c2ccc3ccc(-c4cncnc4)c23)cc1
Brc1ccc(-c2ccccc2)c2ccccc12

  7 Training on 3894 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 20.655726
Reward: 2.122431
Trajectories with max counts:
117	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.31182644
Proportion of valid SMILES: 0.4475414970247416
Sample trajectories:
BP(=O)(NO)c1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BP(=O)(Nc1ccc(Nc2ccc(Nc3ccc(Cl)c(Br)c3)cc2)nc1)Oc1ccc(Br)cc1Br
BP(=O)(O)c1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BP(=O)(OCCC)OC(=O)Nc1ccc(I)cc1Br
Bc1ccc(Nc2ncnc3c(Nc4ccc(F)cc4)nc(-c4ccc(Cl)cc4)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.2709642
Proportion of valid SMILES: 0.5678973717146433
Sample trajectories:
BP(=O)(OCCCCC=C)c1ccc(Br)cc1
BrCc1ccc2sc(Nc3ncnc4ccc(Br)cc34)cc2c1
Brc1cc2cnn(Cc3ccccc3)c2cc1Br
Brc1ccc(-c2c(Br)cccc2Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(-c2cc(Nc3ccc(N4CCCCC4)c(Br)c3)ncn2)cc1
Fine tuning...
Mean value of predictions: 0.25574136
Proportion of valid SMILES: 0.5615023474178403
Sample trajectories:
BP(=O)(OCCCC)C(NC(=O)C(Cl)(Cl)Cl)P(=O)(OCOC(=O)C(F)(F)F)C(F)(F)F
BrBr
BrC(=NNc1ccccc1)c1ccc(Br)cc1
BrCC1=Nc2ncnc(Nc3ccc(Br)cc3)c2N1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Nc1cccc(Nc2ccccc2)c1

  8 Training on 5316 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 22.474771
Reward: 2.776866
Trajectories with max counts:
363	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.38626543
Proportion of valid SMILES: 0.4051266020631447
Sample trajectories:
BrCCOc1cc(Nc2ncnc(Oc3cccc(Br)c3)n2)cs1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Nc2ccccc2)cc(-c2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccccc23)c2nccnc2n1
Brc1cc(Nc2ncnc3ccsc23)cnc1-c1cncnc1
Policy gradient replay...
Mean value of predictions: 0.30051163
Proportion of valid SMILES: 0.550547730829421
Sample trajectories:
B[PH](=O)(NO)(n1cnc(N)c1)C(F)(F)F
BrCC(Br)Br
Brc1cc(-c2nc3cccnc3s2)c2ccccc2n1
Brc1cc(Br)c2c(Br)cc(Nc3ncnc4ccc(Br)cc34)nc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccccc4)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.31438127
Proportion of valid SMILES: 0.5613266583229036
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCC)c1ccc(Nc2nc(N)nc(Br)n2)cc1
Bc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Br
BrC=CCCC=CC=CCNCCCCCN=C1CCCCCCNCCCC1
BrCc1cc2ncnc(Nc3ccc(Br)cc3)-c(c(Nc3ccc(Br)cc3)n1)c1c(Br)cccc21

  9 Training on 6867 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 26.411581
Reward: 3.726011
Trajectories with max counts:
247	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.3958042
Proportion of valid SMILES: 0.35761175367302284
Sample trajectories:
BP(=O)(NC(CO)S(=O)(=O)Oc1cccc(N)c1)P(=O)(OC(C)(C)C)OC(C)(C)C
BP(=O)(OCC(=O)Nc1ccc(Br)cc1)C(F)(F)F
BP(=O)(OCC(F)(F)F)N(C)C=O
BP(=O)(OCC)OCC(=O)C(F)(F)F
BP(=O)(OCC)ON=C(C#N)NS(=O)(=O)N1CCCC(CC(=O)Nc2nc3cc(Br)cc(Br)c3s2)C1
Policy gradient replay...
Mean value of predictions: 0.34471264
Proportion of valid SMILES: 0.5451127819548872
Sample trajectories:
BP(=O)(NC(=O)C=C(F)F)c1ccc(F)cc1
Bc1ccc(Nc2ccc(Nc3ncnc4cc(Br)ccc34)cc2)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1-c1ccccc1
BrCCCC=CC=CC=CCC=CC=Nc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.34936562
Proportion of valid SMILES: 0.5425531914893617
Sample trajectories:
BrC(=Nc1ccc(Nc2ncnc3ccccc23)cc1)c1cccc(Br)c1
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1ccccc1
Brc1ccc(-c2cc(Nc3ncnc4cc(Br)ccc34)ccc2Br)cc1

 10 Training on 8288 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 24.964223
Reward: 3.352347
Trajectories with max counts:
138	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4074442
Proportion of valid SMILES: 0.5042227087894902
Sample trajectories:
BP(=O)(C#C)c1ccc(Nc2cccc(Br)c2)cc1
BP(=O)(OCC)C(=O)Nc1cccc(Br)c1
BrCCc1cc(-c2ccsc2)c2ccccc2n1
BrSc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Br)s1
Policy gradient replay...
Mean value of predictions: 0.4472469
Proportion of valid SMILES: 0.527977492966552
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCC(Br)Br
BrCCNc1cc(Nc2ncnc3ccc(Br)cc23)c2ccccc2c1
BrCCNc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.44051826
Proportion of valid SMILES: 0.5307908721475461
Sample trajectories:
Bc1ccc(Br)cc1-c1cccs1
BrC(=NNc1ccc(Br)cc1)C1CCCCCCC1
Brc1cc(Br)c(Oc2ccc3ccsc3c2)c(Nc2ncnc3cccc(Br)c23)c1
Brc1cc(Nc2ncnc3cc(Br)ccc23)c2ncncc2c1
Brc1cc2c(ncnc1-c1cccnc1)-c1ccccc1N2

 11 Training on 9882 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.536644
Reward: 3.960001
Trajectories with max counts:
504	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.42477396
Proportion of valid SMILES: 0.345625
Sample trajectories:
BP(=O)(NP(=O)(O)OP(=O)(O)O)N(O)CP(=O)(O)CF
BP(=O)(NS(=O)(=O)c1ccccc1)OCC1OC(c2ccccc2)C(N2C=C(F)C(=O)NS2(=O)=O)C(O)C1O
BP(=O)(Nc1cccc(Br)c1)P(=O)(OCC)Oc1ccc(Br)cc1
BP(=O)(OCCCCC)Oc1cccc(Nc2ncnc3ccccc23)c1
BP(=O)(Oc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1)C(F)(F)F
Policy gradient replay...
Mean value of predictions: 0.3416309
Proportion of valid SMILES: 0.5825
Sample trajectories:
Bc1ccc(I)cc1
Brc1ccc(Br)c(-c2ccc(Nc3ncnc4ccccc34)cc2)c1
Brc1ccc(Br)c(-c2cccc(Nc3ncnc4c(Br)ccc(Br)c4s3)c2)c1
Brc1ccc(C=NNc2ccc(Br)cc2)cc1
Brc1ccc(NN=Cc2ccccc2Br)cc1
Fine tuning...
Mean value of predictions: 0.35382935
Proportion of valid SMILES: 0.5716072545340838
Sample trajectories:
BP(=O)(Nc1ccccc1Br)NP(=O)(O)O
BP(=O)(OCC)OCC=CC=C
BP(=O)(Oc1ccccc1Nc1ccc(N)cc1)P(=O)(Oc1ccccc1)Oc1ccc(F)cc1
BrC=C(Br)Br
BrCCNc1ccccc1

 12 Training on 11218 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.053985
Reward: 4.181035
Trajectories with max counts:
233	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.52758336
Proportion of valid SMILES: 0.3840625
Sample trajectories:
BP(=O)(NOC(C(=O)O)N(=O)=O)Nc1ccc(N)cc1
BP1(=O)OCC(CCCCCS(=O)(=O)N(C)C)Cc2cc(Br)ccc2S(=O)(=O)NC1=O
BrC(=NNc1ccccc1)Nc1cccc(Nc2ncnc3ccsc23)c1
Brc1cc(Nc2ncnc3ccsc23)ccn1
Brc1cc2ccccc2cc1Br
Policy gradient replay...
Mean value of predictions: 0.49075428
Proportion of valid SMILES: 0.5142320925868001
Sample trajectories:
BP(=O)(OCC)c1ccc(Nc2ccc(Br)cc2)cc1Br
BrCCN1CCCCCCCCCCCCCCCCCCCCCCCCCCCC1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Nc4ccccc4Br)cc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.49071914
Proportion of valid SMILES: 0.5085964363863708
Sample trajectories:
BP(=O)(CCC(N)=O)NS(=O)(=O)c1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BP(=O)(OCC)n1ccc(Nc2ncnc3cc(Br)cc(Br)c23)n1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrC(=NNc1ccc(Br)cc1)c1ccccc1
BrC(=NNc1ccccc1)c1ccc(Br)cn1

 13 Training on 12979 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.162115
Reward: 4.330141
Trajectories with max counts:
253	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5046177
Proportion of valid SMILES: 0.4128125
Sample trajectories:
BP(=O)(OCC)c1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1
Brc1cc(Nc2ncnc3ccsc23)oc1CCCc1ccccc1
Brc1ccc(-c2ccc(Nc3nc4ccccc4s3)cc2)s1
Brc1ccc(-c2nc3ccccc3nc2Nc2ccccc2Br)c(Br)c1
Brc1ccc(Br)c(-c2ccc(Br)cc2-c2cccc(Nc3ncnc4ccsc34)c2)c1
Policy gradient replay...
Mean value of predictions: 0.49182227
Proportion of valid SMILES: 0.485464207564864
Sample trajectories:
BP(=O)(Cc1ccccc1)Nc1ccc(Br)c(Nc2ccc(Br)cc2)c1
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Br
BrCc1ccc(Nc2ccc(Nc3ncnc4ccccc34)cc2)cc1
Fine tuning...
Mean value of predictions: 0.49579287
Proportion of valid SMILES: 0.4828125
Sample trajectories:
BP(=O)(N=C(Nc1ccc(Br)cc1)P(=O)(O)O)OCC
BP(=O)(NNc1cc(Br)ccc1Br)OCC=C
BP(=O)(OCC)Oc1ccc(Br)cc1Nc1ccc(Br)o1
BP(=O)(OCC)c1cc(Br)c(Br)cc1Br
Bc1ccc(Br)cc1-c1ccc(Br)cc1

 14 Training on 14721 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.624797
Reward: 4.850725
Trajectories with max counts:
460	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5598911
Proportion of valid SMILES: 0.344375
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(Nc1ccc(Nc2cccc(Br)c2)nc1)c1ccc(Br)cc1
BP(=O)(OCC)Oc1cc(Br)cc(Nc2c(Br)cnc3cc(Br)ccc23)c1
BP(=O)(OCc1ccc(Br)cc1)Oc1ccc(Nc2ccc(Br)cc2)cc1
BP(=O)(Oc1ccc(Br)cc1)c1cccc(Br)c1
Policy gradient replay...
Mean value of predictions: 0.53202146
Proportion of valid SMILES: 0.5245542696277761
Sample trajectories:
BP(=O)(CCNC(=O)c1ccc(N)c(Nc2ncnc3cc(Br)cc(Br)c23)n1)OCC
BP(=O)(OCC)N(N=O)c1cc(Br)c(Nc2nc(Cl)cc(Br)c2O)nc1N
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Nc2ccc(Nc3nccnc3-c3cccs3)cc2)cc1Br
Fine tuning...
Mean value of predictions: 0.5516108
Proportion of valid SMILES: 0.49561678146524735
Sample trajectories:
BrBr
BrCc1ccc(Nc2ncnc3ncnc(-c4ccccc4)c23)cc1
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4ccsc34)cc2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2cnc3ccc(Br)nc3n2)c1

 15 Training on 16642 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.274699
Reward: 5.412598
Trajectories with max counts:
837	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5588719
Proportion of valid SMILES: 0.2659375
Sample trajectories:
BP(=O)(N(CC#C)c1ccc(Br)cc1)P(=O)(O)O
BP(=O)(N(O)N(=O)=O)N(=O)=O
BP(=O)(NC(=S)Nc1ccc(Br)cc1)c1cccc(Br)c1
BP(=O)(OCC)OC(=O)C(=O)c1ccc(Br)cc1
BP(=O)(ONC(=O)c1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1)P(=O)(O)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.5451991
Proportion of valid SMILES: 0.5342508601814201
Sample trajectories:
BP(=O)(COc1ccc(Br)cc1)Nc1ccc(Br)cc1
BP(=O)(N(=O)=O)N(=O)=O
Bc1ccc2c(Nc3ccc(Br)cc3)oc(Br)c2c1
BrCC(Br)(Br)Br
BrCc1ccc(Nc2ncnc3cc(Br)sc23)cc1
Fine tuning...
Mean value of predictions: 0.5779977
Proportion of valid SMILES: 0.5370428258830885
Sample trajectories:
BP(=O)(OCCCCCC)c1ccc(N)c(Br)c1
BrC=CC=CC=CC=CC=CCC=CCC=CC=NNc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c2c(NCCN3CCN(CCc4ccccc4)CC3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1

 16 Training on 18614 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.271216
Reward: 5.240570
Trajectories with max counts:
77	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5686436
Proportion of valid SMILES: 0.5323538605814317
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3sccc23)c1
Brc1cc(Nc2cnc3ccsc3c2)cc(Br)c1Br
Brc1cc2ccccc2nc1Nc1ccccc1Nc1ccc(Nc2cccc3ccccc23)cc1
Brc1ccc(Br)c(Nc2ccc(Nc3ncnc4ccc(Br)cc34)cc2)c1
Brc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.5383529
Proportion of valid SMILES: 0.5315822388993121
Sample trajectories:
BP(=O)(OCC)C(=O)O
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(I)c(Br)c(Nc2cccc3nc(Br)ccc23)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1ccc(-c2ccccn2)c(Br)c1
Fine tuning...
Mean value of predictions: 0.5489162
Proportion of valid SMILES: 0.5337711069418386
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Br)cc1
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4cc(Br)ccc34)cc2)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3ccsc23)o1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1

 17 Training on 20931 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.168986
Reward: 5.368397
Trajectories with max counts:
269	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.62202644
Proportion of valid SMILES: 0.42602439787300594
Sample trajectories:
BP(=O)(Oc1ccc(Nc2ncnc3c(Br)cc(Br)cc23)cc1)c1ccc(Br)cc1
Br
BrC(=NNc1ccc(Br)cc1)Nc1cc(Br)ccc1Br
BrC(=NNc1nc2ccc(Br)cc2s1)c1ccccc1
BrSc1cccc(Nc2cc3ccccc3s2)c1
Policy gradient replay...
Mean value of predictions: 0.56854945
Proportion of valid SMILES: 0.4870271959987496
Sample trajectories:
Bc1cc(Br)cc(Br)c1Nc1cc(Nc2ncn[nH]2)c2ccccc2n1
Bc1cc(Br)cc(Br)c1Nc1ccc(Nc2ncnc3sc(Br)cc23)cc1
BrC=CC=CBr
BrCN1CCCCCCCCC1CNc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc(Br)c(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.58675677
Proportion of valid SMILES: 0.4625
Sample trajectories:
BP(=O)(N1CCC(F)(F)CC1)C(F)(F)F
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(OCC)C(=O)Nc1cc(Br)cc(N)c1-c1ccc(Br)cc1
BP(=O)(OCC)C(F)(F)F
B[PH](=O)(=O)C(Br)Br

 18 Training on 23042 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.285092
Reward: 5.335863
Trajectories with max counts:
275	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6348778
Proportion of valid SMILES: 0.371285580231467
Sample trajectories:
BP(=O)(Nc1cc(Br)c(Br)c(Br)c1)P(=O)(Oc1ccc(F)cc1F)C(=O)Nc1c(F)cc(F)c(F)c1F
BP(=O)(OC(=O)c1ccc(Br)c(Br)c1)c1ccc(Br)cc1
BP(=O)(OCC)Oc1ccc(Nc2ncnc3c4ccccc4c23)cn1
BP(=O)(O[SH](=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O)N(O)P(=O)(O)OP(=O)(O)OP(=O)(O)O
Bc1c(Br)cc(Br)cc1Br
Policy gradient replay...
Mean value of predictions: 0.57030237
Proportion of valid SMILES: 0.5796557120500783
Sample trajectories:
BP(=O)(OCC)OC(=O)C(CCCCCCCC)NC(=O)C(N)CCCCCC#N
BrCc1nc2c(Nc3cc(Br)c4ccccc4n3)ncnc2s1
Brc1[nH]ccc1-c1c(-c2ccncc2)cccc2ncnc12
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Nc2ncnc3c(Br)ccc(Br)c23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.5723358
Proportion of valid SMILES: 0.5809881175734835
Sample trajectories:
BP(=O)(OCCCC)OC(=O)CCCCCCn1cnc2c(N)ncnc21
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(-c4c(Br)cc(Br)c(Br)c4Br)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccccc4)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1

 19 Training on 25404 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.822047
Reward: 5.750754
Trajectories with max counts:
214	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6468056
Proportion of valid SMILES: 0.45
Sample trajectories:
BP(=O)(N=C(N)Nc1ccc(Br)cn1)OCC
BP(=O)(OCC)c1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Bc1ccc(Br)c(Br)c1Br
Bc1ccc(Nc2ncnc3cccc(Br)c23)cc1Cl
Bc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.57247823
Proportion of valid SMILES: 0.5023459493274945
Sample trajectories:
BP(=O)(O)c1cccc(Br)c1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1c(Nc2ncnc3ccsc23)sc(-c2ccccc2)c1Br
Brc1cc(Br)c(Nc2ncnc3cccnc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.5890192
Proportion of valid SMILES: 0.5068792995622264
Sample trajectories:
BP(=O)(Nc1ccc(Nc2ncnc3cc(Br)ccc23)cc1)C(=O)OCC
BrC=CBr
BrCC(=NNc1ccccc1)c1cccc(Nc2ccccc2Br)n1
BrCCNc1ccc(Nc2ncnc3ccsc23)cc1
BrCNC=Nc1cccc(Nc2ncnc3cccc(Br)c23)c1

 20 Training on 27758 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.234275
Reward: 5.927921
Trajectories with max counts:
221	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.61715734
Proportion of valid SMILES: 0.3715625
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1cccc(Nc2ncnc3ccsc23)c1
BrC=NNc1ccc(Nc2ncnc3sccc23)cc1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccsc12
Brc1cc(Br)c2ncnc(Nc3cccc(Nc4ccccc4Br)c3)c2c1
Policy gradient replay...
Mean value of predictions: 0.61439735
Proportion of valid SMILES: 0.5601750547045952
Sample trajectories:
BrCc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3cccc(-c4ccccc4Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Br)nc(Nc2ncnc3ccsc23)c1
Fine tuning...
Mean value of predictions: 0.6063509
Proportion of valid SMILES: 0.5609375
Sample trajectories:
B[PH](=O)(Nc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O
BrC(Br)=NNc1ccc(Br)cc1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2c(Br)scc12
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccccc4)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1

Trajectories with max counts:
512	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5634377
Proportion of valid SMILES: 0.4635107247826903
Mean Internal Similarity: 0.47825750138520046
Std Internal Similarity: 0.1129284008485821
Mean External Similarity: 0.4021034035296136
Std External Similarity: 0.06556680423232387
Mean MolWt: 402.2382987208428
Std MolWt: 99.02904468764923
Effect MolWt: -0.9252471224938784
Mean MolLogP: 4.801637296212693
Std MolLogP: 1.6421907786835208
Effect MolLogP: 0.055958626704130696
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.384937% (931 / 956)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 0, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5477.997753381729, 'valid_fraction': 0.4635107247826903, 'active_fraction': 0.5379114948731786, 'max_counts': 512, 'mean_internal_similarity': 0.47825750138520046, 'std_internal_similarity': 0.1129284008485821, 'mean_external_similarity': 0.4021034035296136, 'std_external_similarity': 0.06556680423232387, 'mean_MolWt': 402.2382987208428, 'std_MolWt': 99.02904468764923, 'effect_MolWt': -0.9252471224938784, 'mean_MolLogP': 4.801637296212693, 'std_MolLogP': 1.6421907786835208, 'effect_MolLogP': 0.055958626704130696, 'generated_scaffolds': 956, 'novel_scaffolds': 931, 'novel_fraction': 0.9738493723849372, 'save_path': '../logs/replay_ratio_s1-3.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.990320
Reward: 1.000000
Mean value of predictions: 0.0011400653
Proportion of valid SMILES: 0.7701473816243336
Sample trajectories:
Brc1cc(OCCn2ncc3ccccc32)on1
Brc1ccc(-n2c(-c3cccnc3)nc3ccccc32)cc1
C#CC1(O)CN(C)C(=O)c2c(cnc3cc(Cl)ccc23)C#Cc2nc(N)n3c2ncn3C1
C#CCCCCCCCOP(=O)(C(=O)Nc1ccc(N(=O)=O)cc1)C(N)CCl
C#CCCN(C)P(=O)(OCC)c1ccc(Cl)cc1
Policy gradient replay...
Mean value of predictions: 0.0026343518
Proportion of valid SMILES: 0.599873577749684
Sample trajectories:
BrCC(Br)=CCn1cnc(Br)c1
Brc1ccc2nc(-c3cc(Br)c(OCc4nn[nH]n4)nn3)[nH]c2c1
Brc1cccc(-c2nc(-c3noc(C4CCNC4)n3)no2)c1
Brc1cncc(Br)c1
C#CC#CCCCCCN1CCCCC1=O
Fine tuning...
Mean value of predictions: 0.0009538951
Proportion of valid SMILES: 0.5967741935483871
Sample trajectories:
Brc1cc(C2CCCOCC2)on1
Brc1ccc(NN=Cc2ccncn2)cc1
Brc1nn(CC=CI)c2cc3ccccc3n12
C#CC#CCCCCS(=O)(=O)c1cc(F)c(F)cc1CC(CS(C)(=O)=O)c1nc(N)c(C(O)(C(=O)NC(CCCNC(=N)N)C(N)=O)C(F)(F)F)s1
C#CCC(=O)C(CC(=O)O)NC(=O)N1N=CC(C)=CC1=O

  2 Training on 236 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.782168
Reward: 1.000000
Trajectories with max counts:
2	Cc1n[nH]c(N)n1
2	Cc1nc(C)n(C)n1
Mean value of predictions: 0.0018309101
Proportion of valid SMILES: 0.5869152970922883
Sample trajectories:
BP(=O)(OCC1C=CC(CP(=O)(O)O)O1)n1cnc2c(N)nc(I)nc21
BrCC1CCCCC1=NCc1cnc2c(n1)CCCN2
Brc1ccc2[nH]c3ncnn3CCCCc2c1
Brc1nc(-c2ccsc2)nc(-c2ccc[nH]2)n1
C#CC=CC(=O)NCc1cc(Cl)c(C(C)=O)c(C#N)c1O
Policy gradient replay...
Mean value of predictions: 0.018457033
Proportion of valid SMILES: 0.641202254226675
Sample trajectories:
Brc1ccc(Nc2cn3nc(-c4cccs4)nc3cn2)cc1
Brc1ccc(Nc2nncn3ccnc23)cc1
Brc1ccc2c(c1)N2C1CCC(Nc2ccccn2)C1
Brc1cccc(Nc2ncnc3sc4ccccc4c23)c1
C#CC(N)(CCCC)C(=O)Nc1cccc(CN(C)C)c1
Fine tuning...
Mean value of predictions: 0.017609445
Proportion of valid SMILES: 0.6367052928280614
Sample trajectories:
BP(=O)(OCC)C(=O)C=C1c2cccn2-c2ccccc2NC1C(=O)OP(=O)(O)OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O
BrC1=NC(=NNc2ccc(Nc3ncnc4ccncc34)cc2)N1
Brc1ccc(CN2CCN(Cc3ncn[nH]3)CC2)cc1
Brc1ccc(Nc2nc(-c3ccc(Br)o3)no2)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1

  3 Training on 360 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 20.090283
Reward: 1.141184
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.018488973
Proportion of valid SMILES: 0.6669796557120501
Sample trajectories:
BrC1(c2cc(-c3ccccc3)c(-c3cncnc3)c3cncnc23)CCCCN1
BrC1(c2ccccn2)Oc2ccccc2-c2ccc(-c3csnn3)cc21
Brc1ccc(-c2nc(-c3ccncn3)ccc2-c2nc3ccccc3[nH]2)cc1
Brc1ccc(-c2nc(CCN3CCN(Cc4ncco4)CC3)co2)cc1
Brc1ccc(Br)c(-c2cnc3cccnc3c2)c1
Policy gradient replay...
Mean value of predictions: 0.024484182
Proportion of valid SMILES: 0.682415519399249
Sample trajectories:
Brc1cc(Br)c(Nc2nccc3nn(-c4ccccc4)cc23)cc1Br
Brc1ccc(-c2[nH]c(-c3cc4ccccc4cn3)cc2-c2ccccc2Br)cc1
Brc1ccc(-c2ccc3c(N4CCCCCC4)c(-c4ccccc4)c3n2)cn1
Brc1ccc(-c2ccc3ccccc3c2)cn1
Brc1ccc(Nc2ccc3ncncc3c2)cc1
Fine tuning...
Mean value of predictions: 0.023005566
Proportion of valid SMILES: 0.6748043818466354
Sample trajectories:
Brc1cc2c(s1)N(c1cccc3ccccc13)c1ccccc12
Brc1cc2cc(Nc3nc4ccccc4s3)ccc2cn1
Brc1ccc(-c2cnc(NCc3ccccc3)c3ccccc23)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(N(Nc2cnccn2)c2csc(-c3cccnc3)n2)c1

  4 Training on 570 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.966966
Reward: 1.100434
Trajectories with max counts:
13	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.03190244
Proportion of valid SMILES: 0.6418284283030683
Sample trajectories:
Brc1cc(Nc2ccccc2)nc(-c2ccccc2Br)n1
Brc1ccc(-c2ccc(N=Cc3cccnc3)cc2)cc1
Brc1ccc(Br)c(-c2nc(-c3ccc(Nc4ccncc4)cc3)no2)c1
Brc1ccc(Br)c(Nc2ccc3c(n2)-c2ccccc2N3)c1
Brc1ccc(Br)c(SC2(N3CCCCC3)CCCC2)c1
Policy gradient replay...
Mean value of predictions: 0.029520605
Proportion of valid SMILES: 0.7440550688360451
Sample trajectories:
BrCCCCCSc1nc2c(s1)Oc1ccccc1-2
Brc1ccc(-c2ccc[nH]2)cc1
Brc1ccc(Nc2cc3cccnc3nc2-n2cccn2)cc1
Brc1ccc(Nc2cnc(Nc3cccnc3)c3ccccc23)cc1
Brc1ccc(Nc2ncccn2)c(Br)c1
Fine tuning...
Mean value of predictions: 0.027537437
Proportion of valid SMILES: 0.7514848390121913
Sample trajectories:
Brc1ccc(CCNc2ncnc3c4ccccc4c23)nc1
Brc1ccc(Nc2cccc(Br)c2)cc1
Brc1ccc(Nc2nccc(-c3ccncc3)n2)cc1
Brc1ccc(Nc2ncnc(-c3cccs3)n2)cc1
Brc1ccc2[nH]c(-c3cc4ccccc4o3)nc2c1

  5 Training on 878 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 20.692746
Reward: 1.373650
Trajectories with max counts:
14	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.055037312
Proportion of valid SMILES: 0.6710485133020344
Sample trajectories:
Brc1cc(Nc2ccccc2N2CCCCC2)ccc1Nc1ccc2ncnc(Nc3ccc(Nc4ccccc4)cc3)c2c1
Brc1ccc(Br)cc1
Brc1ccc(C=NNc2cccnc2)o1
Brc1ccc(NN=Cc2ccccn2)cc1
Brc1ccc(Nc2c3c(nc4ccccc24)CCC3)cc1
Policy gradient replay...
Mean value of predictions: 0.038872525
Proportion of valid SMILES: 0.7428125
Sample trajectories:
Brc1ccc(Nc2ccccc2Nc2ncnc3c4ccncc4c23)cc1
Brc1ccc(Nc2cccnc2)cc1
Brc1ccc(Nc2nc3ccccc3[nH]2)cc1Br
Brc1ccc(Nc2ncnc3ccccc23)c2ccccc12
Brc1ccc(Nc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.031509664
Proportion of valid SMILES: 0.7606382978723404
Sample trajectories:
BP(=O)(OCC)OC(=O)C=CC(=O)N1CCCCCCCCC1
BP(=O)(OCCOCCOCCc1ccccc1)P(=O)(OC)OCC
Bc1ccccc1Nc1nccc2cccnc12
BrC1(c2ccccc2)CCCCc2ccccc21
Brc1cc2ccccc2cc1-c1ccc2ccccc2c1

  6 Training on 1310 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.396847
Reward: 1.385487
Trajectories with max counts:
36	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.06997027
Proportion of valid SMILES: 0.6308221319162238
Sample trajectories:
Brc1cc2cnccn2c1-c1ccncc1
Brc1ccc(Nc2ccnc3sc(Nc4ccccc4)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ncnc(-c4c(Br)ccc5cccnc45)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.10121654
Proportion of valid SMILES: 0.5148762918885061
Sample trajectories:
Brc1cc(Br)c(-c2csc3c(Nc4ccccc4)ncnc23)c(Br)c1
Brc1cc2c(Nc3ccccc3)ncnc2cc1N1CCCCC1
Brc1ccc(-c2cc(-c3cncnc3)c3cccc(Nc4ccc(Oc5ccccc5)cc4)c3c2)cc1
Brc1ccc(-c2cnc3cccnc3c2-c2cccnc2)cc1
Brc1ccc(-c2ncnc3c2cnn3-c2ccccn2)c(Br)c1
Fine tuning...
Mean value of predictions: 0.10077243
Proportion of valid SMILES: 0.5269254852849092
Sample trajectories:
Brc1cc(-c2c(Br)cccc2-c2cc3cncn3c(Nc3cncnc3)n2)c2ccccc2n1
Brc1cc(-c2ccc3[nH]nc(-c4ccccc4)c3c2)ncn1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)cc(Nc4cc(Nc5ccccc5)ncn4)c23)c1
Brc1ccc(-c2nc(-c3ccccn3)c3c(Nc4ccc(Br)s4)ccnc3n2)cc1
Brc1ccc(C=NNc2ncc(Br)c(Br)n2)c(Br)c1

  7 Training on 2023 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 18.299630
Reward: 1.715180
Trajectories with max counts:
342	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.11302049
Proportion of valid SMILES: 0.4729603000937793
Sample trajectories:
BrC1=C(c2ccncc2)N2C(=Nc3ccccc32)SC1
BrC=CBr
Brc1cc(-c2cccnc2)c2ccccc2c1-c1nc2ccccc2[nH]1
Brc1cc(Nc2ccncc2)ccc1Nc1ccccc1
Brc1cc2ncnc(Nc3ccc4c(c3)OCO4)c2cc1N=Nc1ccccc1
Policy gradient replay...
Mean value of predictions: 0.07836625
Proportion of valid SMILES: 0.69625
Sample trajectories:
BP(=O)(OCC1OC(CO)C(O)C1O)c1ccccc1
Br
BrC(=NN=C1Nc2cccnc21)c1ccc2ccccc2c1
BrC=CBr
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCN(c2ccccn2)CC1
Fine tuning...
Mean value of predictions: 0.08423444
Proportion of valid SMILES: 0.6882426516572858
Sample trajectories:
BP(=O)(CCl)N[PH](=S)SCCCC(=O)NP(=O)(OCOC(=O)C(=O)CCCl)P(=O)(OCC)OCOCC
BP(=O)(OCC)C1CC(C=C)c2ccc(O)cc2N1
BP(=O)(c1ccc(Cl)c(O)c1)N(CCCl)P(=O)(OCC)OP(=O)(O)OP(=O)(O)Nc1cccc(Cl)c1
BrC#Cc1ncnc2c1ccn2-c1ccccc1
BrC(=Cc1ccc(Br)cc1)c1ccccc1Br

  8 Training on 2797 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 19.909913
Reward: 2.288918
Trajectories with max counts:
479	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.14312541
Proportion of valid SMILES: 0.4660831509846827
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)n1cnc2c(N)ncnc21
Brc1cc(Nc2ncnc3ccccc23)nc2ccccc12
Brc1ccc(-c2ccccc2)cc1Nc1ncnc2ccccc12
Brc1ccc(Nc2c[nH]nc2-c2cncn2-c2cccnc2)cc1
Brc1ccc(Nc2cc(-c3ccccc3)ncn2)cc1
Policy gradient replay...
Mean value of predictions: 0.093832366
Proportion of valid SMILES: 0.5935544430538173
Sample trajectories:
BP(=O)(OCCCC)C(Br)=C(Br)Br
BrCCCCN1CCc2c(Br)cccc21
Brc1cc(Br)c(Nc2ncnc3nc[nH]c23)cc1CN1CCCCC1
Brc1cc2c(Nc3ccccc3)ncnc2cc1-c1ccc2c(c1)Nc1ccccc1O2
Brc1ccc(Nc2ccc3ncnc(Nc4ccccc4Br)c3c2)cc1
Fine tuning...
Mean value of predictions: 0.10332806
Proportion of valid SMILES: 0.5919324577861164
Sample trajectories:
BP(=O)(OCCCCCCCCF)OC(=O)CN
BP(=O)(c1ccc(Nc2cc(Cl)cc(Nc3cncnc3)n2)cc1)C(F)(F)F
BrC1=C(Br)c2ccccc2N(Nc2ccccc2)c2nc3ccccc3n2CC1
BrCCCCCCCCCCCC1(c2ccccc2)CCCCCCCCCCCCCCCCCN1
Brc1cc(Nc2ccccc2)sc1Nc1ncc2ccc3ccccc3c2n1

  9 Training on 3662 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 18.163879
Reward: 2.148294
Trajectories with max counts:
268	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.14603566
Proportion of valid SMILES: 0.5084375
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)P(=O)(O)O
BrCN1CCN(CCNc2ccc3[nH]ccc3c2)CC1
Brc1cc(Nc2ncc3ccccc3n2)ccc1-c1ccncc1
Brc1cc2c(c3ccccc13)CCc1ccccc1N2
Brc1ccc(-c2ccccc2)c2cccnc12
Policy gradient replay...
Mean value of predictions: 0.12242131
Proportion of valid SMILES: 0.6455142231947484
Sample trajectories:
BP(=O)(CCl)Nc1cccc(Nc2ncnc3c([PH](C)(=O)=O)cccc23)c1
BP(=O)(OCCCCCCCC)C(=O)Oc1ccccc1
Brc1ccc(-c2ccc3ncncn23)cc1
Brc1ccc(-c2nc(-c3ccc(Br)c(-c4ccccc4)c3)c(-c3ccc(Br)cc3)o2)cc1
Brc1ccc(-c2nc[nH]c2Br)cc1
Fine tuning...
Mean value of predictions: 0.1307883
Proportion of valid SMILES: 0.6303125
Sample trajectories:
BP(=O)(C(=O)NO)N(O)C(N)=O
BP(=O)(OCCCCCCCCCCCCCCBr)P(=O)(O)O
BrSc1ccccc1-c1ccccc1
Brc1ccc(-c2ccccc2)cc1Nc1ccccc1Nc1ccc2[nH]cnc2c1
Brc1ccc(-c2cccnc2Nc2ccc(-c3ccccc3Br)cc2)cc1

 10 Training on 4520 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 21.191371
Reward: 2.765955
Trajectories with max counts:
68	Fc1ccc(Nc2ncnc3ccc(F)cc23)cc1
Mean value of predictions: 0.3244813
Proportion of valid SMILES: 0.45300751879699247
Sample trajectories:
BP(=O)(Br)OCCCCCBr
BP(=O)(Br)OP(=O)(O)OP(=O)(O)OP(=O)(Oc1cccc(Nc2cc(Br)cc(Br)c2F)c1)C(F)F
BP(=O)(C(=O)NBr)N1CCCCCCC1
BP(=O)(Nc1ccc(F)c2c1OC(c1ccnc(Nc3cc(F)cc(F)c3)c1)=CC2=O)N(=O)=O
BP(=O)(O)C(Nc1ccc(Nc2ccc(F)cc2)cc1)P(=O)(O)O
Policy gradient replay...
Mean value of predictions: 0.047030877
Proportion of valid SMILES: 0.6578125
Sample trajectories:
BP(=O)(Nc1ccccc1)c1ccccc1
Bc1ccccc1-c1ccccc1-c1ccccc1Nc1ccccc1
BrCBr
Brc1cc(-c2ccccc2)c2ccccc2n1
Brc1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.043361988
Proportion of valid SMILES: 0.654375
Sample trajectories:
BP(=O)(OCC1CCC=C(F)CC1F)C(F)(F)F
BP(=O)(OCCCCC)c1ccccc1Nc1ccccc1
BP1(=O)CCC=CCC(CCCCl)(CCCCCCCOP(=O)(O)OCCCl)CO1
Bc1ccccc1-c1ccccc1Nc1ccccc1
BrC1=CCC=C1Nc1ccccc1

 11 Training on 5263 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 21.007299
Reward: 2.826966
Trajectories with max counts:
96	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.29134786
Proportion of valid SMILES: 0.5309375
Sample trajectories:
BP(=O)(NCCCCCCCC(F)F)C(F)(F)F
BP(=O)(Nc1ccc(Br)cc1)P(=O)(O)O
BP(=O)(O)Oc1ccc(Br)cc1Sc1ccccc1Br
BP(=O)(O)P(=O)(O)O
BP(=O)(OCC)C(F)(F)P(=O)(O)CCCCCCCCCCCl
Policy gradient replay...
Mean value of predictions: 0.27371636
Proportion of valid SMILES: 0.5118898623279099
Sample trajectories:
BP(=O)(Nc1ccc(Br)c2c(Br)cnc(Nc3cc(Br)cc(I)c3F)c12)c1cccc(F)c1F
Brc1cc(Br)c2c(c1)C=Nc1cccc(I)c1-2
Brc1cc(Br)c2ncnc(Nc3cccc(CN4CCOCC4)n3)c2c1
Brc1cc2c(Br)cc(I)c(Nc3ccc4ccccc4c3)c2nc1-c1ccsc1
Brc1cc2n(oc1-c1ccc3ncnc(Nc4ccc(Oc5ccccc5)cc4)c3c1)-c1ccccc1N2
Fine tuning...
Mean value of predictions: 0.2520631
Proportion of valid SMILES: 0.5156445556946183
Sample trajectories:
B[PH](=O)(Nc1ccc(Cl)c(F)c1)(c1cccc(Cl)c1)c1ccc(Br)c(N)c1
B[PH](N)(=O)=O
BrC=C1Oc2ccc3ccccc3c2C=C1c1nsc(Nc2ccccc2)n1
BrC=C=CCON=CC=Cc1c(Br)cccc1Br
Brc1cc(-c2nnc(-c3ccccc3)n2-c2ccccc2)c2ccccc2n1

 12 Training on 6754 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 21.411882
Reward: 3.122384
Trajectories with max counts:
404	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.31229
Proportion of valid SMILES: 0.3535479837449203
Sample trajectories:
BP(=O)(NC(=O)c1ccc(Br)cc1)c1ccc2c(Nc3cccc(Br)c3)cccc2c1
BP(=O)(NOCC)C(=O)N(O)c1ccc(Br)cc1
BP(=O)(Nc1ccccc1)P(OP(=O)(O)O)P(=O)(O)O
BP(=O)(c1cccc(Br)c1)N1CCCCC1
B[PH](=O)(COP(=O)(O)O)(NCc1ccc(F)cc1)C(F)(F)F
Policy gradient replay...
Mean value of predictions: 0.28541178
Proportion of valid SMILES: 0.53125
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrC(Nc1ncnc2c1c1ccccc1c1ccccc1c1ccccc1n2-c1ccccc1)c1ccccc1
BrCCCCCCCCCCCCCCCCCN=C1Nc2ncncc2S1
BrSc1ccccc1Nc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc(Nc2ccnc3c(Br)cnnc23)ccc1-c1ccccn1
Fine tuning...
Mean value of predictions: 0.2755294
Proportion of valid SMILES: 0.53125
Sample trajectories:
BC(=O)Nc1ccccc1-c1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BP(=O)(Nc1cc(Br)c(Br)cc1Br)c1cccc(Br)c1
BP(=O)(Nc1ccc(Br)cc1)Oc1ccc(Nc2ccccc2)cc1
BP(=O)(OCC1CCC(O)C1O)n1cnc2c(N)ncnc21
Bc1ccc(Nc2ncnc3c4ccccc4c23)cc1

 13 Training on 8137 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 24.706879
Reward: 3.772398
Trajectories with max counts:
180	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.41272265
Proportion of valid SMILES: 0.3684375
Sample trajectories:
BP(=O)(NCC)c1ccc(Nc2ncnc3ccccc23)cc1
BP(=O)(OCC1Oc2c(Br)cc(Br)cc2N(Cc2c(Br)cc(Br)cc2C(=O)O)C(O)C(O)C1O)c1cccc(Br)c1
B[PH](=O)(=Nc1cc(Br)ccc1O)Nc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccccc23)cc1
BrC(=NNc1cccc(Nc2ncnc3sccc23)c1)c1ccccc1
Policy gradient replay...
Mean value of predictions: 0.39508772
Proportion of valid SMILES: 0.4455909943714822
Sample trajectories:
BP(=O)(OCC(N)=O)n1cnc2c(Nc3ccc(Br)cc3)nc(Cl)nc21
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)N1C=CC(N)=NC1=O
BrCC=C(Br)Br
BrCCNc1cc(Nc2ccnc3c4cc([nH]n4)c23)c2ccccc2n1
BrSc1ccc(Nc2ncnc3cc[nH]c23)cc1
Fine tuning...
Mean value of predictions: 0.38040817
Proportion of valid SMILES: 0.4598060681889271
Sample trajectories:
BP(=O)(OCC(=O)F)P(=O)(O)O
BP(=O)(OCC)c1cccc(Nc2ncnc3c(Cl)cccc23)c1
BrC(=NNc1ccccc1Br)c1ccccc1Nc1ccccc1
BrCCNc1ncccc1-c1ccc(Nc2ncnc3ccccc23)cc1
Brc1[nH]c2ncnc(-c3cnc4ccccc4c3)c2c1Br

 14 Training on 9691 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 24.269973
Reward: 4.675185
Trajectories with max counts:
941	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.3152062
Proportion of valid SMILES: 0.2425
Sample trajectories:
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1ccccc1
Brc1ccc(-c2ccc(-c3ncnc4ccccc34)c(-c3ccccc3)c2)o1
Brc1ccc(NNc2ncnc3ccccc23)cc1
Brc1ccc(Nc2cc(Nc3ccccc3)ncn2)cc1
Brc1ccc(Nc2ccc3c(Br)cccc3c2)cc1
Policy gradient replay...
Mean value of predictions: 0.33898062
Proportion of valid SMILES: 0.4353125
Sample trajectories:
BP(=O)(NC(c1ccccc1)c1ccc(Br)cc1)P(=O)(OS(=O)(=O)O)S(=O)(=O)Oc1ccc(N)cc1
BP(=O)(Nc1ccccc1)Oc1ccc(Nc2ncnc3ccccc23)cc1
BP(=O)(Oc1ccc(Br)c(Br)c1)N(O)C=O
BrC1=CC(=Nc2ccc(Br)cc2)N1
BrCCN(ON=C(Nc1ncnc2ccccc12)c1ccncc1)c1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.32713866
Proportion of valid SMILES: 0.4238824632697718
Sample trajectories:
BP(=O)(C(=O)Nc1ccc(Nc2cc(Br)cc(Br)c2)cc1)N1CCOCC1
BP(=O)(CC=C(Br)Br)OCC
BP(=O)(c1ccc(Nc2ncnc3cc(F)cc(F)c23)cc1)N1CCOCC1
Br
BrCCCCCCNc1ccc(-c2ccc3[nH]cc(Br)c3c2)c2ccccc12

 15 Training on 10486 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 23.313127
Reward: 5.054543
Trajectories with max counts:
766	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.41117316
Proportion of valid SMILES: 0.22375
Sample trajectories:
BP(=O)(Br)OCCBr
BP(=O)(NCCN)n1cnc2c(N)ncnc21
BP(=O)(Nc1ccc(Br)cc1)P(=O)(Oc1ccc(Br)cc1)Oc1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cc1)c1cccc(Br)c1
BP(=O)(Nc1cccc2ccc(Br)cc12)OCCS(=O)(=O)c1ccc(Nc2ccc(Br)cc2)nc1
Policy gradient replay...
Mean value of predictions: 0.4027864
Proportion of valid SMILES: 0.4045084533500313
Sample trajectories:
BP(=O)(COCc1ccc(Br)cc1)P(=O)(O)O
BP(=O)(NC(Nc1cc(Cl)cc(Cl)c1)Nc1cccc(Br)c1)c1ccc(N)cc1
BP(=O)(Nc1ccc(Br)cc1)c1ccccc1
BP(=O)(Nc1ccc(Nc2ncnc3ccccc23)s1)S(=O)(=O)c1ccc(Br)cc1
BP(=O)(OCC)C(=O)Nc1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.4222395
Proportion of valid SMILES: 0.401875
Sample trajectories:
BP(=O)(OCC)c1ccc(Br)s1
BP(=O)(OCCCCCC)OC(=O)NC(c1ccc(Br)cc1)P(=O)(O)O
BP(=O)(Oc1cccc(Nc2ccccc2Br)c1)[PH](=Nc1cccc(Br)c1)Sc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccccc23)cc1
BrC1=C(c2ccco2)Oc2ncnn21

 16 Training on 11439 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.221901
Reward: 5.075340
Trajectories with max counts:
445	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.51653546
Proportion of valid SMILES: 0.23827392120075047
Sample trajectories:
BP(=O)(Nc1cc(Br)cc(Br)c1)P(=O)(Oc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(Nc1ccc(Nc2ccnc3cc(Br)c(Br)cc23)cc1)P1(=O)CCCN(Cc2ccc(Br)cc2)CC1
BP(=O)(OCC)n1c(N2CCN(c3ccc(F)cc3)CC2)nc2nc(N)ncnc2c1Nc1c(F)cc(Br)cc1Br
B[PH](=O)(COP(=O)([O-])OP(=O)(O)OP(=O)(O)Nc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(NS(=O)(=O)c1ccc(N)cc1)Oc1cccc(Br)c1
Bc1cc(Br)c(Nc2ncc(Br)cn2)c(Br)c1I
Policy gradient replay...
Mean value of predictions: 0.39653817
Proportion of valid SMILES: 0.3971875
Sample trajectories:
BP(=O)(Nc1cc(Nc2ccccc2Br)ncn1)c1ccc(F)c(F)c1F
BP(=O)(Nc1ccc(Br)cc1)Nc1cccc(Br)c1
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
BP(=O)(OCC)Oc1cccc(Br)c1
Fine tuning...
Mean value of predictions: 0.41651526
Proportion of valid SMILES: 0.3784375
Sample trajectories:
BP(=O)(C(N)=O)N(Cc1ccc(I)cc1)Cc1cccc(Br)c1
BP(=O)(NC(c1cccc(Br)c1)P(=O)(O)O)C(=O)O
BP(=O)(Nc1cccc(Br)c1)N(O)Cc1ccc(Br)cc1
BP(=O)(Nc1cccc(Br)c1)P(=O)(Oc1ccc(Nc2c(F)cc(Br)cc2Br)cc1)c1ccccc1F
BP(=O)(Nc1ncnc2[nH]cnc12)Oc1ccccc1

 17 Training on 12494 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.744284
Reward: 5.902681
Trajectories with max counts:
851	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.42425197
Proportion of valid SMILES: 0.1984375
Sample trajectories:
BP(=O)(Br)OCC(Br)(Br)Br
BP(=O)(CCCCl)NP(=O)(OCC)Oc1ccc(Br)cc1
BP(=O)(I)Oc1ccc2c(c1)N2Cc1ccc(I)cc1
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)nc1)c1ccc(Nc2ccc(Br)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.38044444
Proportion of valid SMILES: 0.421875
Sample trajectories:
BP(=O)(N(c1ccccc1)c1ccccc1)P(=O)(O)O
BP(=O)(Nc1cccc(Nc2ncnc3sc(Cl)cc23)c1)Oc1cccc(F)c1
BP(=O)(Nc1ccccc1)c1ccc2ncnc(Nc3ccccc3)c2c1
BP(=O)(Nc1ccccc1Br)Nc1cccc(Nc2ccccc2-c2ccc(Br)cc2)c1N(=O)=O
BP(=O)(c1ccccc1)N(O)c1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.38906947
Proportion of valid SMILES: 0.423125
Sample trajectories:
BP(=O)(NC(CC(=O)Nc1ccc(F)cc1)c1ccccc1)C(=O)O
BP(=O)(OCC)C(NP(=O)(O)c1ccccc1)Oc1ccccc1
BP(=O)(OCCCBr)C(F)(F)F
BP(=O)(c1ccccc1)N(O)c1ccccc1
B[PH](=O)(=O)Nc1ccc(Nc2ccc3c(Br)ccc(Br)c3c2)cc1

 18 Training on 13406 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.258162
Reward: 6.033448
Trajectories with max counts:
427	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.51025003
Proportion of valid SMILES: 0.25
Sample trajectories:
BP(=O)(C=C(Br)CBr)OCC
BP(=O)(O)C1CCCC1(C(=O)Nc1ccc(I)cc1)c1ccc(Br)c(Br)c1
BP(=O)(OC(C)=O)c1cc(Br)c2ncnc(Nc3ccc(Br)cc3)c2n1
BP(=O)(OCC)C(=O)Oc1ccc(Br)cc1
BP(=O)(OCC)C(F)=C(Nc1ccc2ncnc(-c3ccc(F)c(Br)c3)c2n1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.4252114
Proportion of valid SMILES: 0.4068167604752971
Sample trajectories:
BP(=O)(OC)OCCCC
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Br
BrBr
BrC(=NNc1ncccc1Br)c1ccccc1Nc1ccccc1
Fine tuning...
Mean value of predictions: 0.4226471
Proportion of valid SMILES: 0.4251328540168803
Sample trajectories:
BP(=O)(O)CCCCC(=O)Nc1cccc(F)c1
BP(=O)(O)OP(=O)(O)OP(=O)(O)CC(Br)Br
BP(=O)(OCC)C(=O)Nc1cccc(Nc2cccc(Br)c2)c1
BP(=O)(OCC)C(=O)Oc1cccc(Nc2ncnc3ccccc23)c1
BP(=O)(OCC)N(O)C(=O)Nc1ccc(Br)cc1

 19 Training on 14526 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.068470
Reward: 6.176172
Trajectories with max counts:
774	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.49155846
Proportion of valid SMILES: 0.1925
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(OCC)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCC)Oc1ccc(Nc2ncnc3c(Br)cc(Br)cc23)cc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)OP(=O)(O)O
BP(=O)(Oc1cc2c(oc3ccccc13)N=C(N)N2)C(F)(F)F
Policy gradient replay...
Mean value of predictions: 0.48610598
Proportion of valid SMILES: 0.3420888055034397
Sample trajectories:
BP(=O)(C=CBr)OCC
BP(=O)(OCC)Oc1ccc(Br)c(Br)c1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)O[PH](=S)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O
Bc1ccc(Nc2ncnc3nsc(-c4cccnc4)c23)cc1
Br
Fine tuning...
Mean value of predictions: 0.49058717
Proportion of valid SMILES: 0.3355222013758599
Sample trajectories:
BP(=O)(Nc1ccc(Nc2ncnc3[nH]c(CCC(=O)OCC)nc23)cc1)OCC(=O)Nc1cccc(Br)c1
BP(=O)(OCC)OC(=O)CN(CCCl)c1ccc(Br)cc1
BP(=O)(OCC)Oc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrC(Br)(Br)Br

 20 Training on 15580 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.114083
Reward: 6.274332
Trajectories with max counts:
223	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4396552
Proportion of valid SMILES: 0.29
Sample trajectories:
BBr
BP(=O)(CO)c1ccc(Nc2ncnc3c(Br)ccc(Br)c23)cc1
BP(=O)(Nc1ccccc1)Oc1ccccc1
BP(=O)(OCC=CC(=O)Nc1ccccc1-c1ccc(F)cc1F)C(F)(F)F
BP(=O)(OCCCl)P(=O)(O)OP(=O)(O)O
Policy gradient replay...
Mean value of predictions: 0.4097166
Proportion of valid SMILES: 0.3859375
Sample trajectories:
Br
BrBr
BrCCC1=CC(C2Oc3cc(Br)ccc3ncnc(c3ccc(Nc4ccnc5ccc(Br)cc45)c(Br)c3)Nc3ccccc32)CC1
BrCCCC=C(I)I
BrCc1ccc(Br)c(-c2ccc(Br)cc2)c1
Fine tuning...
Mean value of predictions: 0.43282136
Proportion of valid SMILES: 0.3865625
Sample trajectories:
BP(=O)(OCC)Oc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Bc1cccc(Nc2ncnc3cccnc23)c1
BrC(Br)(Br)Br
BrC=CBr
BrCc1ccc(Nc2ncnc3cc(Br)ccc23)cc1

Trajectories with max counts:
1019	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.353039
Proportion of valid SMILES: 0.31574670250672
Mean Internal Similarity: 0.4961860926428426
Std Internal Similarity: 0.11257487179887034
Mean External Similarity: 0.4061082729054407
Std External Similarity: 0.0775886700176113
Mean MolWt: 395.71914967967393
Std MolWt: 89.16983202948079
Effect MolWt: -1.0331685389273817
Mean MolLogP: 5.36409385556203
Std MolLogP: 1.4922718346434765
Effect MolLogP: 0.4592828476273876
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 96.631206% (545 / 564)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 20, 'n_policy_replay': 5, 'n_fine_tune': 0, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5442.189335346222, 'valid_fraction': 0.31574670250672, 'active_fraction': 0.3399326865967135, 'max_counts': 1019, 'mean_internal_similarity': 0.4961860926428426, 'std_internal_similarity': 0.11257487179887034, 'mean_external_similarity': 0.4061082729054407, 'std_external_similarity': 0.0775886700176113, 'mean_MolWt': 395.71914967967393, 'std_MolWt': 89.16983202948079, 'effect_MolWt': -1.0331685389273817, 'mean_MolLogP': 5.36409385556203, 'std_MolLogP': 1.4922718346434765, 'effect_MolLogP': 0.4592828476273876, 'generated_scaffolds': 564, 'novel_scaffolds': 545, 'novel_fraction': 0.9663120567375887, 'save_path': '../logs/replay_ratio_s1-4.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.060398
Reward: 1.000000
Mean value of predictions: 0.0012028869
Proportion of valid SMILES: 0.782063342740671
Sample trajectories:
BrCc1ccc[n+](CCCCCCNCCC2CCCC2)c1
Brc1ccc(I)cc1
Brc1ccc(OCCc2ccccn2)nc1
Brc1cccc(C2=NC3=C(C=Cc4cc(Br)ccc4NC=N2)CCCC3)c1
C#CCCN(C)C1Cc2ccc2OCc2cc(F)ccc21
Policy gradient replay...
Mean value of predictions: 0.0013654619
Proportion of valid SMILES: 0.7810539523212046
Sample trajectories:
Brc1cc(C=Cc2ccccc2)nc(-c2ccccc2)c1
Brc1ccc(Sc2[nH]c3cccnc3c2Br)cc1Br
Brc1cccc(-c2cnnc(Nc3ccccc3)n2)c1
Brc1cccc(-c2nnc3ccnc(NCCN4CCCCC4)c3n2)c1
C#CC(O)CCCCC
Fine tuning...
Mean value of predictions: 0.0008028904
Proportion of valid SMILES: 0.7823492462311558
Sample trajectories:
Brc1ccc(C#CCCCCNCc2c(-c3ccc(Br)cc3)[nH]c3ccccc23)cc1
Brc1ccc(CSc2nnc(C3CC3)c3nnnn23)cc1
Brc1ccc2c(c1)C2N1CCOCC1
C#CCC#CC(O)C(Oc1c(O)cc(-c2ccc(Cl)cc2)cc1Cl)C(=O)O
C#CCCC(NC(=O)C(N)CCCC)C(=O)OC1CC2CC=CC=CC(C(C)(C)C)OC(=O)CC(C)C2(C)CCC1C#Cc1ccc(-c2cnccn2)cc1

  2 Training on 227 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.371923
Reward: 1.000000
Trajectories with max counts:
2	Cc1ccc2c(c1)NC(=O)N2
2	O=C1c2ccccc2Oc2ccccc21
Mean value of predictions: 0.00088602497
Proportion of valid SMILES: 0.7783699059561129
Sample trajectories:
BrCC1C2CC3CC3C1CC2Cc1ccc2c(c1)OCO2
BrCCn1ccnc1NCCCn1nnc2c(-c3ccccc3)ncnc21
Brc1cc(C#Cc2cn(C=C(c3ccccc3)c3ccncc3)nc2Cc2ccc3c(c2)OCO3)cc(I)c1Br
Brc1ccc(-c2nc3ccc(-c4cc5ccccc5s4)cc3s2)o1
Brc1ccc(OCc2nnc(-n3ccc4ccccc43)n2Cc2ccccc2-c2ccccc2)cc1
Policy gradient replay...
Mean value of predictions: 0.0011433238
Proportion of valid SMILES: 0.766270337922403
Sample trajectories:
B[PH](=N)(N)(NC(=O)c1ccccc1)P(=O)(Oc1noc(C)n1)OC(C)C
Brc1cnc(NCCCc2ccccc2)nc1-c1ccccc1
C#CCCC(=C)C(C)(CC#N)SC
C#CCCCCCN(CCN)C(=O)CN(CC)C1CCC(C)CC1C
C#CCCCCNC(=S)NCCCC
Fine tuning...
Mean value of predictions: 0.0015599343
Proportion of valid SMILES: 0.7633970542149796
Sample trajectories:
BC(CCc1ccccc1)NCC1CCCC1
Brc1ccc2c(c1)[nH]c1cc(OCCCN3CCCC3)ccc12
Brc1ccc2nnc(N(C#Cc3ccccn3)CCOc3ccc4c(c3)OCO4)c(Br)c2c1
Brc1ccc2oc(-c3ccc(-c4nc5ccccc5[nH]4)cc3)nc2c1
C#CC(=O)NC(C)C1(N(C(=O)c2cccnc2)c2ccc(Cl)cc2)CC1

  3 Training on 242 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.552171
Reward: 1.019292
Trajectories with max counts:
2	Cc1cccc2ccccc12
2	NC(Cc1ccc(-c2ccccc2)cc1)C(=O)O
Mean value of predictions: 0.0015479876
Proportion of valid SMILES: 0.8087636932707355
Sample trajectories:
Brc1ccc(-c2nnc(NCc3ccccc3)o2)cc1
Brc1ccc(NCc2ccccc2)c(Br)c1
Brc1ccc2[nH]c(-c3cc(-c4ccccc4)on3)nc2c1
Brc1ccc2[nH]cc(C3=CCN(CC4CCCC4)CC3)c2c1
Brc1ccc2c(c1)N2CCCCCCN1CCCC1
Policy gradient replay...
Mean value of predictions: 0.0014878622
Proportion of valid SMILES: 0.7993740219092331
Sample trajectories:
Brc1cc(C=Nc2cccc(C=NN3CCCCC3)c2)no1
Brc1ccc(Br)c(Br)c1
Brc1ccc(CN2CCN(c3ncccn3)CC2)cc1
Brc1ccc(Cc2ccc3oc4ccc(Br)cc4c3c2)cc1
Brc1cccc(-c2nn3ncccc3c2-c2ccccc2)c1
Fine tuning...
Mean value of predictions: 0.00047114253
Proportion of valid SMILES: 0.7974326862867878
Sample trajectories:
Brc1ccc(-c2nnn(CCCCCNCc3cccc(-c4ncc[nH]4)c3)n2)cc1
Brc1ccc(C23CC4CC(CC(C4)C2)C3)cc1
Brc1ccc(OCCCCCCCCc2ccccc2)cc1
Brc1ccc(OCCCCCN=CNc2nccc3c2[nH]c2c(Br)cccc23)cc1
Brc1ccc2[nH]c(-c3ccccc3)nc2c1

  4 Training on 256 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.897532
Reward: 1.008640
Trajectories with max counts:
2	CCCN1C(=O)c2ccccc2C1=O
2	Cc1cccc2ccccc12
2	Cn1c2ccccc2c2ccccc21
2	O=C(O)Cc1cccc2ccccc12
Mean value of predictions: 0.0007092199
Proportion of valid SMILES: 0.7936210131332082
Sample trajectories:
Brc1ccc(OC(CCCOc2ccc(Br)s2)=NCCc2c[nH]c3ccccc23)cc1
Brc1ccc(Oc2ccccc2)cc1
Brc1ccc2c(c1)C1CNCCC1N2
Brc1ccc2nc(N3CC(c4cccc5ccccc45)C3)sc2c1
Brc1cccc(-n2ccc3ccccc32)c1
Policy gradient replay...
Mean value of predictions: 0.00070866145
Proportion of valid SMILES: 0.79375
Sample trajectories:
BP(=O)(CCC)NP(=O)(OCC1OC(O)C(O)C1O)n1cnc2c(N)ncnc21
Brc1ccc(-c2nccc3ccccc23)nc1
Brc1ccc(C2=NCCN2)cc1
Brc1cccc(-c2noc(Nc3ccccc3Br)n2)c1
Brc1cccc(CCN2CCNC2)c1
Fine tuning...
Mean value of predictions: 0.0011682243
Proportion of valid SMILES: 0.8035043804755945
Sample trajectories:
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)C(=O)O
BrBr
Brc1ccc(-c2ccc3c(NC4CCC4)n[nH]c3c2)cc1
Brc1ccc(-c2ccc3ncccc3c2)cc1-c1ncccn1
Brc1ccc(-c2cccc3[nH]ccc23)cc1

  5 Training on 269 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.623855
Reward: 1.005139
Trajectories with max counts:
4	Cc1cccc2ccccc12
Mean value of predictions: 0.00076481845
Proportion of valid SMILES: 0.8174429509221631
Sample trajectories:
BrC1=CN2C(=CC=C2Cc2c[nH]c3ccccc23)C=C1
Brc1ccc(-c2cc3ccccn3c2-c2ccc(-c3cnc4[nH]ncc4c3)cc2)cc1
Brc1ccc(Br)cc1
Brc1ccc(N=C2NCCc3ccccc32)o1
Brc1ccc2c(c1)C1(CCNCC1)CCO2
Policy gradient replay...
Mean value of predictions: 0.0013333333
Proportion of valid SMILES: 0.7978723404255319
Sample trajectories:
Brc1ccc(-c2ccc3ncccc3c2)cc1
Brc1ccc(C2=NNC3=C(c4ccccc43)C2c2ccccc2)cc1
Brc1ccc(NCC=Cc2cscn2)cc1
Brc1ccc2c(NCc3ccccc3)ncnc2c1
Brc1ccccc1COc1cccnc1C1=NN(c2ccncc2)CCCN1
Fine tuning...
Mean value of predictions: 0.0011764705
Proportion of valid SMILES: 0.797373358348968
Sample trajectories:
Brc1c[nH]c(-c2c3ccccc3cc3ccccc23)n1
Brc1ccc(-c2cc(Oc3ccccc3)c3ccccc3n2)s1
Brc1ccc(-c2ccc(C#Cc3ccccc3)cc2)cc1
Brc1ccc(-c2ccccc2Nc2ccnc3[nH]ccc23)cc1
Brc1ccc2[nH]c(C3CCN(Cc4ccccc4)CC3)nc2c1

  6 Training on 287 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.565742
Reward: 1.038872
Mean value of predictions: 0.0014996055
Proportion of valid SMILES: 0.7926180794494839
Sample trajectories:
BP1(=O)Oc2ccc(Br)cc2O1
BrCCc1ccc(C2=COc3cc(Br)cc(Br)c3O2)cc1
Brc1ccc(-c2cc(C3=CCCN4CCCC34)ccc2-c2nn[nH]n2)cc1
Brc1ccc(CN2CCN(Cc3ccccc3)CC2)cc1
Brc1ccc(N2CCc3ccccc3C2)nc1
Policy gradient replay...
Mean value of predictions: 0.0012480499
Proportion of valid SMILES: 0.8017510944340213
Sample trajectories:
Brc1cc2cc(Nc3ncc4[nH]nc(C#Cc5ccccc5)cc34)cccc2n1
Brc1ccc(-c2cccc(Nc3cnccn3)c2)cc1Oc1ccccc1
Brc1ccc(-c2nc(Nc3ccncc3)co2)cc1
Brc1ccc2c(c1)C(c1ccccc1)=Nc1ccccc1N2
Brc1ccc2c(c1)N=C(c1nc3ccccc3[nH]1)S2
Fine tuning...
Mean value of predictions: 0.0011538462
Proportion of valid SMILES: 0.8130081300813008
Sample trajectories:
Brc1ccc(Br)c(-c2ccc3ncn(c4ccc(Br)cc24)n3-c2ccccc2)c1
Brc1ccc(Br)c2n[nH]c(n1)N2c1ccc(-c2ccccc2)cc1
Brc1ccc(CN2C=NC3=C2C=NN3C2CC2)cc1
Brc1ccc2[nH]c(-c3ccccc3)nc2c1
Brc1ccccc1C1=NNC(c2ccccc2)=N1

  7 Training on 304 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.496599
Reward: 1.036985
Trajectories with max counts:
4	Cc1cccc2ccccc12
4	O=C1Nc2ccccc2C1=O
Mean value of predictions: 0.0017950636
Proportion of valid SMILES: 0.8358862144420132
Sample trajectories:
BP(=O)(NCCCCOCCOC)c1ccc(C(=O)c2ccccc2OCc2ccccc2)cc1
Brc1ccc(-c2noc3ccccc23)cc1
Brc1ccc(C2=CSC3=NC=CC23)cc1
Brc1ccc(N2CCN(CCCc3ccc4ccccc4c3)CC2)cc1
Brc1ccc2ccccc2c1
Policy gradient replay...
Mean value of predictions: 0.0016460905
Proportion of valid SMILES: 0.835573616755236
Sample trajectories:
Brc1ccc(-n2nc3ccccc3c2NC2=CCCCC2)cc1
Brc1ccc2cc(-c3cccc4c5cn[nH]c5c34)[nH]c2c1
Brc1cccc(Nc2cncn3cc(-c4ccccc4Br)cc23)c1
Brc1cccc2cccnc12
Brc1ccccc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.0018545995
Proportion of valid SMILES: 0.8432905849233656
Sample trajectories:
Brc1ccc(-c2ccc(-c3ccc4ccccc4c3)cc2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)cc1
Brc1ccc(N2C=CC=CC(c3cccs3)=C2)cc1
Brc1ccc(Nc2nccc(-c3ccccn3)n2)cc1

  8 Training on 328 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.079386
Reward: 1.041215
Trajectories with max counts:
2	O=C1c2ccccc2-c2ccccc21
2	Oc1ccc2ccccc2c1
Mean value of predictions: 0.0018896448
Proportion of valid SMILES: 0.826875
Sample trajectories:
Brc1ccc(-c2ccc3ccccc3c2)o1
Brc1ccc(N2CCN(Cc3cccc4c3CO4)CC2)o1
Brc1ccc(Nc2ncnc3ccc(-c4ccccc4)cc23)cc1
Brc1ccc2[nH]cc(CCCNc3cccc4ccccc34)c2c1
Brc1ccc2cc3nc[nH]c3cc2c1
Policy gradient replay...
Mean value of predictions: 0.0010558069
Proportion of valid SMILES: 0.8295276822020644
Sample trajectories:
Brc1[nH]c2ccc3ccccc3c2c1-c1cccs1
Brc1cc(Nc2nccc3ccccc23)cc(OCc2ccccc2)c1
Brc1ccc(-c2ccc3c(c2)OCO3)cc1
Brc1ccc(-c2nsc(C3CCCN(c4cccs4)C3)n2)cc1
Brc1ccc(OCc2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.0009863429
Proportion of valid SMILES: 0.8240075023444826
Sample trajectories:
BrCc1cnc(-c2ccncc2)cc1-c1cccc2ccccc12
Brc1c(C2=NCCO2)n[nH]c1-c1ccc(-c2ccccc2)cc1
Brc1ccc(-n2c3ccccc3c3ccccc32)cc1
Brc1ccc(-n2cc(-c3ccccc3)nn2)cc1
Brc1ccc(C#Cc2c[nH]c3ccc(N4CCOCC4)cc23)cc1

  9 Training on 344 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.645269
Reward: 1.054786
Trajectories with max counts:
3	Fc1ccccc1F
Mean value of predictions: 0.0014240505
Proportion of valid SMILES: 0.79
Sample trajectories:
BrC1=CNc2ccc(NC3C4CCCC43)cc2-c2ccc(cc2)N2CC3=CC=C(N=C3C2)c2c1oc1ccccc21
Brc1[nH]ccc1-c1ccc(C=C2c3ccccc3C(Br)(C=Cc3ccccc3)C3Nc4ccccc4OCC23)cc1
Brc1ccc(-c2nnc3ccccc3n2)cc1
Brc1ccc(-n2cc3ccccc3c2)cc1
Brc1ccc(Sc2ccccc2N2CCN(Cc3ccccc3)CC2)cc1
Policy gradient replay...
Mean value of predictions: 0.0029174665
Proportion of valid SMILES: 0.814571607254534
Sample trajectories:
Brc1c[nH]c2nc(NN=Cc3ccccc3)sc12
Brc1ccc(Cn2cncc2CSc2cncc(Br)[n+]2-c2ccccc2)cc1
Brc1ccc(Nc2nccc(Br)c2Oc2ccccc2)cc1
Brc1ccc(Oc2ccccc2)cc1OCc1ccccc1
Brc1ccc2cc[nH]c2c1
Fine tuning...
Mean value of predictions: 0.0032444957
Proportion of valid SMILES: 0.8100750938673341
Sample trajectories:
BrC1=CSC2=NC(C34CC5CC(CC(C5)C3)C4)=CC=C12
Brc1cc(Br)c2c(c1)C=CN2
Brc1ccc(CN2CCN(Cc3ccc4c(c3)OCO4)CC2)cc1
Brc1ccc(N=Cc2cn(-c3cccc4ccccc34)nn2)cc1
Brc1ccc(OCCn2ccc3c(N4CCCCC4)ncnc32)cc1

 10 Training on 374 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.798677
Reward: 1.048757
Trajectories with max counts:
4	Cc1cccc2ccccc12
Mean value of predictions: 0.002433281
Proportion of valid SMILES: 0.7964989059080962
Sample trajectories:
BrCCN=C1C=CN1
Brc1ccc(-c2ccc3cccnc3c2)cc1
Brc1ccc(Oc2nc3ncccc3s2)cc1
Brc1ccc2c(c1)C1=C(CCC1)N2
Brc1ccc2c(c1)cc1[nH]c3cccnc3CN(C3CCC(c4ccccc4)CC3)CCn12
Policy gradient replay...
Mean value of predictions: 0.0007656968
Proportion of valid SMILES: 0.8170159524554269
Sample trajectories:
Brc1ccc(-c2oc3ccccc3c2-c2c[nH]c3ccccc23)cc1
Brc1ccc(CN2CCCN(c3ccncc3)CC2)cc1
Brc1ccc(Cn2ccc3ccccc32)c(OCc2ccc(C#CC3CCCC3)cc2)n1
Brc1ccc2nc3c(nc2c1)CCCC3
Brc1ccc2oc(N3CCN(c4ccncc4)CC3)cc2c1
Fine tuning...
Mean value of predictions: 0.0015582392
Proportion of valid SMILES: 0.8029402564904599
Sample trajectories:
Brc1cccc(-c2[nH]c3ccccc3c2CN2CCCCC2)c1
Brc1cccc(C2=C3C=CC=CC3=Nc3ccccc3S2)c1
Brc1cccc2[nH]ccc12
Brc1cccs1
Brc1cncc(NC=C2COC3=C2NC=NN2CCN(CC2)C3)c1

 11 Training on 395 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.154685
Reward: 1.038882
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0021434461
Proportion of valid SMILES: 0.7590738423028786
Sample trajectories:
BrC1(c2ccc(C=CC3CCN(c4ccccc4)CC3)cc2)CC(c2ccccc2)=N1
Brc1cc(Br)c(C=Cc2ccncc2)[nH]1
Brc1cc(NC2CNCCN2CN2CCCC2)ccc1Nc1nccs1
Brc1ccc(C2CCN(c3cccs3)C2)c(Br)c1
Brc1ccc(CN2CCN(c3cnccn3)CC2)cc1
Policy gradient replay...
Mean value of predictions: 0.0029588165
Proportion of valid SMILES: 0.782540675844806
Sample trajectories:
BrC=CC=C(CNC1CCCC1)NCCCC=CC=CC=CC=CCCCCCCCCCCBr
Brc1ccc(CN2CCN(C3=Nc4cc(Br)ccc4O3)CC2)cc1
Brc1ccc(CN2CCN(Cc3ccccc3)CC2)cc1
Brc1ccc(N2CCN(c3ccccc3)CC2)c(OCc2ccccc2)c1
Brc1ccc2c(-c3nc4ccccc4[nH]3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.0028301885
Proportion of valid SMILES: 0.7957460118861432
Sample trajectories:
BrC1=CC2=NCCCC2C2(CCN(c3ccccc3)CC2)c2nccn2C1
Brc1c[nH]c2ccc(NC3CCNCC3)cc12
Brc1ccc(-n2ncc3ccc(Br)cc32)cc1
Brc1ccc(C=CC2=C(c3ccccc3)NCCN2)cc1
Brc1ccc(Nc2ncccc2Br)cc1

 12 Training on 424 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.405390
Reward: 1.027202
Mean value of predictions: 0.0021268215
Proportion of valid SMILES: 0.7944305381727159
Sample trajectories:
BP(=O)(NCCCCCN)c1cccc(F)c1
Brc1ccc(-c2ccccc2)s1
Brc1ccc2N=C3CC=CCC3N2C1=Nc1ccccc1
Brc1cccc(Nc2nccs2)c1
C#CCC=CCCCNC(=N)N
Policy gradient replay...
Mean value of predictions: 0.0014751554
Proportion of valid SMILES: 0.8055034396497811
Sample trajectories:
Brc1ccc(N2CCN(CCNc3cccc(C4=NCCCN4)c3)CC2)cc1
Brc1ccc2c(c1)C(c1ccccc1)=NN1CCNCC1=N2
Brc1ccc2c(c1)OC(c1ccc3ccccc3c1)C=N2
Brc1ccc2c(c1)SC(=NC1=NCCCN1)N2
Brc1cccc(-c2ccc3ccccc3c2)c1
Fine tuning...
Mean value of predictions: 0.0010280743
Proportion of valid SMILES: 0.7913016270337923
Sample trajectories:
BrC1=CCC2=C(NCN2)C12CCCN2
Brc1c(CN2CCCNC2=Nc2ccncc2)ccc2ccccc12
Brc1ccc(-c2cnc3ccc(Br)nc3c2NCc2ccco2)cc1
Brc1ccc(Br)c(CN2CCCNCC2)c1
Brc1ccc(N2CCN(c3ccccc3)CC2)c(CNc2ccnc3ccccc23)c1

 13 Training on 444 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.521707
Reward: 1.026095
Trajectories with max counts:
3	CCn1ccc2ccccc21
3	Cc1cccc2ccccc12
3	Nc1cccc2ccccc12
Mean value of predictions: 0.0026244693
Proportion of valid SMILES: 0.8099406064395124
Sample trajectories:
Brc1ccc2c(Oc3cccc4ccccc34)cccc2c1
Brc1ccc2c(Oc3ccccc3N=Cc3ccccc3)cccc2c1
Brc1ccccc1-c1cc2[nH]ccc2cc1Oc1ccc(-c2ccccc2)cc1
C#CCN(C(NCCC1(C)CCCC1)N1CCCC1)S(=O)(=O)C1CCC=CCC1
C#CCN(c1cccc(NS(C)(=O)=O)c1)c1cn(CCC)c(C)c1Nc1ccccc1
Policy gradient replay...
Mean value of predictions: 0.0014643546
Proportion of valid SMILES: 0.811698467313106
Sample trajectories:
BP(=O)(Oc1ccc(F)cc1)c1ccc2ccccc2n1
Brc1ccc(Br)c(Br)c1
Brc1ccc(I)c(Nc2cccc(-c3cnc4[nH]cnc4c3)c2)c1
Brc1ccc2c(c1)[nH]c1c(OCc3ccccc3)cccc12
Brc1cccc2c(N=CN3CCN(c4ccccc4)CC3)Nc3ncc(cc3N=Cc3ccco3)nc12
Fine tuning...
Mean value of predictions: 0.0019290124
Proportion of valid SMILES: 0.8110137672090113
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)OP(=O)(O)O
BrCCBr
Brc1cc(NCc2ccccc2)c(-c2ccc3oc4ccccc4c3c2)cn1
Brc1ccc(-c2ccc3onc(N=Cc4ccccc4)c3c2)cc1
Brc1ccc(-c2nc3ccccc3o2)cc1

 14 Training on 465 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.131396
Reward: 1.032141
Trajectories with max counts:
3	Cc1cccc2ccccc12
Mean value of predictions: 0.0018065487
Proportion of valid SMILES: 0.8316118935837246
Sample trajectories:
Brc1ccc(-c2ccc3sccc3c2)s1
Brc1ccc(CN2CCN(C(c3ccccc3)c3ccccc3)CC2)cc1
Brc1ccc(NC2=NCC=CC2)cc1
Brc1ccc2c(c1)OC(c1ccccc1)=C(Nc1ccc(OCc3ccccc3)cc1)N2
Brc1ccc2nc(-c3ccco3)c(-c3ccccc3)nc2c1
Policy gradient replay...
Mean value of predictions: 0.0017617771
Proportion of valid SMILES: 0.8174702567313713
Sample trajectories:
BP(=O)(OCC)n1cc(Br)c(Br)c1
BrC1=C(c2ccc3cccnc3c2)CCCN1
Brc1ccc(C#Cc2cccnc2)cc1-c1noc(-c2ccncc2)n1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(OCCCCCNc2nc(-c3ccccc3Br)cs2)cc1
Fine tuning...
Mean value of predictions: 0.0006862372
Proportion of valid SMILES: 0.8207133917396746
Sample trajectories:
Brc1ccc(C#CC2=NCCCN2)cc1
Brc1ccc(C#Cc2c[nH]cn2)cc1
Brc1ccc(C#Cc2ccccc2)s1
Brc1ccc(C(Nc2ccccc2)c2ccccc2)cc1
Brc1ccc(Oc2ccccc2)cc1

 15 Training on 482 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.113152
Reward: 1.019955
Trajectories with max counts:
2	CCn1c2ccccc2c2ccccc21
2	Cc1ccc(S(=O)(=O)N2CCN(CC3CC3)CC2)cc1
2	Cn1ccc2ccccc21
Mean value of predictions: 0.002381867
Proportion of valid SMILES: 0.8147104851330204
Sample trajectories:
Brc1ccc(C#Cc2ccco2)cc1
Brc1ccc(C2Oc3ccccc3C2Nc2ccccc2)cc1
Brc1ccc(N2CCN(CCc3ccncc3)CC2)c2ccccc12
Brc1ccc(Sc2ccccc2)c(Br)c1
Brc1ccc2c(c1)CCCO2
Policy gradient replay...
Mean value of predictions: 0.00077790744
Proportion of valid SMILES: 0.8044430538172715
Sample trajectories:
Brc1ccc(-c2cccc3ccccc23)cc1
Brc1ccc(-c2nc3c(ccc4[nH]cc(Br)c43)c3ccccc3[nH]2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(C2CCNCC2NCc2cc3ccccc3o2)o1
Brc1ccc(C=NNC=Cc2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.0016304349
Proportion of valid SMILES: 0.8062597809076683
Sample trajectories:
Brc1cc2[nH]c(CCc3c[nH]c4ccccc34)cc2cn1
Brc1ccc(Br)c(Br)c1
Brc1ccc2[nH]c(-c3ccc4ccccc4c3)nc2c1
Brc1ccc2sc(NCCn3ccc4ccccc43)cc2c1
Brc1cccc(C2=CCc3ccccc3C2)c1

 16 Training on 506 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.083810
Reward: 1.024093
Mean value of predictions: 0.0043894653
Proportion of valid SMILES: 0.7841051314142679
Sample trajectories:
BP(=O)(OCC1OC(N2C=CC=CC2=O)C(O)C1O)C(=O)NCCc1ccccc1
Brc1cc(Nc2ccccn2)ncc1-c1nc2ccccc2o1
Brc1ccc(NC2=NCCN2)cc1Br
Brc1ccc(Nc2nc(-c3ccccn3)[nH]c2Br)o1
Brc1cccc(C2CC=CN(CCCN3CCCCC3)CC2)c1
Policy gradient replay...
Mean value of predictions: 0.0017849898
Proportion of valid SMILES: 0.7705532979055955
Sample trajectories:
B[PH](=O)(Cn1cnc2c(Br)cc(Br)cc21)=NP(N)(=O)OP(=O)(O)OCC1OC(n2cnc3c(Br)cc(Br)cc32)C(O)C1O
Brc1cc[nH]n1
Brc1ccc2[nH]c(C3CCN(Cc4ccncc4)CC3)cc2c1
Brc1ccc2oc(-c3ccc(CN4CCC5CNCC5C4)c4ccccc34)nc2c1
Brc1cccc(N2CCN(Cc3ccc4c(c3)OCO4)CC2)c1
Fine tuning...
Mean value of predictions: 0.0012929293
Proportion of valid SMILES: 0.7748904195366312
Sample trajectories:
BP(=O)(NCCCO)C(=O)N(CCO)CCCC
Brc1cc(NN=Cc2ccccc2Br)ccn1
Brc1ccc(C(c2ccccc2)(c2ccccc2)c2ccccc2)cc1
Brc1ccc2[nH]c(-c3ccccc3)nc2c1
Brc1cccc(Nc2ncnc3cccc(N4CCCC4)c23)c1

 17 Training on 532 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.060359
Reward: 1.015519
Mean value of predictions: 0.0024740624
Proportion of valid SMILES: 0.7858262778300408
Sample trajectories:
Brc1ccc(C2=NOC(c3ccccc3)c3ccccc32)cc1
Brc1ccc(CCNCc2ccccc2)cc1
Brc1ccc(CSC2=Nc3ccccc3-c3ccccc32)cc1
Brc1ccc2c(c1)CC(Nc1ccnc(NCc3ccccn3)n1)O2
Brc1cccc(Oc2ccccc2)c1
Policy gradient replay...
Mean value of predictions: 0.0003952569
Proportion of valid SMILES: 0.7918622848200313
Sample trajectories:
BrC1=NC(=Cc2c[nH]c3ccccc23)Cn2cncc2CN1
Brc1cc(NCCc2cccc3ccccc23)cnc1N1CCCNCC1
Brc1ccc(Nc2c(-c3ccc4nc[nH]c4c3)cnc3ccc(-c4ccc(Br)cc4)cc23)cc1
Brc1cccc(N2CCCNCC2)c1
Brc1cccc(Nc2nc(-c3ccccc3)cs2)n1
Fine tuning...
Mean value of predictions: 0.001607717
Proportion of valid SMILES: 0.7792045098653304
Sample trajectories:
BP(=O)(NC(Cc1ccccc1)C(=O)O)P(=O)(O)O
Brc1ccc(Br)c(Oc2ncc3ccc(NCc4ccccc4)nc3c2N2CCSCC2)c1
Brc1ccc(CN2CCN(c3nsnc3Br)CC2)cc1
Brc1ccc(NCc2cccnc2)c(-c2ccncc2)c1
Brc1ccc(NN=Cc2ccccc2)cc1

 18 Training on 550 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.076512
Reward: 1.009232
Trajectories with max counts:
2	Cc1ccc(S(N)(=O)=O)cc1
2	Fc1ccccc1F
2	Nc1ncnc2c1ncn2C1OC(COP(=O)(O)OP(=O)(O)OP(=O)(O)O)C(O)C1O
Mean value of predictions: 0.0013333333
Proportion of valid SMILES: 0.7981220657276995
Sample trajectories:
Bc1ccc(S(=O)(=O)O)cc1
BrC=CCN=C1CN2CCCC2C(N2CCN(Cc3c[nH]c4ccccc34)CC2)=N1
BrCCCc1ccc(NCc2ccccc2)nc1
Brc1cc2c3ccccc3c1c1cccc(c1)-c1[nH]c3ccccc3c1-2
Brc1ccc(-c2cc(-c3n[nH]cc3Br)cc(Nc3ccccc3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.0013343799
Proportion of valid SMILES: 0.7977457733249843
Sample trajectories:
Brc1ccc(Nc2c(Br)cccc2Br)s1
Brc1ccc2c(Br)cccc2c1
Brc1cnc2[nH]ncc2c1Br
C#CC(C)CCC(C)C
C#CCCC(=O)Nc1ccccc1C(C#N)c1ccc(C(=O)Nc2ccc(c3ccncc3)cc2F)cc1
Fine tuning...
Mean value of predictions: 0.0014251781
Proportion of valid SMILES: 0.7916013788780947
Sample trajectories:
Brc1ccc(CNC2CCCCC2)cc1
Brc1ccc(OCCSc2nc3ccccc3s2)c(Br)c1
Brc1cccc(Nc2ncnc3ccnnc23)c1
Brc1cccc2c(N3CCNCCCNCC3)cc(C#CCNCc3ccccc3)cc12
Brc1ccccc1Br

 19 Training on 566 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.966538
Reward: 1.005491
Mean value of predictions: 0.0008785943
Proportion of valid SMILES: 0.7856918732350172
Sample trajectories:
BrCCc1ccc(-c2ccccc2)c(N2CCOCC2)c1
Brc1ccc(N2CCN(C3=Nc4ccccc4Sc4ccccc43)CC2)cc1
Brc1ccc(NCc2ccco2)cc1
Brc1ccc2c(NCCCNCc3ccccc3)ncnc2c1
Brc1cccc(Nc2nc3ccccc3nc2Sc2ccccc2)c1
Policy gradient replay...
Mean value of predictions: 0.001910828
Proportion of valid SMILES: 0.7877077453747257
Sample trajectories:
BrC1=C[N+][N+]C2=CCCC2C1
Brc1ccc(C2=Nc3ccccc3C3=CCCN(Cc4ccccc4)CCN32)cc1
Brc1ccc(CCNCCc2ccc(C#Cc3c[nH]c4ccc(Br)cc34)cc2)cc1
Brc1ccc(OCC2CCCN2)cc1
Brc1ccc(OCCCN2CCN(c3ccccc3)CC2)cc1
Fine tuning...
Mean value of predictions: 0.0014948859
Proportion of valid SMILES: 0.7961165048543689
Sample trajectories:
BrCCCCCc1ccc(Br)cc1
Brc1cc(CN(Cc2ccco2)CN2C=CC=CN2)c2ccccc2n1
Brc1ccc(-c2cc(-c3csc(N4CCCCC4)n3)ncn2)cc1
Brc1ccc(-c2cc(CN3CCN4C(c5ccccc5)CC34)c3ccccc3n3cccc3n2)cn1
Brc1ccc(-c2nc3ccccc3[nH]2)cc1

 20 Training on 582 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.332183
Reward: 1.003266
Trajectories with max counts:
2	Cc1cccs1
Mean value of predictions: 0.0015625
Proportion of valid SMILES: 0.8017538365173817
Sample trajectories:
BP(=O)(CCCCCCCNC(=O)CCCSC)NC(=O)CN
BP(=O)(NCOCCN1C=C(O)N(O)C(=O)c2ccccc21)c1ccc(-c2ccccc2)cc1
Brc1ccc(-c2nc(-c3ccccc3)no2)cc1
Brc1ccccc1-n1ccc2cnccc21
C#CC1=Cc2ccc(cc2)C(=N)NC(=O)C2=NC=C(C)CN12
Policy gradient replay...
Mean value of predictions: 0.001178782
Proportion of valid SMILES: 0.7973057644110275
Sample trajectories:
Brc1ccc(CN2CCC3CCCCN=C3NC2c2ccccc2)cc1
Brc1ccc(CNC(CCCNC(c2ccccc2)N2CCCC2)c2cccc(Br)c2)cc1
Brc1ccc(NC2=CCCC3C=CCCCC3c3ccccc32)cc1
Brc1ccc(NC2=NCCN2)cc1OCC1CCCC1
Brc1cccc(Cc2cccc3ccccc23)c1
Fine tuning...
Mean value of predictions: 0.00094007055
Proportion of valid SMILES: 0.8003134796238245
Sample trajectories:
Brc1ccc(C2=NN(CCN3CCOCC3)N2)cc1
Brc1ccc(C=Cc2ccccc2Br)cc1
Brc1ccc(NCc2nc3ccccc3[nH]2)cc1
Brc1cccc(C=NNC2CNCCN2)c1
Brc1ccccc1Oc1ncc(CN2CCCCC2)c2sccc12

Trajectories with max counts:
3	Cc1ccc(NC(=O)CN2CCN(c3ccccc3)CC2)cc1
3	NCCCCCN
Mean value of predictions: 0.0012806497
Proportion of valid SMILES: 0.8022301572386143
Mean Internal Similarity: 0.4696270987917516
Std Internal Similarity: 0.18458501019162304
Mean External Similarity: 0.4193448339656822
Std External Similarity: 0.07194854933224125
Mean MolWt: 366.8497500000001
Std MolWt: 53.97418833803807
Effect MolWt: -1.4847556941434634
Mean MolLogP: 4.447790000000003
Std MolLogP: 1.1181964165240716
Effect MolLogP: -0.20461311125194898
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 81.818182% (9 / 11)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 0, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5642.447606086731, 'valid_fraction': 0.8022301572386143, 'active_fraction': 0.0009370607527721381, 'max_counts': 3, 'mean_internal_similarity': 0.4696270987917516, 'std_internal_similarity': 0.18458501019162304, 'mean_external_similarity': 0.4193448339656822, 'std_external_similarity': 0.07194854933224125, 'mean_MolWt': 366.8497500000001, 'std_MolWt': 53.97418833803807, 'effect_MolWt': -1.4847556941434634, 'mean_MolLogP': 4.447790000000003, 'std_MolLogP': 1.1181964165240716, 'effect_MolLogP': -0.20461311125194898, 'generated_scaffolds': 11, 'novel_scaffolds': 9, 'novel_fraction': 0.8181818181818182, 'save_path': '../logs/replay_ratio_s1-5.smi'}
