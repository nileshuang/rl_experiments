starting log


  1 Training on 0 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.698529
Reward: 1.000000
Mean value of predictions: 0.0011881188
Proportion of valid SMILES: 0.7910401002506265
Sample trajectories:
BP(=O)(OC(C)C)C1CCCC=CC1OC(=O)CCC(N)=O
Brc1cc(Br)c2c(c1Br)C1CC=CC(CO2)O1
Brc1ccc(-c2ccc3oc(N4CCCC4)nc3n2)cc1
Brc1ccco1
C#CCC#Cc1ccn(C(=O)N2CCN(c3ccccc3)CC2)c1
Policy gradient replay...
Mean value of predictions: 0.06502405
Proportion of valid SMILES: 0.5209768315591734
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)n2n1
Brc1ccc2c(c1)-c1ccccc1-2
Brc1ccc2ncnc(Nc3ccccc3)c2c1
C#CCC(C)CN(C#N)CC(C)C
C#CCN(CC)CC(NC(=O)COC(=O)CNC(=O)C(CCCNC(=N)N)NC(=O)C=Cc1ccc2nsnc2n1)OCC
Fine tuning...
Mean value of predictions: 0.088901475
Proportion of valid SMILES: 0.5520475148483901
Sample trajectories:
Brc1ccc2nccn2c1
Brc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1ccc2ncnc(Nc3ccccc3)c2n1
C#CCCCCN(CC)c1nc(Nc2ccc(NC(=O)C3CC3)cc2)nc(N2CCN(C)CC2)n1
C#CCN1c2ccc(OC)cc2CC(=O)N(C(C)CO)CC(C)C1CN1CCN(c2ccccc2-c2ccccc2)CC1

  2 Training on 446 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 20.066925
Reward: 1.524475
Trajectories with max counts:
62	COc1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.12208407
Proportion of valid SMILES: 0.5284730913642053
Sample trajectories:
Brc1ccc2nccc(-c3ccccc3)c2n1
Brc1ccc2ncnc(Nc3ccccc3)c2c1
C#CCn1cnc2c1CC(CCc1cc(-c3ccccc3)c(-c3ccccc3)c(-c3ccccc3)c1Nc1ccccn1)N2C
C#Cc1cccc(Nc2ncnc3cc(OC)c(-c4cccnc4)cc23)c1
C#Cc1cccc(Nc2ncnc3cc(OC)c(OC)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.16847885
Proportion of valid SMILES: 0.5712045169385195
Sample trajectories:
Bc1nc(Nc2ccccc2)c2ncnn2n1
Brc1ccc(Nc2ncnc3cccc(Br)c23)cc1
Brc1ccc2nc(-c3ccccc3)c3ccccc3c2n1
Brc1cccc(Nc2ncnc3cc(Br)c4ncnc4nc23)c1
C#CCN(CC)c1cc2c(s1)-c1c(C)c(-c3cc(Cl)c(O)c(Cl)c3)c1-2
Fine tuning...
Mean value of predictions: 0.17443688
Proportion of valid SMILES: 0.596748984057518
Sample trajectories:
Brc1cc(Br)c2c(N3CCN(c4ccco4)CC3)ncnc2c1
Brc1ccc(Nc2cccc(Nc3ncnc4ccc(Br)cc34)c2)cc1
C#CCCOc1cnc(OC)cc1-c1cc(Nc2ccccc2Oc2ccc(F)cc2)ncn1
C#CCN(C)c1cc2nccc(NC)c2cc1Nc1ncc(Br)cn1
C#Cc1cccc(Nc2ncnc3cc(OC)c(-c4ccncc4)cc23)c1

  3 Training on 1729 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 18.504710
Reward: 1.635484
Trajectories with max counts:
78	COc1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.18105975
Proportion of valid SMILES: 0.554548296342607
Sample trajectories:
Brc1cc2cccnc2n1CSc1nc2ccccc2n2cccc12
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc2[nH]cnc2n1
Brc1ccc2ncnc(Nc3ccccc3)c2c1
C#CCC(=O)N1CCN(c2cc3c(Nc4ccccc4Cl)ncnc3cc2OC)CC1
Policy gradient replay...
Mean value of predictions: 0.20380023
Proportion of valid SMILES: 0.5758049390434511
Sample trajectories:
Brc1ccc(Nc2ncnc3ccc(Br)cc23)nc1
Brc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cccc(-n2cnc3ccc(Br)cc32)c1
C#CC(C(=O)NC(CN(CCN=C(Nc1ncc2cnccc2n1)C(C)(C)C)CC(C)C)C(F)(F)F)c1ccnc(Nc2cccc(Nc3ccncc3)c2)c1
C#CCC[n+]1ccc2c1c1cc(OC)n(C#N)c3ccccc3c(ncn1)N2
Fine tuning...
Mean value of predictions: 0.2375546
Proportion of valid SMILES: 0.5728580362726704
Sample trajectories:
Brc1ccc2nc(Nc3ccncc3)oc2c1
Brc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cccc(Nc2ncnc3ccc(Br)nc23)c1
Brc1cccc(Nc2ncnc3cccc(Br)c23)c1
C#CC(NC(=O)NC1(C)CN1C(=O)CC[N+](C)(C)CO)C(=O)N(CC1CC1)c1cc(Nc2ncnc3ccc(-c4cc5c(Nc6ccccc6C(C)C)ncnc5cc4OC)nc23)ccc1F

  4 Training on 3056 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 22.586728
Reward: 2.404513
Trajectories with max counts:
123	COc1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.22700892
Proportion of valid SMILES: 0.56
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1Br
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cnc(Nc2ncccn2)cc1Br
Brc1ncnc2ncnc(NCCn3ccc4ccccc43)c12
C#CCOc1cc2c(Nc3ccccc3Cl)ncnc2cc1CN(CCN(C)C)C(C)C
Policy gradient replay...
Mean value of predictions: 0.30103448
Proportion of valid SMILES: 0.5440900562851783
Sample trajectories:
Brc1cc2ncnc(Nc3ccc(I)cc3)c2cc1N1CCN(CCCCN2CCCCC2)CC1
Brc1ccc(-c2c(Br)ccc3c2c2nccn32)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(c2ncnc3ccc4nc3ccc(c(c3cccc5ncnc(Nc6cccc(Br)c6)c53)CCN4)N2)cc1
Brc1ccc2nccnc2c1
Fine tuning...
Mean value of predictions: 0.29609054
Proportion of valid SMILES: 0.6086412022542267
Sample trajectories:
Brc1ccc(-c2ccc(Br)cc2Nc2ccncc2)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc2nc(-c3cnc4ccccc4c3)nc(NCc3ccc(Br)s3)c2c1
Brc1ccc2ncnc(Nc3cccc4ccncc34)c2c1
Brc1ccc2ncnc(Sc3ccccc3)c2c1

  5 Training on 4617 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 21.559083
Reward: 2.829030
Trajectories with max counts:
282	Oc1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.22158591
Proportion of valid SMILES: 0.4257580493904345
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6ccccc6)c5c4)cc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)nc1
Brc1ccc2nccnc2c1
Brc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1ccc2ncnc(Nc3ccccc3)c2c1-c1c2ccccc2nc2ccccc12
Policy gradient replay...
Mean value of predictions: 0.37157258
Proportion of valid SMILES: 0.6217486681291131
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3cc(OCCN4CCNCC4)ncc23)c1
Brc1ccc(-c2nc(-c3ccccc3)ccc2Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ncc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.3602422
Proportion of valid SMILES: 0.6197623514696685
Sample trajectories:
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)nc23)c1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc2c(Nc3ccc(I)cn3)ncnc2c1
Brc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)sc2c1
Brc1cccc(Nc2ncnc3ccc(I)cc23)c1

  6 Training on 6296 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 23.312187
Reward: 2.864317
Trajectories with max counts:
17	COc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1OC
Mean value of predictions: 0.45803842
Proportion of valid SMILES: 0.6188986232790988
Sample trajectories:
Brc1ccc(Nc2cc(-c3c(Br)cc4ncnc(Nc5ccc(Br)c(Br)c5)c4c3Br)ccn2)nc1
Brc1ccc(Nc2ncnc3ccc(-c4nc5c(cc4Br)ncn5-c4cccc(Br)c4)nc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)nc23)cc1
Brc1cccc(-c2ncnc3ncnc(SCc4ccccc4)c23)c1
Policy gradient replay...
Mean value of predictions: 0.40514562
Proportion of valid SMILES: 0.64375
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccncc1
Brc1cc2ncnc(Sc3ccccc3)c2cc1-c1ccccc1
Brc1ccc(Nc2ccc3nc(Br)ccc3n2)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(OCc4cccc5ncnc(Nc6ccccc6)c45)nc23)cc1
Fine tuning...
Mean value of predictions: 0.45542523
Proportion of valid SMILES: 0.6405760801502818
Sample trajectories:
Brc1ccc(N=Nc2nc(CCn3ccnc3)cs2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4cccc5ncncc45)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccccc4)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(CNCc4ccccn4)cc23)cc1

  7 Training on 8695 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 24.104266
Reward: 3.367090
Trajectories with max counts:
146	Oc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Mean value of predictions: 0.46601605
Proportion of valid SMILES: 0.544375
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncccc5c4)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)c(Br)c1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3nccc(-c4ccc5ncnc(Nc6ccccc6)c5c4)c23)cc1
Brc1ccc(Nc2ncnc3nccc(OCc4cccc5ccccc45)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.51751304
Proportion of valid SMILES: 0.6586433260393874
Sample trajectories:
Brc1ccc(Nc2ncnc3c2ccc2ncnc(Nc4ccccn4)c23)s1
Brc1ccc(Nc2ncnc3ccc(-c4c(Br)cc5ncnc(Nc6ccc(Cc7cccc(Br)c7)cc6)n45)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(-c4cc5c(Nc6ccc(Br)cc6)ncnc5cc4Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(NCc4ccccc4)cc23)cc1
Brc1ccc(Nc2ncnc3nccc(SCc4cccnc4)c23)cc1
Fine tuning...
Mean value of predictions: 0.5121792
Proportion of valid SMILES: 0.6698968427633636
Sample trajectories:
B[PH](=O)(N(c1ccccc1)c1ccccc1)=[PH](Br)Br
BrCCn1cc2ncnc(Nc3ccccc3)c2c1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Cc4ccc5ncnc(Nc6ccc(Br)cc6)c5n4)cc23)cc1
Brc1ccc(Nc2ncnc3cccc(N4CCN(CCc5cc6ncnc(Nc7ccc(Br)c(Br)c7)c6cc5-c5ccccn5)CC4)c23)cc1

  8 Training on 10855 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.243990
Reward: 3.390745
Trajectories with max counts:
22	COc1cc2ncnc(Nc3ccc(Br)cc3)c2cn1
Mean value of predictions: 0.622146
Proportion of valid SMILES: 0.6392107735671781
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Br)nn23)c(Br)c1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3cccc(N=CCN4CCCCC4)c23)cc1
Brc1ccc(Nc2ncnc3ncc(I)cc23)cc1Br
Brc1ccc2c(c1)c1cc(OCN3CCOCC3)c(-c3ccn4ccnc4n3)c(ncn1)N2
Policy gradient replay...
Mean value of predictions: 0.5107589
Proportion of valid SMILES: 0.651031894934334
Sample trajectories:
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(OCc4cnccn4)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Sc4ccc5ncnc(Nc6cccc(Br)c6)c5c4)cc23)cc1
Brc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Fine tuning...
Mean value of predictions: 0.5389413
Proportion of valid SMILES: 0.6614567052203814
Sample trajectories:
BS(=O)(=O)Nc1cc2c(Nc3ccc(Br)s3)ncnc2cc1OC
Brc1cc2ncnc(Nc3ccccc3Br)n2n1
Brc1ccc(Nc2ccc(Br)c(-c3ccc4ncnc(Nc5ccccc5)c4c3)c2-c2ccccc2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Brc1ccc(Nc2ncnc3ccc(Br)c(-c4nc5c(Br)cccc5cc4I)c23)cc1Br

  9 Training on 13526 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.424018
Reward: 4.171525
Trajectories with max counts:
29	COc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cc1Br
Mean value of predictions: 0.6413471
Proportion of valid SMILES: 0.603690960275258
Sample trajectories:
BrCCNc1cc2ncnc(Nc3cccc(Br)c3)c2cc1-c1ccc2c(c1)OCO2
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1cc2ccccc2cn1
Brc1ccc(Nc2ncnc3cc(Br)c(Br)nc23)cc1Br
Brc1ccc(Nc2ncnc3ccc(-c4cc5c(Nc6cccc(Br)c6)ncnc5cc4Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(-c4ccc5ncnc(Nc6cccc(Br)c6)c5n4)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.6067203
Proportion of valid SMILES: 0.66051891216005
Sample trajectories:
BP(=O)(COC(=O)CNc1ccnc2ccccc12)c1cc2ncnc(Br)c2s1
Brc1ccc(-c2ncnc3ccc(NCCCCNc4ccc5ncnc(Nc6ccc(Br)c(Br)c6)c5n4)nc23)cc1
Brc1ccc(Br)nc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ccccc5c4)cc23)cc1Br
Brc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.6088602
Proportion of valid SMILES: 0.6776110068792995
Sample trajectories:
Brc1ccc(NC=Nc2cc3ncnc(Nc4ccc(Br)cc4)c3cc2-c2cccnc2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccnc(Nc5ccccc5)c4)nc23)cc1Br
Brc1ccc(Nc2ncnc3cc(Br)ncc23)cc1
Brc1ccc(Nc2ncnc3ccc(-c4cc5c(Nc6ccc(Br)cc6)ncnc5cc4Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1

 10 Training on 16581 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.723603
Reward: 4.198969
Trajectories with max counts:
20	COc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2nc1-c1ccc(Br)cc1
Mean value of predictions: 0.6519311
Proportion of valid SMILES: 0.6738789589212919
Sample trajectories:
Brc1ccc(-c2nc3c(Nc4cccc(Br)c4)ncnc3cc2Br)nc1
Brc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Brc1ccc(Nc2ncnc3cc(Br)ncc23)cc1
Brc1ccc(Nc2ncnc3ccc(-c4cc5c(Nc6ccc(Br)c(Br)c6)ncnc5cc4Br)cc23)cc1
Brc1ccc(Nc2ncnc3cccc(CNc4cc(Br)c(Br)c5ncnc(Nc6ccc(Br)cc6)c45)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.6383622
Proportion of valid SMILES: 0.702626641651032
Sample trajectories:
BrCCCCCBr
Brc1ccc(-c2cc3c(Nc4ccccc4)ncnc3cc2Br)cc1
Brc1ccc(CN2CCCCN=C(Nc3nccs3)CC2)s1
Brc1ccc(Nc2ncnc3c2c(-c2cc4c(Nc5ccc(Br)c(Br)c5)ncnc4cc2Br)cc2ncncc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4cc5cccc(Br)c5s4)cc23)cc1
Fine tuning...
Mean value of predictions: 0.5992787
Proportion of valid SMILES: 0.6935584740462789
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccc2c(c1)OCO2
Brc1ccc(-c2cc(Nc3ncnc4cccc(Br)c34)ccc2Br)cc1
Brc1ccc(NCCc2ccccc2Nc2ncnc3ccc(Br)nc23)cc1
Brc1ccc(Nc2ncnc3ccc(-c4nc5cc(Nc6cccc(Br)c6)ncc5cc4Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1

 11 Training on 19847 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.908925
Reward: 4.304488
Trajectories with max counts:
21	Oc1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.66086954
Proportion of valid SMILES: 0.6546762589928058
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Brc1cc2ncnc(Nc3cc(Br)c(Br)c(Br)c3Br)c2cc1Nc1ccc2nccnc2c1-c1ccc2ncncc2c1
Brc1ccc(-c2cc3c(Nc4ccc(Br)cc4Br)ncnc3cc2Br)cc1
Brc1ccc(Nc2nc3c(Nc4ccccc4)ncnc3cc2Br)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5nc[nH]c5n4)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.63932437
Proportion of valid SMILES: 0.7224655819774718
Sample trajectories:
Brc1cc(Br)c2nccnc2n1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Brc1ccc(-c2cc3c(Nc4cccc(Br)c4)ncnc3cc2Br)nc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncncc5n4)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(N4CCN(CCNc5nc6ccccc6nc5-c5ccsc5)CC4)cc23)cc1Br
Fine tuning...
Mean value of predictions: 0.6215568
Proportion of valid SMILES: 0.6786495779931229
Sample trajectories:
Brc1ccc(NNc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccccc4)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(-c4cc5c(Nc6ccc(Br)cc6)ncnc5cc4Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(-c4cc5c(Nc6ccc(Br)s6)ncnc5cc4Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1

 12 Training on 23220 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.012376
Reward: 4.518646
Trajectories with max counts:
176	Oc1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.56736994
Proportion of valid SMILES: 0.444375
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2nc1-c1ccccc1
Brc1ccc(CN2N=Nc3ccccc32)cc1
Brc1ccc(Nc2ncnc3cc(ncn3)Nc3ccccc32)cc1
Brc1ccc(Nc2ncnc3ccc(-c4c(Br)ccc5ncccc45)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)c(-c4ccc5nccnc5c4)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.6231884
Proportion of valid SMILES: 0.6684375
Sample trajectories:
Brc1cc2ncnc(Nc3ccccn3)c2cc1OCCc1ccccc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5cccnc5c4)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(-c4ccccn4)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)nc23)cc1
Brc1ccc(Nc2ncnc3ncnc(NCc4ccnc(Nc5ncnc6ccccc56)n4)c23)cc1
Fine tuning...
Mean value of predictions: 0.6217895
Proportion of valid SMILES: 0.6740625
Sample trajectories:
Brc1cc2c(Nc3ccc(Br)c4cnccc34)ncnc2cn1
Brc1ccc(-c2csc3ncnc(Nc4ccc(Br)c(Br)c4)c23)cc1
Brc1ccc(-n2cc3ncnc(Nc4ccc(Br)s4)c3n2)cc1
Brc1ccc(Nc2nc3c(Nc4ccccc4)ncnc3cc2Br)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1

 13 Training on 25948 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.883362
Reward: 5.344051
Trajectories with max counts:
30	COc1cc2ncnc(Nc3ccc(Br)c(Cl)c3)c2cc1-c1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Mean value of predictions: 0.7228974
Proportion of valid SMILES: 0.6456371625863151
Sample trajectories:
BP(=O)(N(O)C=O)N(CC(Cl)Cl)P(=O)(O)O
Brc1cc(Br)c2oc(-c3cc4c(-c5ccc(Br)c(Br)c5)ncnc4cc3Br)cc2c1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4cc5ccccc5nc4-c4ccnc(Nc5ccc(I)cc5)n4)cc23)nc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4cc5ccccc5nc4N4CCCCC4)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccsc4)cc23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.67205185
Proportion of valid SMILES: 0.7236636448890278
Sample trajectories:
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cn1
Brc1ccc(Nc2cc3c(cn2)cc(Br)n3-c2ccc3ncnc(Nc4ccc(Br)cc4)c3n2)cc1
Brc1ccc(Nc2nc3c(Nc4ccccc4)ncnc3cc2Br)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncncc5c4)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncncc5c4)cc3O2)cn1
Fine tuning...
Mean value of predictions: 0.6797045
Proportion of valid SMILES: 0.7195121951219512
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6ccc(Br)c(Br)c6)c5n4)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccccc4)cc23)s1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc(OCCc4cccc5nccnc45)c(Nc4cccc5ccncc45)nc23)cc1
Brc1ccc(Nc2ncnc3ccc(-c4ccccc4Br)nc23)cc1

 14 Training on 29719 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 33.927157
Reward: 5.490017
Trajectories with max counts:
49	Oc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Mean value of predictions: 0.7324201
Proportion of valid SMILES: 0.5475
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccc2ccccc2c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccc2ncnc(Nc3ccccc3)c2n1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1cccc2ccccc12
Brc1ccc(-c2cc3c(Nc4ccccc4)ncnc3cc2Br)cc1
Brc1ccc(-c2cccc(Nc3ncnc4ncc(Br)cc34)c2)c(-c2ccccc2)c1
Policy gradient replay...
Mean value of predictions: 0.70395756
Proportion of valid SMILES: 0.6475
Sample trajectories:
Bc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cc1-c1cc2ccccc2nc1Nc1ccccc1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccc2c(n1)-c1ccccc1N2
Brc1ccc(Nc2ccnc3cc(-c4nc5ccccc5cc4-c4cc5c(Nc6cccc(Br)c6)ncnc5cc4Br)c(Nc4ccccc4)nc23)cc1
Brc1ccc(Nc2ncnc3cc(-c4cc5c(Nc6cccc(Br)c6)ncnc5cc4Br)c(-c4ccc5ncnc(Nc6cccc(Br)c6)c5c4)cc23)cc1
Fine tuning...
Mean value of predictions: 0.7038175
Proportion of valid SMILES: 0.71285580231467
Sample trajectories:
BP(=O)(NCCCN)C(=O)O
Brc1cc2ncnc(Nc3ccc(Br)c(Br)n3)c2cc1Br
Brc1ccc(Nc2cccc(Nc3ncnc4ccc(Br)cc34)c2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4cc5ncncc5s4)cc23)s1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6ccccc6)c5c4)cc23)c(Br)c1

 15 Training on 33399 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.830592
Reward: 5.542977
Trajectories with max counts:
88	Oc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Mean value of predictions: 0.7469751
Proportion of valid SMILES: 0.526875
Sample trajectories:
BP(=O)(OCC)Oc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Nc2ncnc3cc(Br)ncc23)ccn1
Brc1cc2ncnc(-c3cccnc3)c2nc1-c1ccc2ncnc(Nc3cccnc3)c2n1
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1ccc(-c2ncnc3ccc(I)nc23)nc1
Policy gradient replay...
Mean value of predictions: 0.6611681
Proportion of valid SMILES: 0.7013758599124453
Sample trajectories:
Brc1cc2ncnc(Nc3cccc4ccccc34)c2cc1Br
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1cccc2cccnc12
Brc1ccc(-c2nc3c(Nc4cccc(Br)c4)ncnc3cc2Br)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6ccccc6)c5c4)cc23)cc1Br
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncncc5c4)cc23)nc1
Fine tuning...
Mean value of predictions: 0.70769227
Proportion of valid SMILES: 0.6962104603820858
Sample trajectories:
Bc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Brc1ccc(-c2c(Br)ccc3ncnc(Nc4cccc(Br)c4)c23)cc1
Brc1ccc(Nc2ncc(Br)cc2Nc2cccc3c(-c4ccncc4)ccc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6ccc(Br)c(Br)c6)c5n4)cc23)cc1Br
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncncc5c4)cc23)cc1

 16 Training on 37005 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.217262
Reward: 5.809573
Trajectories with max counts:
15	CS(=O)(=O)Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
15	CS(=O)(=O)Nc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Mean value of predictions: 0.67815995
Proportion of valid SMILES: 0.727557084766969
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2nc1Br
Brc1ccc(Br)c(Nc2ncnc3ccc(-c4ncnc5ccc(Br)cc5c5c(Br)c(Br)ccc-5N4)cc23)c1
Brc1ccc(Nc2cc(-c3ccc(Br)c4ncncc34)ncn2)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(I)nc23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.72436684
Proportion of valid SMILES: 0.7171938615721891
Sample trajectories:
Brc1ccc(-c2nc3c(Nc4cccc(Br)c4Br)ncnc3cc2Br)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6ccc(Br)c(Br)c6)c5n4)nc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6ccccc6)c5n4)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccccc4)nc23)cc1Br
Brc1ccc(Nc2ncnc3cc(Br)c(-c4cccnc4)nc23)cc1Br
Fine tuning...
Mean value of predictions: 0.687489
Proportion of valid SMILES: 0.7100406631216766
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Nc4ccc5ncnc(Nc6cccc(Br)c6)c5n4)cc23)c1
Brc1ccc(-c2nc3c(Nc4ccc(Br)c(Br)c4)ncnc3cc2Br)cc1
Brc1ccc(-c2nc3c(Nc4ccc5ccccc5c4)ncnc3cc2Br)cc1
Brc1ccc(Nc2cc(Nc3ncnc4ccc(Br)cc34)ccc2Br)cc1
Brc1ccc(Nc2nc3c(Nc4cc(Br)cc(Br)c4)ncnc3cc2Br)cc1

 17 Training on 41027 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.946223
Reward: 6.058468
Trajectories with max counts:
271	CNc1ccc2ncnc(Nc3cccc(Br)c3)c2n1
Mean value of predictions: 0.6612322
Proportion of valid SMILES: 0.5025015634771732
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2ncncn2c1-c1ccc2ncnc(Nc3ccccc3)c2n1
Brc1ccc(-c2nc3c(Nc4ccc(Br)c(Br)c4)ncnc3cc2Br)nc1
Brc1ccc(Br)c(-c2cc3ncnc(Nc4ccccc4)c3cn2)c1
Brc1ccc(Nc2ncnc3cc(Br)c(Nc4cccc(Br)n4)nc23)cc1
Policy gradient replay...
Mean value of predictions: 0.7134047
Proportion of valid SMILES: 0.730829420970266
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1Br
Brc1ccc(-c2nc3c(Nc4ccc(Br)c(Br)c4)ncnc3cc2Br)cc1
Brc1ccc(Nc2cc3c(Nc4cccc(Br)c4)ncnc3cc2Br)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5cccnc5c4)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Nc4cccnc4)nc23)cc1Br
Fine tuning...
Mean value of predictions: 0.718775
Proportion of valid SMILES: 0.7049436795994993
Sample trajectories:
BP(=O)(NC(CP(=O)(O)CCCCl)NC(=O)CN)C(=O)O
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Brc1ccc(Nc2nc3c(Nc4cccc(Br)c4)ncnc3cc2Br)cc1
Brc1ccc(Nc2ncnc3c2CCCN=C(c2ccc(-c4ccc(Br)cc4)o2)S3)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6cccc(Br)c6)c5c4)cc23)cc1

 18 Training on 44661 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 39.680834
Reward: 5.875485
Trajectories with max counts:
14	COc1cc2ncnc(Nc3cccc(Br)c3)c2cc1-c1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Mean value of predictions: 0.67948306
Proportion of valid SMILES: 0.7825537294563844
Sample trajectories:
BrCCN=CNc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc(Nc2ccc(I)c(Br)c2)cc2cc(Br)n2c1
Brc1ccc(Nc2nc3ccc(Nc4ccc(Br)c(Br)c4)cc3cc2Br)cc1
Brc1ccc(Nc2nccc3cc(Br)c(Br)cc23)cc1Br
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5nccn5c4)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.6632168
Proportion of valid SMILES: 0.6705220381369178
Sample trajectories:
BP(=O)(OCC)C(Nc1cccc2ncnc(Nc3ccc(Br)cc3)c12)OP(=O)(O)O
BP(=O)(c1cc2ccccc2nc1N)N(O)CNc1ccc2ccccc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1ccc(-c2c(-c3cc(Br)ccc3Br)ccc3ncncc23)cc1
Brc1ccc(-c2cc(Br)c(Br)cc2Br)cc1Br
Fine tuning...
Mean value of predictions: 0.689713
Proportion of valid SMILES: 0.708033760550172
Sample trajectories:
Brc1cc2c(nc1-c1ccc3ncnc(Nc4ccccc4)c3c1)N(CCCNc1ccccc1)CC2
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1cccc2ccccc12
Brc1cc2ncnc(Nc3ccccc3)c2nc1-c1ccc2ncncc2c1
Brc1ccc(-c2cc(Br)ccc2-n2cnc3ccccc32)cc1
Brc1ccc(-c2cc3c(Nc4cccc(Br)c4)ncnc3cc2Br)cc1

 19 Training on 48573 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 38.917978
Reward: 6.035144
Trajectories with max counts:
31	COc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2nc1-c1ccc(Br)cc1
Mean value of predictions: 0.7878252
Proportion of valid SMILES: 0.6760917373546969
Sample trajectories:
BP(=O)(N=C(CCl)N[PH](=O)(OCOc1cc(Br)c(Br)c(Br)c1)(P(=O)(O)O)P(=O)(O)O)OCC
BrC(=NNc1ccc(Br)cc1)c1ccncn1
Brc1c[nH]c(-c2c(Br)ccc3ncnc(Nc4cc(Br)c(Br)c(Br)c4)c23)c1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(-c4ccnc(Nc5ccccc5)c4)nc23)cc1Br
Brc1cc(Nc2ncnc3ccc(Br)nc23)ccn1
Policy gradient replay...
Mean value of predictions: 0.75051546
Proportion of valid SMILES: 0.6670834635823695
Sample trajectories:
Bc1cccc(I)c1-c1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cc1-c1cc2ccccc2s1
Brc1ccc(Br)c(Br)c1
Brc1ccc(N=NN2CCN(c3ccc4ncnc(Nc5ccccc5)c4c3)CC2)cc1
Brc1ccc(Nc2ccnc3cc(Br)c(Br)cc23)cc1
Brc1ccc(Nc2ncnc3cc(-c4cc5c(Nc6ccccc6Br)ncnc5cc4Br)c(-c4ccc5ncnc(Nc6cccc(Br)c6)c5c4)cc23)cc1
Fine tuning...
Mean value of predictions: 0.73330265
Proportion of valid SMILES: 0.6790741319987488
Sample trajectories:
Bc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2nc1Br
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(-c4ccc(Br)o4)nc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(-c4ccc5ccccc5n4)cc23)c1
Brc1cc(Nc2ncnc3ccc(Br)c(-c4cccc5ncnc(Nc6ccccc6)c45)c23)c(Br)s1
Brc1ccc(-c2cc3c(Nc4ccccc4)ncnc3cc2I)cc1

 20 Training on 52783 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.711439
Reward: 6.024861
Trajectories with max counts:
44	COc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Mean value of predictions: 0.7439817
Proportion of valid SMILES: 0.5479837449202876
Sample trajectories:
BP(=O)(OCCC)C(F)(F)F
Bc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2nc1-c1ccc2ncncc2c1
Bc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Brc1cc(Br)c(-c2ccc3ncnc(Nc4ccccc4)c3c2)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Nc4cccc5ncnc(Nc6ccc(Br)c(Br)c6)c45)nc23)c1
Policy gradient replay...
Mean value of predictions: 0.74732006
Proportion of valid SMILES: 0.6125703564727955
Sample trajectories:
BP(=O)(OCC)OC(=O)CN(CCCl)NS(=O)(=O)c1ccc2ncn(CCN3CCCCC3)c2n1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1ccc(-c2nc3c(Nc4ccc(Br)c(Br)c4)ncnc3cc2Br)o1
Fine tuning...
Mean value of predictions: 0.7527307
Proportion of valid SMILES: 0.6641651031894934
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1ccc(-c2cc3c(Nc4cccc(Br)c4)ncnc3cc2Br)cc1
Brc1ccc(Br)s1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4cc5ccccc5nc4-c4ccc(Br)cc4)cc23)cc1

Trajectories with max counts:
94	COc1cc2ncnc(Nc3cccc(Br)c3)c2cc1-c1ccc(Br)cc1
Mean value of predictions: 0.7301094
Proportion of valid SMILES: 0.5430161310491435
Mean Internal Similarity: 0.5435369220784486
Std Internal Similarity: 0.09976365706121024
Mean External Similarity: 0.45459583135875975
Std External Similarity: 0.0757959234777908
Mean MolWt: 559.8091777231779
Std MolWt: 146.18463003928454
Effect MolWt: 0.4524201643188788
Mean MolLogP: 6.805852709254711
Std MolLogP: 2.2204220163498736
Effect MolLogP: 0.9627160653726812
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 98.695951% (1438 / 1457)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/empty.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5878.6037940979, 'valid_fraction': 0.5430161310491435, 'active_fraction': 0.7029360967184801, 'max_counts': 94, 'mean_internal_similarity': 0.5435369220784486, 'std_internal_similarity': 0.09976365706121024, 'mean_external_similarity': 0.45459583135875975, 'std_external_similarity': 0.0757959234777908, 'mean_MolWt': 559.8091777231779, 'std_MolWt': 146.18463003928454, 'effect_MolWt': 0.4524201643188788, 'mean_MolLogP': 6.805852709254711, 'std_MolLogP': 2.2204220163498736, 'effect_MolLogP': 0.9627160653726812, 'generated_scaffolds': 1457, 'novel_scaffolds': 1438, 'novel_fraction': 0.9869595058339052, 'save_path': '../logs/replay_data_s2-1.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.698529
Reward: 1.000000
Mean value of predictions: 0.0011881188
Proportion of valid SMILES: 0.7910401002506265
Sample trajectories:
BP(=O)(OC(C)C)C1CCCC=CC1OC(=O)CCC(N)=O
Brc1cc(Br)c2c(c1Br)C1CC=CC(CO2)O1
Brc1ccc(-c2ccc3oc(N4CCCC4)nc3n2)cc1
Brc1ccco1
C#CCC#Cc1ccn(C(=O)N2CCN(c3ccccc3)CC2)c1
Policy gradient replay...
Mean value of predictions: 0.0028673834
Proportion of valid SMILES: 0.7846875
Sample trajectories:
Brc1ccc2c(c1)Sc1ccccc1O2
Brc1ccc2ccccc2c1
Brc1ccccc1
Brc1ccccc1-c1cc2ccccc2cc1-c1ccccc1
Brc1ccccc1-c1cc2ccccc2nc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.018753069
Proportion of valid SMILES: 0.6377582968065122
Sample trajectories:
Brc1ccc(-c2[nH]ncc2-c2cccc(-c3cccc(Br)c3)c2)cc1
Brc1ccc(Nc2ncnc3cnccc23)nc1
C#CCCC12CC1(CO)CC1C2N1CCCc1nsc(CN2CCC(N3CCN(CC)CC3)O2)n1
C#CCNC(=O)C1=NN(Cc2cn(CC)cn2)Nc2ccccc21
C#Cc1c(-c2ccccc2)ccc(Nc2cc(-c3ccccc3)c(C)nn2)c1C#N

  2 Training on 287 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.377350
Reward: 1.040467
Trajectories with max counts:
6	Oc1ccc2ccccc2c1
Mean value of predictions: 0.022521008
Proportion of valid SMILES: 0.7439824945295405
Sample trajectories:
Brc1ccc(-c2ccccc2)o1
Brc1ccc(Nc2nc(Nc3ccccc3)nc3ccccc23)cc1
Brc1ccc(Nc2ncc3sc(-c4ccc(Br)o4)nc3n2)cc1
Brc1ccc(Nc2ncnc3cnc(Nc4cccc(Br)c4)nc3-2)cc1
Brc1ccc2ncc(Nc3ccccc3Br)nc2c1
Policy gradient replay...
Mean value of predictions: 0.005430211
Proportion of valid SMILES: 0.8176985616010006
Sample trajectories:
Brc1cc2ccccc2cc1-c1ccc2ccccc2c1
Brc1ccc(Nc2ccccc2Nc2ccccc2Br)cc1
Brc1ccc(Nc2cccnc2)cc1
Brc1cccc(Nc2ncccc2-c2ccccc2)c1
Brc1ccccc1-c1cccc2ccccc12
Fine tuning...
Mean value of predictions: 0.04758621
Proportion of valid SMILES: 0.6359649122807017
Sample trajectories:
BrCCOc1ccc(-c2ccccn2)c2ccccc12
Brc1ccc(-c2nc3cccnc3s2)o1
Brc1ccc(-c2ncc[nH]2)s1
Brc1ccc(Br)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1

  3 Training on 531 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.292418
Reward: 1.334564
Trajectories with max counts:
11	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.055350196
Proportion of valid SMILES: 0.6425
Sample trajectories:
BP(=O)(CCC)C(=O)Nc1ccc(Br)cc1
Brc1ccc(-c2ccncc2)o1
Brc1ccc(-c2nc(-c3cncnc3)c3ccccc3n2)cc1
Brc1ccc(Nc2ccncn2)cc1
Brc1ccc(Nc2nc3ccccc3[nH]2)cc1
Policy gradient replay...
Mean value of predictions: 0.064326465
Proportion of valid SMILES: 0.7126328955597249
Sample trajectories:
BP(=O)(c1ccccc1)N1CCOCC1
Brc1cc2c(Br)cccc2c2ccccc12
Brc1ccc(N2C=C(Nc3nccs3)c3ccccc3C23COC(N2CCN(c4ccccc4)CC2)C3)cc1
Brc1ccc(N=Nc2ccncc2)cc1
Brc1ccc(Nc2nc3ccccc3nc2c2ncnc3ccccc32)cc1
Fine tuning...
Mean value of predictions: 0.10121774
Proportion of valid SMILES: 0.641963727329581
Sample trajectories:
BP(=O)(CCN1CCCC(F)C(F)C1)OCC
BP(=O)(N(O)COP(=O)(O)OP(=O)(O)O)P(=O)(Nc1cccc(F)c1)OCOC(=O)N(O)C(CC(=O)O)NS(=O)(=O)CCl
BP(=O)(NO)c1cccc(Cl)c1
BrCc1ccc2c(Nc3ccccc3-c3cccnc3-c3cncnc3-c3ccccc3Br)ncnc2c1
Brc1ccc(-c2ccc3[nH]ccc3c2)c2ccccc12

  4 Training on 1203 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 20.076134
Reward: 1.582864
Trajectories with max counts:
90	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.17416807
Proportion of valid SMILES: 0.554582421019706
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(cc1CSc1nc3ccccc3nc1Nc1ncnc3sc4ccccc4c13)Sc1ccccc1N2
Brc1ccc(-c2cnc(Nc3ccccc3)nc2)cc1
Brc1ccc(Br)c(-c2ccc(Nc3ccc(Br)s3)cc2)c1
Brc1ccc(C=C2NC(c3c[nH]c4ccccc34)=NSS2)cc1
Policy gradient replay...
Mean value of predictions: 0.16084905
Proportion of valid SMILES: 0.5304973412574289
Sample trajectories:
Bc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc2ncnc(Nc3ccccc3)c2nc1CN1CCCCC1
Brc1ccc(-c2ccccc2Br)cc1
Brc1ccc(-c2ncnc3c(Nc4ccccc4Br)ncnc23)s1
Brc1ccc(Br)c(Nc2ncc3ncnc(-c4ccccc4)c3n2)c1
Fine tuning...
Mean value of predictions: 0.17446353
Proportion of valid SMILES: 0.5828642901813633
Sample trajectories:
BP(=O)(C=O)OCC
BrCCOc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(-c2ccccc2)cc1Nc1ncnc2ccccc12
Brc1ccc(-c2ccccc2)nc1Nc1ncc2ccccc2n1
Brc1ccc(-c2nc(-c3ccccc3)c3c(-c4ccccc4Br)ncnc3n2)s1

  5 Training on 2438 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 19.652863
Reward: 2.065749
Trajectories with max counts:
422	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.09520295
Proportion of valid SMILES: 0.508125
Sample trajectories:
Bc1cnccc1N=C(Nc1ccccc1N)C(N)=O
Brc1cc2ccccc2cc1-c1cccc(Nc2nccs2)c1
Brc1ccc(-c2ccc(Nc3cccs3)cc2)cc1
Brc1ccc(-c2ccccc2)c2ccccc12
Brc1ccc(NCCCN2CCN(c3ccccc3Br)CC2)s1
Policy gradient replay...
Mean value of predictions: 0.17282984
Proportion of valid SMILES: 0.6373866833385433
Sample trajectories:
BrCc1ccccc1-n1ccc2ccccc21
Brc1ccc(Br)c(Nc2cccc(Nc3ccccc3N=Cc3ccccc3Br)n2)c1
Brc1ccc(Br)c(Nc2ncnc3ncnc(Nc4ccccc4Br)nc3n2)c1
Brc1ccc(Nc2ncnc(Nc3cccc(Br)c3)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.18297435
Proportion of valid SMILES: 0.609375
Sample trajectories:
BP(=O)(OCC)OC(=O)CCC(=O)OC(C)C=CC=CBr
Bc1ccccc1-c1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(-c2cccnc2)c2ncnc(Nc3ccccc3)c2c1Nc1ccccc1
Brc1cc(Br)c(Br)c(-c2ccccc2-c2ccccc2)c1
Brc1ccc(-c2[nH]ccc2Br)cc1

  6 Training on 3542 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 19.323853
Reward: 2.696473
Trajectories with max counts:
1084	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.18702596
Proportion of valid SMILES: 0.313125
Sample trajectories:
Brc1cccc(Nc2ncnc3ccccc23)c1
Brc1ccccc1-c1ccccc1-c1ccccc1Nc1ccccc1Nc1ncnc2ccccc12
Brc1ccccc1-c1ccccc1Nc1ncccc1Nc1ccccc1
Brc1ccccc1-c1ccccc1Nc1ncnc2ccccc12
Brc1ccccc1-c1cnccc1Nc1ncnc2ccccc12
Policy gradient replay...
Mean value of predictions: 0.26840797
Proportion of valid SMILES: 0.5028142589118199
Sample trajectories:
BP(=O)(NC(C)(COC(=O)CCCl)OC)OCCF
BP(=O)(OCC)OC(=O)CCCCCCC(=O)CI
BP(=O)(OCC1OC(OP(=O)(O)O)C(O)C1O)n1cnc2c(N)ncnc21
BrC1=NNc2ncc(Br)c3cccc1c23
BrC1=Nc2sc3c(c2C=C1)CCCCC3
Fine tuning...
Mean value of predictions: 0.26655114
Proportion of valid SMILES: 0.5417840375586854
Sample trajectories:
BP(=O)(C(=O)Oc1ccc(F)cc1)N1CCOCC1
Brc1cc(Nc2ncnc3ncnc(Nc4ccncc4Br)c23)ccn1
Brc1ccc(-c2ccccc2)cc1Nc1ncnc2ccccc12
Brc1ccc(-c2nc3ncccc3s2)cc1
Brc1ccc(Br)c(Nc2ncnc3cc(Br)ccc23)c1

  7 Training on 4760 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 21.623724
Reward: 2.868256
Trajectories with max counts:
439	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.22054055
Proportion of valid SMILES: 0.4625
Sample trajectories:
BP(=O)(OCCCC)OCOP(=O)(O)OP(=O)(O)O
BP(=O)(Oc1ccccc1P(=O)(Oc1ccccc1)c1ccccc1)C(O)CCC
BP1(=O)CCN1CCS(=O)(=O)Nc1ccc(Br)cc1
BrC1=Nc2ccccc2Nc2ccccc2S1
BrSc1ccccc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Policy gradient replay...
Mean value of predictions: 0.3390625
Proportion of valid SMILES: 0.5209768315591734
Sample trajectories:
BP(=O)(N=Nc1ccc(I)cc1)N(O)CP(=O)(O)O
BrCCOc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(N2CCCC2CN2CCC(N3CCOCC3)CC2)c2c(Nc3cccc(-c4ccccc4Br)c3)ncnc2c1
Brc1ccc(-c2cnc3sc(Nc4ccccc4)nc3c2)cn1
Brc1ccc(Br)c(Nc2cc(Br)c(Br)cn2)c1
Fine tuning...
Mean value of predictions: 0.27404994
Proportion of valid SMILES: 0.5758049390434511
Sample trajectories:
BP(=O)(Nc1ccc(F)cc1)c1ccc(F)cc1F
Bc1cccc(Nc2ncnc3cc(Cl)ccc23)c1
BrCCc1ccncc1Nc1ncnc2ccccc12
Brc1cc2c(nc1Nc1ncnc3sc(-c4ccccc4)cc13)Sc1ccccc1N2
Brc1ccc(-c2ccccc2Nc2ccccc2Br)o1

  8 Training on 6190 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 22.429478
Reward: 3.021695
Trajectories with max counts:
82	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.33915246
Proportion of valid SMILES: 0.4945261182358461
Sample trajectories:
BP(=O)(OCC)OC(=O)C1OP(F)(=S)OC1(F)F
Brc1cc2ccc1Oc1cccc3nc[nH]c3c(ncn1)N2
Brc1ccc(-c2nc(Nc3cccs3)[nH]c2-c2ccccc2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ncnc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.25866947
Proportion of valid SMILES: 0.5965625
Sample trajectories:
BP(=O)(Nc1cccc(Nc2ccccc2Cl)c1)NS(=O)(=O)c1ccc(Br)cc1
BP(=O)(Oc1ccccc1Br)P(=O)(Oc1ccccc1)Oc1ccc(Br)cc1
Bc1ccc(Nc2ccccc2Br)c(Br)c1Nc1ccccc1Br
BrC1=Nc2ccccc2-c2ccc(Br)c(Br)c2S1
BrCCCCC1=Nc2sc3ccccc3c2OC1=Nc1ccccc1
Fine tuning...
Mean value of predictions: 0.2955665
Proportion of valid SMILES: 0.5712945590994372
Sample trajectories:
BP(=O)(NC(=O)OCC)c1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CSCCP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(c1ccccc1Nc1c(N)cc(Cl)cc1O)N(O)CC=O
Bc1[nH]c2ccccc2c1-c1nc2cc(Nc3ccc(Br)cc3)cc(Br)cn12
BrC=C1N=Nc2ccc(Br)cc2Nc2cc(Br)ccc21

  9 Training on 7707 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 23.830911
Reward: 3.064039
Trajectories with max counts:
62	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.2935501
Proportion of valid SMILES: 0.5961237886839638
Sample trajectories:
BP(=O)(OCC)C1Oc2sc(Br)cc2N1C(=O)c1cc(Br)ccc1Br
BrCBr
Brc1c(-c2ccccc2)cccc1-c1cnc2ccccn12
Brc1cc(-c2ccccc2)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2sccc12
Policy gradient replay...
Mean value of predictions: 0.39168197
Proportion of valid SMILES: 0.5112570356472795
Sample trajectories:
BP(=O)(N1CCN(C(=O)Oc2ccc(Cl)c(F)c2)CC1)C(F)(F)F
BP(=O)(OCC1OC(n2cnc3c(N)cc(Br)cc32)C(O)(O)C1O)C(=O)NS(=O)(=O)Oc1ccccc1
BP(=O)(OCCC)OCOP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BrC1(c2ccccc2)CC1=NNc1ncnc2sc3cccs3c2n1
Brc1cc(Br)c2ncnc(Nc3ccc(I)cc3)c2c1
Fine tuning...
Mean value of predictions: 0.38305473
Proportion of valid SMILES: 0.5423569865582994
Sample trajectories:
BP(=O)(NCc1ccccc1)P(=O)(O)O
BP(=O)(OCC)OP(=O)(O)OP(=O)(O)Nc1ccc(F)c(F)c1
BrC1=Nc2scnc2Nc2ccccc21
BrCc1nc2ncnc(Nc3cccc(-c4ccccc4Br)c3)c2s1
Brc1cc(-c2cccs2)c2c(Br)c(Br)cnc2c1-c1ccccc1

 10 Training on 9282 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 22.537999
Reward: 3.240157
Trajectories with max counts:
176	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.5040068
Proportion of valid SMILES: 0.36679174484052535
Sample trajectories:
BP(=O)(CC(F)(F)F)NO
BrBr
Brc1cc(Br)c2ncsc2c1
Brc1cc(Nc2nc3ccccc3s2)sc1Br
Brc1cc(Nc2ncnc3sccc23)sc1Nc1ncnc2sc3ccccc3c12
Policy gradient replay...
Mean value of predictions: 0.3842627
Proportion of valid SMILES: 0.504375
Sample trajectories:
BP(=O)(NC(C1CCC1)P(=O)(O)O)n1cnc2c(N)ncnc21
BP(=O)(NCc1ccc(F)cc1)OP(=O)(Nc1cc(Br)cc(Br)c1)Oc1ccccc1Br
BP(=O)(OCC)Oc1nc2c(Br)cc(Br)cc2s1
BP(=O)(OCC)c1nc(-c2ccccc2)c2nc(-c3ccccc3)c(-c3ccccc3)c(Br)c(Br)c2[nH]1
BP(=O)(OCCCC)Oc1ccc(Nc2ncnc3sc4scnc4c3s2)cc1
Fine tuning...
Mean value of predictions: 0.38331428
Proportion of valid SMILES: 0.546875
Sample trajectories:
BP(=O)(OCC(=O)Nc1ccc(F)cc1F)c1cccc(F)c1
BP(=O)(OCC)C(=O)NNc1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CCCCOP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)CC(F)(F)F
B[PH](=O)(Br)(Br)C(Br)=C(Br)Br
Bc1ccc(Nc2ncnc3ccsc23)cc1

 11 Training on 10634 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.040359
Reward: 3.436797
Trajectories with max counts:
323	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4588517
Proportion of valid SMILES: 0.391875
Sample trajectories:
BP(=O)(CCC=C(Br)Br)OCC
BP(=O)(NCc1ccc(Br)cc1)[PH](=O)(Nc1cncs1)(Oc1ccccc1Nc1ccc(N)cc1)C(F)(F)F
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1O
Bc1cccc(Br)c1Nc1ncnc2ccccc12
Bc1cccc(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.440882
Proportion of valid SMILES: 0.524867062871442
Sample trajectories:
BP(=O)(COP(=O)(O)O)N(CCN)P(=O)(O)O
BP(=O)(OCBr)OC(=O)Cc1cnc(Br)[nH]1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Br
Fine tuning...
Mean value of predictions: 0.41147733
Proportion of valid SMILES: 0.5501719287277275
Sample trajectories:
BP1(=O)OCC2OC(NC(=O)OCC(COc3c(Br)cc(Br)cc3Br)O1)C(O)(n1cnc3c(Br)cc(Br)cc31)C(O)C2O
B[PH](=O)(=NO)Nc1ccc(Br)cc1
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc2c(N=Cc3ccccc3Br)ncnc2s1

 12 Training on 12073 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.384958
Reward: 3.817497
Trajectories with max counts:
291	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.49982083
Proportion of valid SMILES: 0.3488590184432635
Sample trajectories:
BP(=O)(CCC=CBr)OCC
BP(=O)(Cl)OP(=O)(O)OP(=O)(O)OP(O)(F)(F)Br
BP(=O)(N(O)C=O)n1cnc2c(N)ncnc21
BP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC)OC(=O)Nc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.47692308
Proportion of valid SMILES: 0.463849765258216
Sample trajectories:
BP(=O)(OCC1OC(O)C(O)C1O)OP(=O)(O)OP(=O)(O)Oc1ccc(Nc2ncc(Br)c(Br)c2Br)nc1
Bc1ccc(Nc2ncnc3c2c(Br)sc3c2nc3ccc(Br)cc3s2)cc1
Bc1ccc(Nc2ncnc3sc(Nc4ccccc4)cc23)cc1
BrC1=Nc2ncnc(Br)[n+]2s1
BrCc1nc2c(Nc3nc(-c4ccc(Br)cc4)c(Br)cc3Br)ncnc2s1
Fine tuning...
Mean value of predictions: 0.4195923
Proportion of valid SMILES: 0.5520475148483901
Sample trajectories:
BP(=O)(CC(=O)ON=C(Nc1ccc(Br)nc1)Oc1ccccc1)NO
Bc1ccccc1Nc1ncnc2sc(Nc3ccc(Br)c(Br)c3)nc12
BrC(=NNc1ncnc(Nc2ccc(Br)cc2)n1)c1ncnc2sc(Br)cc12
BrCCI
BrCc1ccccc1-c1nc(N2CCOCC2)c2ncnc(Nc3ccccc3)c2n1

 13 Training on 13568 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.813995
Reward: 3.502899
Trajectories with max counts:
66	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4378855
Proportion of valid SMILES: 0.567677399187246
Sample trajectories:
BP(=O)(C(=O)O)N1CCC(=O)Nc2sc(C(=O)Oc3ccccc3)cc21
Bc1cccc(Nc2ncnc3ccsc23)c1
BrCCNc1ccc(Nc2ncnc3ncnc(Nc4ccc(Br)cc4)c23)cc1
BrCc1ccccc1-c1ccccc1Nc1ncnc2sc(Nc3ccccc3Br)nc12
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2n1
Policy gradient replay...
Mean value of predictions: 0.40839326
Proportion of valid SMILES: 0.5214129415442326
Sample trajectories:
BP(=O)(NC(c1ccccc1)c1ccccc1)c1ccccc1
BP(=O)(OCCS)C(=O)N1CCC(Br)(CBr)CC1
BrC(Br)=Nc1ccccc1Nc1ncnc2nc(Br)ccc12
Brc1cc(Br)c2c(c1)Oc1cc(Br)sc1-2
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Fine tuning...
Mean value of predictions: 0.39943945
Proportion of valid SMILES: 0.5575
Sample trajectories:
Brc1cc(Br)c2sc3ncnc(Nc4ccccc4Oc4ccccc4Br)c3c2c1
Brc1cc(Nc2ccsc2)ccc1I
Brc1cc(Nc2ncnc3ccccc23)nc2ccccc12
Brc1cc2c(Nc3cccc(-c4ccccc4Br)c3)ncnc2s1
Brc1cc2c(Nc3cccc(I)c3)ncnc2s1

 14 Training on 15102 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.593015
Reward: 3.773487
Trajectories with max counts:
317	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.5105079
Proportion of valid SMILES: 0.356875
Sample trajectories:
BP(=O)(O)Oc1ccccc1Nc1ncnc2ccccc12
BP(=O)(O)c1ccccc1Nc1ncnc2c(Br)c(Br)ccc12
BP(=O)(OC)OC(=O)CBr
BP(=O)(OCC)C1(Cc2ccc(Br)cc2)OC(=O)C(C)(C)Oc2c1ccc(Br)c2Br
BP(=O)(OCC)OCC1NC(O)=CC(=N)O1
Policy gradient replay...
Mean value of predictions: 0.27874362
Proportion of valid SMILES: 0.4278125
Sample trajectories:
BP(=O)(CCOCCOCCC(=O)N1CCCN(Cc2ccc(Br)s2)O1)OCC
Bc1ccccc1-c1ccccc1Nc1ncnc2ccccc12
Bc1ccccc1N1cc2c3cccnc3Nc3ccccc3Nc3ccccc3N=CC=C2N(c2ccccc2)CC1
BrC=CC=CC=CCNc1ncnc2sc3ccccc3c12
BrCCOc1ccccc1Nc1ncnc2ccccc12
Fine tuning...
Mean value of predictions: 0.4305263
Proportion of valid SMILES: 0.534375
Sample trajectories:
BP(=O)(OCC)c1cc(Br)cc(Br)c1Br
BrC1=C(c2ccccc2)c2nc(-c3ccccc3)sc2-c2cc(Br)ccc2N=C1CN1CCOCC1
BrCCBr
BrCCCCCCCCBr
Brc1c(Br)n(-c2ccccc2)c2ccc(Nc3ncnc4ccccc34)cc12

 15 Training on 16316 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.354739
Reward: 3.694452
Trajectories with max counts:
167	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4548632
Proportion of valid SMILES: 0.41125
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCC=CCC=C)OCC
BP(=O)(OCC)OCCCCCC
BP(=O)(Oc1cc(Br)ccc1Br)OP(=O)(O)O
Bc1ccc(Nc2ccsc2)cc1Cl
BrBr
Policy gradient replay...
Mean value of predictions: 0.4954717
Proportion of valid SMILES: 0.49703032197561736
Sample trajectories:
BP(=O)(CCC(=O)Oc1ccccc1Nc1ncnc2sccc12)COCC
BP(=O)(CCC(=O)c1cc(Br)c(OP(=O)(O)O)c(Br)c1)OCC
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC)OC1CCC[P+](=O)([O-])Oc2ccc(Br)cc21
BP(=O)(OCC)Oc1ccc(Br)cc1S(=O)(=O)Nc1nccs1
Fine tuning...
Mean value of predictions: 0.45218405
Proportion of valid SMILES: 0.5367302281963113
Sample trajectories:
BP(=O)(N(O)C(F)F)P(=O)(Oc1ccccc1)Oc1ccc(Br)c(Br)c1
BP(=O)(OCC)OC(=O)Nc1ccc(Br)c(Cl)c1
BP(=O)(OCC)c1ccc(Br)cc1
BrC=Cc1ccccc1Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1Br
Brc1cc(Br)c2c(Br)n(-c3ccccc3Nc3ncnc4sccc34)c(-c3cccs3)c2c1

 16 Training on 17898 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.409496
Reward: 4.006271
Trajectories with max counts:
406	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5596014
Proportion of valid SMILES: 0.34510784620193813
Sample trajectories:
BP(=O)(NC1CCCC(C(=O)N2CCCCC2)C1)OS(=O)(=O)O
BP(=O)(Nc1cc(Br)c(Br)cc1Br)C(P(=O)(O)O)P(=O)(O)P(=O)(O)O
BP(=O)(Nc1cc(Br)cc(Nc2ccc(Br)c(Br)c2)c1)C(F)(F)P(=O)(O)O
BP(=O)(Nc1ccc(Br)c(Br)c1)c1ccc(Br)c(Br)c1
BP(=O)(OCC)c1ccc(Nc2ncnc3c(Br)c(Br)c(Br)c(Br)c(Br)c(Br)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.4288961
Proportion of valid SMILES: 0.5775
Sample trajectories:
Bc1ccccc1Nc1ncnc2cc(Br)ccc12
BrCCNc1ccc2ncnc(Nc3ccccc3-c3ccccc3Br)c2n1
BrCc1nc2c(-c3ccccc3Br)ncnc2s1
BrCc1sc2ncnc(Nc3ccc(Br)cc3)c2c1Br
BrSc1ccccc1-c1ccccc1Nc1ccccc1
Fine tuning...
Mean value of predictions: 0.43252948
Proportion of valid SMILES: 0.5571875
Sample trajectories:
BP(=O)(CCCO)c1cc2ncnc(Nc3ccc(Br)cc3F)c2s1
BP(=O)(OCCCC)Oc1ccc(Nc2ncnc3ccccc23)cc1
Bc1ccc(Nc2nc(Nc3cc4ccccc34)nc(-c3cccs3)c2-c2cncnc2)cc1
Bc1ccccc1Nc1ncnc2sc(Nc3ccccc3Br)nc12
BrC(=Nc1ccsc1)c1ccccc1Nc1cccc2c(Nc3ccccc3Br)ncnc12

 17 Training on 19549 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.230745
Reward: 3.590635
Trajectories with max counts:
40	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49202454
Proportion of valid SMILES: 0.5095342294467021
Sample trajectories:
BP(=O)(CCCCCl)NO
BP(=O)(Nc1ccc(Nc2ncnc3ccccc23)cc1)c1ccc(Br)cc1
BP(=O)(OCC)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCC1CCCN1C(=O)OP(=O)(O)CCC1CC1)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)OP(=O)(O)OP(=O)(OCC)OCCCCCC
Policy gradient replay...
Mean value of predictions: 0.49682155
Proportion of valid SMILES: 0.5114098155673648
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Br)cc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)c1cccc(Br)c1
Bc1cnc(Nc2cc3ccc(Br)cc3nc3c(Br)c(Br)nc23)[nH]1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccsc12
Fine tuning...
Mean value of predictions: 0.49479166
Proportion of valid SMILES: 0.5403377110694184
Sample trajectories:
BP(=O)(C=COc1ccc(Br)cc1)OC
BP(=O)(CCC=CC(C)=O)OCCC
BP(=O)(NOP(=O)(O)OP(=O)(O)O)P(=O)(O)OP(=O)(O)Oc1ccccc1Br
Bc1cccc(Nc2ncnc3ccsc23)c1
BrC=CBr

 18 Training on 21412 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.229218
Reward: 4.113505
Trajectories with max counts:
115	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.54767823
Proportion of valid SMILES: 0.4778125
Sample trajectories:
BIc1ccccc1Nc1ncnc2nc(-c3ccccc3)sc12
BP(=O)(OCC1CCCCC1)Oc1ccccc1Nc1ncnc(Nc2ccc(Br)c(Br)c2O)n1
Bc1ccccc1-c1cc(Nc2cccc(Br)c2)ncn1
Bc1ccccc1Nc1ncnc2sc(Br)cc12
Bc1ccccc1Nc1ncnc2sccc12
Policy gradient replay...
Mean value of predictions: 0.49370462
Proportion of valid SMILES: 0.51625
Sample trajectories:
Bc1ccc2c(Nc3ccccc3)ccnc2c1
Bc1ccccc1-c1ccccc1-c1ncnc2sc3ccccc3c12
Bc1ccccc1-c1ccccc1Nc1ncnc2sc(Nc3ccc(Br)s3)cc12
Bc1ccccc1Nc1ncnc2sccc12
BrCC=CCC=CC=NNc1ncnc2sc(Br)cc12
Fine tuning...
Mean value of predictions: 0.48607877
Proportion of valid SMILES: 0.5636136292591435
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)OCC1OC(c2cnc(Br)cc2Br)C(O)(CC)O1
Brc1c(-c2ccc(Br)c3c(Nc4ccccc4)ncnc23)ccc2c1CCC2
Brc1cc(Br)c2sc3ncnc(Nc4ccccc4Br)c3c2c1
Brc1cc(CN2CCCC2)c2c(Nc3ccc(Nc4ccccc4Br)cc3)ncnc2c1
Brc1cc(Nc2ncnc3sc(-c4ccsc4)cc23)cc2ccccc12

 19 Training on 23340 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.129191
Reward: 4.499421
Trajectories with max counts:
326	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.6182163
Proportion of valid SMILES: 0.329375
Sample trajectories:
BP(=O)(NCc1ccccc1)c1ccc(Nc2ccc(Br)cc2)s1
BP(=O)(Nc1ccc(Br)cc1Br)OCC=C
BP(=O)(Nc1nc(N)cs1)Nc1ccc(Br)cc1F
BP(=O)(OCC)OC(=O)C(Br)Br
BP(=O)(OCC)OC(=O)CN(c1ccccc1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.44122905
Proportion of valid SMILES: 0.559375
Sample trajectories:
BP(=O)(CCCl)NO
BP(=O)(Nc1ccccc1)c1ccc(Br)cc1
Bc1ccccc1Nc1ncnc2sc(Nc3cccc(Br)c3)nc12
BrC12Cc3ccccc3C1CNc1ccncc12
BrCc1nc2c(-c3ccccc3Br)ncnc2s1
Fine tuning...
Mean value of predictions: 0.5047934
Proportion of valid SMILES: 0.5671875
Sample trajectories:
BP(=O)(Nc1cc(Br)cc(Br)c1Br)c1nc2c(Br)c(Br)c(Br)c(Br)c2s1
B[PH](=O)(Cl)(Cl)OCCCl
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1

 20 Training on 25129 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.623426
Reward: 4.858248
Trajectories with max counts:
274	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.5507274
Proportion of valid SMILES: 0.3221875
Sample trajectories:
BP(=O)(NCCCCCCl)c1ccccc1Nc1cccc(Nc2ncnc3sccc23)c1
BP(=O)(c1ccccc1)N1CCC(F)(F)C1
BP(=O)(c1ccccc1Nc1ccccc1)N1CCCCC1
BP1(=O)OCC(O)C(Oc2ccccc2)N1Cc1cncnc1Cl
B[PH](=O)(OCC)=C(Br)Br
Policy gradient replay...
Mean value of predictions: 0.4937616
Proportion of valid SMILES: 0.5072055137844611
Sample trajectories:
BP(=O)(OCC)Oc1cccc(Br)c1Nc1c2c(F)c(Br)c(Br)cc2nc2c(F)c(F)c(F)c(F)c12
BP(=O)(c1cc(Br)cc(Nc2cncnc2)c1)S(=O)(=O)Nc1cc(F)cc(Cl)c1
BP1(=O)OCC2OC(=O)OCC2c2cc(N)c(F)c(F)c21
Bc1ccc(Nc2cc(-c3ccc4ccc(Br)cc4n3)nc(Br)c2F)cc1
BrCCC=CC=CC=CC=CC=C=NNc1ncnc2sccc12
Fine tuning...
Mean value of predictions: 0.51342136
Proportion of valid SMILES: 0.5684803001876173
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCl
Bc1ccccc1Nc1ncnc2sccc12
BrC(=Nc1ccc(Br)cc1)c1ccc(Nc2ncnc3sc(Br)cc23)cc1
BrC=CC(Br)Br
Brc1cc(-c2cccs2)sc1-c1ccsc1-c1cncc(Nc2ccc3ncnc(Br)c3c2)c1

Trajectories with max counts:
231	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.46509284
Proportion of valid SMILES: 0.4713383759454898
Mean Internal Similarity: 0.4655441047686505
Std Internal Similarity: 0.10073750341490033
Mean External Similarity: 0.4167015223187768
Std External Similarity: 0.06696205085652614
Mean MolWt: 414.1099825878116
Std MolWt: 93.67718164568349
Effect MolWt: -0.8567850736331483
Mean MolLogP: 5.700259867907537
Std MolLogP: 1.60665689782518
Effect MolLogP: 0.6390134862536615
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.936371% (1139 / 1163)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5549.702456712723, 'valid_fraction': 0.4713383759454898, 'active_fraction': 0.44177718832891244, 'max_counts': 231, 'mean_internal_similarity': 0.4655441047686505, 'std_internal_similarity': 0.10073750341490033, 'mean_external_similarity': 0.4167015223187768, 'std_external_similarity': 0.06696205085652614, 'mean_MolWt': 414.1099825878116, 'std_MolWt': 93.67718164568349, 'effect_MolWt': -0.8567850736331483, 'mean_MolLogP': 5.700259867907537, 'std_MolLogP': 1.60665689782518, 'effect_MolLogP': 0.6390134862536615, 'generated_scaffolds': 1163, 'novel_scaffolds': 1139, 'novel_fraction': 0.9793637145313844, 'save_path': '../logs/replay_data_s2-2.smi'}


  1 Training on 219 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.698529
Reward: 1.000000
Mean value of predictions: 0.0011881188
Proportion of valid SMILES: 0.7910401002506265
Sample trajectories:
BP(=O)(OC(C)C)C1CCCC=CC1OC(=O)CCC(N)=O
Brc1cc(Br)c2c(c1Br)C1CC=CC(CO2)O1
Brc1ccc(-c2ccc3oc(N4CCCC4)nc3n2)cc1
Brc1ccco1
C#CCC#Cc1ccn(C(=O)N2CCN(c3ccccc3)CC2)c1
Policy gradient replay...
Mean value of predictions: 0.036534447
Proportion of valid SMILES: 0.5996870109546166
Sample trajectories:
BP(=O)(OCC=Cc1csc(NC(=O)c2ncc(F)cc2Cl)c1)C1=C(NN=C=S)c2ccc(F)c(F)c2C1=O
Brc1ccc(-c2nccc(NN=Cc3cccnc3)n2)s1
Brc1ccc(-n2c(Nc3cccnc3)nnc2SCc2ccco2)cc1
Brc1ccc(CN2CCCn3nc(nc3-c3ccco3)C2)cc1
Brc1ccc(Nc2cc(Nc3ccnc4cncnc34)ncn2)cc1
Fine tuning...
Mean value of predictions: 0.03972675
Proportion of valid SMILES: 0.5950594121325828
Sample trajectories:
Brc1ccc(-c2nc(CNC3CCOCC3)n2)cc1
Brc1ccc(-c2nc3ccc(N4CCOCC4)c(Br)c3s2)cc1
Brc1ccc2[n+](c1)[N-]Cc1sccc1N2
Brc1ccc2c(Oc3ccccc3)ncnc2c1
Brc1cccc(N2CCN(c3ccccc3)CCCCNc3c(I)cccc32)c1

  2 Training on 423 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.526095
Reward: 1.043742
Trajectories with max counts:
3	Cc1ccc(Nc2ncnc3scnc23)cc1
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.058310326
Proportion of valid SMILES: 0.5662914321450907
Sample trajectories:
BP(=O)(NC1CCCCC1)c1ccc(Br)cc1
Brc1cc(Br)cc(C=NNc2nc(Br)cs2)c1
Brc1ccc(CSc2ncnc3c(Br)cnn23)cc1
Brc1ccc[n+](CCOCCc2ccccc2)n1
Brc1cccc(Nc2nc(Nc3ccc4c(c3)OCO4)nc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.03045085
Proportion of valid SMILES: 0.4242709313264346
Sample trajectories:
BP(=O)(OCOC(=O)c1cc(Nc2cncc(Cl)c2)n2nc(N)c(Cl)c2n1)C1CCOCC1
Brc1cc2nc(CN3CCCCC3)nc(Nc3cncnc3)n2n1
Brc1ccc(Nc2ncnc3sc(Br)c(-c4ncns4)c23)s1
Brc1ccc(Nc2ncnc3sc(Nc4ccc(Br)s4)nc23)cc1
Brc1cncs1
Fine tuning...
Mean value of predictions: 0.10485208
Proportion of valid SMILES: 0.5284552845528455
Sample trajectories:
Brc1cc(Nc2ncnc3cc(Br)c(Br)cc23)c(Br)s1
Brc1cc2nc(NC3CCCCN3)sc2s1
Brc1ccc(CNc2ccc3c(c2)c2[nH]ccc2-3)cc1
Brc1ccc(Nc2nc(-c3ccccc3)nc(-c3cscn3)n2)cc1
Brc1ccc(Nc2ncnc3ccnn23)cc1

  3 Training on 855 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.329274
Reward: 1.351441
Trajectories with max counts:
8	Oc1ccc(Nc2ncnc3sc4ccccc4c23)cc1
Mean value of predictions: 0.13394594
Proportion of valid SMILES: 0.578305720537668
Sample trajectories:
BP(=O)(c1ccc(Nc2cc(N(=O)=O)ccc2N2CCOCC2)cc1)N1CCOCC1
Brc1cc2c(Nc3cncnc3)ncnc2s1
Brc1ccc(C(Nc2cccnc2)c2ccccn2)cc1
Brc1ccc(N=Nc2ccc3c(c2)c2nc[nH]c2O3)cc1
Brc1ccc(NN=C(Nc2ncnc3scnc23)c2cccs2)cc1
Policy gradient replay...
Mean value of predictions: 0.200818
Proportion of valid SMILES: 0.4588676884579293
Sample trajectories:
Brc1cc2oc(CCN3CCOCC3)nc2s1
Brc1ccc(-c2cccc3cnccc23)o1
Brc1ccc(-n2c(Nc3cc(Br)on3)nnc2C2CCCCC2)cc1
Brc1ccc(CSc2nnc(Nc3ccnc4ccccc34)s2)cc1
Brc1ccc(Nc2ccccn2)cc1Oc1ccccc1
Fine tuning...
Mean value of predictions: 0.21572211
Proportion of valid SMILES: 0.5129728040012503
Sample trajectories:
Brc1cc(Nc2ncnc3sccc23)ccc1N1CCOCC1
Brc1cc2ncsc2c2ccc(C3CCCNC3)cc12
Brc1ccc(-c2nc3ccnnc3s2)c2ccccc12
Brc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(Nc2ncnc3sc(Nc4ccccc4)cc23)cc1

  4 Training on 2018 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 23.714322
Reward: 1.928377
Trajectories with max counts:
110	Cc1ccc(Nc2ncnc3sc(C)cc23)cc1
Mean value of predictions: 0.29070634
Proportion of valid SMILES: 0.504375
Sample trajectories:
BP(=O)(OCC)OCCOCCOCCOCCOCOP(=O)(NC(=O)CC(C)O)OCOCCCOCCOC(C)C
BP(=O)(OCC1CCOP(=O)(O)C1OP(=O)(O)O)c1cc(F)cc(Nc2ccc(F)cc2)c1
Brc1ccc(N(CCn2cccn2)c2cccc(Nc3cc(CC4CCCCO4)ncc3Br)c2)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4ccccc4)cc3s2)cc1
Brc1ccc(Nc2ccccc2)cc1
Policy gradient replay...
Mean value of predictions: 0.3044868
Proportion of valid SMILES: 0.5084375
Sample trajectories:
Bc1cc(Nc2ncnc3scnc23)ccc1OCCCO
BrCCN(Br)c1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(NCCn2cncn2)cc1Br
Brc1cc(Nc2cccnc2)ncn1
Brc1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.29792845
Proportion of valid SMILES: 0.49796811503594873
Sample trajectories:
Brc1[nH]c2ncnc(Nc3cccc4ccccc34)c2c1CCc1cccs1
Brc1cc(Nc2ccc(Nc3nc4ccccc4s3)nc2)n[nH]1
Brc1ccc(Nc2cc(Nc3ncnc4ccsc34)ccn2)cc1
Brc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(Nc2ncnc3ccsc23)nc1

  5 Training on 3709 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 23.997197
Reward: 2.481251
Trajectories with max counts:
129	Cc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.2580013
Proportion of valid SMILES: 0.4784375
Sample trajectories:
Brc1ccc(NCc2cccs2)cc1
Brc1ccc(Nc2ccc3ncncc3n2)cc1
Brc1ccc(Nc2ccccc2)Nc2ncnc3ncnc(cc1)Nc1cccc1cc23
Brc1ccc(Nc2ccnc3ccsc23)cc1
Brc1ccc(Nc2nc3ccccc3s2)cc1Br
Policy gradient replay...
Mean value of predictions: 0.35530445
Proportion of valid SMILES: 0.4978125
Sample trajectories:
BP(=O)(O)Oc1ccc(Nc2ncnc3sc4ccccc4c23)cc1
Brc1cc(Nc2ncnc3ccsc23)nc2ccccc12
Brc1cc(Nc2ncsc2Br)ncn1
Brc1cc2c(Nc3ccc(CN4CCC5CCCCC5C4)nc3)cncc2s1
Brc1ccCc2c(c1)c1ccc(Br)cc21
Fine tuning...
Mean value of predictions: 0.3436464
Proportion of valid SMILES: 0.509540193931811
Sample trajectories:
BP1(=O)c2ccccc2C2(Oc3ccc(Br)cc3)CCN1C2
Bc1cc(Nc2cccnc2)ccc1-c1ccnc2cc[nH]c12
Bc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
Brc1cc(Br)c2ncnc(Nc3ccc4ccccc4c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3scnc23)c1

  6 Training on 5377 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 25.236299
Reward: 3.216936
Trajectories with max counts:
226	Cc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.40605095
Proportion of valid SMILES: 0.3925
Sample trajectories:
Brc1ccc(Nc2ncnc3c(Nc4ccccc4)cc23)cc1
Brc1ccc(Nc2ncnc3cncnc23)cc1
Brc1ccc(Nc2ncnc3cscc23)cc1
Brc1ccc(Nc2ncnc3sc(Nc4ccccc4)cc23)cc1
Brc1ccc(Nc2ncnc3sccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.31882903
Proportion of valid SMILES: 0.46983432322600815
Sample trajectories:
BP(=O)(CCCCCCO)N(Cc1ccc(Br)cc1)C(=O)c1cc(Nc2ncccc2Br)cnc1Cl
Brc1ccc(Br)c(Nc2ncnc3ncnc(Nc4ccccc4)c23)c1
Brc1ccc(CNc2ncnc3sccc23)cc1
Brc1ccc(Nc2cc3c2sc2ccc(Br)cc23)cc1
Brc1ccc(Nc2ccc3c(c2)OCO3)cc1
Fine tuning...
Mean value of predictions: 0.37107387
Proportion of valid SMILES: 0.5066037735849057
Sample trajectories:
Bc1ccc(Nc2ncnc3sc4ccccc4c23)nc1
Brc1cc(CN2CCCCC2)cc2sccc12
Brc1ccc(C2CCCCCCCCC2)o1
Brc1ccc(Nc2ccncn2)cc1
Brc1ccc(Nc2ncnc3c2sc2ccncc23)cc1

  7 Training on 7023 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 27.006627
Reward: 3.427868
Trajectories with max counts:
148	Cc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.37616956
Proportion of valid SMILES: 0.428169014084507
Sample trajectories:
BP(=O)(OCC)OCC(F)(F)F
BP(=O)(c1ccc(NS(=O)(=O)c2ccc(F)cc2)cc1)N(CCC)S(=O)(=O)c1ccc(Nc2ccc(F)cc2)cc1
Bc1ccc(Nc2ncnc3sc(Nc4cccs4)ncnc23)cc1
Bc1ccc(Nc2ncnc3sccc23)cc1F
Bc1cccc(Nc2ncnc3sccc23)c1
Policy gradient replay...
Mean value of predictions: 0.36935484
Proportion of valid SMILES: 0.503907471084714
Sample trajectories:
BP(=O)(OC)OCC
Bc1cc(Br)cc(Nc2ncnc3ccsc23)c1
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCOCCOCc1cc(Br)c2c(c1)OCO2
BrCCON=C1CCCC1
Brc1cN2CCOc(c3ccc(CN4CCOCC4)cc3)c2cc1
Fine tuning...
Mean value of predictions: 0.41271752
Proportion of valid SMILES: 0.466875
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BrCCCOCCCOc1ccc(Nc2ncnc3sccc23)cc1
Brc1cc(-c2ccccc2)c2ccccc2n1
Brc1cc2c(Nc3ccc(I)cc3)ncnc2s1
Brc1cc2c(Nc3ccnc4ccncc34)ncnc2s1

  8 Training on 8604 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 25.645114
Reward: 3.972230
Trajectories with max counts:
361	Cc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.37354147
Proportion of valid SMILES: 0.3803125
Sample trajectories:
BP(=O)(C(=O)OCO)N1CCOCC1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3cscc23)cc1
Brc1ccc(Nc2ccc(Br)cc2)cc1
Brc1ccc(Nc2cccc3ccccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.40415224
Proportion of valid SMILES: 0.4515625
Sample trajectories:
BP(=O)(OCCC)Oc1ccc(Nc2ncnc3sccc23)cn1
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCCNc1cc2c(Nc3cncc(I)c3)ncnc2s1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc2c(Nc3ccc(C4CCCCCC4)s3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.4593031
Proportion of valid SMILES: 0.4753125
Sample trajectories:
BP(=O)(NCc1ccc(Br)cc1)P(=O)(O)O
BP1(=O)OCC(OC(=CO)NC(c2ccc(Br)cc2)c2cc(F)c(F)c(F)c2F)O1
BrCc1cc2c(N3CCOCC3)cccc2[nH]1
BrSc1ccc(Nc2ncnc3scc(-c4ccccc4)c23)cc1
Brc1c2c(c3nc(Nc4ccccc4)ncnc13)OCO2

  9 Training on 9942 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 26.007612
Reward: 3.756431
Trajectories with max counts:
78	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.5125607
Proportion of valid SMILES: 0.4504532666458268
Sample trajectories:
BP(=O)(OCC)OCCOCCOCOCOCCOC(=O)CCCCCCC1CCP(=N)(NO)Oc2c(F)cc(F)cc21
Bc1cc(Nc2cc(Nc3ccsc3)ncn2)ccc1Br
BrCCN(c1ccc(Br)c(Br)c1)c1cccs1
Brc1cc(-c2ncnc3sc(CC4CCCCC4)cc23)c2sccn12
Brc1cc(N2CCCC3(COC3)C2)cnc1Br
Policy gradient replay...
Mean value of predictions: 0.45291075
Proportion of valid SMILES: 0.4841841528343251
Sample trajectories:
BP(=O)(OC)OCCBr
Bc1cc(Nc2cccc(Br)c2)ccc1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3sc4c(Cl)cccc4c23)cc1Br
Bc1sc2ncnc(Nc3cccc(I)c3)c2c1C
Fine tuning...
Mean value of predictions: 0.4827103
Proportion of valid SMILES: 0.5356695869837297
Sample trajectories:
BP(=O)(NC(CCCN)CCCCCCCCCCC#N)Oc1c(Br)cc(Br)cc1Br
Bc1ccc(Nc2ncnc3sc4ccsc4c23)cc1Br
BrCCCc1cc2c(Nc3ccc(CNc4ccccc4Br)cc3)ncnc2s1
BrCCOc1cc(Nc2ncnc3sccc23)ccc1Br
BrCc1ccc2sc(c1)c1ncnc(Nc3cc(Br)cc(Nc4ccccc4)c3)c1s2

 10 Training on 11637 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.767055
Reward: 3.744414
Trajectories with max counts:
122	Cc1ccc(Nc2ncnc3sc(C)cc23)cc1
Mean value of predictions: 0.5645489
Proportion of valid SMILES: 0.49233656553018457
Sample trajectories:
BP(=O)(CCCCc1ccccc1)P(=O)(O)OCC1OC(CO)C(O)C1O
BP(=O)(OCC)OCCCCCCCCCCCCC
BP(=O)(OCC)Oc1ccc(Nc2ncnc3sc(C)cc23)cc1
B[PH](=O)(Nc1cc(Br)c(Br)c(Br)c1)(P(=O)(O)O)P(=O)(O)O
Bc1cc(Nc2ncnc3sc(CCCCCCCCC)cc23)ccc1Br
Policy gradient replay...
Mean value of predictions: 0.53643864
Proportion of valid SMILES: 0.5343415248897291
Sample trajectories:
BP(=O)(CCCN)n1cnc2c(N)ncnc21
BP(=O)(CCc1cccc(Br)c1)c1ccc(Nc2ccc(N(=O)=O)cc2)cc1
BP(=O)(OCC1OC(S(=O)(=O)N2CCCCCCCCCCCCCCC2)C(O)C1O)ON1C(=O)N=C1Cc1cc(F)ccc1F
BP(=O)(OCCOCCOP(=O)(O)OP(=O)(O)O)OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O
Bc1cc(Br)c2c(Nc3cccc(I)c3)ccnc2c1
Fine tuning...
Mean value of predictions: 0.51068944
Proportion of valid SMILES: 0.4940625
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3sc(Nc4ccccc4)cc23)cc1
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCc1ccc2ncsc2c1
Brc1cc(Nc2ccnc3cccnc23)ccc1CCN1CCCC1
Brc1cc(Nc2ncnc3scnc23)co1

 11 Training on 13701 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.479417
Reward: 4.286845
Trajectories with max counts:
374	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49616164
Proportion of valid SMILES: 0.309375
Sample trajectories:
BC1=CNC(Nc2ccc(F)cc2F)C1=O
BP(=O)(NCO)C(=O)Oc1ccccc1
BP(=O)(OCC)C(=O)N(c1ccccc1)c1ccc(Br)cc1
Bc1ccc(Nc2cc(Nc3cc(Br)ccc3O)ccn2)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.5294393
Proportion of valid SMILES: 0.5360050093926112
Sample trajectories:
Bc1cc(Nc2ncnc3sc4ccccc4c23)cc2sc3ccsc3c12
Bc1cnc(Nc2ncnc3sc(Br)cc23)cc1Br
Brc1cc(Br)cc(Nc2ncnc3sc(-c4sccc4Br)nc23)c1
Brc1cc(Br)cc(Nc2ncnc3sc(Br)cc23)c1
Brc1cc(Nc2cc(Nc3ccsc3)ncn2)sc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.517757
Proportion of valid SMILES: 0.5020331560838286
Sample trajectories:
BP(=O)(CS)OCCOCOCCOCCOCCOCCOCCOCCOCCCOCCOCCOCCOCCOCCOCCCCOCCOCCOCCOCP(=O)(O)O
Bc1cc(Nc2ncnc3scc(C(C)C)c23)ccc1Br
BrCCCOCCOCCOc1ccc(Nc2ncnc3sc4ccccc4c23)cc1
BrCCCc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(-c2ccncc2)c2ccccc2n1

 12 Training on 15411 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.966504
Reward: 4.436782
Trajectories with max counts:
187	Cc1ccc(Nc2ncnc3sc(C)cc23)cc1
Mean value of predictions: 0.5722013
Proportion of valid SMILES: 0.41606752110034384
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)P(=O)(O)OCCc1ccc(Br)cc1
BP(=O)(OCC)OCCCCC1OC(c2c(Br)cc(Br)cc2Br)C(O)C1O
BP(=O)(OCC)c1ccc(Nc2cc(Br)cc(F)c2F)cc1
B[PH](=O)(=Nc1ccc(Br)cc1)Nc1ccc(Br)cc1
Bc1cc(Nc2ncnc3sc(C)cc23)ccc1Br
Policy gradient replay...
Mean value of predictions: 0.6
Proportion of valid SMILES: 0.5317485142320926
Sample trajectories:
B[PH](=O)(Nc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O
Brc1cc(Br)c2c(Br)c3c(Nc4ccccc4Br)c4ccccc4c3c2c1
Brc1ccc(-c2csc3ncnc(Nc4ccc(Br)c5ccccc45)c23)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2cc3c(cc2Br)OCO3)c1
Fine tuning...
Mean value of predictions: 0.5281437
Proportion of valid SMILES: 0.5222013758599124
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCCC=CCC(=O)O)NO
BP(=O)(OC)OCC
Bc1cc(Nc2ncnc3c(Br)sc(Br)c23)cc(Br)c1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1

 13 Training on 17454 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.251894
Reward: 4.818968
Trajectories with max counts:
137	Cc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.63105136
Proportion of valid SMILES: 0.5115697310819262
Sample trajectories:
Bc1ccc(Nc2ncnc3sc(CN4CCCC4)cc23)cc1
Brc1cc(Br)c(Nc2ncnc3ccsc23)cc1Br
Brc1cc(Br)cc(CNc2ncnc3ccsc23)c1
Brc1cc(Br)cc(Nc2nc3ccccc3s2)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Policy gradient replay...
Mean value of predictions: 0.6011229
Proportion of valid SMILES: 0.5009375
Sample trajectories:
BC(=O)Nc1ccc(Nc2ncnc3scc(-c4cccs4)c23)cc1
BP(=O)(Nc1ccc(Nc2ncnc3sc(C)cc23)cc1)c1ccc(Br)cc1
BP(=O)(O)Oc1cc(Br)c(Br)cc1Br
BP(=O)(OCC)Oc1ccc(Nc2ncnc3sc(N)cc23)cc1
BP(=O)(OCCOP(=O)(O)OP(=O)(O)O)n1cnc2c(N)ncnc21
Fine tuning...
Mean value of predictions: 0.57569194
Proportion of valid SMILES: 0.5196998123827392
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCCCCCCCCCN)NO
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3sccc23)cc1
Bc1cccc(Nc2ncnc3sc(Br)cc23)c1
BrCCCCCCCCSc1nc2c(Nc3ccc(Br)cc3Br)ncnc2s1

 14 Training on 19862 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 33.188079
Reward: 4.696580
Trajectories with max counts:
30	COc1ccc(Nc2ncnc3sc(Nc4ccccc4)cc23)cc1
Mean value of predictions: 0.62532467
Proportion of valid SMILES: 0.5778611632270169
Sample trajectories:
BrCCBr
Brc1cc(Br)cc(Nc2ncnc3scc(Br)c23)c1
Brc1cc(Br)cc(Nc2ncnc3scnc23)c1
Brc1cc(Nc2cc3c(Nc4ccc(Br)nc4)ncnc3s2)cc(-c2ccccc2)c1
Brc1cc(Nc2ncnc3sc(Br)cc23)cs1
Policy gradient replay...
Mean value of predictions: 0.56280786
Proportion of valid SMILES: 0.5082942097026604
Sample trajectories:
BP(=O)(OCOCCOCP(=O)(O)OP(=O)(O)O)P(=O)(O)O
B[PH](=O)(Nc1ccc(Br)cc1)=[PH](N)(=O)Oc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1
Bc1ccc(Nc2ncnc3sc(C(N)=O)c(Nc4ccccc4)c23)cc1
Bc1ccc(Nc2ncsc2I)cc1Br
Fine tuning...
Mean value of predictions: 0.58853215
Proportion of valid SMILES: 0.5451703657392936
Sample trajectories:
BP(=O)(NC(CC#C)c1ccc(Br)cc1)c1cccc(Br)c1
BP(=O)(c1cccc(Nc2ncnc3ccccc23)c1)N1CCOCC1
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCCCCCCBr
BrCc1ccc(Nc2ncnc3sc(Nc4ccccc4)cc23)cc1

 15 Training on 22337 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.666115
Reward: 4.849384
Trajectories with max counts:
363	Cc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4519149
Proportion of valid SMILES: 0.440625
Sample trajectories:
Bc1ccc(Nc2ncnc3ccccc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Bc1cccc(Nc2ncnc3ccsc23)c1
BrSc1ccccc1-c1ccc(Nc2ncnc3ccccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.5994855
Proportion of valid SMILES: 0.48715538847117795
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Nc2nccs2)cc1Br
BP(=O)(OCC)c1cc(Br)cs1
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1Br
Bc1ccc(Nc2ncnc3sc4ccccc4c23)cc1
Bc1ccc(Nc2ncnc3sccc23)cc1
Fine tuning...
Mean value of predictions: 0.607147
Proportion of valid SMILES: 0.5426962777604004
Sample trajectories:
BP(=O)(NCc1ccc(Br)cc1)c1ccc(F)cc1
BP(=O)(Nc1ccc(Br)cc1)N1CCOCC1
Bc1ccc(Br)cc1Nc1cncc(Nc2c(F)cc(Br)cc2Br)c1
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1Br
BrC(=NNc1cccc(Br)c1)c1cccnc1

 16 Training on 24415 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.951075
Reward: 5.162639
Trajectories with max counts:
423	Cc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6232519
Proportion of valid SMILES: 0.3709375
Sample trajectories:
Bc1cc(Br)cc(Br)c1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(-c2cc3c(Nc4ccccc4)ncnc3s2)cc1
Brc1ccc(-c2csc3sc(Nc4ccccc4)nc23)cc1
Brc1ccc(Nc2cc(Br)ccn2)cc1
Policy gradient replay...
Mean value of predictions: 0.5931345
Proportion of valid SMILES: 0.5570532915360502
Sample trajectories:
Bc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Bc1cc(F)cc(Nc2ncnc3sccc23)c1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCc1sc2ncnc(Nc3ccc(Br)cc3)c2c1-c1cccs1
Brc1cc(Br)c(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.60998917
Proportion of valid SMILES: 0.5783359497645212
Sample trajectories:
BP(=O)(CCCCCCCCCCCC=CC#CCCCCCCCCCCCCCCCCCCCCCC)OCC
BP(=O)(Nc1ccc(Br)cc1)P(=O)(O)O
BP(=O)(OCCCCCCO)c1ccc(Br)cc1
BP1(=O)OCCN(C=CC(=O)NO)C2OC(CO1)C(O)C2O
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1

 17 Training on 26799 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 33.440230
Reward: 4.878441
Trajectories with max counts:
70	Cc1ccc(Nc2ncnc3sc(C)c(C)c23)cc1
Mean value of predictions: 0.6917258
Proportion of valid SMILES: 0.5307402760351317
Sample trajectories:
BP(=O)(OCC)OCCN1CCCC1
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3sc(SC(C)=O)nc23)cc1Br
BrCCBr
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.5396661
Proportion of valid SMILES: 0.5488151658767773
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3sc(SCc4cccc(Br)c4)nc23)cc1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c2c3ccccc3c2c1
Brc1cc(Nc2ncnc3sc(CCCCCCCCNc4nccs4)cc23)cs1
Fine tuning...
Mean value of predictions: 0.60424817
Proportion of valid SMILES: 0.5604636591478697
Sample trajectories:
BP(=O)(O)c1ccc(Nc2ncnc3sc(C)c(Br)c23)cc1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCCCCCc1ccc(Br)c2ncnc(Nc3ccncn3)c12
BrCCCc1ccc(Nc2ncnc3ccsc23)s1
BrCc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1

 18 Training on 29437 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 33.550025
Reward: 5.006166
Trajectories with max counts:
327	Cc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.63316834
Proportion of valid SMILES: 0.5067419253684541
Sample trajectories:
BrCc1ccc(Nc2ccnc3c(Br)cccc23)cc1C1=NCCCCCCC1
Brc1cc(Br)cc(Nc2ncnc3sc(C#CCCCCNCCOc4ccccc4)cc23)c1
Brc1ccc(Br)c(Cc2ccc(Nc3ncnc4ccccc34)cc2)c1
Brc1ccc(Br)c(Nc2ncnc3ccsc23)c1
Brc1ccc(CN(c2cccc(Br)c2)C2CCCCC2)cc1
Policy gradient replay...
Mean value of predictions: 0.6031847
Proportion of valid SMILES: 0.5907808090310442
Sample trajectories:
BrCCCCCCCCBr
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cN(c2ncnc3sc(Nc4cccs4)nc23)Nc(c2ccccc2)ccc1Br
Brc1cc(Br)c(Nc2ncnc3sc(Br)c(-c4cccs4)c23)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3scc(Br)c23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.63335145
Proportion of valid SMILES: 0.578021978021978
Sample trajectories:
BP(=O)(OCC)OCC1OC(Oc2ccc(Br)cc2)C(F)C1=O
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3scnc23)cc1
Brc1cc(Br)c(Nc2ncnc3sc(Br)cc23)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3cncnc23)c1

 19 Training on 32175 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.050473
Reward: 5.267330
Trajectories with max counts:
475	Cc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4528372
Proportion of valid SMILES: 0.3359375
Sample trajectories:
Bc1ccccc1Br
Brc1ccc(Nc2cc3nccc(Nc4ccccc4Br)c3c(Nc3ccccc3)n2)cc1
Brc1ccc(Nc2cccc3cc(Nc4ccccc4Br)ccc23)cc1
Brc1ccc(Nc2ccccc2Br)c(Nc2cccc(Nc3ncnc4ccsc34)c2)c1
Brc1ccc(Nc2ccncc2Br)cc1Br
Policy gradient replay...
Mean value of predictions: 0.64360183
Proportion of valid SMILES: 0.5301507537688442
Sample trajectories:
BP(=O)(OCCCCCF)Oc1ccc(Nc2ncnc3ccsc23)cc1
BrCc1cc(Nc2ncnc3sc(Br)cc23)ccc1Br
BrCc1cc2c(Nc3cc(Br)c(Br)s3)ncnc2s1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCc1sc2ncnc(Nc3ccc(Br)cc3)c2c1Br
Fine tuning...
Mean value of predictions: 0.6937008
Proportion of valid SMILES: 0.5564945226917057
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1Br
Bc1ccc(Nc2ncnc3sc4ccccc4c23)cc1
Bc1cccc(Nc2ncnc3scc(-c4ccccc4)c23)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1

 20 Training on 34505 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.687576
Reward: 5.280107
Trajectories with max counts:
222	Cc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.72630197
Proportion of valid SMILES: 0.4755485893416928
Sample trajectories:
Bc1ccc(Nc2ncnc3sc(C(=O)OC)cc23)c2ccccc12
Brc1cc(Br)c(Nc2ncnc3ccsc23)cc1Br
Brc1ccc(-c2csc3ncnc(Nc4ccccc4)c23)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Cc2ccc(Nc3ncnc4sccc34)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.7174194
Proportion of valid SMILES: 0.58125
Sample trajectories:
BP(=O)(OCC)OCCCCCCCCCN1CCCCCCCCCCCCC1
Bc1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCBr
BrCc1cc(Nc2ncnc3sc(Br)cc23)ccc1I
Brc1cc(Br)c(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.6853644
Proportion of valid SMILES: 0.5389692017598995
Sample trajectories:
B[PH](=O)(Nc1ccc(F)c(F)c1)(P(=O)(O)O)P(=O)(O)O
BrCCOCCCCCCCCCCCCOc1ccc(Nc2ccnc3ccc(Br)cc23)cc1
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
BrCc1cccc2c1sc1ncnc(Nc3cccc(Br)c3)c12
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1

Trajectories with max counts:
557	Cc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.64371395
Proportion of valid SMILES: 0.46736063209381074
Mean Internal Similarity: 0.5384303532529471
Std Internal Similarity: 0.122552566366805
Mean External Similarity: 0.4058324032312567
Std External Similarity: 0.05347362042331335
Mean MolWt: 395.5127005807701
Std MolWt: 143.27901986660896
Effect MolWt: -0.7974356397849848
Mean MolLogP: 5.878167907076792
Std MolLogP: 3.5988671681471525
Effect MolLogP: 0.40882956018635935
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 98.718861% (1387 / 1405)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/egfr_enamine.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5449.852677106857, 'valid_fraction': 0.46736063209381074, 'active_fraction': 0.6237756608077284, 'max_counts': 557, 'mean_internal_similarity': 0.5384303532529471, 'std_internal_similarity': 0.122552566366805, 'mean_external_similarity': 0.4058324032312567, 'std_external_similarity': 0.05347362042331335, 'mean_MolWt': 395.5127005807701, 'std_MolWt': 143.27901986660896, 'effect_MolWt': -0.7974356397849848, 'mean_MolLogP': 5.878167907076792, 'std_MolLogP': 3.5988671681471525, 'effect_MolLogP': 0.40882956018635935, 'generated_scaffolds': 1405, 'novel_scaffolds': 1387, 'novel_fraction': 0.9871886120996441, 'save_path': '../logs/replay_data_s2-3.smi'}


  1 Training on 435 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.698529
Reward: 1.000000
Mean value of predictions: 0.0011881188
Proportion of valid SMILES: 0.7910401002506265
Sample trajectories:
BP(=O)(OC(C)C)C1CCCC=CC1OC(=O)CCC(N)=O
Brc1cc(Br)c2c(c1Br)C1CC=CC(CO2)O1
Brc1ccc(-c2ccc3oc(N4CCCC4)nc3n2)cc1
Brc1ccco1
C#CCC#Cc1ccn(C(=O)N2CCN(c3ccccc3)CC2)c1
Policy gradient replay...
Mean value of predictions: 0.0018698871
Proportion of valid SMILES: 0.8024382619568615
Sample trajectories:
Brc1ccc(-c2ccccc2)s1
Brc1ccc(-n2cc3ccccc3c2-c2ccccc2)cc1
Brc1ccc(N2CCNCc3ccccc3C2)cc1
Brc1ccc(Nc2ccc3ccncc3c2)cc1
Brc1ccc(Nc2ccccc2OCc2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.02344358
Proportion of valid SMILES: 0.6435054773082942
Sample trajectories:
Brc1cc(-c2ccnc(Nc3ccccc3)c2)c(I)s1
Brc1ccc(CNc2ncccn2)cc1
Brc1ccc(N2N=C3C=CC=CN3c3ncnc(Nc4ccccc4)c32)o1
Brc1ccc(Nc2ncnc3c2ncn3-c2ccncc2)cc1
Brc1ccc2[nH]nc(-c3ccncc3)c2c1

  2 Training on 515 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.476183
Reward: 1.033067
Trajectories with max counts:
7	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.032596685
Proportion of valid SMILES: 0.6793869252424147
Sample trajectories:
Brc1ccc(-c2sc(-c3ccccc3)nc2Cc2ccccc2)cc1
Brc1ccc2[nH]cc(-c3ccccn3)c2c1
Brc1ccc2[nH]ccc2c1-c1ccc(-c2ccncc2)cc1
Brc1ccc2c(c1)oc1ccccc12
Brc1ccc2c(c1)sc1nccn12
Policy gradient replay...
Mean value of predictions: 0.047486033
Proportion of valid SMILES: 0.67125
Sample trajectories:
Brc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(Nc2ncnc3ncccc23)cc1
Brc1ccc2cccc3cc4ccc(Br)cc4c3[nH]c2c1
Brc1ccc2sccc2c1-c1cc2ccccc2nc1N1CCN(Cc2ccccn2)CC1
Brc1cccc(Nc2nc(NCc3ccccc3)nc3ccccc23)c1
Fine tuning...
Mean value of predictions: 0.056893203
Proportion of valid SMILES: 0.6441525953721076
Sample trajectories:
BrC(Br)=C1CCCC1
Brc1c(CNCc2ccccn2)sc2ccccc12
Brc1c2ccccc2cc2ccccc12
Brc1ccc(-c2ncnc3c2ncn3C2CC2)c2ccccc12
Brc1ccc(NN=Cc2cccc(Br)c2)cc1

  3 Training on 916 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.210340
Reward: 1.303665
Trajectories with max counts:
15	Cc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.10436242
Proportion of valid SMILES: 0.5594493116395495
Sample trajectories:
Brc1cc(Nc2ncnc3c2sc2ccccc2C=N3)cs1
Brc1cc2c(Nc3nccs3)cnc2sc1-c1csc(Nc2ccccc2)n1
Brc1ccc(C2CCC(c3ncncc3I)O2)cc1
Brc1ccc(Nc2ccnc(Nc3ncnc4ccccc34)c2)cc1
Brc1ccc(Nc2ncnc3ccncc23)cc1
Policy gradient replay...
Mean value of predictions: 0.011245314
Proportion of valid SMILES: 0.7503125
Sample trajectories:
Brc1ccc(Nc2ccccc2)cc1
Brc1ccc(Nc2ccccc2-c2ncccc2-c2ccccc2)cc1
Brc1ccc2ccccc2c1
Brc1cccc(Nc2cccc3ccccc23)c1
Brc1cccc(Nc2ccccc2-c2cnccc2Sc2ccccn2)c1
Fine tuning...
Mean value of predictions: 0.13043916
Proportion of valid SMILES: 0.6192560175054704
Sample trajectories:
Brc1ccc(-c2nc3cc(Nc4ccc(NC5CCCC5)nc4)ccc3s2)s1
Brc1ccc(Nc2ccc3Cc2c(Nc2ccc(Br)cc2)ncnc2ccc-3cs2)cc1
Brc1ccc(Nc2cnc3ccccc3n2)cc1
Brc1ccc(Nc2nc3ncnc(Nc4cccc(Br)c4)c3s2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1

  4 Training on 1542 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 23.781591
Reward: 1.799715
Trajectories with max counts:
9	Cc1ccc(Nc2ncnc3sc(C)cc23)cc1
Mean value of predictions: 0.19073144
Proportion of valid SMILES: 0.5603879849812266
Sample trajectories:
Brc1ccc(Br)c(-c2sc3ncnc(Nc4cccs4)c3c2C2CCNCC2)c1
Brc1ccc(Nc2ncnc3c(N4CCOCC4)ccnc23)cc1
Brc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(Nc2ncnc3sc(-c4cn[nH]c4)nc23)cc1
Brc1ccc(Nc2ncnc3scc(-c4ccccc4)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.1001773
Proportion of valid SMILES: 0.705
Sample trajectories:
Brc1ccc(Nc2ccccc2)cc1
Brc1ccc(Nc2ncnc3c2[nH]nc3c2ccccc2)cc1
Brc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(Nc2ncnc3sc4ccccc4c23)cc1
Brc1ccc2c(Nc3ccccn3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.18152568
Proportion of valid SMILES: 0.6021875
Sample trajectories:
Brc1ccc(Nc2cc(Nc3ccsc3)[nH]n2)cc1
Brc1ccc(Nc2ncnc(Nc3cccs3)n2)cc1
Brc1ccc(Nc2ncnc3ccccc23)s1
Brc1ccc(Nc2ncnc3ncnc(Nc4ccccc4Br)c23)cc1
Brc1ccc(Nc2ncnc3sc(-c4ccsc4)nc23)cc1

  5 Training on 2733 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 20.973340
Reward: 1.719336
Trajectories with max counts:
37	Cc1ccc(Nc2ncnc3ccccc23)cc1
37	Cc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.17532468
Proportion of valid SMILES: 0.5775
Sample trajectories:
Brc1ccc(Br)c(Nc2cccc(Nc3nccs3)c2)c1
Brc1ccc(N(c2ccccc2)c2ccccc2)c(N2CCCCC2)c1
Brc1ccc(Nc2ccc3ncsc3c2)cc1
Brc1ccc(Nc2ccnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2cnccn2)cc1
Policy gradient replay...
Mean value of predictions: 0.24366517
Proportion of valid SMILES: 0.5528455284552846
Sample trajectories:
BrCCOc1ccc(CNc2ncnc3ccc(Oc4ccccc4)cc23)cc1
Brc1cc(Nc2nccc3cnc(-c4ccccc4)nc23)cc(-c2ccccc2)c1
Brc1ccc(-c2ccc3ncnc(Nc4ccncc4)c3c2)s1
Brc1ccc(NC(=Nc2ccncc2)C2CCCCC2)cc1
Brc1ccc(Nc2ccccc2N2CCCC2)cc1
Fine tuning...
Mean value of predictions: 0.23739658
Proportion of valid SMILES: 0.5665625
Sample trajectories:
Bc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
Brc1cc(-c2nc3ccccc3s2)c2c(Nc3ccccc3)ncnc2c1
Brc1cc(Nc2nccc3ncnc(Nc4ccccc4Br)c23)ccc1Nc1ccccc1
Brc1cc2c(-c3ccncc3)c3ccccc3nc2c2sc3c(c12)Cc1ccccc1-3
Brc1ccc(-c2nc3ccc(Nc4ncnc5ccccc45)cc3s2)cc1

  6 Training on 4106 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 23.688840
Reward: 2.377225
Trajectories with max counts:
226	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.25232244
Proportion of valid SMILES: 0.4575
Sample trajectories:
BrC1CC2(C1)Nc1ccccc12
Brc1ccc(-c2ccc(Nc3ncnc4ccsc34)cc2)o1
Brc1ccc(-c2ccc3ncnc(Nc4ccccc4)c3c2)s1
Brc1ccc(-c2cnc(Nc3ccccc3)nc2Nc2ccccc2Br)cc1
Brc1ccc(NN=Cc2ccncc2)cc1
Policy gradient replay...
Mean value of predictions: 0.36022174
Proportion of valid SMILES: 0.45150187734668334
Sample trajectories:
BP(=O)(NO)C(=O)c1ccc(I)cc1
BP(=O)(OCC)OC(=O)CCCCCCC(=O)CO
Brc1c[nH]c2ncc(Nc3sc4c(c3-c3ccccc3)CCC4)c2c1
Brc1cc(Br)c2c(c1)C(CCN1CCCC1)O2
Brc1ccc(Nc2cc3c(Nc4ccc(Br)cc4)ncnc3s2)cc1
Fine tuning...
Mean value of predictions: 0.30875364
Proportion of valid SMILES: 0.5390625
Sample trajectories:
BrC1=Cc2ccccc2Oc2ncnc(Nc3ccccc3)c2N=C1
Brc1cc2[nH]cc(Nc3ccc(Nc4ccccc4)cc3)nc2c1Br
Brc1ccc(-c2cscn2)c2c1sc1ncccc12
Brc1ccc(CNc2ccc(Nc3ncnc4nsnc34)cc2)cc1
Brc1ccc(NCc2ccc(-c3ccccc3)cc2)cc1

  7 Training on 5644 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 25.104969
Reward: 3.046037
Trajectories with max counts:
382	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.36556524
Proportion of valid SMILES: 0.359375
Sample trajectories:
BP(=O)(NCCCSSCC(N)P(=O)(O)O)OCCCSS(N)(=O)=O
BP(=O)(Nc1ccccc1)C(F)(F)F
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3sccc23)cc1
Brc1ccc(NN=Nc2ccc(Br)cn2)cc1
Policy gradient replay...
Mean value of predictions: 0.33439288
Proportion of valid SMILES: 0.4915625
Sample trajectories:
BP(=O)(CCC=CCC=C(Br)Br)OCC
BP(=O)(NCCO)Nc1nc(Nc2ccccc2)sc1C(=O)OP(=O)(O)O
BP(=O)(NO)c1ccc(Br)cc1
BP(=O)(Oc1ccc(Cl)cc1)Oc1ccc(Nc2ccc(F)c(F)c2F)nc1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.32092747
Proportion of valid SMILES: 0.5257893091591123
Sample trajectories:
BP(=O)(OCC)OCC1OC(n2cnc3c(Nc4ccc(F)c(F)c4)nc(Cl)nc32)C(O)OC1O
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
Bc1c(Nc2ncnc3cc(Cl)ccc23)sc2ccccc12
Bc1ccc(Nc2ncnc3sc4ccccc4c23)cc1
BrCCc1ccc(Nc2ncnc3ccccc23)cc1

  8 Training on 7186 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 27.111943
Reward: 3.304428
Trajectories with max counts:
233	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.42061856
Proportion of valid SMILES: 0.4546875
Sample trajectories:
BP(=O)(C(=O)OCC)N(O)COCC
Bc1ccc(N2Cc3cc(Br)ccc3S2(=O)=O)c(CN)c1
Bc1ccc(Nc2ncnc3scnc23)cc1
Brc1cc(Br)c2ncnc(Nc3cccc(CN4CCOCC4)c3)c2c1Br
Brc1cc(N2CCOCC2)ccc1Nc1c(-c2cccs2)c2ccccc2sc2ccoc12
Policy gradient replay...
Mean value of predictions: 0.20939335
Proportion of valid SMILES: 0.63875
Sample trajectories:
BP(=O)(C=O)OCC
BP(=O)(OCC(=O)Nc1ccccc1)c1cccc(Br)c1Br
Brc1cc(Br)c2c(Br)cccc2c1Br
Brc1cc(Nc2ncnc3sc(Nc4ccccc4)cc23)c2ccccc2c1
Brc1cc2c(cc1Br)c1ncnc(-c3ccccc3Br)c1N2
Fine tuning...
Mean value of predictions: 0.38035426
Proportion of valid SMILES: 0.5823694904657706
Sample trajectories:
Bc1ccc(Nc2ncnc3sccc23)cc1
Brc1c(Nc2ncnc3scnc23)sc2ccc(CC3CCCC3)cc12
Brc1ccc(-c2cc3cncnc3s2)cc1
Brc1ccc(-c2cccc(Nc3ncnc4sccc34)c2)o1
Brc1ccc(N(CCNc2ccnc3ccccc23)c2ccccc2)cc1

  9 Training on 8853 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 28.698145
Reward: 3.889068
Trajectories with max counts:
423	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.49121064
Proportion of valid SMILES: 0.376875
Sample trajectories:
BP(=O)(Nc1ccc(Nc2ncnc3sccc23)cc1)c1cccc(F)c1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(NC(C)C)OP(=O)(O)O
BP(=O)(ON=C(N)N)c1cccc(Nc2cc(NC(=O)CC)ccc2Br)c1
Bc1cc(Nc2ncnc3sc(Br)cc23)ccc1Br
Bc1ccc(Nc2ncnc3sccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.47932962
Proportion of valid SMILES: 0.504225352112676
Sample trajectories:
BP(=O)(OCC1CCCOP(=O)(N2C=C(F)C(=O)NC2=O)N1)Oc1ccc(Br)cc1
BP(=O)(OCCO)C(Br)Br
Bc1ccc(Nc2ncnc3sc(C(=O)OCC)c(Br)c23)c(F)c1
Brc1cc(Nc2ncnc3ccccc23)cs1
Brc1cc(Nc2ncnc3sc(Br)cc23)cs1
Fine tuning...
Mean value of predictions: 0.44239953
Proportion of valid SMILES: 0.5365625
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCOP(=O)(O)O
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1cccc(Nc2ncnc(Nc3ccc(F)cc3)n2)c1
BrCCCc1ccc(Nc2ncnc3sc(Nc4ccccc4)ncnc23)cc1
Brc1cc(Nc2cc(Nc3cccs3)ncc2Br)cc(Nc2ccncc2)n1

 10 Training on 10570 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 26.984729
Reward: 4.078314
Trajectories with max counts:
327	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.53909975
Proportion of valid SMILES: 0.3541731791184745
Sample trajectories:
BP(=O)(NN=Cc1cccc(Br)c1)c1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3scc(-c4ccsc4)c23)cc1
BrC(=NN(Cc1ccccc1)c1ccc(Nc2ncnc3ccccc23)cc1)c1ccsc1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.45067823
Proportion of valid SMILES: 0.5761175367302281
Sample trajectories:
BP(=O)(O)CCCCCOCCOCCOCCCOCCCCCOCCCCCCOCCOCCOCCCF
BP(=O)(OCC)C(=O)NCCCCCCCCCCCCC(=O)NO
Bc1cc(Nc2ncnc3sc(Br)cc3s2)cc(Br)c1F
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1cccc(Nc2ncnc3ccsc23)c1
Fine tuning...
Mean value of predictions: 0.42372093
Proportion of valid SMILES: 0.5375
Sample trajectories:
BP(=O)(OCC)OCCCCCC
BP(=O)(OCC)OCCCCCCCCCCCCC
BP(=O)(OCC)Oc1ccc(Br)cc1
BP(=O)(OCC1OC(OOP(=O)(O)O)C(O)C1N)S(=O)(=O)O
B[PH](=O)(=CCl)OCC(F)F

 11 Training on 12200 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.549565
Reward: 4.412575
Trajectories with max counts:
882	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5083151
Proportion of valid SMILES: 0.285625
Sample trajectories:
BP(=O)(NC1CCCCC1)c1csc(Nc2ccc(F)cc2)n1
BP(=O)(OCC)c1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(-c2ccc(Nc3ncnc4sc(Br)cc34)cc2)cc1
Brc1ccc(NN=Cc2cccs2)cc1
Brc1ccc(Nc2ccnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.50392157
Proportion of valid SMILES: 0.5103189493433395
Sample trajectories:
BC(CO)Nc1nc2c(s1)CCN2c1ccc(F)cc1
BP(=O)(CCCNc1ccc2ccsc2c1)Nc1cc(Br)cc(Br)c1O
BP(=O)(OCCCCCCCC)Oc1ccc(O)c(F)c1
B[PH](=O)(Br)(OCBr)c1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Fine tuning...
Mean value of predictions: 0.47905046
Proportion of valid SMILES: 0.5273865414710485
Sample trajectories:
BP(=O)(Nc1cccc(Nc2ccc(Br)cc2O)c1)c1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cc(Br)c(Nc2ncnc3ccsc23)s1
Brc1cc(Nc2ccsc2)cc2cnccc12
Brc1cc(Nc2ncnc3ccsc23)cs1

 12 Training on 13788 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.205874
Reward: 4.370662
Trajectories with max counts:
376	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5480478
Proportion of valid SMILES: 0.3925555208007507
Sample trajectories:
BP(=O)(CCC=CC)OCC
BP(=O)(Nc1ccc(Br)c(Br)c1)c1cc(Br)c(Br)cc1F
BP(=O)(Nc1ccc(F)cc1)Oc1ccc(F)c(F)c1
BP(=O)(OCC)OC(=O)C(Br)Br
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCCCCCCC(=O)I
Policy gradient replay...
Mean value of predictions: 0.47077444
Proportion of valid SMILES: 0.5528125
Sample trajectories:
BP(=O)(C(=O)O)N(Cc1ccc(Br)cc1)P(=O)(O)O
BP(=O)(CCCNc1ccc(Br)cc1)c1ccc(Br)cc1Br
BP(=O)(OC(C)=O)Oc1ccc(Br)cc1Br
BP(=O)(OCC)C(=O)O
B[PH](=O)(=C[PH](F)(F)Oc1ccc(F)cc1Br)N1CCOCC1
Fine tuning...
Mean value of predictions: 0.4817192
Proportion of valid SMILES: 0.5453125
Sample trajectories:
BP(=O)(C=Cc1ccc(Nc2nc3ccccc3s2)cc1)OCC
BP(=O)(CCC=C(I)C(=NO)c1ccc(Br)cc1)OCC
BP(=O)(CCCCCCCCCCCCCCCCCCC)NCCCCCCO
BP(=O)(Nc1ccc(Br)cc1)C(=O)Nc1c(Br)cc(Br)cc1Br
BP(=O)(OCC)C(F)(F)F

 13 Training on 15596 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.268599
Reward: 4.430608
Trajectories with max counts:
416	Cc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49479687
Proportion of valid SMILES: 0.43871169480925576
Sample trajectories:
BP(=O)(OCCC)n1cc(Nc2c(-c3ccc(F)cc3F)nc(Br)c(Br)c2Br)cc1F
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2nncs2)cc1
Policy gradient replay...
Mean value of predictions: 0.52149
Proportion of valid SMILES: 0.5454829634260706
Sample trajectories:
BP(=O)(NCCCCCCCC)c1ccccc1
BP1(=O)OCC2OC([N-][N+]#N)OC2(C)CC1O
Bc1cc(Br)cc(Nc2ncnc3sc4ccccc4c23)c1
Bc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1F
Fine tuning...
Mean value of predictions: 0.5181818
Proportion of valid SMILES: 0.50875
Sample trajectories:
BP(=O)(Cl)Oc1cc(Br)cc(Nc2ncnc3[nH]cnc23)c1
BP(=O)(OCC)OC(=O)C=Cc1c(Br)c(Br)c(Br)c(Br)c1Br
BP(=O)(OCC)OC(=O)COC=C
BP(=O)(OCC)OCCn1cc(Br)nc1OP(=O)(Oc1ccc(Br)cc1)Oc1ccc(Br)cc1
BP(=O)(OCC)Oc1ccc(Br)c(Nc2c(Br)cnc3c(Br)ccc(Br)c23)c1

 14 Training on 17485 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.415880
Reward: 4.503164
Trajectories with max counts:
81	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6210594
Proportion of valid SMILES: 0.48375
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)c1cc(Br)ccc1Br
BP(=O)(OCC)OCOC(=O)C(Cc1ccc(P(=O)(O)OP(=O)(O)OP(=O)(O)O)cc1)c1ccc(Br)cc1F
BP(=O)(OCC1OC(CO)C(O)C1O)N1c2ncnc(N)c2S1(=O)=O
B[PH](=O)(=O)Nc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.56796587
Proportion of valid SMILES: 0.513125
Sample trajectories:
BP(=O)(CCCc1cccc(F)c1)OCOC(=O)COc1ccc(F)cc1
BP(=O)(OCC)OC(=O)Nc1ccc(Br)c(Br)c1
BP(=O)(OCC)c1ccc(Nc2ncnc3c(F)ccc(F)c23)cc1
BP(=O)(OCC1OC(Cc2ccc(O)c(I)c2)O1)C(=O)O
BP(=O)(Oc1ccccc1)c1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.5408959
Proportion of valid SMILES: 0.5371875
Sample trajectories:
BP(=O)(Oc1ccccc1)Oc1ccc(Br)cc1Br
BP(=O)(c1sc(Nc2nccc3c4ccccc4c23)c2c1CCCC2)N1CCOCC1
Bc1ccc(Nc2ncnc3c(Br)sc(Br)c23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCc1cccc(Nc2nc3c(Nc4ccc(Br)cc4)ncnc3s2)c1

 15 Training on 19759 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.969265
Reward: 5.238034
Trajectories with max counts:
503	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.59585744
Proportion of valid SMILES: 0.3771875
Sample trajectories:
BP(=O)(=Nc1ccc(F)cc1F)(Nc1ccc(Br)cc1)Sc1ccc(Br)cc1
BP(=O)(CCC(=S)Nc1ccc(Br)cc1Oc1ccc(Br)cc1)OCC
BP(=O)(N=O)c1ccc(Br)cc1Br
BP(=O)(NCc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O[PH](F)(F)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
Policy gradient replay...
Mean value of predictions: 0.6344232
Proportion of valid SMILES: 0.5073552425665102
Sample trajectories:
BP(=O)(NCCCC)c1cc(Br)c(Br)c(Br)c1Br
BP(=O)(OCC)OC(=O)Cc1ccc(Br)cc1
BP(=O)(OCC)Oc1ccc(Cl)cc1
Bc1ccc(Nc2ncnc3c(Nc4ccccc4)cc23)cc1
Bc1cnc2ncnc(Nc3ccc4ccccc4c3)c2c1
Fine tuning...
Mean value of predictions: 0.5794841
Proportion of valid SMILES: 0.5209375
Sample trajectories:
BP(=O)(O)C(=O)O
BP(=O)(OCC)OC(=O)CCCCc1ccccc1
BP(=O)(OCC)OC(=O)Nc1ccc(Nc2ncnc3cc(F)c(F)cc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1

 16 Training on 21970 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.196372
Reward: 5.536472
Trajectories with max counts:
349	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6752669
Proportion of valid SMILES: 0.35135979993748045
Sample trajectories:
BBr
BP(=O)(Cl)P(=O)(OCC)OCOCCOP(=O)(O)OP(=O)(O)O
BP(=O)(N(O)COP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)CF)P(=O)(ON)OP(F)(F)(F)F
BP(=O)(Nc1cc(Br)c(Br)cc1Br)c1c(Br)cc(Br)cc1Br
BP(=O)(Nc1ccc(F)c(F)c1)c1ccc(F)cc1F
Policy gradient replay...
Mean value of predictions: 0.6083492
Proportion of valid SMILES: 0.4945261182358461
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCC(=O)Nc1ccc(Br)cc1
Bc1cc(Nc2ncc(Br)c(Br)c2Br)cc(Br)c1O
Bc1cc(Nc2ncnc3sc(Br)c(Br)c23)cc(Br)c1O
Brc1cc(Br)c(C[n+]2c(Br)cc(Br)c3ccc(Br)cc32)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3sccc23)cc1Br
Fine tuning...
Mean value of predictions: 0.559789
Proportion of valid SMILES: 0.533125
Sample trajectories:
Bc1cc(Nc2ncnc3sc(Br)cc23)cc(Br)c1Br
Bc1ccc(Nc2ncnc3ccc(Br)c(Br)c23)cc1
Bc1cccc(Nc2cc(Nc3ncnc4ccsc34)ccn2)c1
BrCCc1sc2ncnc(Br)c2c1Nc1cccc(Br)c1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1

 17 Training on 24139 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.877833
Reward: 5.155100
Trajectories with max counts:
47	Brc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Mean value of predictions: 0.641787
Proportion of valid SMILES: 0.4548296342607065
Sample trajectories:
BP(=O)(CC(Br)Br)OCC
BP(=O)(OCC)OC(=O)CCC(=O)Nc1c(F)c(F)c(F)c(F)c1F
BP(=O)(OCC)Oc1cc(Br)c(Br)[nH]1
BP(=O)(OCc1ccc(Br)cc1)c1ccc(Br)cc1
BP1(=O)CCN1CCC1(Br)CC1
Policy gradient replay...
Mean value of predictions: 0.52016366
Proportion of valid SMILES: 0.5346875
Sample trajectories:
BP(=O)(OCC[N+](C)(C)C)[n+]1ccc(Br)c(Br)c1
Bc1ccc(Nc2ncnc3ccsc23)c(F)c1
Bc1ccc(Nc2ncnc3sccc23)cc1
BrCCCOc1ccccc1Nc1ncnc2sccc12
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.6119351
Proportion of valid SMILES: 0.539712320200125
Sample trajectories:
BP(=O)(OCC)Oc1ccc(F)c(Nc2ncnc3c(F)cccc23)c1
BP1(=O)NS(=O)(=O)N(CCN)P(=O)(c2ccc(Br)cc2)O1
Bc1cc(Nc2ncnc3ccsc23)cc(Br)c1O
Bc1cc(O)c(Br)c(Br)c1Br
Bc1ccc(Nc2ncnc3c(Br)cc(Br)cc23)cc1

 18 Training on 26483 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.874173
Reward: 5.059152
Trajectories with max counts:
96	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.66610646
Proportion of valid SMILES: 0.5581613508442776
Sample trajectories:
BP(=O)(CC)CCc1c(N)ccc2c(Br)cccc12
BP(=O)(NO)c1ccc(Br)cc1Br
BP(=O)(c1ccc(F)cc1)N(CCCl)Nc1sc2c(Br)c(F)c(F)cc2c1-c1ccc(F)cc1F
Bc1cc(Nc2cc(I)ccc2Br)ccc1Br
BrCc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.6528144
Proportion of valid SMILES: 0.5220381369177868
Sample trajectories:
BP(=O)(C=C(Br)Br)OCCO
BP(=O)(N(CC)CCC=C)P(=O)(OCC)OCC
BP(=O)(Nc1ccc(Cl)c(Nc2nccs2)c1)c1ccc(F)c(F)c1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1-c1ccc(Br)cc1Br
Fine tuning...
Mean value of predictions: 0.62590915
Proportion of valid SMILES: 0.55
Sample trajectories:
BP(=O)(C(=O)NCCc1cccs1)N1CC(O)c2cc(Br)cc(Br)c2OC1=O
Bc1ccc(Nc2ncnc3sc(Nc4ccccc4)cc23)cc1
Bc1ccc(Nc2ncnc3sc4ccccc4c23)cc1
BrC=CC=CC=CC=CC=CCCCCBr
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1

 19 Training on 29295 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 33.332100
Reward: 5.593866
Trajectories with max counts:
305	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5337579
Proportion of valid SMILES: 0.3925
Sample trajectories:
BP(=O)(CBr)COCCCBr
BP(=O)(N=C(NO)c1ccccc1)OCC
BP(=O)(Nc1ccc(Br)cc1)OCc1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cc1Br)OCC
BP(=O)(c1cccc(F)c1)c1ccc(F)c(F)c1F
Policy gradient replay...
Mean value of predictions: 0.5841686
Proportion of valid SMILES: 0.54875
Sample trajectories:
BP(=O)(Nc1ccc(F)cc1)c1ccc(Br)cn1
BP(=O)(c1ccccc1)N(O)CCCl
Bc1ccc(Nc2ncnc3sc4ccccc4c23)s1
Brc1cc(Br)c2ncnc(Nc3ncnc4ccccc34)c2c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.6351981
Proportion of valid SMILES: 0.53625
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)s1
Bc1cccc(Nc2nncn2-c2ccc(Br)cc2Br)c1
Brc1cc(Br)c(-n2ccc(C=NNc3ncnc4ccccc34)c2)s1
Brc1cc(Br)c(Nc2ncnc3ccsc23)cn1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ccnc2c1

 20 Training on 31597 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 33.072978
Reward: 5.434934
Trajectories with max counts:
74	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.676474
Proportion of valid SMILES: 0.5461081587996249
Sample trajectories:
BP(=O)(Br)OCC1(OCC=C)CCC(F)C1
BP(=O)(NCCCCCCCC)C(=O)Nc1ccc(OCC(F)(F)F)c(Br)c1
BP(=O)(OCC)c1cc2c(Nc3ccc(Cl)c(Br)c3)ncnc2n1Cc1cc(F)c(F)c(F)c1
Bc1cc(Br)c(C#N)c(Br)c1Br
Bc1cc(Br)c2c(Nc3cc(Br)c(Br)s3)c(Br)cc(Br)c2c1
Policy gradient replay...
Mean value of predictions: 0.68845236
Proportion of valid SMILES: 0.525
Sample trajectories:
BP(=O)(OCC)OC(=O)c1ccc(Nc2ncnc3c(F)c(F)c(F)cc23)cc1F
BP1(=O)OCCc(Br)c2cc(Nc3cc(Br)c(Br)cc3C(=O)N(O)OC(=O)Nc3cc(F)c(F)c(F)c3)c(Br)c21
Bc1cc(Nc2ncnc3ccsc23)sc1-c1ccccc1Br
Bc1cccc(Nc2ncnc3ccsc23)c1
BrC=C=C(Br)C=NOCCBr
Fine tuning...
Mean value of predictions: 0.6301514
Proportion of valid SMILES: 0.536875
Sample trajectories:
Brc1c(-c2ncnc3ccsc23)ccc2ccccc12
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1ccc(-c2cc(Nc3ncnc4ccsc34)ccc2Br)cc1
Brc1ccc(-c2cc3c(Nc4cccc(Br)c4)ncnc3s2)cc1

Trajectories with max counts:
303	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5880576
Proportion of valid SMILES: 0.43442930366295784
Mean Internal Similarity: 0.5124204264641719
Std Internal Similarity: 0.11698029868243766
Mean External Similarity: 0.4150526431647002
Std External Similarity: 0.058800296907884124
Mean MolWt: 371.09322301425664
Std MolWt: 77.58288561921366
Effect MolWt: -1.2215272163629938
Mean MolLogP: 5.080285809572303
Std MolLogP: 1.367187951868603
Effect MolLogP: 0.26683299461581694
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 98.054819% (1109 / 1131)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/egfr_mixed.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5521.633421421051, 'valid_fraction': 0.43442930366295784, 'active_fraction': 0.5651798561151079, 'max_counts': 303, 'mean_internal_similarity': 0.5124204264641719, 'std_internal_similarity': 0.11698029868243766, 'mean_external_similarity': 0.4150526431647002, 'std_external_similarity': 0.058800296907884124, 'mean_MolWt': 371.09322301425664, 'std_MolWt': 77.58288561921366, 'effect_MolWt': -1.2215272163629938, 'mean_MolLogP': 5.080285809572303, 'std_MolLogP': 1.367187951868603, 'effect_MolLogP': 0.26683299461581694, 'generated_scaffolds': 1131, 'novel_scaffolds': 1109, 'novel_fraction': 0.9805481874447391, 'save_path': '../logs/replay_data_s2-4.smi'}
