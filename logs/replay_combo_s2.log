starting log


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.747517
Reward: 1.000000
Mean value of predictions: 0.0012588513
Proportion of valid SMILES: 0.7956181533646323
Sample trajectories:
BP(=O)(O)COP(=O)(O)O
Brc1ccc(-n2cccc2-c2ccccn2)cc1
Brc1cnc(NCC2CCN(Cc3ncccn3)CC2)s1
C#CC(=O)C(C)=NNC(=O)C1C=C(S(=O)(=O)c2ccc3ccccc3c2)CC=CC1
C#CC1C(OCC)OC(c2ccncc2)=CC1c1ccc(C(=O)c2cccc(Cl)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.020521894
Proportion of valid SMILES: 0.7751114158381899
Sample trajectories:
Brc1ccc(C2=Nc3ncnn3CCCc3ccccc32)o1
Brc1ccc(Nc2ncnc3ccncc23)cc1
Brc1ccc2[nH]c3c(c2c1)CCCC3
Brc1ccc2c(c1)NCc1cccnc1N2
Brc1ccc2nc(NN=Cc3ccccc3SCCCCCCCCCCCCCN3CCOCC3)sc2c1
Fine tuning...
Mean value of predictions: 0.020882087
Proportion of valid SMILES: 0.763573883161512
Sample trajectories:
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc2ncnc(Nc3ccnc(Br)c3)c2c1
Brc1ccc2oc(-c3ccccc3)c(-c3ccccc3)c2c1
C#CCCCCCCCCCCCCCNC(=O)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCBr
C#CCCCCCCCCCCCCN(CCCCCCCCCCCCCCCCCCCCCCCCCCCCC)C(=O)CCCCCCCCCCCN(CCCC)CCCCCC

  2 Training on 354 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.352624
Reward: 1.090000
Trajectories with max counts:
5	Cc1ccc(Nc2ncnc3ccccc23)cc1
5	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.02397915
Proportion of valid SMILES: 0.7637690776376908
Sample trajectories:
BrC1CCC2(CCN(Cc3ccco3)CC2)O1
BrCCCN1CCCCC1
Brc1ccc2nc3cc(Br)c(Br)cc3c(CCN3CCCCC3)cnc2c1
Brc1cccc2c(Nc3ccc(I)cc3)ccnc12
Brc1cccc2c1NCCc1c(ccc3c1CCCO3)OCCCCO2
Policy gradient replay...
Mean value of predictions: 0.04266239
Proportion of valid SMILES: 0.7823086574654956
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCC1CCNCC1
Brc1ccc(Nc2ccccc2-c2ccccc2OCC2CNc3ncccc3-c3ccccc32)cc1
Brc1ccc(Nc2nccnc2-c2cnccn2)cc1
Brc1ccc(Nc2ncnc3c2CCC3)cc1
Brc1ccc2ccc(Nc3ccncc3)cc2c1
Fine tuning...
Mean value of predictions: 0.038571432
Proportion of valid SMILES: 0.7904642409033877
Sample trajectories:
Brc1cc2c(nc1CCCCCc1ccsc1)CCCCO2
Brc1ccc(-c2ccccc2)c2ncccc12
Brc1ccc(-c2cscc2Nc2ccccc2)cc1
Brc1ccc(-c2nc3ccccc3o2)c2ncnc(Nc3ccccc3)c12
Brc1ccc(Nc2ncnc3cccc(Br)c23)cn1

  3 Training on 729 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.310076
Reward: 1.123967
Trajectories with max counts:
21	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.044851445
Proportion of valid SMILES: 0.7685329996872068
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCN1CCCCCCCC1
BrCCCCCCCCN=C(Nc1ccccc1)c1ccccc1
Brc1ccc(-c2c3cccc2c2ccc4ccccc4-c32)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc2c(c1)-c1ccccc1O2
Policy gradient replay...
Mean value of predictions: 0.09591161
Proportion of valid SMILES: 0.7445495680789799
Sample trajectories:
BP(=O)(OCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)C(F)(F)F
BrCCN1CCCCC1
BrCc1nc(NC2=NCCCCCn(c3nc4ccccc4[nH]3)cc(Br)cn2)n[nH]1
Brc1ccc(NN=Cc2cncnc2)cc1
Brc1ccc(Nc2ncnc3scnc23)nc1
Fine tuning...
Mean value of predictions: 0.09847953
Proportion of valid SMILES: 0.7310816588285592
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3Br)n2n1
Brc1ccc(Nc2cc3ccncc3cn2)o1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc2cc1Nc1ncnc(cc(Nc3ccc4[nH]cnc4c3)ncn1)N2
Brc1ccc2ncnc(Nc3ccccn3)c2c1

  4 Training on 1360 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.229288
Reward: 1.193818
Trajectories with max counts:
10	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.0960396
Proportion of valid SMILES: 0.698961937716263
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC1CCCCCCC1NCCCCNc1ncnc2nsnc12
Brc1ccc(Nc2ncnc3cc(Br)ccc23)c(Br)c1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(NCc4ccccc4Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.15930736
Proportion of valid SMILES: 0.5986394557823129
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ncc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)nc23)cc1
Brc1ccc(Nc2ncnc3cncnc23)cc1
Fine tuning...
Mean value of predictions: 0.16413105
Proportion of valid SMILES: 0.5822825735531846
Sample trajectories:
Brc1cc2ncnc(N3CCCCCC3)c2cc1-c1nc2nc(Nc3ccncc3)ccc2s1
Brc1ccc(Nc2ncnc3c2c(I)cc2ncncc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)c(-c2ccsc2)c1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccsc23)cc1

  5 Training on 2450 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 18.001479
Reward: 1.168541
Trajectories with max counts:
16	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.16215031
Proportion of valid SMILES: 0.5911154345006485
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCCCCCCCCCCCCCCCCC)OCC
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)cnc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.21032807
Proportion of valid SMILES: 0.5258785942492012
Sample trajectories:
Brc1cc2c(c(Br)c1Br)CCCCCCCCCCCCCCN1CCCCCC21
Brc1cc2nc(Nc3ncccn3)sc2nc1N1CCN(CCc2ccncc2)CC1
Brc1cc2ncnc(-c3ccncn3)c2s1
Brc1cc2ncnc(Nc3ccnc4cnccc34)c2cn1
Brc1cc2nnc(Nc3ccccc3)nc(cs1)n2
Fine tuning...
Mean value of predictions: 0.19356348
Proportion of valid SMILES: 0.5461808884627677
Sample trajectories:
BP(=O)(C(=O)Nc1ccc(Br)cc1)N(O)CCCl
BP(=O)(OCC)C(=O)COC(=O)C(F)(F)F
BrC1C2C=C(c3cnccn3)C1CC2
BrCC(Br)(Br)CBr
Brc1cc(Br)c2ncnc(Nc3ccnc4ccccc34)c2n1

  6 Training on 3703 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 19.986287
Reward: 1.428418
Trajectories with max counts:
10	Nc1ncnc2ncnc(Nc3ccc(Cl)cc3)c12
10	Nc1ncnc2ncnc(Nc3ccc(F)cc3)c12
Mean value of predictions: 0.2507822
Proportion of valid SMILES: 0.5261158594491928
Sample trajectories:
B[PH](=O)(Nc1nc2nc(Cl)nc(N)c2nc1NC(=O)OCc1cc(Br)cc(Br)c1)(P(=O)(O)O)P(=O)(O)O
BrCc1ccc(Nc2ncnc3cc(Br)c(Nc4nnc5ncnc(Nc6ccc(Br)cc6)c5n4)cc23)cc1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1ccc(-c2cc(Nc3ccncc3)ncn2)c2ncnc(Nc3cccc4ccncc34)c12
Brc1ccc(Nc2ncnc3[nH]cnc23)cc1
Policy gradient replay...
Mean value of predictions: 0.32517081
Proportion of valid SMILES: 0.6008841174613199
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(NCc2ccnc3cncnc23)ccc1Nc1ncnc2ccccc12
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCN(CCN2CCCCC2)CC1
Brc1cc2ncnc(Nc3ccnc(Nc4ccccc4)c3)c2s1
Brc1ccc(-c2cc(Br)ccc2Br)c(COc2ccncc2)c1
Fine tuning...
Mean value of predictions: 0.3090625
Proportion of valid SMILES: 0.6066350710900474
Sample trajectories:
BP(=O)(Br)CCCCCCCCCCCNS(=O)(=O)c1cc(Br)c(NC(=O)CNC(=O)CN(C)S(=O)(=O)c2ccc(F)cc2)c2ccccc12
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNc1ccc(-c2nc3ccccc3[nH]2)c2ccccc12
BrCCCCNCCCCCCCCCCCNCCCCNc1cnc(Nc2cc(Br)c(Br)cn2)nc1
BrCCNCCCCCCCCNCCCCCNCCCCCNCCCCCCNc1ccnc2cnc3cnc[nH]c3c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1

  7 Training on 5531 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 21.425286
Reward: 1.769435
Trajectories with max counts:
37	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.35987023
Proportion of valid SMILES: 0.5958749597164035
Sample trajectories:
BP(=O)(OP(=O)(O)O)N(CCCCNS(=O)(=O)c1cc(F)c(Br)cc1Br)C(Cl)Cl
Bc1cc(Nc2ncnc3cc(F)ccc23)cnc1-c1cccs1
BrC=C(Br)c1ccc(-c2cc3ccnc(Nc4cccc5ccccc45)nc3n2)nc1
BrC=C=CCCC=CCCCCCCCCCCCCCNc1ncnc2ncnc(Nc3ccc(Br)cn3)c12
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC1CC=CCCCCCCCCCCCCCCCCCCCCCC1
Policy gradient replay...
Mean value of predictions: 0.33480346
Proportion of valid SMILES: 0.6553565818410305
Sample trajectories:
BP(=O)(OCC1CCCCC1)Oc1cc2ncnc(Nc3cccc(Br)c3)c2cc1Br
BrCCCNc1ccc(Nc2ccnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ncc23)c1
Brc1cc(Br)c(Br)c2c3c(c1)C3c1ccccc12
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2nc2ncnc2n1
Fine tuning...
Mean value of predictions: 0.3377778
Proportion of valid SMILES: 0.6636335111390022
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccccc3)ncnc2c1
Brc1cc2ncnc(Nc3cccc4ccccc34)c2cn1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccc2ncnc(Nc3ccsc3)c2c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCNCC1
Brc1cc2ncnc(Nc3ccccc3)c2cn1

  8 Training on 7649 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 22.199151
Reward: 1.761926
Trajectories with max counts:
26	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.22294243
Proportion of valid SMILES: 0.7465775230818211
Sample trajectories:
Brc1[nH]c(Br)c2c1sc1ccccc12
Brc1cc(Br)c2ncnc(Nc3ccc4ccccc4c3)c2c1
Brc1cc2c(Nc3ccccc3)ncnc2cc1N1CCCCC1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Brc1ccc(-c2ncnc3ccccc23)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.398053
Proportion of valid SMILES: 0.5785356695869838
Sample trajectories:
BP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)NP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O[PH](=O)(O)(O)OP(=O)(O)O
BP(=O)(OCC1(O)OCCN1CC(F)(F)F)C(F)(F)C(F)F
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCN=C1CCCCCC1
BrCCCNc1ccc(Nc2ncnc3ncnc(C4CCCCCCCC4)c23)cc1
BrCCN1CCCCC1CNc1ccccc1Nc1cccc(Br)c1
Fine tuning...
Mean value of predictions: 0.3899842
Proportion of valid SMILES: 0.5937402190923318
Sample trajectories:
Br
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)c(Nc2nc(-c3ccccc3)nc3cnccc23)c(Br)c1
Brc1cc(Br)cc(Nc2nc(-c3ccncc3Br)c3c(Nc4ccccc4)ncnc3n2)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

  9 Training on 9590 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 22.930321
Reward: 2.080327
Trajectories with max counts:
41	Fc1ccc(Nc2ncnc3cc(F)ccc23)cc1
Mean value of predictions: 0.42617255
Proportion of valid SMILES: 0.5408706545568431
Sample trajectories:
BP(=O)(NS(=O)(=O)c1ccc(Br)cc1)c1ccc2ncnc(N)c2n1
BP(=O)(OCC)Oc1c(Br)c(Br)c(Br)c(Br)c1Br
B[PH](=O)(NO)(Nc1cc(Br)c(Br)c(Br)c1)P(=O)(O)O
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCc1c[nH]c2ccccc12
Brc1cc(Br)c(Br)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.39098278
Proportion of valid SMILES: 0.6880446148483792
Sample trajectories:
BrCCCCCCCCCCCCNCCCCCCCNCCCCCCCCCCCCCCNCCCNCCCCCCNCCCN1CCCCCc2c(Nc3ccc(Br)cc3)nc21
BrCCCCCCCCCCNc1c2c(nCCCC2)nc2ncnc12
BrCCCNCC1CCCCCCCCCCCCCCCCCCCCCCCCC1CCCN1CCCCCCCCCCC1
Brc1cc(Nc2ncnc3cc(Br)c(Br)cc23)ccn1
Brc1cc2c(s1)c(Nc1nc3nnnnc3cc1Br)ncnc1cc(ncn1)N2
Fine tuning...
Mean value of predictions: 0.3734767
Proportion of valid SMILES: 0.6901060070671378
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCCCCCCC=CCCCF)OCC
BP(=O)(O)P(=O)(O)OP(=O)(O)OP(=O)(OCC)OCCCCCCCCCCCCCCCCCCCCCCCC=C
BrCCCNc1nc2ncnc(Nc3cccc(Br)c3)c2s1
BrCCNc1ncc2ncnc(Nc3ccccc3)c2n1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1

 10 Training on 11566 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 25.489784
Reward: 2.208421
Trajectories with max counts:
6	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4071321
Proportion of valid SMILES: 0.6737732656514382
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCCCNc1c2c(nCCCC2)nc2ncnc12
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCc1c(N2CCCCCC2)nc2ccccc2c1Br
BrCCN(CBr)c1cccc(Br)c1Nc1ncnc2ccc(Br)cc12
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.45958254
Proportion of valid SMILES: 0.6773778920308483
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC(N)=O)NCCCCCCCCCCCCCCCCCCCCCCCCCCCN
BP(=O)(NCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC(=O)Nc1cc(Br)c(Br)cc1Br)N1CCCCC1
BrC1=CC2=Nc3scc(Br)c3CCC2C1
BrC=CC=CC=Nc1cccc(Nc2cc3cc(Br)cnc3c3cccnc23)c1
BrCCCCOc1cc(Br)c(Br)cc1Nc1ncnc2ccc(Br)cc12
Fine tuning...
Mean value of predictions: 0.4460076
Proportion of valid SMILES: 0.6791478373143964
Sample trajectories:
BP(=O)(OCC)OCC=CCCC
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BrCCCCCCCCCCCCCCCCCCCNCCCSc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BrCCNc1nc2ncnc(Nc3cccc(Br)c3)c2s1
BrCCNc1ncnc2cc3c4ccccc4N=Nc(n1)c3s2

 11 Training on 13599 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.811623
Reward: 2.234413
Trajectories with max counts:
19	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.46557713
Proportion of valid SMILES: 0.6733149931224209
Sample trajectories:
BP(=O)(CCBr)C(F)(F)F
BP(=O)(CCCCCCCCCCCCCCCC)Nc1ncnc2ncnc(Nc3ccc(Br)cc3)c12
BP(=O)(Nc1ccc(Br)cc1)P(=O)(OCC)OCCCCCCCCF
BP(=O)(OCC)OCCC=CCCCCCCCCC(=O)c1ccc(Br)o1
BP(=O)(OCC1NC(=O)N(CCCl)C(=O)N(O)C(=O)O1)C(O)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
Policy gradient replay...
Mean value of predictions: 0.4919333
Proportion of valid SMILES: 0.7042208822596001
Sample trajectories:
BrCCNc1ncnc2ncnc(Nc3ccccc3)s12
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Brc1cc2c(cc1Nc1ncnc3ncnc(Nc4ccccc4)c13)Sc1ccccc1N2
Fine tuning...
Mean value of predictions: 0.48811567
Proportion of valid SMILES: 0.7034329307056579
Sample trajectories:
BC(=O)Oc1cc2ncnc(Nc3cc(Br)c4[nH]ccc4c3)c2s1
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCCCCCNc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Brc1cc(Br)c2ncnc(Nc3ccc4ccccc4c3)c2n1

 12 Training on 15928 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.917572
Reward: 2.136992
Trajectories with max counts:
12	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.44522044
Proportion of valid SMILES: 0.7526281208935611
Sample trajectories:
BP(=O)(OCCCCCCCCCCCCBr)P(=O)(O)OP(=O)(O)OP(=O)(O)NP(=O)(O)COCCCl
Bc1cnc(Nc2ccc(Br)cc2)c2sccc12
BrC1=CN(Cc2cccc(Br)c2)Nc2cc(Br)c(Br)cc2N1CCCCCCCCCCCCCNCC1CCCCCC1
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1Br
Brc1ccc(-c2ncnc3ccsc23)c2ccccc12
Policy gradient replay...
Mean value of predictions: 0.50818664
Proportion of valid SMILES: 0.7133437990580848
Sample trajectories:
BP(=O)(CCCCC)c1ccccc1O
BP(=O)(Nc1ncc(CC(=O)Nc2cc(F)c(F)cc2F)s1)C(F)(F)F
BrC(Br)Br
BrC=CCN1CCCC1CCCCCCCCCCCNc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BrCCCCCCCCCCCCNCCCCCCNc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.5202317
Proportion of valid SMILES: 0.7038895859473023
Sample trajectories:
BP(=O)(OCCCc1ccccc1)c1ccccc1
BrCCCCCC1CCCN1c1ncnc2ncnc(Nc3ccc(Br)cc3)c12
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(-c3ccccc3Br)ncnc2s1
Brc1cc2ncnc(Nc3cccc(Br)c3Br)c2cc1-c1ccccc1

 13 Training on 18514 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.757130
Reward: 2.416214
Trajectories with max counts:
16	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5055938
Proportion of valid SMILES: 0.7798657718120805
Sample trajectories:
BP1(=O)Nc2ncnc(Nc3ccc(Br)cc3)c2N1
Brc1cc(Br)c2c(Nc3ccc(Br)c4cncnc34)ncnc2c1
Brc1cc(Br)c2c(Nc3cnc(-c4ccccc4Br)s3)ncnc2c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1nc2ncnc(Nc3ccccc3)c2s1
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1Br
Policy gradient replay...
Mean value of predictions: 0.56349707
Proportion of valid SMILES: 0.7044444444444444
Sample trajectories:
Brc1cc(Br)c(Br)c(Nc2ncnc3scc(Br)c23)c1
Brc1cc(Br)c2ncnc(Br)c2c1Br
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c1
Brc1ccc(Br)c(-c2ccc3ncnc(Nc4cccc5ccccc45)c3c2)c1
Brc1ccc(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.5562528
Proportion of valid SMILES: 0.7096055979643766
Sample trajectories:
BP(=O)(c1cc2ccccc2c2c(Br)ccnc2c2ncnc(Nc3cc(Br)cs3)c12)P(=O)(Oc1ccccc1)O[PH](O)(F)F
Brc1cc(Br)c2cc(Br)c(Br)c(Br)c2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3c4ccccc4c23)cs1

 14 Training on 21439 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.469432
Reward: 3.066045
Trajectories with max counts:
9	CN(C)CCCNc1ncnc2ncnc(Nc3ccc(Br)cc3)c12
9	CN(C)CCCNc1ncnc2ncnc(Nc3cccc(Cl)c3)c12
Mean value of predictions: 0.5619658
Proportion of valid SMILES: 0.7398039835599115
Sample trajectories:
BP(=O)(N(CCC(=O)Nc1cc(Br)c(Br)cc1Br)c1ccccc1)P(=O)(O)O
BP(=O)(OCC)C(=O)Nc1ccc(Br)cn1
Br
Brc1cc2ncnc(Nc3cccc(Br)c3-c3ccncc3)c2c(Br)c1Br
Brc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.55279666
Proportion of valid SMILES: 0.752311125278929
Sample trajectories:
BP(=O)(Nc1ccc(Br)c2ncnc(Br)c12)c1ccc(Br)cc1
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCCCCCCNc1ccnc2ccc(Nc3ccc(Br)cc3)cc12
Brc1cc(Br)c2c(Nc3ncnc4ccc(Br)cc34)ccnc2c1
Brc1cc(Br)cc(Nc2ncnc3sc(Br)nc23)c1
Brc1cc(Nc2ncnc3cc(Br)c(Br)cc23)ccn1
Fine tuning...
Mean value of predictions: 0.5472326
Proportion of valid SMILES: 0.76479949077021
Sample trajectories:
BP(=O)(CNc1ccc(Br)cc1)c1ccc(CN(CCCl)CCCl)cc1
BP(=O)(N(O)C=O)P(=O)(O)c1ccccc1
BrBr
BrCCCCCCCCCCCCCCCCCCCCCCCCCNCCCCNCCCCCNCCCSc1cc2ncnc(Nc3ccccc3)c2c2ccccc12
BrCCN=C(Br)c1ccc2ncnc(Nc3cccc(Br)c3)c2c1

 15 Training on 24666 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.291933
Reward: 2.924920
Trajectories with max counts:
16	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5799649
Proportion of valid SMILES: 0.7130325814536341
Sample trajectories:
BP(=O)(OCC)OCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
Br
BrCCN(CCCNc1ccc2ncnc(Nc3cccc(Br)c3)c2n1)c1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BrCCc1cc2ncnc(Nc3cccc(Br)c3)c2s1
BrCN1CCCCCCCCCCC1CCNCc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.64101046
Proportion of valid SMILES: 0.7456828885400314
Sample trajectories:
BP(=O)(NC(CN)CCCCN)OCCCCCCCCCCCCCCCCNc1ccccc1Br
BP(=O)(OCC)S(=O)(=O)CCNC(=O)c1ccc(Br)cc1
BrC=CBr
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCN1CCCCC1CCCCc1ccccc1
BrCCCCCCCCCCCCCCCCCCCCCCCCCCNCCCNc1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.6355383
Proportion of valid SMILES: 0.7278162366268093
Sample trajectories:
Bc1cc(Br)c2ncnc(Nc3cccc(Br)c3)c2n1
BrC1CCCC1N1CCc2c1ncn2C1CCCCC1
BrCCCBr
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCN1CC=C(c2cc3ncnc(Nc4ccc5ccccc5c4)c3nc2-c2ccccc2)CC1

 16 Training on 28216 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.717413
Reward: 3.199688
Trajectories with max counts:
34	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.63237476
Proportion of valid SMILES: 0.6739606126914661
Sample trajectories:
BrCCCNc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BrCCNc1nc2ccc(Br)nc2s1
BrSc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2n1
Policy gradient replay...
Mean value of predictions: 0.6051865
Proportion of valid SMILES: 0.6873045653533458
Sample trajectories:
BP(=O)(NO)c1ccc2nc(Nc3ncnc4sc(Br)c(Br)c34)cc(Br)c2c1
Bc1ccccc1-c1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCc1cccc(Nc2ncnc3ccccc23)c1
BrCN1CCCCCNCC1
BrCc1ccc2ncnc(Nc3ccsc3)c2c1
Fine tuning...
Mean value of predictions: 0.5953551
Proportion of valid SMILES: 0.7007824726134585
Sample trajectories:
BP(=O)(N(CC#N)c1cccc(Br)c1)P(=O)(Oc1cccc(Br)c1)c1ccc(Br)cc1
BP(=O)(OCC)N1CCN(CCOc2cccc(Br)c2)CC1
BrC(Br)(Br)Br
BrSc1ccc(Nc2ncnc3sccc23)cc1
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(Br)c1

 17 Training on 31525 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 33.307371
Reward: 3.362458
Trajectories with max counts:
35	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5906508
Proportion of valid SMILES: 0.6837981823879662
Sample trajectories:
BP(=O)(Nc1ccccc1)OCC1CCCCC1
Bc1ccc2nc(Nc3ccc(I)cc3)ccc2c1
BrCCNc1nccc(Br)c1-c1cccc2ncncc12
Brc1cc(-c2nc3ccc(Br)nc3c3ccccc23)ccn1
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.5906559
Proportion of valid SMILES: 0.7279267495094833
Sample trajectories:
BP(=O)(OCCCC)C(F)(F)F
Bc1cc(Nc2ncnc3cc(Br)cc(Br)c23)c(Br)c(Br)c1Br
Bc1ccc(Br)cc1Br
BrCCCCCCCNc1ccc2c(c1)CN2Cc1ccccc1
BrCCCN(Nc1ncncn1)c1ccc(Br)c2c(Nc3ccc(Br)cc3Br)ncnc12
Fine tuning...
Mean value of predictions: 0.60587156
Proportion of valid SMILES: 0.7091737150292778
Sample trajectories:
BP(=O)(OCCO)c1ccc(Br)cc1
Bc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2c(N)n1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Nc2cc(-c3cc(Br)nc4ncncc34)ncn2)ccn1
Brc1cc(Nc2ncnc3c(Br)c(Br)nc(Nc4ccccc4)c23)cs1

 18 Training on 34741 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.374297
Reward: 3.643903
Trajectories with max counts:
29	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.62099344
Proportion of valid SMILES: 0.7010512483574245
Sample trajectories:
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(I)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.60212046
Proportion of valid SMILES: 0.737566468564279
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ccc(Br)c2n1
Brc1cc(Nc2nc3ccc(Br)c(Br)c3s2)ccn1
Brc1cc2ncnc(N3CCCCC3)Nc3cccc2nc2cnccc3c2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Brc1cc2ncnc(Nc3ccnc4cccnc34)c2cn1
Fine tuning...
Mean value of predictions: 0.6232519
Proportion of valid SMILES: 0.7428035043804756
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3cccc(Br)c23)c1
Brc1cc(Nc2ncnc3ccccc23)c2ncnc(Nc3ccncc3)c2n1
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCCCCCC1
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1cc2ncnc(Nc3ccccn3)sc2nc1Nc1ccccc1

 19 Training on 38285 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 39.144082
Reward: 3.904469
Trajectories with max counts:
20	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.63260317
Proportion of valid SMILES: 0.741875
Sample trajectories:
Brc1cc(Nc2ncnc3sccc23)ccc1-c1ccccc1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1ccc(-c2ncnc3ccc(NCCc4cccc5ncnc(Nc6ccccc6)c45)cc23)cc1Br
Brc1ccc(Nc2ccnc3cccnc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6ccccc6)c5c4)cc3[nH]2)cc1
Policy gradient replay...
Mean value of predictions: 0.685203
Proportion of valid SMILES: 0.7591119946984758
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccccc23)cs1
Brc1cc2c(Nc3ccc(Br)c(Br)c3Br)cccc2s1
Brc1cc2ncnc(Nc3cc(Br)c(Br)cn3)c2s1
Fine tuning...
Mean value of predictions: 0.6662437
Proportion of valid SMILES: 0.7791694133157547
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNc1cc2ncnc(Nc3ccccc3)c2s1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Nc2ncnc3ccsc23)cs1
Brc1cc(Nc2ncnc3ncsc23)nc2ccccc12
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCCCCC1

 20 Training on 42237 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 37.584382
Reward: 3.886337
Trajectories with max counts:
12	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.7010135
Proportion of valid SMILES: 0.7479469361970941
Sample trajectories:
Brc1cc(Br)c(Nc2ncnc3ccsc23)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc2c(c(Br)c1Br)CCc1c-2ccc(Br)c1Br
Brc1cc2ncnc(Nc3ccc4[nH]cnc4c3)c2s1
Policy gradient replay...
Mean value of predictions: 0.5815579
Proportion of valid SMILES: 0.7636655948553055
Sample trajectories:
BP(=O)(NCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)C(=O)Nc1ccccc1Br
BP(=O)(Nc1ccccc1Br)P(=O)(O)Oc1cc(Br)cc(Br)c1F
BP(=O)(OCC)C(CCCCCCCCCCCCCCCCCNC(=O)c1ccccc1)c1ccccc1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc2ncnc(Nc3cnc4ccccc4c3)c2s1
Fine tuning...
Mean value of predictions: 0.5888982
Proportion of valid SMILES: 0.7741518578352181
Sample trajectories:
BIc1ccccc1Nc1nc2ncnc(Nc3ccccc3)sc2cc1-c1ccccc1
Brc1cc(Br)c(-c2nccc3cncnc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)cc(CNc2ncnc3sc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

Trajectories with max counts:
50	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5779982
Proportion of valid SMILES: 0.7009075111025295
Mean Internal Similarity: 0.49060732184221056
Std Internal Similarity: 0.11024464010702348
Mean External Similarity: 0.4218304963322064
Std External Similarity: 0.07541109173249697
Mean MolWt: 532.3229030713343
Std MolWt: 248.2641820279887
Effect MolWt: 0.1559493894916026
Mean MolLogP: 8.268886114597096
Std MolLogP: 6.430093560260611
Effect MolLogP: 0.665841981968202
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.785349% (1148 / 1174)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 0, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5628.670406103134, 'valid_fraction': 0.7009075111025295, 'active_fraction': 0.5561065197428834, 'max_counts': 50, 'mean_internal_similarity': 0.49060732184221056, 'std_internal_similarity': 0.11024464010702348, 'mean_external_similarity': 0.4218304963322064, 'std_external_similarity': 0.07541109173249697, 'mean_MolWt': 532.3229030713343, 'std_MolWt': 248.2641820279887, 'effect_MolWt': 0.1559493894916026, 'mean_MolLogP': 8.268886114597096, 'std_MolLogP': 6.430093560260611, 'effect_MolLogP': 0.665841981968202, 'generated_scaffolds': 1174, 'novel_scaffolds': 1148, 'novel_fraction': 0.9778534923339012, 'save_path': '../logs/replay_combo_s2-1.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.747517
Reward: 1.000000
Mean value of predictions: 0.0012588513
Proportion of valid SMILES: 0.7956181533646323
Sample trajectories:
BP(=O)(O)COP(=O)(O)O
Brc1ccc(-n2cccc2-c2ccccn2)cc1
Brc1cnc(NCC2CCN(Cc3ncccn3)CC2)s1
C#CC(=O)C(C)=NNC(=O)C1C=C(S(=O)(=O)c2ccc3ccccc3c2)CC=CC1
C#CC1C(OCC)OC(c2ccncc2)=CC1c1ccc(C(=O)c2cccc(Cl)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.020521894
Proportion of valid SMILES: 0.7751114158381899
Sample trajectories:
Brc1ccc(C2=Nc3ncnn3CCCc3ccccc32)o1
Brc1ccc(Nc2ncnc3ccncc23)cc1
Brc1ccc2[nH]c3c(c2c1)CCCC3
Brc1ccc2c(c1)NCc1cccnc1N2
Brc1ccc2nc(NN=Cc3ccccc3SCCCCCCCCCCCCCN3CCOCC3)sc2c1
Fine tuning...
Mean value of predictions: 0.03316537
Proportion of valid SMILES: 0.6856155778894473
Sample trajectories:
BrCCN=C(Nc1nc(-c2ccccc2)c2ncnc(Nc3ccccc3)c2n1)c1ccccc1
Brc1ccc(-c2cncnc2)c(CNc2ccccc2)c1
Brc1ccc(Nc2ncnc(Nc3ccc(Br)cc3)n2)cc1
Brc1ccc(Nc2ncnc3cccc(Br)c23)cc1
Brc1ccc(Nc2nnc(NCCN3CCOCC3)c3ccccc23)cc1

  2 Training on 397 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.318637
Reward: 1.111429
Trajectories with max counts:
14	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.032444645
Proportion of valid SMILES: 0.6972274732199117
Sample trajectories:
Brc1ccc(Br)c(Br)c1
Brc1ccc(N=Nc2nccs2)cc1
Brc1ccc(Nc2ccnc(-c3nc4ccccc4s3)n2)cc1
Brc1ccc2ccc(Br)c(CCCN3CCCCCC3)c2c1
Brc1ccc2nc(-c3ccccc3)ccc2c1
Policy gradient replay...
Mean value of predictions: 0.072287664
Proportion of valid SMILES: 0.5097977243994943
Sample trajectories:
Bc1ccc(Nc2ncnc3c(Nc4ccc(Br)cc4)ncnc23)cc1
Brc1ccc(-c2cnc(Nc3c4ccccc4cc(Br)ccc3n3cnnn3)nc2)cc1
Brc1ccc(Nc2cnc3cccc(Br)c3c2)cc1
Brc1ccc(Nc2nc(NC3CCCCC3)oc3ncnc23)cc1
Brc1ccc(Nc2nc(Nc3ccsc3)ncc2-c2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.1053209
Proportion of valid SMILES: 0.5707576706324358
Sample trajectories:
Brc1cc(Nc2ncnc3ccccc23)ncn1
Brc1cc2c(Br)c(Br)ccc2o1
Brc1ccc(C=NNc2ncnc3sccc23)cc1
Brc1ccc(Nc2ccnc3cc(Br)nc(Nc4ccccc4)c23)cc1
Brc1ccc(Nc2nc(-c3ccccc3)nc3ccccc23)cc1

  3 Training on 955 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.181208
Reward: 1.076488
Trajectories with max counts:
16	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.12014134
Proportion of valid SMILES: 0.5321215919774366
Sample trajectories:
BP(=O)(CC(F)F)C(F)(F)F
Brc1cC(Nc2cccs2)=Nc2nc3ccccc3nc2c2c(Br)c(cc1)Nc1ncnc2c2ccc(Br)cc12
Brc1ccc(Br)c(Br)c1
Brc1ccc(NN=Cc2cccnc2)cc1
Brc1ccc(Nc2ccnc(N3CCCC3)c2CNc2cnc3cccc(Br)c3n2)nc1
Policy gradient replay...
Mean value of predictions: 0.14801179
Proportion of valid SMILES: 0.43207126948775054
Sample trajectories:
Brc1cc(Br)c2c(Nc3nc(Br)nc(Nc4ccnc5cc(Br)c(Br)c(Br)c45)n3)ccnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1ccc(Br)c(Br)c1
Brc1ccc(CCNc2nc3c(Nc4ccc(Br)cc4)ncnc3s2)cc1
Brc1ccc(Nc2ncnc(Nc3cncc(Br)c3)n2)cc1
Fine tuning...
Mean value of predictions: 0.15877192
Proportion of valid SMILES: 0.5721455457967378
Sample trajectories:
BrCCCCCCCCCNc1ccc2nc3ccccc3nc2c1
BrCc1cccc(Nc2nc(Nc3ccsc3)nc3ccccc23)n1
Brc1ccc(-c2nc3ccccc3nc2-c2occc2-c2cc3ccccc3cc2-c2ccccc2)cc1
Brc1ccc(-c2ncnc3ccc(-c4ccc5nc(N6CCCCC6)c(-c6cccc(-c7ccccc7)c6)nc5c4)cc23)cc1
Brc1ccc(-c2nsc(-c3ccc(Br)cc3)n2)cc1

  4 Training on 1905 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 20.481602
Reward: 1.413937
Trajectories with max counts:
31	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.17705788
Proportion of valid SMILES: 0.5891739674593242
Sample trajectories:
BP(=O)(SCC#C)c1ccc(Br)cc1
Brc1cc(Br)c(Nc2cc(-c3ccccc3)ccn2)c(Br)c1
Brc1ccc(-c2ccccc2)o1
Brc1ccc(-n2c(Nc3ccc4c(c3)CCCC4)nc3ccccc32)cc1
Brc1ccc(Br)c(Nc2nccc(-c3ccc(Nc4ncnc5ccc(Br)cc45)cc3)n2)c1
Policy gradient replay...
Mean value of predictions: 0.23048246
Proportion of valid SMILES: 0.5712496085186345
Sample trajectories:
BP(=O)(Nc1ccc(F)cc1)N(=O)=O
BP(=O)(OC(C)C)C(F)F
Brc1cc(-c2ccccn2)c(Br)c(-c2ncnc3cccnc23)n1
Brc1cc2ncnc(N3CCOCC3)c2nc1NCc1cccnc1
Brc1ccc(-c2cc(Nc3cccs3)ncn2)cc1
Fine tuning...
Mean value of predictions: 0.21884531
Proportion of valid SMILES: 0.5753682231275462
Sample trajectories:
BP(=O)(CCl)NO
BP(=O)(NCCCCCO)n1cc(-c2ccc3ccccc3c2)nc1SCC(=O)N(CC)CC
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BrC1=C2C3=C(Br)C2CCC3C1=CNc1cccc2cnc3ccccc3c12

  5 Training on 3278 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 20.438533
Reward: 1.485387
Trajectories with max counts:
26	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.26340026
Proportion of valid SMILES: 0.5312010034493572
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrC1=CC=Nc2scc(Br)c2O1
BrCCCCSc1ccc(Nc2ncnc3ccccc23)nc1
BrCCN(CCC=Cc1ccc(Br)cc1)c1ccc(Br)cn1
Brc1cc(-c2ccc(Br)c(Br)c2)c2ncnc(Nc3ccccc3)c2c1
Policy gradient replay...
Mean value of predictions: 0.28513286
Proportion of valid SMILES: 0.5545454545454546
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)cc(Nc2nc3cc(Br)ccc3s2)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc(Nc2ncnc3ccsc23)n2ncnc2n1
Brc1cc2c(s1)c(Nc1cncnc1Br)c(Br)c1ccsc12
Fine tuning...
Mean value of predictions: 0.25876236
Proportion of valid SMILES: 0.6018779342723005
Sample trajectories:
BrC=CC=CCC=CC=CC=CCBr
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(Br)c(Br)[nH]1
Brc1cc(Br)cc(Nc2nc3cc(Br)ccc3s2)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1

  6 Training on 4855 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 21.079592
Reward: 1.626860
Trajectories with max counts:
24	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.29904762
Proportion of valid SMILES: 0.5908096280087527
Sample trajectories:
Bc1ccc(Nc2ncnc3cnccc23)cc1
Brc1cc(Br)c2ccc(Br)c(Br)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Brc1cc(Br)nc(Nc2ncnc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.33431858
Proportion of valid SMILES: 0.5736263736263736
Sample trajectories:
BP(=O)(OCC)OCCCCCCCCl
BP(=O)(OCCCCCl)c1ccc(Br)c(Br)c1
Bc1ccc(OC2CC2CCC(Br)Br)c(Br)c1
BrC1CCc2ncnc(N3CCOCC3)c2C1
BrCC1CCCCN1c1ncnc2c(-c3ccc(Br)cc3)csc12
Fine tuning...
Mean value of predictions: 0.34228483
Proportion of valid SMILES: 0.5795418889237527
Sample trajectories:
Brc1cc(-c2ccccn2)c2nc3ccccc3nc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c4scnc34)ncnc2c1
Brc1cc(Br)c2c(Nc3cncc(Br)n3)ncnc2c1
Brc1cc(Nc2ncnc3ccccc23)c2ncncc2n1
Brc1cc2ncnc(Nc3ccc(Br)s3)c2c(Br)n1

  7 Training on 6710 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 24.584910
Reward: 1.976519
Trajectories with max counts:
32	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.32523862
Proportion of valid SMILES: 0.5908521303258145
Sample trajectories:
BrCC(Br)Br
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCC=CCC=CCCCCCCN=C(CCCCCCCCCCCCC(Br)Br)NBr
BrCN1CCC(=Nc2ccc(Br)cc2)c2ccc(Br)cc2C1
Brc1cc(Br)c(Br)c(-c2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Nc1ccccc1
Policy gradient replay...
Mean value of predictions: 0.37165952
Proportion of valid SMILES: 0.5820006271558482
Sample trajectories:
BP(=O)(CCl)NP(=O)(OCC)OCCl
BP(=O)(NCCCCO)c1ccc(Br)cc1
BP(=O)(O)C=C(Br)C(F)(F)P(=O)(O)O
BP(=O)(O)CCCCC(O)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
BP(=O)(OCC1OC(N2C=C(Br)C(=O)NC2=O)C(O)C(O)C1O)OC(C(N)=O)C(=O)Nc1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.32812837
Proportion of valid SMILES: 0.5865746549560853
Sample trajectories:
BP(=O)(CCl)SCCCCCCCCCl
Bc1cc(Nc2ncnc3cc(Br)ccc23)ccc1O
BrCCCCCCCCCCNCCCCCNc1c2cnccc2nc2ccc(Br)cc12
Brc1cc(Br)c(Nc2nc3ncnc(Nc4cccc(I)c4)c3cc2Br)s1
Brc1cc(Br)c(Nc2ncnc3[nH]cnc23)c(Br)c1Br

  8 Training on 8526 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 22.159798
Reward: 1.840832
Trajectories with max counts:
75	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.3260305
Proportion of valid SMILES: 0.5536105032822757
Sample trajectories:
BP(=O)(CCCl)N[PH](=O)OP(=O)(OCCl)OCOP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCCOCCF)Oc1cc(Br)c(Br)cc1Br
Bc1cc(Br)c(Br)cc1Nc1ccc(Br)cn1
BrC=CBr
BrCCCc1c(Br)ccc2ncnc(Nc3ccc(Br)cc3)c12
Policy gradient replay...
Mean value of predictions: 0.39581206
Proportion of valid SMILES: 0.6126408010012515
Sample trajectories:
Brc1cc(-c2cncnc2Br)n2ncnc2n1
Brc1cc(Br)c(Nc2ncnc3cc(Br)cc(Br)c23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(I)cc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.38315132
Proportion of valid SMILES: 0.6028213166144201
Sample trajectories:
BP(=O)(Br)C(=O)Nc1cc(Br)cc(Nc2ncnc3ccccc23)n1
BP(=O)(CCCCCCCCCCCCC(=O)NO)n1cnc2c(N)ncnc21
Br
BrC=Cc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCC12CCCCCCC1c1ccc(Br)cc1OCCC2

  9 Training on 10162 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 24.634701
Reward: 2.095372
Trajectories with max counts:
12	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
12	Clc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.399681
Proportion of valid SMILES: 0.5896551724137931
Sample trajectories:
BP(=O)(NCCC(O)P(=O)(O)OP(=O)(O)O)P(=O)(O)Oc1ccc(Br)cc1
BP(=O)(OCC)OCCCCCCC=CCCC(Br)Br
BP(=O)(OCC1CCC(O)C1O)n1cnc2c(N)ncnc21
Brc1cc(Br)c2c(Nc3ccc(Br)c(I)c3)ncnc2n1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.42410764
Proportion of valid SMILES: 0.6048185231539425
Sample trajectories:
BP(=O)(C=O)OCC
BP(=O)(OCCCCCC#CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)N1C=C(Br)C(=O)NC1=O
BrC(=Nc1ccc(Br)cc1)c1ccc(Br)cc1
BrCCCCCNc1c(Br)cccc1Nc1ncnc2ccc3ccccc3c12
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.4068006
Proportion of valid SMILES: 0.6075117370892019
Sample trajectories:
BC=NS(=O)(=O)Oc1ccc(Cl)cc1
BrCCBr
BrCCCCCCNc1ccc(Nc2ncnc3ccsc23)cc1
BrCCN(CCBr)c1ccc(Br)c(Br)c1
Brc1cc(Br)c(-c2ccc(Nc3ncnc4ccsc34)cc2)s1

 10 Training on 11881 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.064492
Reward: 1.976129
Trajectories with max counts:
42	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.3656887
Proportion of valid SMILES: 0.6292240300375469
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC(=O)OCCCCCCCCCCCBr
Bc1cccc(Nc2nc3ccccc3c3c4ccccc4c23)c1
Bc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
BrC=CCCCCCCBr
BrCc1nc2ncnc(Nc3ccc(Br)cc3)c2s1
Policy gradient replay...
Mean value of predictions: 0.4643319
Proportion of valid SMILES: 0.581453634085213
Sample trajectories:
BP1(=O)OCC2OC(C(O)C2O)N(C=CC(=O)NS(=O)(=O)Nc2ncncn2)C(=O)N(C=O)C1=O
B[PH](=O)(ONCc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O
Bc1cc(Br)c2c(Nc3ccc(Br)cc3)ncnc2c1Br
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c(Br)cc1Br
Fine tuning...
Mean value of predictions: 0.43965518
Proportion of valid SMILES: 0.5818181818181818
Sample trajectories:
BP(=O)(COc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(NCOCCCCCO)c1cc(Br)cc(Br)c1
BP(=O)(OCC)C(=O)Nc1ccc(Br)c(Br)c1
BP(=O)(OCC)c1nc2cc(Br)c(Br)c(Br)c2s1
BP(=O)(OCCC(=O)Nc1ccc(Br)cc1)S(=O)(=O)Nc1ccc(NC(=O)c2c(Br)cc(Br)c(Br)c2O)cc1

 11 Training on 13719 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.154795
Reward: 2.201835
Trajectories with max counts:
24	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.40903732
Proportion of valid SMILES: 0.6832214765100671
Sample trajectories:
BrCCCCCCCCCCCCC=CCC=CCC=CCCCCCCCCCCCCCCCCCCCCCBr
BrCCCCCCCCCCCCCCCCCCCCCCC=CCC=CCC=CCCCCC1CCCCCC1
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCCCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCCCCCNc1ccc(Br)cc1
Brc1cc(-c2ccccc2)c(Br)cn1
Policy gradient replay...
Mean value of predictions: 0.4517897
Proportion of valid SMILES: 0.5590994371482176
Sample trajectories:
BP(=O)(CCCl)Nc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BP(=O)(Nc1cccc(C(=O)Nc2ccc(Br)cc2)c1)C(=O)OCC
BP(=O)(c1ccc(Br)cc1)N1CCN(C(=O)Oc2ccc(Br)cc2)CC1
B[PH](=O)(Nc1cccc(Nc2ncnc3cc(F)c(Br)cc23)c1)(N1CCC(F)C1)C(F)(F)F
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)nc1
Fine tuning...
Mean value of predictions: 0.4532778
Proportion of valid SMILES: 0.6013767209011264
Sample trajectories:
BP(=O)(O)C=NO
BP(=O)(O)CCCCCCCCCCCCCCCCCCCl
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cn1
BrCCCC=CCC=NNc1nc(Nc2ccccc2)c2ccccc2n1
BrCCNc1ccc2c(Nc3ccc(Br)cc3)ncnc2c1

 12 Training on 15642 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.214389
Reward: 2.794909
Trajectories with max counts:
99	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49750596
Proportion of valid SMILES: 0.5274036955840902
Sample trajectories:
B=C(Br)NNC(=O)c1ccc(Nc2ncnc3scnc23)c(Br)c1
BP(=O)(Br)OP(=S)(N(=O)(O)OP(=O)(O)OCCCCl)P(=O)(O)C(F)(F)F
BP(=O)(CCl)NO
BP(=O)(NCCCCl)c1cc(Br)c(Br)c(Br)c1Oc1ccc(Br)cc1
BP(=O)(NO)P(=O)(O)N(O)N(O)Oc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.4578947
Proportion of valid SMILES: 0.649367088607595
Sample trajectories:
BP(=O)(C(=O)Nc1ccc(Br)cc1F)N1CCN(c2ccc(Br)cc2)CC1
BP(=O)(CC)Nc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BP(=O)(CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)N=O
BP(=O)(OCOC(=O)CCCCCCl)P(=O)(O)O
Bc1ccc(NN=C2sc3ccccc32)cc1Br
Fine tuning...
Mean value of predictions: 0.48413393
Proportion of valid SMILES: 0.6454744754149703
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCNCCCCNCCCCNc1ccc(Nc2ncnc3ccccc23)cc1
BrCCNc1ccc2c(Nc3ccc(Br)cc3)ncnc2c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Nc2nc(Nc3ccccc3)nc3ccncc23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1

 13 Training on 17827 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.321784
Reward: 2.752071
Trajectories with max counts:
54	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.48501334
Proportion of valid SMILES: 0.5872220482305043
Sample trajectories:
BOc1ccc(Nc2ncnc3ncnc(N4CCCCC4)c23)cc1
BP(=O)(OCC)OC(=O)C=CC=CC(Br)Br
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCCCCCCCl
BP(=O)(OCCS(=O)(=O)c1ccc(Br)cc1)C(=O)c1nc(Br)cc(Br)c1Br
Bc1sc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1Cl
Policy gradient replay...
Mean value of predictions: 0.49954292
Proportion of valid SMILES: 0.6176286072772899
Sample trajectories:
BP(=O)(CCC(=O)Nc1ccc(Br)cc1)Nc1ccnc(Cl)c1
BP(=O)(Nc1ccc(Br)cc1)Nc1cc2c(Br)cc(Nc3cc(Br)ncn3)cc2s1
BP(=O)(OCC)C(=O)N1CCCC1
BP(=O)(OCC)Oc1ccc(Nc2ncnc3c(Br)c(Br)c(Br)c(Br)c23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Fine tuning...
Mean value of predictions: 0.48518708
Proportion of valid SMILES: 0.610641627543036
Sample trajectories:
BP(=O)(NO)N(O)C(=O)OP(=O)(O)C(F)(F)F
Bc1cc(Nc2ncnc3cccc(Br)c23)nc2ccc(Br)cc12
BrC(Br)=NNc1ccc(Br)s1
BrCCCBr
BrCCNc1nc2cc(Br)c(Br)cn2c1Nc1scnc1Br

 14 Training on 20019 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.998605
Reward: 2.539051
Trajectories with max counts:
42	Oc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.42338365
Proportion of valid SMILES: 0.58
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Br
BrC(=NNc1cccc(Br)c1)Nc1ccc(Nc2ncnc3ccsc23)cc1
BrCCN1CCC(Nc2ncnc3ccc(Br)cc23)CC1
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.539397
Proportion of valid SMILES: 0.622848200312989
Sample trajectories:
BP(=O)(CCCCC)OCCCCCCCl
Bc1cc(Br)cc(Br)c1Nc1ncnc2cc(Br)c(Br)cc12
Brc1cc(Br)c(Nc2ncnc3scnc23)cn1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)n3)ccnc2n1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.5250894
Proportion of valid SMILES: 0.6125195618153365
Sample trajectories:
BP(=O)(Nc1cc(Br)c(Br)cc1Br)OCC
BP(=O)(OCC(=O)Nc1ccc(Br)cc1)c1ccc(Br)cc1
Bc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1

 15 Training on 22266 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.072017
Reward: 3.134859
Trajectories with max counts:
50	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.5495075
Proportion of valid SMILES: 0.6048918156161807
Sample trajectories:
Bc1ccc(Nc2ncnc3cc(Br)sc23)cc1
Brc1c(I)sc2c(Nc3ccccc3)ncnc12
Brc1cc(Br)c2c(Nc3ccc(Br)c4cc(Br)ccc34)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.5682106
Proportion of valid SMILES: 0.5948653725735754
Sample trajectories:
BP(=O)(NO)n1cnc2c(Nc3ccc(Br)cc3)ncnc21
Bc1cc(Br)c(Br)cc1Oc1nc2ncnc(Nc3cc(Br)c(Br)c(Br)c3)c2s1
BrCCNc1ccc(Nc2ncnc3ccsc23)cc1
BrCCNc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.5653239
Proportion of valid SMILES: 0.61415596617601
Sample trajectories:
BP(=O)(N(CC(=O)Nc1ccc(Br)c(Br)c1)C(=O)Nc1cc(Br)c(Br)cc1F)C(F)(F)F
BP(=O)(NC(=O)OCC)c1cc(Br)cc(Br)c1Br
BP(=O)(OCC(=O)Nc1ccc(Br)cn1)C(=O)N(CC(=O)O)c1ccc(Br)cc1
BP(=O)(OCCCOc1cccc(Nc2nccc(Br)n2)c1)c1ccc(Br)cc1
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1

 16 Training on 24905 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.238282
Reward: 3.121662
Trajectories with max counts:
97	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5727425
Proportion of valid SMILES: 0.5585839598997494
Sample trajectories:
BP(=O)(C=C(Br)CCS(=O)(=O)Nc1cc(Br)cc(Br)c1O)OCC
BP(=O)(N(O)CCCC(=O)N(O)CP(=O)(O)CCCCCl)P(=O)(O)O
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCc1ccc2sc(NCc3cc4cc(Br)ccc4nc3Br)nc2c1
Policy gradient replay...
Mean value of predictions: 0.5567865
Proportion of valid SMILES: 0.6291993720565149
Sample trajectories:
BP(=O)(CCCCCCCCCC(=C)c1ccc(Br)cc1)NO
Bc1ccc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)cc1
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCN1CCN(CCCCNc2cc(Br)cc3ncnc(Nc4ccc(Br)cc4)c23)CC1
BrCCCOc1ccc(Br)cc1Nc1ncnc2ccc(Br)cc12
BrCCNc1cc2ncnc(Nc3cccc(Br)c3)c2nc1Nc1cccc(NCc2cccnc2-c2ncnc3[nH]cnc23)c1
Fine tuning...
Mean value of predictions: 0.5544821
Proportion of valid SMILES: 0.6286787726988102
Sample trajectories:
Bc1cc(Br)c2nc(C3CCN(c4nc5ccc(N)nc5nc4-c4ccc(Br)cc4)C3)[nH]c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1
Brc1cc(Br)c2ncsc2c1

 17 Training on 27554 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.943294
Reward: 3.395566
Trajectories with max counts:
21	Clc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5875782
Proportion of valid SMILES: 0.6531446540880503
Sample trajectories:
BP(=O)(OC(=O)N(O)P(B)(=O)OCC)OP(=O)(O)OP(B)(F)(F)OP(=O)(O)OP(=O)(O)NP(=O)(OCC)Oc1sc2nc(I)cc(Nc3cc(Br)cc(Br)c3)c2c1Br
B[PH](=O)(=C[PH](=O)Br)OCCCl
BrCCNc1nc2ncnc(Nc3ccc(Br)o3)c2s1
BrCc1cc(Br)c(Br)c(C=Nc2nc3c(Br)c(Br)c(Br)cc3s2)c1
Brc1cc(Br)c(N=Nc2ccc(Br)s2)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.49217477
Proportion of valid SMILES: 0.6151922475773679
Sample trajectories:
Bc1ccc(Nc2ncnc3cc(Br)sc23)cc1
Bc1nc(Nc2ccccc2Br)c2sccc2n1
BrCCCCCCCCCNCc1ccc(Br)cc1
BrCCCNc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(Br)c(Br)c(Br)c1Br
Fine tuning...
Mean value of predictions: 0.5511593
Proportion of valid SMILES: 0.635423197492163
Sample trajectories:
BP(=O)(NCCC(=O)NO)C(=O)O
BP(=O)(OCC)C1C=CC(Nc2cccc(Br)c2)O1
BrCCNc1nc2c(Nc3cccc(Br)c3)ncnc2s1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1

 18 Training on 30191 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.396593
Reward: 3.148774
Trajectories with max counts:
32	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5390043
Proportion of valid SMILES: 0.6473717146433041
Sample trajectories:
BC=CC(=O)Nc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BrCCCCCCCCCCCCCCCN=C(NNc1nc2ncnc(Nc3ccccc3)c2s1)Nc1ccccc1
Brc1cc(Br)c(-c2cccc(Nc3ncnc4cc(Br)ccc34)c2)c(Br)c1
Brc1cc2ncnc(Nc3ccc(Br)c4cncnc34)c2s1
Brc1ccc(-c2cn(-c3ccccc3Br)c3cc(Br)ccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.62102735
Proportion of valid SMILES: 0.6517521902377973
Sample trajectories:
BP(=O)(OCC)OCCCCn1c(N)nc2ncnc(I)c2c2ncnc(Nc3ccc(Br)cc3F)c21
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Brc1cc(Br)c(Nc2ncnc3cccc(Br)c23)s1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ccnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c4ccsc34)ncnc2c1
Fine tuning...
Mean value of predictions: 0.5746124
Proportion of valid SMILES: 0.6452016255079712
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCCNc1ccc(Nc2ncnc3c(Br)cccc23)cc1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Nc2cc3ccncc3cn2)n1

 19 Training on 33134 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.267148
Reward: 3.071981
Trajectories with max counts:
28	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5922481
Proportion of valid SMILES: 0.6452016255079712
Sample trajectories:
BP(=O)(OCC)Oc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
BP(=O)(c1ccc(Br)cc1)c1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Policy gradient replay...
Mean value of predictions: 0.62280095
Proportion of valid SMILES: 0.6391331658291457
Sample trajectories:
BrC=CCCCCCCCCCCCBr
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c(Nc2cc(Nc3ncnc4cc(Br)c(Br)c(Br)c34)c(Br)cc2Br)c(Br)c1
Brc1cc(Br)c(Nc2ccc(Br)nc2)c(I)c1
Brc1cc(Br)c(Nc2nc(Nc3ccc(Br)c(Br)c3Br)ncc2Br)cc1Br
Fine tuning...
Mean value of predictions: 0.5802348
Proportion of valid SMILES: 0.63875
Sample trajectories:
Bc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
BrCCCCCCCCCCCCCCCCCCCCNCCCCNc1ccccn1
BrCCN(c1ccc(Br)cc1)c1cccc(-c2ccc3ncnc(Br)c3c2)c1
BrCc1ccc2c(Nc3ccc(Br)cc3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1

 20 Training on 36137 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.900376
Reward: 3.325039
Trajectories with max counts:
24	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.60996544
Proportion of valid SMILES: 0.6340319049108539
Sample trajectories:
BP(=O)(Br)C(=O)Nc1cc(Br)c(Br)c(Br)c1
BP(=O)(Nc1cc(Br)cnc1F)C(=O)NC(P(=O)(O)O)P(=O)(O)O
Bc1cccc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c1
Brc1cc(Br)c2c(Nc3cnc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1
Policy gradient replay...
Mean value of predictions: 0.63824546
Proportion of valid SMILES: 0.6344590368980613
Sample trajectories:
BP(=O)(OCC)OCCCCCCCCCCCCC
BP(=O)(Oc1ccc(Br)cc1)c1ccc(Br)cc1
BrCCBr
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc2nccc(Nc3ccc(I)cc3)c2cc1Br
Fine tuning...
Mean value of predictions: 0.6059052
Proportion of valid SMILES: 0.6462308414138255
Sample trajectories:
BP(=O)(Nc1ccc(Br)c(Br)c1)c1cc(Br)cc(Br)c1
Bc1cc(Br)c2c(Nc3ccc(Br)cc3)ncnc2c1
BrCCCCCCCCNc1ccnc2ccc(Br)cc12
BrCNc1nc2ccccc2s1
Brc1cc(Br)c(Nc2ncnc3scc(-c4cncnc4)c23)[nH]1

Trajectories with max counts:
152	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.57111263
Proportion of valid SMILES: 0.5484556708765788
Mean Internal Similarity: 0.46247271027559717
Std Internal Similarity: 0.09663587415197027
Mean External Similarity: 0.4129487276084224
Std External Similarity: 0.07193301743066241
Mean MolWt: 428.1520749531544
Std MolWt: 108.80474266575338
Effect MolWt: -0.6808175507969388
Mean MolLogP: 4.881321936289821
Std MolLogP: 1.8171775063245859
Effect MolLogP: 0.10120948826787321
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.512039% (1215 / 1246)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5669.518181800842, 'valid_fraction': 0.5484556708765788, 'active_fraction': 0.5475376196990424, 'max_counts': 152, 'mean_internal_similarity': 0.46247271027559717, 'std_internal_similarity': 0.09663587415197027, 'mean_external_similarity': 0.4129487276084224, 'std_external_similarity': 0.07193301743066241, 'mean_MolWt': 428.1520749531544, 'std_MolWt': 108.80474266575338, 'effect_MolWt': -0.6808175507969388, 'mean_MolLogP': 4.881321936289821, 'std_MolLogP': 1.8171775063245859, 'effect_MolLogP': 0.10120948826787321, 'generated_scaffolds': 1246, 'novel_scaffolds': 1215, 'novel_fraction': 0.9751203852327448, 'save_path': '../logs/replay_combo_s2-2.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.747517
Reward: 1.000000
Mean value of predictions: 0.0012588513
Proportion of valid SMILES: 0.7956181533646323
Sample trajectories:
BP(=O)(O)COP(=O)(O)O
Brc1ccc(-n2cccc2-c2ccccn2)cc1
Brc1cnc(NCC2CCN(Cc3ncccn3)CC2)s1
C#CC(=O)C(C)=NNC(=O)C1C=C(S(=O)(=O)c2ccc3ccccc3c2)CC=CC1
C#CC1C(OCC)OC(c2ccncc2)=CC1c1ccc(C(=O)c2cccc(Cl)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.020521894
Proportion of valid SMILES: 0.7751114158381899
Sample trajectories:
Brc1ccc(C2=Nc3ncnn3CCCc3ccccc32)o1
Brc1ccc(Nc2ncnc3ccncc23)cc1
Brc1ccc2[nH]c3c(c2c1)CCCC3
Brc1ccc2c(c1)NCc1cccnc1N2
Brc1ccc2nc(NN=Cc3ccccc3SCCCCCCCCCCCCCN3CCOCC3)sc2c1
Fine tuning...
Mean value of predictions: 0.04227297
Proportion of valid SMILES: 0.5908377784750549
Sample trajectories:
Brc1cc(Nc2ncnc3ccccc23)no1
Brc1ccc(Br)c(N2CCNCCNCC2)c1
Brc1ccc(Nc2cc(-c3ccco3)nc3ccccc23)cc1
Brc1ccc(Nc2ccc(I)nc2Nc2nc(-c3ccccc3)nc(-c3cccnc3)n2)cc1
Brc1ccc(Nc2nc(Nc3ccccc3)nc(-c3ccc(Br)cc3)n2)cc1

  2 Training on 415 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.663393
Reward: 1.133929
Trajectories with max counts:
9	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.05065196
Proportion of valid SMILES: 0.6252743806835999
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)c1cc(F)c(F)c(F)c1
BrCC1CC=CCC1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ccnc3cccnc23)cc1
Brc1ccc(Nc2ncnc(Nc3ccc(N4CCNCC4)nc3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.01625817
Proportion of valid SMILES: 0.7661971830985915
Sample trajectories:
Brc1ccc(-c2ccccc2-c2ccccc2)c2ccccc12
Brc1ccc(Nc2ccc3cc(-c4ccccc4)cnc3c2)cc1
Brc1ccc(Nc2ccnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(SCc2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.12049272
Proportion of valid SMILES: 0.561458660798491
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)CCCl
BrCc1cc(Nc2ncnc3cc(Br)ccc23)c2ccccc2n1
Brc1cc(Br)n(Nc2ccc(Br)nc2)c1
Brc1cc(Nc2cccc(CNCCC=CCCCCCc3ccccc3)c2)c2ccccc2c1
Brc1ccc(-c2nc(-c3ccccc3)c3cc(Br)ccc3n2)cc1

  3 Training on 938 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 20.448373
Reward: 1.392903
Trajectories with max counts:
48	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.124772616
Proportion of valid SMILES: 0.5901484054310072
Sample trajectories:
BP(=O)(NC(c1ccc(Br)cc1)P(=O)(O)O)P(=O)(O)O
BP(=O)(OCC1OC(CO)C(O)C(O)C1O)OC(=S)Nc1ccc(Br)cc1
BrCCCn1cnc2ccccc21
Brc1ccc(-c2nc(Nc3nccs3)n2)cc1
Brc1ccc(-n2ccnc2)nc1
Policy gradient replay...
Mean value of predictions: 0.15383844
Proportion of valid SMILES: 0.6265325369380698
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)c1ccc(I)cc1
BrCCCCCCCCCCCCCCCCCCCCCCCCC=CCCCCCCN1CCCCC1
Brc1ccc(C2CCCCN2CCNc2ncnc3sc(Br)cc23)o1
Brc1ccc(Nc2cc3c(Nc4cccc(Br)c4)ncnc3cn2)cc1
Brc1ccc(Nc2ccc(Nc3ncnc4ccccc34)cc2)cc1
Fine tuning...
Mean value of predictions: 0.20519619
Proportion of valid SMILES: 0.5928953159383842
Sample trajectories:
BP(=O)(O)CCNC(=O)c1ccc(I)cc1Br
BP(=O)(OCCCCCCCCCCCCC(F)(F)F)c1ccc(Br)cc1
BrC1CCN(Cc2ccccc2)CC1
BrC=COc1ccc2cnccc2c1
BrCCBr

  4 Training on 2221 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 21.014693
Reward: 1.407904
Trajectories with max counts:
33	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.23483023
Proportion of valid SMILES: 0.5731324544883867
Sample trajectories:
BP(=O)(NO)c1cccc(Cl)c1
BP(=O)(OCC)C(=O)Nc1ccc(Cl)c(Br)c1
Brc1ccc(-c2ccc3ncnc(Nc4cccc(Br)c4)c3c2)cc1
Brc1ccc(-c2ncnc3ncnc(Nc4cccc(Br)c4)c23)cc1
Brc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.24623063
Proportion of valid SMILES: 0.6269170579029734
Sample trajectories:
BrC1CCN(c2ccc(Nc3ncnc4ncnc(Nc5ccccc5)c34)cn2)CC1
BrCCCCCCCCCCCCC=CCCCCCCCCCCCN1CCOCC1
BrCCNc1ncc2ccc(Nc3ccnc4ccccc34)cc2n1
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.24818183
Proportion of valid SMILES: 0.6203007518796992
Sample trajectories:
BrC=Cc1cccc(Nc2nc3ccccc3s2)c1
BrCCN1C(=Nc2ccc(Nc3ncnc4ccccc34)cc2)N(Cc2ccccc2)Cc2c1ncn2-c1ccc(Br)cc1
BrCc1nc(Nc2ncnc3sc(Nc4ccccc4)cc23)cs1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccccc23)nc2ccccc12

  5 Training on 3831 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 22.034254
Reward: 1.612230
Trajectories with max counts:
45	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.24970528
Proportion of valid SMILES: 0.6364488902782119
Sample trajectories:
BP(=NO)(c1ccccc1)c1ccccc1
BP(=O)(NCCCC(=O)c1ccccc1Br)N(=O)=O
Br
BrC1=CN(Nc2ccccc2)c2ncccc2C1c1ccccc1
BrCC12CCC(CCN3Cc4ccc(Br)cc43)=Nc1nc1ccccc1C2
Policy gradient replay...
Mean value of predictions: 0.29514667
Proportion of valid SMILES: 0.587958607714017
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCC)ON(C(=O)C(F)(F)F)c1nc(Nc2cc(F)c(F)c(Br)c2F)cs1
BrCCCCCCCCCCCCCCCCBr
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCN=C1CCNCC1
BrCCN(Nc1ccc2ccncc2c1)c1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.3504781
Proportion of valid SMILES: 0.6219092331768388
Sample trajectories:
BP(=O)(OCC)OC(=O)CN(C(=O)OP(=O)(O)CCl)P(=O)(O)CP(=O)(O)O
BrC=CBr
BrCCCCCCCCCCCCCCCCCCCCCCCCCN1CCCC1
Brc1c(Nc2ncnc3cccc(Br)c23)cc2ncnc(Nc3ccccc3)c2c1Br
Brc1cc(Br)c2[nH]cnc2c1

  6 Training on 5693 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 22.523742
Reward: 1.773275
Trajectories with max counts:
66	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.30703086
Proportion of valid SMILES: 0.618005626758362
Sample trajectories:
BP(=O)(Br)Oc1cccc(Nc2ncnc3cc(Br)c(Br)cc23)c1
BP(=O)(CCCN(C(=O)c1ccc(Br)cc1)c1ccc(Br)cc1)N1CCN(Cc2cccc(Cl)c2)CC1
BP(=O)(c1ccc(Br)cc1)N(CC(=O)N(C)c1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(c1ccccc1)c1ccc(Br)o1
BrCCCNc1ncnc2ncnc(Nc3ccccc3)c12
Policy gradient replay...
Mean value of predictions: 0.35660473
Proportion of valid SMILES: 0.6235515189476981
Sample trajectories:
BP(=O)(NO)C(=O)c1ccccc1Nc1ccc(Br)cc1
BP(=O)(c1ccc(F)cc1)N1CCC(CCC(=O)Nc2ncnc3cc(Br)ccc23)CC1
Brc1cc(Br)cc(Nc2ncnc3ccc(Nc4cc(Br)ccc4Br)cc23)c1
Brc1cc(Nc2ncnc3[nH]ncc23)ccc1-c1ccccc1
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1Br
Fine tuning...
Mean value of predictions: 0.3675838
Proportion of valid SMILES: 0.6256651017214397
Sample trajectories:
BrC(=NN1CCc2ncnc(Nc3ccc(Br)s3)c2C1)c1cc2c(Br)cc(Br)cc2[nH]1
Brc1cc(Br)c(Nc2ncnc3sccc23)c(Br)n1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2n1
Brc1cc(Br)c2ncnc(Nc3cccs3)c2c1
Brc1ccc(-c2ccc(Br)cc2Oc2cccnc2)c(Br)c1

  7 Training on 7722 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 26.448717
Reward: 2.264570
Trajectories with max counts:
13	Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
13	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.33481175
Proportion of valid SMILES: 0.6065060994682515
Sample trajectories:
BrCN1CCC(=Nc2ccc(Br)cc2)c2ccc(Br)cc2C1
Brc1ccc(-c2cc3c(Nc4ccccc4)ncnc3nc2-c2ccccc2Br)cc1
Brc1ccc(Cc2ncnc(Nc3ccc4[nH]cnc4c3)n2)cc1
Brc1ccc(Nc2cc(Nc3ncnc4ccc(Br)cc34)ccc2Br)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4ccc(Br)cc4)c3c2)cc1
Policy gradient replay...
Mean value of predictions: 0.4240907
Proportion of valid SMILES: 0.6646781789638933
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
BrC1CCCC1(Br)Br
BrCC1CC1c1ccc2cccc(Br)c2c1Br
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNC1CCCCC1
BrCCN1CCc2ccccc2C1
Fine tuning...
Mean value of predictions: 0.39292452
Proportion of valid SMILES: 0.6637445209768316
Sample trajectories:
BrCc1cccc(Nc2ncnc3cc(Br)c(Br)cc23)c1
BrCc1cccc(Nc2ncnc3ccccc23)n1
Brc1cc2c(Nc3c(Br)cncc3Br)ncnc2s1
Brc1ccc(Br)c(-c2ncccc2-c2cccc(Nc3ncnc4ccc5ccccc5c34)c2)c1
Brc1ccc(Br)c(Nc2nc3cc(Br)cnc3nc2-c2ccccc2)c1

  8 Training on 9916 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 26.321019
Reward: 2.257010
Trajectories with max counts:
23	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4160876
Proportion of valid SMILES: 0.6606918238993711
Sample trajectories:
BSc1ccc(Nc2ncnc3ncsc23)cc1
Bc1ccc(Nc2ncnc(Nc3ccc(Br)cc3)c2C#N)cc1-c1ccc(Br)cc1
Brc1c[nH]c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c2c(Nc3cc(Br)c(Br)s3)ncnc2c1
Brc1cc(Br)cc(Nc2cccc(Nc3ncnc4ccccc34)c2)c1
Policy gradient replay...
Mean value of predictions: 0.44199526
Proportion of valid SMILES: 0.6594611528822055
Sample trajectories:
BP(=O)(CCCCCCCCNC(=O)c1cc(Br)ccc1Br)c1cccc(Br)c1
BP(=O)(OCC)Oc1ccc(Br)cc1Br
Bc1csc2ncnc(Nc3ccc(Br)cc3)c12
BrC(=NNc1cc(Nc2ncnc3ccccc23)ccn1)c1ncccc1Br
BrC1CCCC1c1cccc(Nc2ccnc3ccccc23)c1
Fine tuning...
Mean value of predictions: 0.44018912
Proportion of valid SMILES: 0.6617647058823529
Sample trajectories:
BP(=O)(OCCN)C(=O)Nc1ccc(Br)cc1
Bc1nccc(Nc2ncnc3ccccc23)c1N(=O)=O
BrC(=NN1CCCC1)c1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BrCN1CCN(Cc2ccccc2)CC1
Brc1cc(Br)cc(Nc2ncnc3ncnc(-c4ccccc4Br)c23)c1

  9 Training on 11848 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.712156
Reward: 2.432137
Trajectories with max counts:
15	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4725975
Proportion of valid SMILES: 0.67264
Sample trajectories:
BP(=O)(CCCCCNc1ccc(Br)cc1)P(=O)(O)O
BP(=O)(CCl)N1CC(=N)Oc2cc(Br)c(Br)cc2O1
BrCC1CCCN1c1nc2cnccc2nc1-c1ccc(Nc2cccc(Br)c2)cc1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc2ncnc(Nc3ccc(I)cc3)c2s1
Policy gradient replay...
Mean value of predictions: 0.41424683
Proportion of valid SMILES: 0.6891807379612258
Sample trajectories:
BP(=O)(N(O)Cc1ccc(Br)cc1)P(=O)(O)CN(C(=O)Nc1ccc(Br)c(Br)c1)c1ccccc1
BrC1=CNc2cc(N3CCC(NCCc4cscn4)CC3)ccc21
BrCCCCCC1=C(c2ccc(Nc3ncnc4ccccc34)cc2)C2=CCCCC21
BrCCCCCNc1ncnc2ncnc(Nc3ccccc3)c12
BrCCCNc1ncc2ccc(Br)cc2c1-c1ccc2ncnc(Nc3ccccc3)c2c1
Fine tuning...
Mean value of predictions: 0.47061568
Proportion of valid SMILES: 0.6712586098935505
Sample trajectories:
Brc1cc(Br)c(Nc2ncnc3ccsc23)cc1Br
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1ccc(-c2ccccc2)c(Nc2ccc(Nc3ncnc4ccccc34)o2)c1
Brc1ccc(-c2ccccc2Nc2nc(Nc3ccccc3)nc(-c3ccccc3Br)n2)cc1
Brc1ccc(-c2cn(-c3ccccc3Br)c3ncnc(Nc4ccccc4)c23)cc1

 10 Training on 13986 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.218086
Reward: 2.457468
Trajectories with max counts:
15	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.4489544
Proportion of valid SMILES: 0.657705532979056
Sample trajectories:
BrCCOI
Brc1ccc(-c2ccc(Nc3ncnc4[nH]cnc34)cc2)c(Br)c1
Brc1ccc(-c2ccc(Nc3ncnc4ccccc34)c3ccccc23)cc1
Brc1ccc(Br)cc1
Brc1ccc(CNc2ncnc3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.5054695
Proportion of valid SMILES: 0.7027546444586803
Sample trajectories:
BrCCN(CCCNc1ccnc2ccc(Br)cc12)Cc1ccc2ccccc2c1Br
BrCCNc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3Br)ncnc2c1
Brc1cc(Nc2ncnc3ccccc23)c2ncnc(Nc3ccccc3)c2c1
Brc1cc2ncnc(Nc3ccc(NC4CCCC4CCN4CCCCCC4)cc3)n2c1SCCN1CCCCC1
Fine tuning...
Mean value of predictions: 0.52777016
Proportion of valid SMILES: 0.6857410881801126
Sample trajectories:
BrCCCCC=NNc1ccc(Nc2ncnc3[nH]ncc23)cc1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc2ncnc(Nc3ccccc3)c12
Brc1cc2ncnc(Nc3ccccc3Br)c2s1

 11 Training on 16447 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.644682
Reward: 2.584544
Trajectories with max counts:
15	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5577301
Proportion of valid SMILES: 0.7017873941674506
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Nc4c(Br)cccc4Br)cc3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(Br)c(-c2ccc(Nc3ncnc4c5[nH]ccc5c34)cc2)c1
Brc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.5937769
Proportion of valid SMILES: 0.6604489408789124
Sample trajectories:
B[PH](=O)(=O)CC(=NO)c1ccc(Br)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)cc(Nc2ccc(Br)c(Br)c2)c1
Brc1cc(Br)cc(Nc2ncnc3c(Br)cc(Br)nc23)c1
Fine tuning...
Mean value of predictions: 0.5718635
Proportion of valid SMILES: 0.6791979949874687
Sample trajectories:
BC=C1C(=O)N(C2CCN(CC3CN3Cc3ccccc3)CC2)c2ccccc21
BrCCN1CCCC1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3cccc(-c4c(Br)ccc5[nH]cnc45)c3)ncnc2c1

 12 Training on 19387 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.505506
Reward: 2.766699
Trajectories with max counts:
65	Clc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.57451665
Proportion of valid SMILES: 0.6318922305764411
Sample trajectories:
BP(=O)(CCl)NP(=O)(OC)OC(=O)COc1ccc(Nc2ncnc3sc(Nc4ccc(Br)cc4)cc23)cc1
Br
BrC(=NNc1ccc2ncnnc2c1)c1ccc(Br)cc1Br
BrCCCC1CC12CCN(CCNc1ncnc3cc(Br)ccc13)CC2
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC[N+]12CCCCC1CCC2
Policy gradient replay...
Mean value of predictions: 0.59634835
Proportion of valid SMILES: 0.6689633573441904
Sample trajectories:
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Nc2ncnc3ccnc(NCCCN4CCCCC4)c23)cc1Br
Brc1cc(Br)c(Nc2ncnc3ncnc(Nc4ccccc4)c23)c(Br)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1ccc(-c2sc(Br)cc2Nc2cc3cc(Br)ccc3o2)cc1
Fine tuning...
Mean value of predictions: 0.5563969
Proportion of valid SMILES: 0.7187988739443228
Sample trajectories:
BP(=O)(COC(=O)C(NC(=O)CBr)c1ccc(Br)cc1)N(c1cccc(Br)c1)c1ncnc2sc(Br)cc12
BP(=O)(NCCCO)c1ccccc1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(Nc2ncnc3ccccc23)ncn1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br

 13 Training on 22304 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.844430
Reward: 3.111530
Trajectories with max counts:
18	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.57688177
Proportion of valid SMILES: 0.7859154929577464
Sample trajectories:
BC(=O)c1ccc(Nc2ncnc3ccsc23)c(F)c1
BrCCCCCCCCCCCCBr
BrCCCCCCCCCCCCCCc1ccc(Nc2ncnc3c(Br)cccc23)cc1
BrCCNc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(-c2cccc(Nc3ncnc4ccccc34)c2)cc1Br
Policy gradient replay...
Mean value of predictions: 0.5754611
Proportion of valid SMILES: 0.7808390732623669
Sample trajectories:
Brc1ccc(Nc2cc(Br)ccc2Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2cc(Nc3cccc(Br)c3)ncn2)cc1
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)cnc23)cc1
Fine tuning...
Mean value of predictions: 0.605148
Proportion of valid SMILES: 0.7291210509852987
Sample trajectories:
BP(=O)(NCCCCCCCCCCCCNc1cc(Br)cnc1Br)OCC
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Nc2ncnc3ccccc23)nc(Br)c1Br
Brc1ccc(Nc2ccc(Br)c(-c3ccccc3Br)c2)cc1

 14 Training on 25526 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.516085
Reward: 3.344769
Trajectories with max counts:
23	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.60634923
Proportion of valid SMILES: 0.7100814026299311
Sample trajectories:
BrC1CCCN(Cc2cccnc2)C1
BrCCCCCCNc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br
Brc1cc(Br)c2ncnc(Nc3cccc(Nc4ccc(Br)c(Br)c4)c3)c2n1
Brc1cc(CNc2ncnc3ccncc23)nnc1Nc1ccc(Br)c2ccccc12
Policy gradient replay...
Mean value of predictions: 0.41668877
Proportion of valid SMILES: 0.708033760550172
Sample trajectories:
Bc1cccc(Nc2ncnc3ccccc23)c1
BrCC1CC1c1ccccc1-c1ccc(Br)cc1
BrCCNc1ccc2cccc3c(n1)Nc1ccccc1-c2c3Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.60978895
Proportion of valid SMILES: 0.6972448340638697
Sample trajectories:
BrC1CC(=Nc2ccnc3ccccc23)c2ccc(Nc3ncnc4cccc(I)c34)cc2C1
BrCN1CCN(Cc2nc(Nc3cccc(Br)c3)nc3ccc(Br)cc23)CC1
BrCc1cc2ncnc(Nc3cccc(Br)c3)c2cn1
Brc1cc(Br)c2c(Nc3nc(Br)cs3)ncnc2c1
Brc1cc(Br)c2cccc(Nc3ncnc4ccccc34)c2c1

 15 Training on 28430 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.632587
Reward: 3.380882
Trajectories with max counts:
10	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.62515724
Proportion of valid SMILES: 0.7884297520661157
Sample trajectories:
Brc1ccc(-c2cccc(Nc3ncnc4ccccc34)c2)c(Br)c1
Brc1ccc(N2CCN=C(Nc3ncnc4cnc(Cc5ccccc5)c(Br)c34)CC2)s1
Brc1ccc(Nc2nc(-c3ccc(Br)cc3Nc3ccc(Br)cc3)c3sccc3n2)cc1
Brc1ccc(Nc2ncnc3c(Br)ccc(Br)c23)cc1Br
Brc1ccc(Nc2ncnc3cc(-c4ccccc4Br)sc23)cc1
Policy gradient replay...
Mean value of predictions: 0.6384245
Proportion of valid SMILES: 0.7151799687010955
Sample trajectories:
BP(=O)(c1ccc(Br)cc1)C(c1ccc(Br)cc1)c1ccc(Br)cc1
BrCCCC(=NNc1ccc(Br)cc1)c1cc(Nc2ncnc3ccccc23)ncn1
Brc1cc(-c2cncnc2)c2sccc2n1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)c(Br)c23)c1
Brc1ccc(-c2csc3ncnc(Nc4ccc(Br)o4)c23)cc1
Fine tuning...
Mean value of predictions: 0.63599145
Proportion of valid SMILES: 0.739514348785872
Sample trajectories:
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Brc1cc(Br)cc(Nc2ncnc3cccc(Br)c23)c1
Brc1cc(Nc2ncnc3ccccc23)c2ncnc(Nc3ccccc3)c2c1
Brc1ccc(-c2c3cc(Br)ccc3c3cc(Br)cc23)cc1

 16 Training on 31995 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.844262
Reward: 3.651942
Trajectories with max counts:
41	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6174838
Proportion of valid SMILES: 0.6764705882352942
Sample trajectories:
BP(=O)(NO)S(=O)(=O)Cc1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cc1)(Nc1cccc(Br)c1)(P(=O)(O)O)P(=O)(O)OP(=O)(O)OP(=O)(O)O
BrCc1cc2c(Nc3ccc(Br)cc3Br)ncnc2cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Policy gradient replay...
Mean value of predictions: 0.6149474
Proportion of valid SMILES: 0.7774140752864157
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2sc(NCCCN3CCCCC3)nc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)cs1
Brc1cc(Nc2ncnc3sc(CCN4CCCC4)nc23)cc2cncnc12
Fine tuning...
Mean value of predictions: 0.6496616
Proportion of valid SMILES: 0.7401377582968065
Sample trajectories:
BrCN1CCC(=Cc2ccc(Br)cc2)c2ccccc2C1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)ncn1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc2c(n1)sc1nc(Nc3cnc(Br)s3)cc12

 17 Training on 34792 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.211852
Reward: 3.740564
Trajectories with max counts:
26	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.61762315
Proportion of valid SMILES: 0.6919949968730457
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2[nH]1
Brc1ccc(Br)c(C2Sc3ncnc4cccc2c34)c1
Brc1ccc(I)cc1Nc1ncnc(Nc2ccccc2)n1
Brc1ccc(NCc2ccnc(Br)c2)nc1
Policy gradient replay...
Mean value of predictions: 0.6567815
Proportion of valid SMILES: 0.717010631644778
Sample trajectories:
BrBr
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3Br)ncnc2c1
Brc1cc(Nc2ncc(-c3cccc4ncncc34)c(-c3ccccc3Br)n2)c2ccccc2c1
Brc1cc(Nc2ncnc3cnc(Br)cc23)ncn1
Fine tuning...
Mean value of predictions: 0.65851915
Proportion of valid SMILES: 0.7329192546583851
Sample trajectories:
BP(=O)(NCCCCCCCCCCCCC)C(=O)C(Cl)(Cl)Cl
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1ccc(-c2cc(Nc3ncnc4ccccc34)c3ccsc3n2)nc1
Brc1ccc(-c2ccc(Br)cc2Nc2ncnc3ccsc23)cc1
Brc1ccc(Br)c(Nc2nccc(Nc3ncnc4ccsc34)n2)c1

 18 Training on 38384 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 40.846468
Reward: 4.161279
Trajectories with max counts:
39	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.685641
Proportion of valid SMILES: 0.7428571428571429
Sample trajectories:
BP(=O)(Nc1ccc(Br)c(O)c1)P(=O)(O)O
Bc1cc2c(cc1Br)-c1cc(Br)ccc1O2
BrCCNCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCN1CCN(CCN2CCCC2)CC1
Brc1cc(-c2ccccc2)n2ncnc2n1
Policy gradient replay...
Mean value of predictions: 0.59958893
Proportion of valid SMILES: 0.7641331658291457
Sample trajectories:
BrC=CCN1CCOCC1COc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1ccccc1
Brc1ccc(-c2ccccn2)c2ccccc12
Brc1ccc(Nc2ccc(Br)c(-c3ccnc(Nc4ncnc5ccccc45)c3)c2)cc1
Brc1ccc(Nc2ccc(Nc3ncnc4ccccc34)cc2)cc1NCCCN1CCNCC1
Fine tuning...
Mean value of predictions: 0.6805128
Proportion of valid SMILES: 0.752895752895753
Sample trajectories:
BP(=O)(CC(=O)Nc1ccc(Br)c(Br)c1)NO
BP(=O)(NCCCCCCCCCCCCNCCCCCCCCCCCCCCCCN)C(=O)Nc1ccc2nsnc2c1
B[PH](=O)(Nc1ccc(Br)cc1)(C(=O)NS(=O)(=O)c1ccc(Br)cc1)c1ccc(Br)cc1
Brc1cc(Br)c2c(Nc3csc(-c4ccccc4Br)c3)ncnc2c1
Brc1cc2c(Nc3c(Br)c(Br)cc4c(Nc5ccccc5)ncnc34)ncnc2s1

 19 Training on 42278 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 41.422554
Reward: 4.361709
Trajectories with max counts:
21	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.70803726
Proportion of valid SMILES: 0.7715404699738904
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3cnc(Br)c(Br)c3)ncnc2c1
Brc1cc(Nc2ncnc3sccc23)cc(I)c1Br
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1ccc(Br)c(Nc2ncccc2Br)c1
Policy gradient replay...
Mean value of predictions: 0.6311111
Proportion of valid SMILES: 0.8148558758314856
Sample trajectories:
BrCCCNc1ncnc2ncnc(Nc3ccccc3)c12
Brc1cc2ncnc(Nc3ccccc3)c2cn1
Brc1ccc(Br)c(Nc2ncnc3ccc(Nc4ncnc5ccccc45)cc23)c1
Brc1ccc(Br)c(Nc2ncnc3cccc(Br)c23)c1
Brc1ccc(Nc2cc(Nc3ccnc4ccccc34)ccn2)cc1
Fine tuning...
Mean value of predictions: 0.67872256
Proportion of valid SMILES: 0.7929724596391263
Sample trajectories:
BP(=O)(Br)Oc1ccc(Nc2ncnc3sccc23)cc1
BP(=O)(CCCO)N(Cc1ccccc1Br)C(=O)c1ccc(Br)cc1Br
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc3s2)cc1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1

 20 Training on 46330 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 41.211871
Reward: 4.361079
Trajectories with max counts:
18	Brc1cccc(Nc2ncnc3ccsc23)c1
Mean value of predictions: 0.59991616
Proportion of valid SMILES: 0.7460913070669168
Sample trajectories:
BP(=O)(CC(=O)Nc1cccc(Br)c1)c1ccc(Br)cc1
Bc1ccccc1Nc1ccccc1Br
BrCCCNC1CCCCN1Cc1ccccc1
BrCCNc1ccccc1-c1cccc2ncnc(Nc3ccccc3)c12
BrCc1cc(Br)cc2ncnc(Nc3ccc(Br)cc3)c12
Policy gradient replay...
Mean value of predictions: 0.72180855
Proportion of valid SMILES: 0.7452923686818632
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCN=CNc1ccc(Br)c(Nc2ncnc3ccccc23)c1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)c(Br)c23)nc1Br
Brc1cc(Nc2ncnc3cc(Br)c(Br)cc23)cs1
Brc1cc(Nc2ncnc3nc(-c4ccccc4Br)sc23)c(Br)s1
Brc1cc2c(Nc3cccc(I)c3)ncnc2cc1I
Fine tuning...
Mean value of predictions: 0.70626044
Proportion of valid SMILES: 0.7529855436832181
Sample trajectories:
Bc1ncc(Br)c(Nc2ncnc3ccc(Br)cc23)c1Br
BrCCNc1nc2ncnc(Nc3cccc(Br)c3)c2s1
BrCCOc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCN1CCc2ccc(Nc3cc(Br)cc(Br)c3)cc2O1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1

Trajectories with max counts:
98	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6833649
Proportion of valid SMILES: 0.6668349318525997
Mean Internal Similarity: 0.5055842182011198
Std Internal Similarity: 0.11244179086574693
Mean External Similarity: 0.421800567104196
Std External Similarity: 0.07787987075680379
Mean MolWt: 492.7224310640733
Std MolWt: 187.56473337816442
Effect MolWt: -0.05215567523348811
Mean MolLogP: 6.739120324656752
Std MolLogP: 4.552085213781947
Effect MolLogP: 0.5274556794238806
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 98.449132% (1587 / 1612)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 100, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 6373.009729385376, 'valid_fraction': 0.6668349318525997, 'active_fraction': 0.6616199848599545, 'max_counts': 98, 'mean_internal_similarity': 0.5055842182011198, 'std_internal_similarity': 0.11244179086574693, 'mean_external_similarity': 0.421800567104196, 'std_external_similarity': 0.07787987075680379, 'mean_MolWt': 492.7224310640733, 'std_MolWt': 187.56473337816442, 'effect_MolWt': -0.05215567523348811, 'mean_MolLogP': 6.739120324656752, 'std_MolLogP': 4.552085213781947, 'effect_MolLogP': 0.5274556794238806, 'generated_scaffolds': 1612, 'novel_scaffolds': 1587, 'novel_fraction': 0.9844913151364765, 'save_path': '../logs/replay_combo_s2-3.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.698529
Reward: 1.000000
Mean value of predictions: 0.0011881188
Proportion of valid SMILES: 0.7910401002506265
Sample trajectories:
BP(=O)(OC(C)C)C1CCCC=CC1OC(=O)CCC(N)=O
Brc1cc(Br)c2c(c1Br)C1CC=CC(CO2)O1
Brc1ccc(-c2ccc3oc(N4CCCC4)nc3n2)cc1
Brc1ccco1
C#CCC#Cc1ccn(C(=O)N2CCN(c3ccccc3)CC2)c1
Policy gradient replay...
Mean value of predictions: 0.0028673834
Proportion of valid SMILES: 0.7846875
Sample trajectories:
Brc1ccc2c(c1)Sc1ccccc1O2
Brc1ccc2ccccc2c1
Brc1ccccc1
Brc1ccccc1-c1cc2ccccc2cc1-c1ccccc1
Brc1ccccc1-c1cc2ccccc2nc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.0018297534
Proportion of valid SMILES: 0.785625
Sample trajectories:
Brc1ccc2[nH]ccc2c1
Brc1ccc2ccccc2c1
Brc1ccc2ccccc2c1Cc1ccccc1
Brc1ccc2ccccc2n1
Brc1cccc(-c2ccccc2)c1

  2 Training on 239 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 15.047628
Reward: 1.000000
Trajectories with max counts:
31	O=C(Nc1ccccc1)c1ccccc1
Mean value of predictions: 0.0013524265
Proportion of valid SMILES: 0.7858705845576742
Sample trajectories:
BrCCc1ccccc1-c1ccccc1
Brc1ccc(-c2ccccc2-c2nc3ccccc3o2)cc1
Brc1ccc2ccccc2c1
Brc1ccc2ccccc2c1-c1ccc2ccccc2c1
Brc1cccc(Nc2c(-c3ccccc3)cnc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.026653882
Proportion of valid SMILES: 0.6543287327478042
Sample trajectories:
BrCI
Brc1cc2nc(-c3ncccc3Br)sc2c(Nc2ccccc2)n1
Brc1ccc(-n2cccc2)nc1
Brc1ccc(Nc2nc3ccccc3nc2-n2ccnc2)cc1
Brc1ccc(Nc2ncnc3cccc(-c4ccc(Br)cc4)c23)cc1
Fine tuning...
Mean value of predictions: 0.023438258
Proportion of valid SMILES: 0.6481481481481481
Sample trajectories:
BrCCNc1cccc(Br)c1
Brc1ccc(-c2nc3ncccc3s2)cc1
Brc1ccc(-n2ncnn2)s1
Brc1ccc(Br)cc1
Brc1ccc(C(=CCSc2ncncc2Br)n2ccnc2)cc1

  3 Training on 405 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.692166
Reward: 1.131414
Trajectories with max counts:
5	COc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.026808701
Proportion of valid SMILES: 0.7049436795994993
Sample trajectories:
BrC1=NN=C(c2ccccc2)Oc2ccccc21
BrCc1ccccc1-c1nccc(Nc2ccccc2)n1
Brc1ccc(-c2ccnc(Nc3ccc(CN4CCCC4)s3)c2)cc1
Brc1ccc(-c2nc3ccccc3s2)cc1
Brc1ccc(C2CCCCC2CCNCCN2CCCCC2)cc1
Policy gradient replay...
Mean value of predictions: 0.04448996
Proportion of valid SMILES: 0.7630509534229447
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(O)C=CC(=O)C(I)=CP(=O)(O)O
BP(=O)(OCCCCCCOCCO)C(O)CO
BrCCCOc1ccccc1Nc1ncnc2ccc(N3CCN(Cc4ccccc4)CC3)nc12
Brc1ccc(C=Cc2ccccc2)cc1
Brc1ccc(CN2CCN(Cc3ccccc3)CC2)cc1
Fine tuning...
Mean value of predictions: 0.045098037
Proportion of valid SMILES: 0.7657178604942133
Sample trajectories:
Brc1cc2ccccc2s1
Brc1ccc(N=NNc2ccc(Br)cc2)cc1
Brc1ccc(NN=Nc2ccc3ccccc3c2)cn1
Brc1ccc(Nc2nccc(-c3ccc4cnccc4c3)n2)cc1
Brc1ccc(Nc2ncnc3[nH]cc(Br)c23)cc1

  4 Training on 815 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.456278
Reward: 1.159296
Trajectories with max counts:
9	COc1ccccc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Mean value of predictions: 0.05230639
Proportion of valid SMILES: 0.7384375
Sample trajectories:
BrC1=Nc2sc(Br)cc2Nc2ccccc2Nc2ccccc2C(=Nc2ccccc2Br)S1
BrCCCCCCCc1ccc2c(n1)Nc1cnccc1-2
BrCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(-c2ncccn2)c2cccnc2c1
Brc1ccc(-c2cc(-c3ccncc3)c3ccnc(Nc4ccc5ccccc5n4)c3n2)cc1
Policy gradient replay...
Mean value of predictions: 0.09935484
Proportion of valid SMILES: 0.6783369803063457
Sample trajectories:
BP(=O)(c1ccc(Br)cc1)C(O)c1cccc(O)c1
Bc1ccccc1Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrC=CBr
BrCc1ccc[n+]2c1nc1cc(Nc3ccccc3)c(Br)cc1c(Nc1ccc(Br)s1)c1ccc(Br)cc1CC2
Brc1cc(-c2ncnc3cscc23)c2ccccc2n1
Fine tuning...
Mean value of predictions: 0.09912804
Proportion of valid SMILES: 0.6811503594873398
Sample trajectories:
BrCCCn1cncn1
BrI
Brc1cc(Nc2c(Br)cc3ccc4ncc(Br)c(Nc5ccccc5)nc4c23)ccn1
Brc1ccc(-c2ccc3[nH]cnc3n2)s1
Brc1ccc(-c2ccccc2)o1

  5 Training on 1628 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.351938
Reward: 1.445312
Trajectories with max counts:
20	COc1ccc(Nc2ncnc3ccccc23)cc1
20	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.11672224
Proportion of valid SMILES: 0.6563477173233271
Sample trajectories:
BrCCCCBr
BrCn1ccc(Nc2nc3ccccc3nc2-c2nc3ccccc3c3ccccc23)c1
Brc1cc(-c2ccccc2)c2cncnc2n1
Brc1ccc(C=Nc2ncnc3ncncc23)cc1
Brc1ccc(I)cc1Nc1ccc(I)cc1
Policy gradient replay...
Mean value of predictions: 0.1455867
Proportion of valid SMILES: 0.6022514071294559
Sample trajectories:
BrC=CC=CC=CC=CC=CCN=C(c1ccccc1)N1CCN(c2ccccn2)CC1
BrCCCCCCCCCCCCCCCCCCC=Cc1ncccc1Nc1ccc(Br)cc1
Br[n+]1ccccc1Nc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc(-c2ccc3c(c2)SCc2ccccc2C3)c2ccccc2n1
Brc1cc(Br)c(Nc2ccc(Oc3ncnc4ccccc34)cc2)cc1Br
Fine tuning...
Mean value of predictions: 0.13962656
Proportion of valid SMILES: 0.6030653737879261
Sample trajectories:
BP(=O)(OCCC)c1ccc(Cl)cc1
BP(F)(F)(F)P(=O)(O)OP(=O)(O)O
BrCC[n+]1ccc2ccccc2c1
Brc1cc(Br)c2c(c1)OCO2
Brc1cc2ccccc2N1c1ncnc2cc(Br)sc12

  6 Training on 2736 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 18.009611
Reward: 1.674607
Trajectories with max counts:
33	Oc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.13062014
Proportion of valid SMILES: 0.6452016255079712
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1OP(=O)(O)O)C(O)CP(=O)(O)O
BP(=O)(OCCCn1ccnc1)c1ccccc1
BP(=O)(Oc1ccccc1)Oc1ccccc1Cl
BrCNP1c2ccccc2-c2ccccc21
Brc1ccc(-c2c[nH]c3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.24117647
Proportion of valid SMILES: 0.5647132560325917
Sample trajectories:
BrCN1c(OCCN2CCOCC2)cc2c(Nc3ccccc3)ncnc21
Brc1cc(-c2ccc(Nc3ncnc4scnc34)cc2)on1
Brc1ccc(-c2ccc(Br)cc2-c2nnc(-c3ccc(Br)cc3)n2-c2ccncc2)cc1
Brc1ccc(-c2ccnc3ccc(N4CCCCC4)cc2N3C=Nc2ccc3ccccc3n2)cc1
Brc1ccc(-c2nc3cc(Br)ccc3s2)cc1
Fine tuning...
Mean value of predictions: 0.23222223
Proportion of valid SMILES: 0.5628517823639775
Sample trajectories:
Brc1cc(-c2ccc3c(c2)CC3)c2c(Nc3ccc(CCN4CCCCCC4)cc3)ncnc2c1
Brc1cc2ncnc(Nc3ccccc3)c2cn1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2cc(-c3nc4cc(Br)ccc4s3)[nH]n2)cc1
Brc1ccc(Nc2cc(Nc3ccsc3)ncn2)cc1

  7 Training on 4069 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 21.288716
Reward: 2.255516
Trajectories with max counts:
109	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.23658931
Proportion of valid SMILES: 0.5794246404002501
Sample trajectories:
BP(=O)(CCNc1ccc(Br)cc1)c1ccc(Cl)cc1
BP(=O)(NP(=O)(O)OP(=O)(O)O)OCC(=O)O
BP(=O)(Oc1ccccc1)Oc1ccccc1
BrSc1cc(Nc2ncnc3ccc(Br)cc23)ccc1-c1ccccc1
Brc1ccc(-c2cccc(Nc3ncc(Br)cn3)c2)cc1
Policy gradient replay...
Mean value of predictions: 0.2548881
Proportion of valid SMILES: 0.5321215919774366
Sample trajectories:
BrC=CCCC=CCC=CCC=CC=CCC=CC=CC=CC=CCC=CCBr
BrCCCOc1ccc(Nc2cc3sc(Nc4ccc(Br)cc4)ncnc23)cc1
BrCCNc1cccc(Br)c1
BrCCOc1ccc(Nc2ncnc3cc4cc(cccc23)Nc2cc3sccc3nc2c4)cc1
BrCCOc1ccc(Nc2ncnc3scc(-c4ccc(Br)cc4)c23)o1
Fine tuning...
Mean value of predictions: 0.23265307
Proportion of valid SMILES: 0.5377861398557542
Sample trajectories:
BP(=O)(CCNC(=O)C(CCC(=O)OCCCP(=O)(O)O)C(=O)O)OCC
BP(=O)(Nc1cc(Nc2ccc(Br)cc2)c2ccccc2n1)c1ccc(Br)cc1
BP1(=O)OCC(OC(=O)Nc2cccc(Br)c2)O1
BrC1=C(Nc2ccc(Br)cc2)c2ccccc2S1
BrCCCCCn1cc(-c2ccc3ccccc3c2)c2c(Nc3ccccc3)ncnc21

  8 Training on 5456 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 22.560096
Reward: 2.629334
Trajectories with max counts:
24	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.34700966
Proportion of valid SMILES: 0.48746081504702193
Sample trajectories:
Brc1cc(-c2ccsc2)n2nccc2n1
Brc1cc(-c2ncnn2Cc2ccncc2)c2sccc2n1
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Nc4ccccc4Br)cc3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3scnc23)c1
Policy gradient replay...
Mean value of predictions: 0.28140312
Proportion of valid SMILES: 0.5621283255086073
Sample trajectories:
BP(=O)(OCC)C(=O)O
BrC=CC=C(Br)Br
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1Nc1ccccc1
Brc1cc2c(Nc3ccccc3Br)cccc2c2nc3ccccc3cc2cco1
Fine tuning...
Mean value of predictions: 0.2601888
Proportion of valid SMILES: 0.5631644777986241
Sample trajectories:
BP(=O)(c1ccc(Br)cc1)N(O)C(F)(F)F
Brc1cc(Nc2ccc(Br)c(Br)c2)nc(Nc2nccc(Br)c2-c2ccccc2)n1
Brc1cc(Nc2ncnc(Nc3ccccc3)c2Cn2cncn2)ncc1I
Brc1cc(Nc2ncnc3ccccc23)ccc1Oc1ccccc1
Brc1ccc(Br)c(Nc2ncnc3ccccc23)c1

  9 Training on 7038 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 24.579941
Reward: 3.208020
Trajectories with max counts:
111	Fc1ccc(Nc2ncnc3ccc(F)cc23)cc1
Mean value of predictions: 0.3931673
Proportion of valid SMILES: 0.4397496087636933
Sample trajectories:
BP(=O)(O)CC(NP(=O)(O)Nc1ccc(OP(=O)(O)O)cc1)c1cc2cc(Br)cnc(N)c2c(N)[nH]1
Bc1cc(Br)cc(Br)c1-c1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Bc1cc(Nc2nccc(Br)c2Br)c(Br)c(Br)c1O
Brc1c(Nc2ncnc3ccccc23)cccc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Brc1cc(Br)c(-c2ccccc2)cc1Oc1cccnc1
Policy gradient replay...
Mean value of predictions: 0.29104722
Proportion of valid SMILES: 0.5762976860537836
Sample trajectories:
B[PH](=O)(=NNc1ccc(Br)cc1)Nc1ccc(Br)cc1
Bc1ccc(Nc2ccc(Br)cc2)cc1Br
Bc1cccc(Nc2ncnc3ccccc23)c1
BrC1=C2Oc3ccc(Br)cc3C2CC=C1
BrCc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.28445172
Proportion of valid SMILES: 0.572991559862457
Sample trajectories:
BP(=O)(CCC=C(C)Cl)OCCO
BP(=O)(NCc1ccc(F)c(F)c1)C(=O)Nc1cc(Cl)c(F)cc1F
BrCCNc1cc2c(Br)cccc2nc1Nc1ccccc1
BrCN1CCOCC1
Brc1cc(Br)c2sc3c(Br)ccc(Br)c3c2c1

 10 Training on 8617 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 24.322538
Reward: 3.865993
Trajectories with max counts:
229	Fc1ccc(Nc2ncnc3cc(F)ccc23)cc1
Mean value of predictions: 0.46700612
Proportion of valid SMILES: 0.3070669168230144
Sample trajectories:
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2cnc3ccc(Br)cc3c2)c1
Brc1ccc(-c2cc(Nc3ncnc4ccsc34)c(Br)cn2)c(Br)c1
Brc1ccc(Nc2cc(Nc3ccc(Br)cc3)ncn2)cc1
Brc1ccc(Nc2cc(Nc3cccc(Br)c3)ncn2)cc1
Policy gradient replay...
Mean value of predictions: 0.37043956
Proportion of valid SMILES: 0.5696400625978091
Sample trajectories:
BP(=O)(OCCCC)OC(=O)CCS(=O)(=O)NO
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Nc4ccccc4Br)cc23)c1
Brc1cc(Nc2cnccc2Nc2ccccc2)ccc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.36584824
Proportion of valid SMILES: 0.5607008760951189
Sample trajectories:
BP(=O)(OCC)OCC=CC=CC
BrNc1cccc(Nc2nc(Nc3cc(Br)ccc3Br)nc3c(Br)cccc23)c1
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3sc(-c4ccccc4Br)cc3Br)ncnc2c1
Brc1cc(Nc2ncnc3cc(Br)c(Br)cc23)c(Br)s1

 11 Training on 9990 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 23.243200
Reward: 3.445355
Trajectories with max counts:
54	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.32458156
Proportion of valid SMILES: 0.5978736710444027
Sample trajectories:
BP1(=O)OCC(OC(=N)C(CCCl)OCCCl)C(O)C(O)C1O
B[PH](=O)(Nc1ccc(Br)cc1)=C(Br)C(=O)NCc1ccc(Br)cc1
BrCc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
BrSc1ncccc1-c1ccccc1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.41163892
Proportion of valid SMILES: 0.5277342525853964
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1cccc(Br)c1
Bc1ccc(Nc2cc(Br)ccc2Nc2ncnc3ccsc23)cc1
BrC(Nc1ncnc2sccc12)c1ccccc1
BrCCSc1ccc(Nc2ncnc3sccc23)cc1
BrCN1CCC(Nc2cc(Nc3ncnc4cc(Br)cc(Br)c34)ccc2Br)CC1
Fine tuning...
Mean value of predictions: 0.4174863
Proportion of valid SMILES: 0.5167869469720741
Sample trajectories:
BP(=O)(N(O)C=O)P(=O)(O)O
BP(=O)(NCCCCCCCCCCCCCCCCCC(F)F)Oc1ccc(Br)cc1
BP(=O)(OCC#C)OP(=O)(O)OP(=O)(O)CCCCCCCCCCCCC(Br)Br
BP(=O)(OCC1OC(N(O)S(=O)(=O)O)C(O)C1O)C(=S)C(=O)NCCCl
Bc1ccc(Nc2ncnc3c(Br)cccc23)cc1-c1ccc(O)cc1

 12 Training on 11324 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.635425
Reward: 4.009817
Trajectories with max counts:
181	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.46629396
Proportion of valid SMILES: 0.3918622848200313
Sample trajectories:
BP(=O)(NC(=O)CCC(=O)Nc1cc(Br)c(Br)cc1Br)C(=O)Nc1cc(Br)c(Br)c(Br)c1F
BP(=O)(Nc1ccc(Br)c(Br)c1)c1cc(Br)c(Br)cc1Br
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC1OC(=O)C(C)=CC1=O)OP(=O)(O)Oc1ccccc1
Policy gradient replay...
Mean value of predictions: 0.3992891
Proportion of valid SMILES: 0.5299843014128729
Sample trajectories:
BP(=O)(CCCF)Oc1ccc(Nc2ncnc3sc(Cl)cc23)cc1
BP(=O)(NCCCCl)Nc1ccc(Br)cc1
BP(=O)(O)CCCCNCCCCCCCCC
BP(=O)(OC(=O)c1cccc(Br)c1)N(O)C=O
BrCCCBr
Fine tuning...
Mean value of predictions: 0.4107077
Proportion of valid SMILES: 0.5094043887147336
Sample trajectories:
BP(=O)(O)CCCCS
BP(=O)(OCC)c1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)cc(Nc2ccc(Nc3ncnc4c(Br)cc(Br)cc34)cc2Br)c1

 13 Training on 12705 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.646054
Reward: 3.962015
Trajectories with max counts:
433	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.37829596
Proportion of valid SMILES: 0.3484375
Sample trajectories:
BP(=O)(C=CC(=O)Nc1ccc(Cl)cc1)N1CCCS1(=O)=O
BP(=O)(CCCO)C(F)(F)P(=O)(O)O
BP(=O)(Nc1cccc(Br)c1)OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O
BP(=O)(Nc1ccccc1)NS(=O)(=O)c1ccc(Nc2ccccc2Oc2ccncc2)cc1
BP(=O)(Nc1ccccc1)OCC1OC(=O)c2c(cc(I)c(N)c2I)C1(C)C
Policy gradient replay...
Mean value of predictions: 0.114285715
Proportion of valid SMILES: 0.6724030037546934
Sample trajectories:
BP(=O)(Br)Nc1ccccc1Br
BrCc1ccccc1-c1ccccc1-c1ccccc1Br
Brc1cc(Nc2ncnc3ccc(Nc4ccccc4-c4ccccc4-c4ccccc4-c4ccccc4Br)cc23)cs1
Brc1cc2c(Nc3ccccc3-c3ccccc3Br)ncnc2s1
Brc1ccc(-c2ccccc2Nc2ccc3ncnc(Nc4ccccc4)c3c2)s1
Fine tuning...
Mean value of predictions: 0.10991502
Proportion of valid SMILES: 0.6633260256811776
Sample trajectories:
BP(=O)(Nc1ccccc1)Oc1ccccc1
BP(=O)(OCC1=C(O)N(c2ccccc2)c2ccccc21)c1ccccc1
Bc1ccccc1Nc1ncnc2cccnc12
BrC(=NNc1ccccc1-c1ccccc1)c1ccccc1
Brc1cc2sc(Nc3ccccc3)cc2c2ccccc12

 14 Training on 13272 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.177415
Reward: 3.739568
Trajectories with max counts:
55	COc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.28961176
Proportion of valid SMILES: 0.595625
Sample trajectories:
BP(=O)(COP(=O)(N(C)C)P(=O)(CC=C)OCC=C)N1CCCCC1
BP(=O)(Nc1ccccc1)c1ccccc1
Brc1cc(Br)c2cc(Nc3ccccc3-c3ccccc3-c3ccccc3-c3ccccc3Br)ccc2c1
Brc1cc2c(Nc3ccc(-c4ccccc4-c4ccccc4Br)cc3)ncnc2s1
Brc1cc2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.2301775
Proportion of valid SMILES: 0.6341463414634146
Sample trajectories:
Bc1cccc(Nc2ncnc3ccsc23)c1
Bc1ccccc1-c1cc2ncnc(Nc3ccccc3)c2cc1C
Bc1ccccc1-c1cccc2ncnc(Nc3ccccc3)c12
Bc1ccccc1-c1ccccc1-c1ccc(Nc2ncnc3sccc23)cc1
Bc1ccccc1-c1ccccc1-c1ccccc1Br
Fine tuning...
Mean value of predictions: 0.23224156
Proportion of valid SMILES: 0.6108158799624883
Sample trajectories:
BP(=O)(NCCCCCCCCCBr)C(=O)Nc1ccccc1Br
BP(=O)(O)CCCCCCCCCCCCCCCCS(=O)(=O)NCC=O
Bc1cc(Nc2ccccc2Br)c2ncnc(Nc3ccccc3)c2c1
Bc1ccc(Nc2ncnc3cc(Br)cnc23)cc1
Bc1ccc(Nc2ncnc3ccccc23)cc1

 15 Training on 14235 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.188856
Reward: 3.956188
Trajectories with max counts:
359	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.40879926
Proportion of valid SMILES: 0.3409375
Sample trajectories:
BP(=O)(NC(=O)c1ccc(Br)cc1Br)N(=O)=O
BP(=O)(OCC)C1(N=C(Br)P(=O)(Oc2ccccc2)c2ccc(F)cc2)CCCCC1
B[PH](=O)(NCc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O
B[PH](=O)(OC(=O)Nc1ccc(F)cc1)(N1CCC(c2ccccc2)CC1)P(=C)(O)O
BrBr
Policy gradient replay...
Mean value of predictions: 0.46875
Proportion of valid SMILES: 0.5111180707798308
Sample trajectories:
Bc1ccc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)s1
Bc1ccc(Nc2ncnc3scnc23)cc1
BrC=Cc1cc2ncnc(Nc3ccc(Br)s3)c2cc1-c1ccccc1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3cnc(Br)cc3Br)ncnc2c1
Fine tuning...
Mean value of predictions: 0.46964815
Proportion of valid SMILES: 0.5248826291079812
Sample trajectories:
BP(=O)(OCC)OC(=O)c1ccc2ncnc(Nc3ccc(I)cc3)c2c1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1
Br
BrCCC(Br)(Br)CCCC(Br)(Br)Br

 16 Training on 15652 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.808948
Reward: 4.195711
Trajectories with max counts:
386	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5612586
Proportion of valid SMILES: 0.3178125
Sample trajectories:
BP(=O)(CCC(O)P(=O)(O)O)OCC
BP(=O)(NC(=O)C(F)(F)F)c1c(F)cc(F)cc1-c1ncccn1
BP(=O)(NCCCCCCCCCCCCCCCC)C(=O)Nc1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Nc2cc(Br)c(Br)cc2Br)cc1
BP(=O)(OCC(=O)Nc1ccc(Br)cc1)P(=O)(Oc1cc(Br)cc(Br)c1)Oc1cc(Br)c(O)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.49586883
Proportion of valid SMILES: 0.47730829420970267
Sample trajectories:
B[PH]1(=O)=C(N)NN(C=CC(N)=O)C2OC(CO1)C(O)C2O
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
BrSc1ccc(Nc2ncnc3scnc23)cc1
Brc1cc(-c2cccs2)c2ncn(CCCCCCCCCCCCN3CCOCC3)c2n1
Brc1cc(Br)c2c(Nc3cc(Br)c(-c4ccccc4)cn3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.49834397
Proportion of valid SMILES: 0.49123904881101377
Sample trajectories:
BP(=O)(CCCCCC(=O)Nc1ccc(Br)cc1)OCCCO
BP(=O)(N1CCCCC1)P(=O)(O)O
BP(=O)(Nc1cccc(Br)c1)P(=O)(Oc1ccccc1)Oc1cccc(Br)c1
BP(=O)(OCC)C(O)C(F)(F)F
Brc1cc(-c2ncnc3ccccc23)c2ccccc2n1

 17 Training on 17291 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.251560
Reward: 4.816998
Trajectories with max counts:
531	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5336323
Proportion of valid SMILES: 0.27875
Sample trajectories:
BP(=O)(NC(=O)C(F)(F)F)OCC
BP(=O)(NO)Nc1cc(Br)cc(Br)c1Br
BP(=O)(OCC(CCl)P(=O)(Oc1ccc(F)cc1)c1ccc(F)cc1F)P(=O)(O)O
BP(=O)(c1ccc(Br)cc1)c1cc(Br)c(Br)c(Br)c1
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.43748015
Proportion of valid SMILES: 0.5906816760475297
Sample trajectories:
BP(=O)(NC(CCC#N)NP(=O)(O)c1ccccc1)C(=O)OCCCCC
BP(=O)(OCC)C(F)(F)F
Bc1ccccc1-c1ccccc1Nc1ncnc2ccccc12
Brc1cc(-c2ccc3ccccc3c2)c2sccc2n1
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccccc4)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.44055203
Proportion of valid SMILES: 0.5896713615023474
Sample trajectories:
Bc1ccc(Nc2ncnc3cc(Br)sc23)cc1
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1
BrN(c1ccccc1)c1ccccc1-c1ccccc1
Brc1cc(-c2nc3ccccc3s2)c2ccccc2n1
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1

 18 Training on 18832 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.992017
Reward: 4.814268
Trajectories with max counts:
221	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5610183
Proportion of valid SMILES: 0.3931811072880826
Sample trajectories:
BP(=O)(NO)c1ccc(Br)cc1
BP(=O)(NS(=O)(=O)c1cccc(Br)c1)C(NS(=O)(=O)c1ccc2c(c1)CCc1ccccc1-2)C(=O)O
BP(=O)(O)C=O
BP(=O)(c1ccc(Br)cc1)N(CCO)c1ccc(Br)cc1
B[PH](Br)(OP(=O)(Br)Oc1ccc(Br)cc1)C(Br)(Br)Br
Policy gradient replay...
Mean value of predictions: 0.43008357
Proportion of valid SMILES: 0.5612883051907442
Sample trajectories:
BP(=O)(C(=O)O)N(O)Cc1ccc(Br)cc1
BP(=O)(NO)S(=O)(=O)C(F)(F)F
BrC=CBr
Brc1cc(Br)c(-c2cccc(Nc3ncnc4cccc(Br)c34)c2)cc1Br
Brc1cc(Br)c(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.43542394
Proportion of valid SMILES: 0.5382932166301969
Sample trajectories:
BP(=O)(CCF)P(=O)(C(=O)Nc1cc(Br)c(Br)cc1F)N1CCOCC1
BP(=O)(NC(=O)c1ccc(NC(=O)c2cc(Br)nc3ccccc23)cc1)c1ccc2ncnc(-c3ccc(Br)cc3)c2c1
BP(=O)(NC(C(=O)OCCl)c1ccccc1)c1ccccc1
Brc1cc(-c2ccc3ccc(Nc4ccc(-c5ccncc5)c(-c5ccccc5Br)c4)cc3c2)ccc1-c1ccccc1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccccc12

 19 Training on 20492 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.869722
Reward: 5.015412
Trajectories with max counts:
260	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.46722975
Proportion of valid SMILES: 0.37011566114410754
Sample trajectories:
BP(=O)(Br)Oc1cccc(Br)c1-c1ccccc1
BP(=O)(COc1ccccc1)P(=O)(O)O
BP(=O)(Nc1ccc(Br)cc1)P(=O)(O)C(F)(F)F
BP(=O)(Nc1ccc(F)cc1)c1cc(F)ccc1-c1cccc(F)c1
BP(=O)(Nc1cccc(Br)c1)Nc1cccc(Br)c1
Policy gradient replay...
Mean value of predictions: 0.27445933
Proportion of valid SMILES: 0.606875
Sample trajectories:
BP(=O)(F)(F)(F)C(F)(F)F
BP(=O)(Nc1cccc(Nc2ncnc3ccc(Br)cc23)c1)c1ccccc1
BP(=O)(Nc1ccccc1-c1ccccc1)Oc1ccccc1
Bc1ccccc1Nc1ccccc1Nc1ccccc1Nc1ccccc1Nc1ccc(Nc2cc(Nc3ccccc3)ccn2)cc1
Brc1cc(Nc2ncnc3ccccc23)ncn1
Fine tuning...
Mean value of predictions: 0.2795939
Proportion of valid SMILES: 0.6158174429509221
Sample trajectories:
BP(=O)(C(Br)=C(Br)Br)c1ccccc1P(=O)(c1ccccc1)c1ccccc1
BP(=O)(Nc1ccccc1)NC(Cc1ccccc1)C(=O)NCCCl
Bc1ccccc1-c1ccccc1-c1ccccc1Nc1ncnc2ncnc(Nc3cccc(Br)c3)c12
Bc1ccccc1Nc1cccc2ncnc(Nc3cccc(Br)c3)c12
BrC=CBr

 20 Training on 21585 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.410029
Reward: 4.894816
Trajectories with max counts:
398	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5085653
Proportion of valid SMILES: 0.2919662394498281
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(OCC)n1cnc2c1Nc1cc(Br)ccc12
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1cccc(Nc2ncnc3c(-c4ccccc4)ncnc23)c1
Policy gradient replay...
Mean value of predictions: 0.5210851
Proportion of valid SMILES: 0.5103838892385147
Sample trajectories:
BP(=O)(C=CC=CC=C(O)CCCCCCCCCCCCCCCCCCCCCCCCC)OCC
BP(=O)(NC(=O)c1ccc(Br)c(Nc2nc(-c3cc(Br)c(Br)cc3F)nc3cccc(Br)c23)n1)OCC
Bc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Fine tuning...
Mean value of predictions: 0.5104193
Proportion of valid SMILES: 0.49715729627289956
Sample trajectories:
BP(=O)(OCC)c1ccc(-c2ccc(Br)cc2Br)c(Br)c1
BP(=O)(OP(=O)(OCc1ccc(F)cn1)c1ccccc1)N(O)CCCCCC
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2cc1Br
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1

Trajectories with max counts:
447	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4678375
Proportion of valid SMILES: 0.4237512613521695
Mean Internal Similarity: 0.4933725403073951
Std Internal Similarity: 0.10331215853093204
Mean External Similarity: 0.4151604080754208
Std External Similarity: 0.07493484756597842
Mean MolWt: 425.70264635761595
Std MolWt: 115.13852433114123
Effect MolWt: -0.6963682105571888
Mean MolLogP: 5.848965397350994
Std MolLogP: 2.120818493683446
Effect MolLogP: 0.629109156415721
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 96.855346% (770 / 795)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 0, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5496.356374263763, 'valid_fraction': 0.4237512613521695, 'active_fraction': 0.4494716475666022, 'max_counts': 447, 'mean_internal_similarity': 0.4933725403073951, 'std_internal_similarity': 0.10331215853093204, 'mean_external_similarity': 0.4151604080754208, 'std_external_similarity': 0.07493484756597842, 'mean_MolWt': 425.70264635761595, 'std_MolWt': 115.13852433114123, 'effect_MolWt': -0.6963682105571888, 'mean_MolLogP': 5.848965397350994, 'std_MolLogP': 2.120818493683446, 'effect_MolLogP': 0.629109156415721, 'generated_scaffolds': 795, 'novel_scaffolds': 770, 'novel_fraction': 0.9685534591194969, 'save_path': '../logs/replay_combo_s2-4.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.698529
Reward: 1.000000
Mean value of predictions: 0.0011881188
Proportion of valid SMILES: 0.7910401002506265
Sample trajectories:
BP(=O)(OC(C)C)C1CCCC=CC1OC(=O)CCC(N)=O
Brc1cc(Br)c2c(c1Br)C1CC=CC(CO2)O1
Brc1ccc(-c2ccc3oc(N4CCCC4)nc3n2)cc1
Brc1ccco1
C#CCC#Cc1ccn(C(=O)N2CCN(c3ccccc3)CC2)c1
Policy gradient replay...
Mean value of predictions: 0.0028673834
Proportion of valid SMILES: 0.7846875
Sample trajectories:
Brc1ccc2c(c1)Sc1ccccc1O2
Brc1ccc2ccccc2c1
Brc1ccccc1
Brc1ccccc1-c1cc2ccccc2cc1-c1ccccc1
Brc1ccccc1-c1cc2ccccc2nc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.018753069
Proportion of valid SMILES: 0.6377582968065122
Sample trajectories:
Brc1ccc(-c2[nH]ncc2-c2cccc(-c3cccc(Br)c3)c2)cc1
Brc1ccc(Nc2ncnc3cnccc23)nc1
C#CCCC12CC1(CO)CC1C2N1CCCc1nsc(CN2CCC(N3CCN(CC)CC3)O2)n1
C#CCNC(=O)C1=NN(Cc2cn(CC)cn2)Nc2ccccc21
C#Cc1c(-c2ccccc2)ccc(Nc2cc(-c3ccccc3)c(C)nn2)c1C#N

  2 Training on 287 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.377350
Reward: 1.040467
Trajectories with max counts:
6	Oc1ccc2ccccc2c1
Mean value of predictions: 0.022521008
Proportion of valid SMILES: 0.7439824945295405
Sample trajectories:
Brc1ccc(-c2ccccc2)o1
Brc1ccc(Nc2nc(Nc3ccccc3)nc3ccccc23)cc1
Brc1ccc(Nc2ncc3sc(-c4ccc(Br)o4)nc3n2)cc1
Brc1ccc(Nc2ncnc3cnc(Nc4cccc(Br)c4)nc3-2)cc1
Brc1ccc2ncc(Nc3ccccc3Br)nc2c1
Policy gradient replay...
Mean value of predictions: 0.005430211
Proportion of valid SMILES: 0.8176985616010006
Sample trajectories:
Brc1cc2ccccc2cc1-c1ccc2ccccc2c1
Brc1ccc(Nc2ccccc2Nc2ccccc2Br)cc1
Brc1ccc(Nc2cccnc2)cc1
Brc1cccc(Nc2ncccc2-c2ccccc2)c1
Brc1ccccc1-c1cccc2ccccc12
Fine tuning...
Mean value of predictions: 0.04758621
Proportion of valid SMILES: 0.6359649122807017
Sample trajectories:
BrCCOc1ccc(-c2ccccn2)c2ccccc12
Brc1ccc(-c2nc3cccnc3s2)o1
Brc1ccc(-c2ncc[nH]2)s1
Brc1ccc(Br)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1

  3 Training on 531 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.292418
Reward: 1.334564
Trajectories with max counts:
11	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.055350196
Proportion of valid SMILES: 0.6425
Sample trajectories:
BP(=O)(CCC)C(=O)Nc1ccc(Br)cc1
Brc1ccc(-c2ccncc2)o1
Brc1ccc(-c2nc(-c3cncnc3)c3ccccc3n2)cc1
Brc1ccc(Nc2ccncn2)cc1
Brc1ccc(Nc2nc3ccccc3[nH]2)cc1
Policy gradient replay...
Mean value of predictions: 0.064326465
Proportion of valid SMILES: 0.7126328955597249
Sample trajectories:
BP(=O)(c1ccccc1)N1CCOCC1
Brc1cc2c(Br)cccc2c2ccccc12
Brc1ccc(N2C=C(Nc3nccs3)c3ccccc3C23COC(N2CCN(c4ccccc4)CC2)C3)cc1
Brc1ccc(N=Nc2ccncc2)cc1
Brc1ccc(Nc2nc3ccccc3nc2c2ncnc3ccccc32)cc1
Fine tuning...
Mean value of predictions: 0.10121774
Proportion of valid SMILES: 0.641963727329581
Sample trajectories:
BP(=O)(CCN1CCCC(F)C(F)C1)OCC
BP(=O)(N(O)COP(=O)(O)OP(=O)(O)O)P(=O)(Nc1cccc(F)c1)OCOC(=O)N(O)C(CC(=O)O)NS(=O)(=O)CCl
BP(=O)(NO)c1cccc(Cl)c1
BrCc1ccc2c(Nc3ccccc3-c3cccnc3-c3cncnc3-c3ccccc3Br)ncnc2c1
Brc1ccc(-c2ccc3[nH]ccc3c2)c2ccccc12

  4 Training on 1203 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 20.076134
Reward: 1.582864
Trajectories with max counts:
90	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.17416807
Proportion of valid SMILES: 0.554582421019706
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(cc1CSc1nc3ccccc3nc1Nc1ncnc3sc4ccccc4c13)Sc1ccccc1N2
Brc1ccc(-c2cnc(Nc3ccccc3)nc2)cc1
Brc1ccc(Br)c(-c2ccc(Nc3ccc(Br)s3)cc2)c1
Brc1ccc(C=C2NC(c3c[nH]c4ccccc34)=NSS2)cc1
Policy gradient replay...
Mean value of predictions: 0.16084905
Proportion of valid SMILES: 0.5304973412574289
Sample trajectories:
Bc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc2ncnc(Nc3ccccc3)c2nc1CN1CCCCC1
Brc1ccc(-c2ccccc2Br)cc1
Brc1ccc(-c2ncnc3c(Nc4ccccc4Br)ncnc23)s1
Brc1ccc(Br)c(Nc2ncc3ncnc(-c4ccccc4)c3n2)c1
Fine tuning...
Mean value of predictions: 0.17446353
Proportion of valid SMILES: 0.5828642901813633
Sample trajectories:
BP(=O)(C=O)OCC
BrCCOc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(-c2ccccc2)cc1Nc1ncnc2ccccc12
Brc1ccc(-c2ccccc2)nc1Nc1ncc2ccccc2n1
Brc1ccc(-c2nc(-c3ccccc3)c3c(-c4ccccc4Br)ncnc3n2)s1

  5 Training on 2438 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 19.652863
Reward: 2.065749
Trajectories with max counts:
422	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.09520295
Proportion of valid SMILES: 0.508125
Sample trajectories:
Bc1cnccc1N=C(Nc1ccccc1N)C(N)=O
Brc1cc2ccccc2cc1-c1cccc(Nc2nccs2)c1
Brc1ccc(-c2ccc(Nc3cccs3)cc2)cc1
Brc1ccc(-c2ccccc2)c2ccccc12
Brc1ccc(NCCCN2CCN(c3ccccc3Br)CC2)s1
Policy gradient replay...
Mean value of predictions: 0.17282984
Proportion of valid SMILES: 0.6373866833385433
Sample trajectories:
BrCc1ccccc1-n1ccc2ccccc21
Brc1ccc(Br)c(Nc2cccc(Nc3ccccc3N=Cc3ccccc3Br)n2)c1
Brc1ccc(Br)c(Nc2ncnc3ncnc(Nc4ccccc4Br)nc3n2)c1
Brc1ccc(Nc2ncnc(Nc3cccc(Br)c3)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.18297435
Proportion of valid SMILES: 0.609375
Sample trajectories:
BP(=O)(OCC)OC(=O)CCC(=O)OC(C)C=CC=CBr
Bc1ccccc1-c1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(-c2cccnc2)c2ncnc(Nc3ccccc3)c2c1Nc1ccccc1
Brc1cc(Br)c(Br)c(-c2ccccc2-c2ccccc2)c1
Brc1ccc(-c2[nH]ccc2Br)cc1

  6 Training on 3542 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 19.323853
Reward: 2.696473
Trajectories with max counts:
1084	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.18702596
Proportion of valid SMILES: 0.313125
Sample trajectories:
Brc1cccc(Nc2ncnc3ccccc23)c1
Brc1ccccc1-c1ccccc1-c1ccccc1Nc1ccccc1Nc1ncnc2ccccc12
Brc1ccccc1-c1ccccc1Nc1ncccc1Nc1ccccc1
Brc1ccccc1-c1ccccc1Nc1ncnc2ccccc12
Brc1ccccc1-c1cnccc1Nc1ncnc2ccccc12
Policy gradient replay...
Mean value of predictions: 0.26840797
Proportion of valid SMILES: 0.5028142589118199
Sample trajectories:
BP(=O)(NC(C)(COC(=O)CCCl)OC)OCCF
BP(=O)(OCC)OC(=O)CCCCCCC(=O)CI
BP(=O)(OCC1OC(OP(=O)(O)O)C(O)C1O)n1cnc2c(N)ncnc21
BrC1=NNc2ncc(Br)c3cccc1c23
BrC1=Nc2sc3c(c2C=C1)CCCCC3
Fine tuning...
Mean value of predictions: 0.26655114
Proportion of valid SMILES: 0.5417840375586854
Sample trajectories:
BP(=O)(C(=O)Oc1ccc(F)cc1)N1CCOCC1
Brc1cc(Nc2ncnc3ncnc(Nc4ccncc4Br)c23)ccn1
Brc1ccc(-c2ccccc2)cc1Nc1ncnc2ccccc12
Brc1ccc(-c2nc3ncccc3s2)cc1
Brc1ccc(Br)c(Nc2ncnc3cc(Br)ccc23)c1

  7 Training on 4760 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 21.623724
Reward: 2.868256
Trajectories with max counts:
439	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.22054055
Proportion of valid SMILES: 0.4625
Sample trajectories:
BP(=O)(OCCCC)OCOP(=O)(O)OP(=O)(O)O
BP(=O)(Oc1ccccc1P(=O)(Oc1ccccc1)c1ccccc1)C(O)CCC
BP1(=O)CCN1CCS(=O)(=O)Nc1ccc(Br)cc1
BrC1=Nc2ccccc2Nc2ccccc2S1
BrSc1ccccc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Policy gradient replay...
Mean value of predictions: 0.3390625
Proportion of valid SMILES: 0.5209768315591734
Sample trajectories:
BP(=O)(N=Nc1ccc(I)cc1)N(O)CP(=O)(O)O
BrCCOc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(N2CCCC2CN2CCC(N3CCOCC3)CC2)c2c(Nc3cccc(-c4ccccc4Br)c3)ncnc2c1
Brc1ccc(-c2cnc3sc(Nc4ccccc4)nc3c2)cn1
Brc1ccc(Br)c(Nc2cc(Br)c(Br)cn2)c1
Fine tuning...
Mean value of predictions: 0.27404994
Proportion of valid SMILES: 0.5758049390434511
Sample trajectories:
BP(=O)(Nc1ccc(F)cc1)c1ccc(F)cc1F
Bc1cccc(Nc2ncnc3cc(Cl)ccc23)c1
BrCCc1ccncc1Nc1ncnc2ccccc12
Brc1cc2c(nc1Nc1ncnc3sc(-c4ccccc4)cc13)Sc1ccccc1N2
Brc1ccc(-c2ccccc2Nc2ccccc2Br)o1

  8 Training on 6190 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 22.429478
Reward: 3.021695
Trajectories with max counts:
82	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.33915246
Proportion of valid SMILES: 0.4945261182358461
Sample trajectories:
BP(=O)(OCC)OC(=O)C1OP(F)(=S)OC1(F)F
Brc1cc2ccc1Oc1cccc3nc[nH]c3c(ncn1)N2
Brc1ccc(-c2nc(Nc3cccs3)[nH]c2-c2ccccc2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ncnc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.25866947
Proportion of valid SMILES: 0.5965625
Sample trajectories:
BP(=O)(Nc1cccc(Nc2ccccc2Cl)c1)NS(=O)(=O)c1ccc(Br)cc1
BP(=O)(Oc1ccccc1Br)P(=O)(Oc1ccccc1)Oc1ccc(Br)cc1
Bc1ccc(Nc2ccccc2Br)c(Br)c1Nc1ccccc1Br
BrC1=Nc2ccccc2-c2ccc(Br)c(Br)c2S1
BrCCCCC1=Nc2sc3ccccc3c2OC1=Nc1ccccc1
Fine tuning...
Mean value of predictions: 0.2955665
Proportion of valid SMILES: 0.5712945590994372
Sample trajectories:
BP(=O)(NC(=O)OCC)c1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CSCCP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(c1ccccc1Nc1c(N)cc(Cl)cc1O)N(O)CC=O
Bc1[nH]c2ccccc2c1-c1nc2cc(Nc3ccc(Br)cc3)cc(Br)cn12
BrC=C1N=Nc2ccc(Br)cc2Nc2cc(Br)ccc21

  9 Training on 7707 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 23.830911
Reward: 3.064039
Trajectories with max counts:
62	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.2935501
Proportion of valid SMILES: 0.5961237886839638
Sample trajectories:
BP(=O)(OCC)C1Oc2sc(Br)cc2N1C(=O)c1cc(Br)ccc1Br
BrCBr
Brc1c(-c2ccccc2)cccc1-c1cnc2ccccn12
Brc1cc(-c2ccccc2)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2sccc12
Policy gradient replay...
Mean value of predictions: 0.39168197
Proportion of valid SMILES: 0.5112570356472795
Sample trajectories:
BP(=O)(N1CCN(C(=O)Oc2ccc(Cl)c(F)c2)CC1)C(F)(F)F
BP(=O)(OCC1OC(n2cnc3c(N)cc(Br)cc32)C(O)(O)C1O)C(=O)NS(=O)(=O)Oc1ccccc1
BP(=O)(OCCC)OCOP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BrC1(c2ccccc2)CC1=NNc1ncnc2sc3cccs3c2n1
Brc1cc(Br)c2ncnc(Nc3ccc(I)cc3)c2c1
Fine tuning...
Mean value of predictions: 0.38305473
Proportion of valid SMILES: 0.5423569865582994
Sample trajectories:
BP(=O)(NCc1ccccc1)P(=O)(O)O
BP(=O)(OCC)OP(=O)(O)OP(=O)(O)Nc1ccc(F)c(F)c1
BrC1=Nc2scnc2Nc2ccccc21
BrCc1nc2ncnc(Nc3cccc(-c4ccccc4Br)c3)c2s1
Brc1cc(-c2cccs2)c2c(Br)c(Br)cnc2c1-c1ccccc1

 10 Training on 9282 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 22.537999
Reward: 3.240157
Trajectories with max counts:
176	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.5040068
Proportion of valid SMILES: 0.36679174484052535
Sample trajectories:
BP(=O)(CC(F)(F)F)NO
BrBr
Brc1cc(Br)c2ncsc2c1
Brc1cc(Nc2nc3ccccc3s2)sc1Br
Brc1cc(Nc2ncnc3sccc23)sc1Nc1ncnc2sc3ccccc3c12
Policy gradient replay...
Mean value of predictions: 0.3842627
Proportion of valid SMILES: 0.504375
Sample trajectories:
BP(=O)(NC(C1CCC1)P(=O)(O)O)n1cnc2c(N)ncnc21
BP(=O)(NCc1ccc(F)cc1)OP(=O)(Nc1cc(Br)cc(Br)c1)Oc1ccccc1Br
BP(=O)(OCC)Oc1nc2c(Br)cc(Br)cc2s1
BP(=O)(OCC)c1nc(-c2ccccc2)c2nc(-c3ccccc3)c(-c3ccccc3)c(Br)c(Br)c2[nH]1
BP(=O)(OCCCC)Oc1ccc(Nc2ncnc3sc4scnc4c3s2)cc1
Fine tuning...
Mean value of predictions: 0.38331428
Proportion of valid SMILES: 0.546875
Sample trajectories:
BP(=O)(OCC(=O)Nc1ccc(F)cc1F)c1cccc(F)c1
BP(=O)(OCC)C(=O)NNc1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CCCCOP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)CC(F)(F)F
B[PH](=O)(Br)(Br)C(Br)=C(Br)Br
Bc1ccc(Nc2ncnc3ccsc23)cc1

 11 Training on 10634 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.040359
Reward: 3.436797
Trajectories with max counts:
323	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4588517
Proportion of valid SMILES: 0.391875
Sample trajectories:
BP(=O)(CCC=C(Br)Br)OCC
BP(=O)(NCc1ccc(Br)cc1)[PH](=O)(Nc1cncs1)(Oc1ccccc1Nc1ccc(N)cc1)C(F)(F)F
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1O
Bc1cccc(Br)c1Nc1ncnc2ccccc12
Bc1cccc(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.440882
Proportion of valid SMILES: 0.524867062871442
Sample trajectories:
BP(=O)(COP(=O)(O)O)N(CCN)P(=O)(O)O
BP(=O)(OCBr)OC(=O)Cc1cnc(Br)[nH]1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Br
Fine tuning...
Mean value of predictions: 0.41147733
Proportion of valid SMILES: 0.5501719287277275
Sample trajectories:
BP1(=O)OCC2OC(NC(=O)OCC(COc3c(Br)cc(Br)cc3Br)O1)C(O)(n1cnc3c(Br)cc(Br)cc31)C(O)C2O
B[PH](=O)(=NO)Nc1ccc(Br)cc1
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc2c(N=Cc3ccccc3Br)ncnc2s1

 12 Training on 12073 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.384958
Reward: 3.817497
Trajectories with max counts:
291	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.49982083
Proportion of valid SMILES: 0.3488590184432635
Sample trajectories:
BP(=O)(CCC=CBr)OCC
BP(=O)(Cl)OP(=O)(O)OP(=O)(O)OP(O)(F)(F)Br
BP(=O)(N(O)C=O)n1cnc2c(N)ncnc21
BP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC)OC(=O)Nc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.47692308
Proportion of valid SMILES: 0.463849765258216
Sample trajectories:
BP(=O)(OCC1OC(O)C(O)C1O)OP(=O)(O)OP(=O)(O)Oc1ccc(Nc2ncc(Br)c(Br)c2Br)nc1
Bc1ccc(Nc2ncnc3c2c(Br)sc3c2nc3ccc(Br)cc3s2)cc1
Bc1ccc(Nc2ncnc3sc(Nc4ccccc4)cc23)cc1
BrC1=Nc2ncnc(Br)[n+]2s1
BrCc1nc2c(Nc3nc(-c4ccc(Br)cc4)c(Br)cc3Br)ncnc2s1
Fine tuning...
Mean value of predictions: 0.4195923
Proportion of valid SMILES: 0.5520475148483901
Sample trajectories:
BP(=O)(CC(=O)ON=C(Nc1ccc(Br)nc1)Oc1ccccc1)NO
Bc1ccccc1Nc1ncnc2sc(Nc3ccc(Br)c(Br)c3)nc12
BrC(=NNc1ncnc(Nc2ccc(Br)cc2)n1)c1ncnc2sc(Br)cc12
BrCCI
BrCc1ccccc1-c1nc(N2CCOCC2)c2ncnc(Nc3ccccc3)c2n1

 13 Training on 13568 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.813995
Reward: 3.502899
Trajectories with max counts:
66	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4378855
Proportion of valid SMILES: 0.567677399187246
Sample trajectories:
BP(=O)(C(=O)O)N1CCC(=O)Nc2sc(C(=O)Oc3ccccc3)cc21
Bc1cccc(Nc2ncnc3ccsc23)c1
BrCCNc1ccc(Nc2ncnc3ncnc(Nc4ccc(Br)cc4)c23)cc1
BrCc1ccccc1-c1ccccc1Nc1ncnc2sc(Nc3ccccc3Br)nc12
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2n1
Policy gradient replay...
Mean value of predictions: 0.40839326
Proportion of valid SMILES: 0.5214129415442326
Sample trajectories:
BP(=O)(NC(c1ccccc1)c1ccccc1)c1ccccc1
BP(=O)(OCCS)C(=O)N1CCC(Br)(CBr)CC1
BrC(Br)=Nc1ccccc1Nc1ncnc2nc(Br)ccc12
Brc1cc(Br)c2c(c1)Oc1cc(Br)sc1-2
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Fine tuning...
Mean value of predictions: 0.39943945
Proportion of valid SMILES: 0.5575
Sample trajectories:
Brc1cc(Br)c2sc3ncnc(Nc4ccccc4Oc4ccccc4Br)c3c2c1
Brc1cc(Nc2ccsc2)ccc1I
Brc1cc(Nc2ncnc3ccccc23)nc2ccccc12
Brc1cc2c(Nc3cccc(-c4ccccc4Br)c3)ncnc2s1
Brc1cc2c(Nc3cccc(I)c3)ncnc2s1

 14 Training on 15102 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.593015
Reward: 3.773487
Trajectories with max counts:
317	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.5105079
Proportion of valid SMILES: 0.356875
Sample trajectories:
BP(=O)(O)Oc1ccccc1Nc1ncnc2ccccc12
BP(=O)(O)c1ccccc1Nc1ncnc2c(Br)c(Br)ccc12
BP(=O)(OC)OC(=O)CBr
BP(=O)(OCC)C1(Cc2ccc(Br)cc2)OC(=O)C(C)(C)Oc2c1ccc(Br)c2Br
BP(=O)(OCC)OCC1NC(O)=CC(=N)O1
Policy gradient replay...
Mean value of predictions: 0.27874362
Proportion of valid SMILES: 0.4278125
Sample trajectories:
BP(=O)(CCOCCOCCC(=O)N1CCCN(Cc2ccc(Br)s2)O1)OCC
Bc1ccccc1-c1ccccc1Nc1ncnc2ccccc12
Bc1ccccc1N1cc2c3cccnc3Nc3ccccc3Nc3ccccc3N=CC=C2N(c2ccccc2)CC1
BrC=CC=CC=CCNc1ncnc2sc3ccccc3c12
BrCCOc1ccccc1Nc1ncnc2ccccc12
Fine tuning...
Mean value of predictions: 0.4305263
Proportion of valid SMILES: 0.534375
Sample trajectories:
BP(=O)(OCC)c1cc(Br)cc(Br)c1Br
BrC1=C(c2ccccc2)c2nc(-c3ccccc3)sc2-c2cc(Br)ccc2N=C1CN1CCOCC1
BrCCBr
BrCCCCCCCCBr
Brc1c(Br)n(-c2ccccc2)c2ccc(Nc3ncnc4ccccc34)cc12

 15 Training on 16316 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.354739
Reward: 3.694452
Trajectories with max counts:
167	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4548632
Proportion of valid SMILES: 0.41125
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCC=CCC=C)OCC
BP(=O)(OCC)OCCCCCC
BP(=O)(Oc1cc(Br)ccc1Br)OP(=O)(O)O
Bc1ccc(Nc2ccsc2)cc1Cl
BrBr
Policy gradient replay...
Mean value of predictions: 0.4954717
Proportion of valid SMILES: 0.49703032197561736
Sample trajectories:
BP(=O)(CCC(=O)Oc1ccccc1Nc1ncnc2sccc12)COCC
BP(=O)(CCC(=O)c1cc(Br)c(OP(=O)(O)O)c(Br)c1)OCC
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC)OC1CCC[P+](=O)([O-])Oc2ccc(Br)cc21
BP(=O)(OCC)Oc1ccc(Br)cc1S(=O)(=O)Nc1nccs1
Fine tuning...
Mean value of predictions: 0.45218405
Proportion of valid SMILES: 0.5367302281963113
Sample trajectories:
BP(=O)(N(O)C(F)F)P(=O)(Oc1ccccc1)Oc1ccc(Br)c(Br)c1
BP(=O)(OCC)OC(=O)Nc1ccc(Br)c(Cl)c1
BP(=O)(OCC)c1ccc(Br)cc1
BrC=Cc1ccccc1Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1Br
Brc1cc(Br)c2c(Br)n(-c3ccccc3Nc3ncnc4sccc34)c(-c3cccs3)c2c1

 16 Training on 17898 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.409496
Reward: 4.006271
Trajectories with max counts:
406	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5596014
Proportion of valid SMILES: 0.34510784620193813
Sample trajectories:
BP(=O)(NC1CCCC(C(=O)N2CCCCC2)C1)OS(=O)(=O)O
BP(=O)(Nc1cc(Br)c(Br)cc1Br)C(P(=O)(O)O)P(=O)(O)P(=O)(O)O
BP(=O)(Nc1cc(Br)cc(Nc2ccc(Br)c(Br)c2)c1)C(F)(F)P(=O)(O)O
BP(=O)(Nc1ccc(Br)c(Br)c1)c1ccc(Br)c(Br)c1
BP(=O)(OCC)c1ccc(Nc2ncnc3c(Br)c(Br)c(Br)c(Br)c(Br)c(Br)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.4288961
Proportion of valid SMILES: 0.5775
Sample trajectories:
Bc1ccccc1Nc1ncnc2cc(Br)ccc12
BrCCNc1ccc2ncnc(Nc3ccccc3-c3ccccc3Br)c2n1
BrCc1nc2c(-c3ccccc3Br)ncnc2s1
BrCc1sc2ncnc(Nc3ccc(Br)cc3)c2c1Br
BrSc1ccccc1-c1ccccc1Nc1ccccc1
Fine tuning...
Mean value of predictions: 0.43252948
Proportion of valid SMILES: 0.5571875
Sample trajectories:
BP(=O)(CCCO)c1cc2ncnc(Nc3ccc(Br)cc3F)c2s1
BP(=O)(OCCCC)Oc1ccc(Nc2ncnc3ccccc23)cc1
Bc1ccc(Nc2nc(Nc3cc4ccccc34)nc(-c3cccs3)c2-c2cncnc2)cc1
Bc1ccccc1Nc1ncnc2sc(Nc3ccccc3Br)nc12
BrC(=Nc1ccsc1)c1ccccc1Nc1cccc2c(Nc3ccccc3Br)ncnc12

 17 Training on 19549 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.230745
Reward: 3.590635
Trajectories with max counts:
40	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49202454
Proportion of valid SMILES: 0.5095342294467021
Sample trajectories:
BP(=O)(CCCCCl)NO
BP(=O)(Nc1ccc(Nc2ncnc3ccccc23)cc1)c1ccc(Br)cc1
BP(=O)(OCC)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCC1CCCN1C(=O)OP(=O)(O)CCC1CC1)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)OP(=O)(O)OP(=O)(OCC)OCCCCCC
Policy gradient replay...
Mean value of predictions: 0.49682155
Proportion of valid SMILES: 0.5114098155673648
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Br)cc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)c1cccc(Br)c1
Bc1cnc(Nc2cc3ccc(Br)cc3nc3c(Br)c(Br)nc23)[nH]1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccsc12
Fine tuning...
Mean value of predictions: 0.49479166
Proportion of valid SMILES: 0.5403377110694184
Sample trajectories:
BP(=O)(C=COc1ccc(Br)cc1)OC
BP(=O)(CCC=CC(C)=O)OCCC
BP(=O)(NOP(=O)(O)OP(=O)(O)O)P(=O)(O)OP(=O)(O)Oc1ccccc1Br
Bc1cccc(Nc2ncnc3ccsc23)c1
BrC=CBr

 18 Training on 21412 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.229218
Reward: 4.113505
Trajectories with max counts:
115	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.54767823
Proportion of valid SMILES: 0.4778125
Sample trajectories:
BIc1ccccc1Nc1ncnc2nc(-c3ccccc3)sc12
BP(=O)(OCC1CCCCC1)Oc1ccccc1Nc1ncnc(Nc2ccc(Br)c(Br)c2O)n1
Bc1ccccc1-c1cc(Nc2cccc(Br)c2)ncn1
Bc1ccccc1Nc1ncnc2sc(Br)cc12
Bc1ccccc1Nc1ncnc2sccc12
Policy gradient replay...
Mean value of predictions: 0.49370462
Proportion of valid SMILES: 0.51625
Sample trajectories:
Bc1ccc2c(Nc3ccccc3)ccnc2c1
Bc1ccccc1-c1ccccc1-c1ncnc2sc3ccccc3c12
Bc1ccccc1-c1ccccc1Nc1ncnc2sc(Nc3ccc(Br)s3)cc12
Bc1ccccc1Nc1ncnc2sccc12
BrCC=CCC=CC=NNc1ncnc2sc(Br)cc12
Fine tuning...
Mean value of predictions: 0.48607877
Proportion of valid SMILES: 0.5636136292591435
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)OCC1OC(c2cnc(Br)cc2Br)C(O)(CC)O1
Brc1c(-c2ccc(Br)c3c(Nc4ccccc4)ncnc23)ccc2c1CCC2
Brc1cc(Br)c2sc3ncnc(Nc4ccccc4Br)c3c2c1
Brc1cc(CN2CCCC2)c2c(Nc3ccc(Nc4ccccc4Br)cc3)ncnc2c1
Brc1cc(Nc2ncnc3sc(-c4ccsc4)cc23)cc2ccccc12

 19 Training on 23340 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.129191
Reward: 4.499421
Trajectories with max counts:
326	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.6182163
Proportion of valid SMILES: 0.329375
Sample trajectories:
BP(=O)(NCc1ccccc1)c1ccc(Nc2ccc(Br)cc2)s1
BP(=O)(Nc1ccc(Br)cc1Br)OCC=C
BP(=O)(Nc1nc(N)cs1)Nc1ccc(Br)cc1F
BP(=O)(OCC)OC(=O)C(Br)Br
BP(=O)(OCC)OC(=O)CN(c1ccccc1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.44122905
Proportion of valid SMILES: 0.559375
Sample trajectories:
BP(=O)(CCCl)NO
BP(=O)(Nc1ccccc1)c1ccc(Br)cc1
Bc1ccccc1Nc1ncnc2sc(Nc3cccc(Br)c3)nc12
BrC12Cc3ccccc3C1CNc1ccncc12
BrCc1nc2c(-c3ccccc3Br)ncnc2s1
Fine tuning...
Mean value of predictions: 0.5047934
Proportion of valid SMILES: 0.5671875
Sample trajectories:
BP(=O)(Nc1cc(Br)cc(Br)c1Br)c1nc2c(Br)c(Br)c(Br)c(Br)c2s1
B[PH](=O)(Cl)(Cl)OCCCl
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1

 20 Training on 25129 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.623426
Reward: 4.858248
Trajectories with max counts:
274	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.5507274
Proportion of valid SMILES: 0.3221875
Sample trajectories:
BP(=O)(NCCCCCCl)c1ccccc1Nc1cccc(Nc2ncnc3sccc23)c1
BP(=O)(c1ccccc1)N1CCC(F)(F)C1
BP(=O)(c1ccccc1Nc1ccccc1)N1CCCCC1
BP1(=O)OCC(O)C(Oc2ccccc2)N1Cc1cncnc1Cl
B[PH](=O)(OCC)=C(Br)Br
Policy gradient replay...
Mean value of predictions: 0.4937616
Proportion of valid SMILES: 0.5072055137844611
Sample trajectories:
BP(=O)(OCC)Oc1cccc(Br)c1Nc1c2c(F)c(Br)c(Br)cc2nc2c(F)c(F)c(F)c(F)c12
BP(=O)(c1cc(Br)cc(Nc2cncnc2)c1)S(=O)(=O)Nc1cc(F)cc(Cl)c1
BP1(=O)OCC2OC(=O)OCC2c2cc(N)c(F)c(F)c21
Bc1ccc(Nc2cc(-c3ccc4ccc(Br)cc4n3)nc(Br)c2F)cc1
BrCCC=CC=CC=CC=CC=C=NNc1ncnc2sccc12
Fine tuning...
Mean value of predictions: 0.51342136
Proportion of valid SMILES: 0.5684803001876173
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCl
Bc1ccccc1Nc1ncnc2sccc12
BrC(=Nc1ccc(Br)cc1)c1ccc(Nc2ncnc3sc(Br)cc23)cc1
BrC=CC(Br)Br
Brc1cc(-c2cccs2)sc1-c1ccsc1-c1cncc(Nc2ccc3ncnc(Br)c3c2)c1

Trajectories with max counts:
231	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.46509284
Proportion of valid SMILES: 0.4713383759454898
Mean Internal Similarity: 0.4655441047686505
Std Internal Similarity: 0.10073750341490033
Mean External Similarity: 0.4167015223187768
Std External Similarity: 0.06696205085652614
Mean MolWt: 414.1099825878116
Std MolWt: 93.67718164568349
Effect MolWt: -0.8567850736331483
Mean MolLogP: 5.700259867907537
Std MolLogP: 1.60665689782518
Effect MolLogP: 0.6390134862536615
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.936371% (1139 / 1163)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5550.56104516983, 'valid_fraction': 0.4713383759454898, 'active_fraction': 0.44177718832891244, 'max_counts': 231, 'mean_internal_similarity': 0.4655441047686505, 'std_internal_similarity': 0.10073750341490033, 'mean_external_similarity': 0.4167015223187768, 'std_external_similarity': 0.06696205085652614, 'mean_MolWt': 414.1099825878116, 'std_MolWt': 93.67718164568349, 'effect_MolWt': -0.8567850736331483, 'mean_MolLogP': 5.700259867907537, 'std_MolLogP': 1.60665689782518, 'effect_MolLogP': 0.6390134862536615, 'generated_scaffolds': 1163, 'novel_scaffolds': 1139, 'novel_fraction': 0.9793637145313844, 'save_path': '../logs/replay_combo_s2-5.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.698529
Reward: 1.000000
Mean value of predictions: 0.0011881188
Proportion of valid SMILES: 0.7910401002506265
Sample trajectories:
BP(=O)(OC(C)C)C1CCCC=CC1OC(=O)CCC(N)=O
Brc1cc(Br)c2c(c1Br)C1CC=CC(CO2)O1
Brc1ccc(-c2ccc3oc(N4CCCC4)nc3n2)cc1
Brc1ccco1
C#CCC#Cc1ccn(C(=O)N2CCN(c3ccccc3)CC2)c1
Policy gradient replay...
Mean value of predictions: 0.0028673834
Proportion of valid SMILES: 0.7846875
Sample trajectories:
Brc1ccc2c(c1)Sc1ccccc1O2
Brc1ccc2ccccc2c1
Brc1ccccc1
Brc1ccccc1-c1cc2ccccc2cc1-c1ccccc1
Brc1ccccc1-c1cc2ccccc2nc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.034373228
Proportion of valid SMILES: 0.5556255909234163
Sample trajectories:
Brc1ccc(CNc2ncnc3ccc(Nc4ccc(Br)cc4)ncnc23)cc1
Brc1ccc(Nc2cnc3c(Nc4cccc(Br)c4)ccnc3c2)cc1
Brc1ccc(Nc2nc(Nc3ccccc3)sc2-c2ncnc3[nH]ccc23)c(Br)c1
Brc1ccc2c(Nc3ccccc3Br)ncnc2c1
Brc1ccc2c(c1)C(Cc1ccccc1)C(N(N=Cc1cccnc1)c1ccccc1Br)CO2

  2 Training on 322 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.129485
Reward: 1.046309
Trajectories with max counts:
2	COc1cc2ncnc(Nc3ccc(Cl)cc3)c2cc1OC
2	Cc1ccccc1N(=O)=O
Mean value of predictions: 0.03198708
Proportion of valid SMILES: 0.5835952231301068
Sample trajectories:
Brc1ccc(Nc2ccnc(Nc3cccnc3)c2)cc1
Brc1ccc(Nc2ncnc3cccnc23)cc1
Brc1cccc(Nc2ncnc3cc(Br)c(N4CCCCC4)cc23)c1
Brc1cncc(Nc2ccc3ccccc3c2)n1
C#CCN(CCC=C)N=Nc1cc2ncnc(Nc3ccc(Cl)nc3)c2s1
Policy gradient replay...
Mean value of predictions: 0.025136612
Proportion of valid SMILES: 0.40939597315436244
Sample trajectories:
Brc1cc(Nc2csnn2)nc(Nc2ncc(Br)s2)c1
Brc1cc2nc(no2)-c2cnc3cnnn3c2nc1Nc1nccnc1N1CCOCC1
Brc1ccc(CN2CCC(CNc3ncnc4c3ncn4C3CC3)CC2)c(Br)c1
Brc1ccc(Nc2nncn2-c2ccsc2Br)nc1
Brc1ccc(Nc2nnn[nH]2)cc1
Fine tuning...
Mean value of predictions: 0.09763594
Proportion of valid SMILES: 0.5322428436615287
Sample trajectories:
BrC=CBr
Brc1ccc(Nc2ncnc3ccc(-c4ccccc4)cc23)cc1Br
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3oc(-c4ccc5[nH]ncc5c4)nc23)cc1

  3 Training on 696 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 24.212073
Reward: 1.507320
Trajectories with max counts:
8	Clc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
Mean value of predictions: 0.12630346
Proportion of valid SMILES: 0.534942024443748
Sample trajectories:
Brc1cc2nc3cccnc3[nH]c2nc2cccnc12
Brc1ccc(Cn2cnc(NCCSc3nnc(-c4cccs4)n3-c3ccccc3)n2)cc1
Brc1ccc(Nc2nc(-c3nnn[nH]3)nc3ccccc23)cc1Br
Brc1ccc(Nc2nc(Nc3ccc(OC4CC4)cc3)nc3ncnc(Nc4cccc(Br)n4)nc3n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.12088889
Proportion of valid SMILES: 0.5646173149309912
Sample trajectories:
Brc1cc2c(Nc3ncnc4ccccc34)ncnc2cn1
Brc1ccc(-c2ccc(Nc3nc4ccccc4s3)cc2)cc1
Brc1ccc(CNc2cc3[nH]c(-c4ccccc4)nc3cc2Br)cc1
Brc1ccc(Nc2ccnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2nc(Nc3nnc(C4=CC=CC=CC4)s3)cc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.17023934
Proportion of valid SMILES: 0.6013767209011264
Sample trajectories:
BP(=O)(OCC)OCCS(=O)(=O)c1cnc(NC(C)(C)CBr)c2ccccc12
Brc1cc(Br)cc(-c2ccc(Br)c(Br)c2)c1
Brc1cc(NC=CC2Nc3ccccc3S2)cc2c(Nc3ccccc3)ncnc12
Brc1cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)sc1Br
Brc1cc2c(s1)-c1ccccc1O2

  4 Training on 1725 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 22.895167
Reward: 1.661728
Trajectories with max counts:
12	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.18468606
Proportion of valid SMILES: 0.6127619643415703
Sample trajectories:
Brc1c[nH]cn1
Brc1ccc(Nc2nc(-c3ccccc3)c3nc(Br)ccc3n2)cc1
Brc1ccc(Nc2ncnc3c(Nc4ccccc4Br)ncnc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Nc4ncnc5ccccc45)cc23)s1
Policy gradient replay...
Mean value of predictions: 0.2545761
Proportion of valid SMILES: 0.5588327580796988
Sample trajectories:
BrCc1nn2cc(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Br)c(CSc2ncnc3ncnc(c2Br)Nc2ccc(Br)cc2s3)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(I)c2nc3ccccc3c12
Brc1ccc(Nc2n[nH]c3ncnc(Nc4cccc(Br)c4)c23)cc1
Fine tuning...
Mean value of predictions: 0.24094093
Proportion of valid SMILES: 0.6245701781806815
Sample trajectories:
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c2c(Nc3cc(Br)c4c(Nc5ccccc5)ncnc4n3)ncnc2c1
Brc1cc2ncnc(Nc3cncnc3)c2cc1Br
Brc1ccc(CNc2ncnc3cnccc23)cc1
Brc1ccc(Nc2ccc3[nH]c(-c4cncnc4-c4ccccc4Br)nc3c2)cc1

  5 Training on 3258 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 23.656112
Reward: 1.762929
Trajectories with max counts:
31	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.2457971
Proportion of valid SMILES: 0.647077211628634
Sample trajectories:
Brc1ccc(NN=Cc2cccc(Br)c2)cc1
Brc1ccc(NN=Cc2ccccc2Br)cc1
Brc1ccc(Nc2nc3ccc(Br)cc3s2)cc1
Brc1ccc(Nc2nccnc2-c2ccccc2Br)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.27199617
Proportion of valid SMILES: 0.6536295369211514
Sample trajectories:
BP(=O)(CCCCCC)NCCCCCC(=O)O
BP(=O)(O)CP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
Brc1cc2n(nc3ccccc13)-c1ccccc1N2c1cccc2c(Nc3ccccc3)ncnc12
Brc1ccc(Br)c(Nc2ncnc3ccccc23)c1
Brc1ccc(Nc2cc(Nc3ccnc4ccccc34)ccn2)cc1
Fine tuning...
Mean value of predictions: 0.30492958
Proportion of valid SMILES: 0.62125
Sample trajectories:
BP(=O)([N-][N+]#N)OCCC
BrC(=NNc1ncncn1)Nc1cccc2ccccc12
Brc1cc2c(N3CCOCC3)ccnc2cc1-c1ccoc1
Brc1ccc(Br)c(Nc2ncnc3scnc23)c1
Brc1ccc(Nc2cc(Br)cc(Br)c2)cc1

  6 Training on 5092 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 27.934109
Reward: 2.932027
Trajectories with max counts:
127	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.42820683
Proportion of valid SMILES: 0.46589486858573215
Sample trajectories:
BrC=CBr
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(Nc2nc3ccccc3nc2Nc2ccc(Br)cc2Br)cc1
Brc1ccc(Nc2nc3sc(Br)cc3[nH]2)cc1
Brc1ccc(Nc2ncnc3c(Br)c(Br)ccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.32541704
Proportion of valid SMILES: 0.636875
Sample trajectories:
BrC=Cc1ccccc1Br
Brc1ccc(C=Nc2ncnc3cscc23)c(Nc2cccc(Br)n2)c1
Brc1ccc(CNc2cnc3ccccc3n2)cc1
Brc1ccc(N2CCN(CCCCNc3ncnc4cnccc34)CC2)cc1
Brc1ccc(Nc2ccnc(Nc3ccc(Br)cc3)n2)cc1
Fine tuning...
Mean value of predictions: 0.353297
Proportion of valid SMILES: 0.6309039724741946
Sample trajectories:
BP(=O)(Nc1ccc(Br)c(Nc2nc(I)nc(Nc3ccc(Br)cc3)c2F)c1)C(=O)N1CCC(F)(F)CC1
BP(=O)(OCC)OC(=O)C(C)S(=O)(=O)Nc1ccc(F)cc1
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1Cl
BrC1Cc2ccccc2CN1c1ccnc(Nc2ccc3[nH]ncc3c2)c1
BrCCOc1cccc(Nc2ccccc2-c2ccccc2Br)c1Br

  7 Training on 7188 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 27.787303
Reward: 3.466912
Trajectories with max counts:
324	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.38136092
Proportion of valid SMILES: 0.4225
Sample trajectories:
B[PH](=O)(=Nc1ccc(Nc2ncnc3cccc(Br)c23)cc1)OCC
Brc1cc(I)ccc1Nc1ncnc2ccc(Nc3ccccc3)nc12
Brc1ccc(-c2ncnc3ccccc23)c2ccccc12
Brc1ccc(NN=Cc2ccc(Nc3ccccc3)cc2)cc1
Brc1ccc(Nc2nc(Nc3nc4ccccc4s3)nc(-c3ccc(Br)cc3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.34721944
Proportion of valid SMILES: 0.6183182244451391
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCCCC(=O)Oc1ccc(Br)cc1Br
BP(=O)(SCCS)C(P(=O)(O)O)P(=O)(O)OP(=O)(O)OP(=O)(O)N(O)P(=O)(O)CCC(F)F
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.38006043
Proportion of valid SMILES: 0.620625
Sample trajectories:
BP(=O)(Oc1ccc(Br)c(Nc2cc(Br)cc(Br)c2)c1)OC(C)Br
Bc1ccc2[nH]cc(-c3csc(N)n3)c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)[nH]3)ncnc2c1
Brc1ccc(Br)c(-c2ncnc3ccccc23)c1
Brc1ccc(Br)c(N=Nc2ccc(Nc3ncnc4ccsc34)cc2)c1

  8 Training on 9042 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 29.777351
Reward: 3.659671
Trajectories with max counts:
95	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.38034487
Proportion of valid SMILES: 0.54375
Sample trajectories:
Brc1ccc(-c2csc3ccccc23)c2ccccc12
Brc1ccc(Br)c(Br)c1
Brc1ccc(C=C(Sc2ccccc2)c2ccccc2)cc1
Brc1ccc(N(c2ncncc2Br)c2ncnc3ncncc23)cc1
Brc1ccc(Nc2ccccc2Br)cc1Br
Policy gradient replay...
Mean value of predictions: 0.38651794
Proportion of valid SMILES: 0.644576430134417
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc2cccc(Br)c2cc1Nc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(-c2sc(Nc3ncnc4cc(Br)ccc34)cc2Br)cc1
Brc1ccc(CSc2ncccn2)cc1
Brc1ccc(Nc2ccc3cc(Br)ccc3n2)cc1
Fine tuning...
Mean value of predictions: 0.40976077
Proportion of valid SMILES: 0.6533291653641763
Sample trajectories:
BP(=O)(NCC(=O)O)C(=O)N(Cc1ccccc1-c1ccccc1)P(=O)(O)O
BP(=O)(OCC)OC(=O)CSc1nc2cc(Br)c(Br)cc2s1
BrCC1CCN(Cc2nc3ccccc3nc2-c2cc(-c3ccccc3)ccc2Br)CC1
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(Nc2ncnc3ccccc23)c2ccccc2n1

  9 Training on 10741 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 29.350791
Reward: 3.514452
Trajectories with max counts:
83	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5122603
Proportion of valid SMILES: 0.5389915440025055
Sample trajectories:
BP(=O)(C(=O)NCC1CCCC1)N1CCC(F)(F)CC1
BP(=O)(OCC)C(=O)NC(CO)CNC(=O)C(=O)Nc1cc(Cl)c(N)cc1-c1cc2c(Nc3ccc(F)cc3)ccnc2s1
Brc1cc(Br)c(Nc2ncnc3[n+]2CCc2cncnc2N3)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3cc(Br)c(Br)c(Br)c3)c2c1
Policy gradient replay...
Mean value of predictions: 0.45748267
Proportion of valid SMILES: 0.630625
Sample trajectories:
BP(=O)(C=C(Nc1ccc(Br)cc1)S(=O)(=O)N1CC=CCC1)OCC
BP(=O)(NCCCCCCO)P(=O)(O)N(C)O
BP(=O)(NCc1ccc(Nc2nc(Br)c(F)c(Br)c2F)cc1)C(F)(F)F
Bc1ccc2c(Nc3ccc(Br)cc3)ncnc2c1Br
BrCCc1cnc(Nc2c(Br)ncc(Br)c2Br)nc1
Fine tuning...
Mean value of predictions: 0.45250738
Proportion of valid SMILES: 0.635625
Sample trajectories:
BP(=O)(CP(=O)(O)O)OCCO
BP(=O)(Nc1ccc(Nc2nc(Br)cc(Br)c2F)cc1)OCc1ccc(F)cc1
Brc1cc(-c2ccc3nsnc3c2)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)cc(Nc2ccc(Br)s2)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1

 10 Training on 12600 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.316470
Reward: 3.894931
Trajectories with max counts:
131	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5290621
Proportion of valid SMILES: 0.4735689709102283
Sample trajectories:
BP(=O)(Nc1ccc(F)cc1)N(CCO)NS(=O)(=O)c1sccc1Br
BrC=CBr
BrCCCOc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cc(-c2ncnc3c(Br)csc23)cnc1Br
Brc1cc(Br)c(Nc2ncnc3ccccc23)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.530765
Proportion of valid SMILES: 0.5683453237410072
Sample trajectories:
BP(=O)(NP(=O)(NO)c1ccc(N2CCOCC2)c(Br)c1)SCCCl
BP(=O)(OCC)C(F)(F)P(=O)(O)O
BP(=O)(OCC)OC(=O)Nc1cccc(Nc2ncnc3sc(Br)cc23)c1
BP(=O)(OCC)OCCCCCCCCNC(=O)C(C)C(CC(=O)NC(NP(=O)(O)OCC)C(Cl)P(=O)(O)CCCl)OCCCC
BP(=O)(OCCS)C(F)(F)F
Fine tuning...
Mean value of predictions: 0.45549738
Proportion of valid SMILES: 0.5970615817442951
Sample trajectories:
BrCCC=CC=CCNc1ccc(Br)cc1Br
Brc1cc(Br)c2c(Nc3cc(Br)c(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc3s2)c1
Brc1cc(Nc2ncnc3cc(Br)ccc23)cc(-c2ccccc2Br)c1
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1Br

 11 Training on 14445 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.841966
Reward: 3.596744
Trajectories with max counts:
53	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.48077327
Proportion of valid SMILES: 0.5984990619136961
Sample trajectories:
BrCN1CCCC1CNc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc2ncnc(Nc3ccc(Nc4cc(Nc5ncnc6sc(NCC7CCCS7)cc56)ccc4Br)cc3)c2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCCCC1
Brc1ccc(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Policy gradient replay...
Mean value of predictions: 0.41515574
Proportion of valid SMILES: 0.6721875
Sample trajectories:
Brc1cc(Br)c2ncnc(Nc3ccc(-c4ccccc4Br)cc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ccc3ncnc(-c4ccccc4Br)c3c2)c(Br)s1
Brc1cc2ncnc(Nc3ccc(-c4ccccc4Br)cc3)c2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Fine tuning...
Mean value of predictions: 0.4764535
Proportion of valid SMILES: 0.6452016255079712
Sample trajectories:
Brc1cc(Nc2nccc3nccc(-c4nc5ccccc5nc4Nc4ccccc4Br)c2C3)ncn1
Brc1cc2c(Nc3ccccc3Br)cccc2s1
Brc1ccc(-c2ccc(Nc3ncnc4ccsc34)cn2)cc1Br
Brc1ccc(-c2ccc3ncnc(Nc4ccccc4Br)c3c2)cc1
Brc1ccc(NC=Cc2ccc(Br)s2)cc1

 12 Training on 16416 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.809324
Reward: 3.849826
Trajectories with max counts:
118	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.54837644
Proportion of valid SMILES: 0.5679899968740232
Sample trajectories:
BP(=O)(NO)OCC
BP(=O)(NO)P(=O)(O)OCCCCF
BP(=O)(c1cc(Nc2ccc(I)c(F)c2F)nc2c(Nc3ccc(F)c(F)c3F)nc(Cl)nc12)C(F)(F)F
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.35522655
Proportion of valid SMILES: 0.6965625
Sample trajectories:
BP(=O)(Br)OCCCBr
BP(=O)(OP(=O)(O)P(=O)(O)O)N(O)CBr
Bc1ccc(Nc2ccccc2Br)cc1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrC#Cc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1Br
Fine tuning...
Mean value of predictions: 0.4958498
Proportion of valid SMILES: 0.6326977180368866
Sample trajectories:
Brc1cc(Br)cc(Nc2cnc(Nc3ccccc3)cn2)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccsc23)ncn1
Brc1cc2ncnc(Nc3ccc(-c4ccccc4-c4ccccc4Br)cc3)c2cc1Br
Brc1ccc(-c2cscn2)c2ccccc12

 13 Training on 18430 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.084175
Reward: 3.753588
Trajectories with max counts:
46	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5656829
Proportion of valid SMILES: 0.6248827758674586
Sample trajectories:
Bc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)c(Nc2cc3ncnc(Nc4ccc(Br)s4)c3s2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(CN4CCCCC4)c3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.4938283
Proportion of valid SMILES: 0.69875
Sample trajectories:
BP(=O)(Br)CCBr
BP(=O)(OCCOP(=O)(O)OP(=O)(O)O)c1ccc(Br)cc1Nc1c(Cl)cccc1Nc1ncnc2c(F)c(F)c(Br)cc12
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.5389685
Proportion of valid SMILES: 0.6556042579837195
Sample trajectories:
BrCCN=C(NC1=C(c2ccc(Br)c(Br)c2)N1)Nc1ccc(Br)cc1
Brc1cc(Br)c(Nc2ccc3ncnn3c2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Nc4cc(Br)c(Br)c(Br)c4)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3c(Br)ncnc3I)c2c1
Brc1cc(Br)cc(Nc2ncnc(Br)n2)c1

 14 Training on 20902 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.263027
Reward: 4.028103
Trajectories with max counts:
42	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.5641732
Proportion of valid SMILES: 0.6351984995311035
Sample trajectories:
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1
BrCCCCCCCCCCC=C(I)I
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Nc2nc3c(Nc4ccccc4Br)ncnc3s2)cc1Br
Policy gradient replay...
Mean value of predictions: 0.5302638
Proportion of valid SMILES: 0.6521739130434783
Sample trajectories:
BP(=O)(NC(=O)C(Br)Br)Oc1ccc(C(N)=O)c(Nc2cc(Br)cc(Br)c2)c1
Brc1cc(Br)c(-c2ccc(Nc3ncnc4ccsc34)cc2)cc1Br
Brc1cc(Br)c(Nc2cc(Nc3ncnc4ccccc34)cs2)cc1Br
Brc1cc(Br)c(Nc2ncnc3ccsc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.48952112
Proportion of valid SMILES: 0.6594746716697936
Sample trajectories:
BP(=O)(O)CNC(=O)c1cc(Br)nc2sc(Br)c(Br)c12
Brc1c(Br)c(Cc2ccccc2)c2c(c1Br)N=CN(C1CC1)CCO2
Brc1cc(Br)c2ncnc(Nc3ccccc3Nc3ccccc3Nc3ccccc3Br)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccsc23)ncn1

 15 Training on 23341 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 33.133665
Reward: 4.214619
Trajectories with max counts:
56	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.50530976
Proportion of valid SMILES: 0.635625
Sample trajectories:
Bc1ccc(Nc2ncnc3sc4ccccc4c23)cc1
Brc1ccc(Br)c(Nc2ncnc3ccccc23)c1
Brc1ccc(N=Cc2ccccc2)cc1
Brc1ccc(Nc2ccc3c(ncn2)oc2c(-c4ccccc4Br)nccc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Nc4ccccc4)cc23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.57813126
Proportion of valid SMILES: 0.6295369211514393
Sample trajectories:
BP(=O)(C=C(F)F)NO
Bc1ccc(Nc2ncnc3c(-c4cccc(Br)c4)c3n2)cc1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc2ncnc(Nc3cccs3)c2s1
Brc1ccc(Br)c(Nc2cccc(Nc3ncnc4ccc(Br)cc34)c(Br)cc2)c1
Fine tuning...
Mean value of predictions: 0.54454285
Proportion of valid SMILES: 0.635625
Sample trajectories:
BrCCNc1ncnc2cc(Br)ccc12
Brc1cc(-c2ncnc3cc(Br)c(C4CC4)cc23)ccn1
Brc1cc(Nc2ncnc3ccc(N4CCCCC4)nc23)n1
Brc1cc2c(Nc3cc(Br)c(Br)c(I)c3)ncnc2s1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1

 16 Training on 25835 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.523501
Reward: 4.350293
Trajectories with max counts:
101	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6173378
Proportion of valid SMILES: 0.5594493116395495
Sample trajectories:
BP(=O)(CCCCCS(=O)(=O)Nc1ccc(Br)cc1F)NO
BP(=O)(NC(=O)C(F)(F)F)c1ccc(Nc2c(F)ccc(F)c2F)cn1
BP(=O)(NC(=O)OCc1ccc(Br)c(Nc2cc(Br)cc(Br)c2)c1)c1ccc(Nc2ncnc3c(Br)c(Br)c(Br)cc23)cc1
BP(=O)(NO)c1ccc(Br)cc1Br
BP(=O)(NS(=O)(=O)c1ccc(Br)cc1)NS(=O)(=O)c1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.6022605
Proportion of valid SMILES: 0.5811698467313106
Sample trajectories:
BP(=O)(CCl)Nc1ccc(Nc2ncnc3nc(F)c(Cl)cc23)c(F)c1
BP(=O)(NCC(F)F)Nc1ccc(F)cc1Br
BP(=O)(NCc1ccc(Br)cc1)P(=O)(Oc1cc(F)c(F)c(F)c1)c1c(Br)c(Br)c(Br)c(Br)c1Br
BP(=O)(OC)OC(=O)CBr
BP(=O)(OCC)OC(=O)C(CCCCCP(=O)(O)OP(=O)(O)O)P(=O)(O)O
Fine tuning...
Mean value of predictions: 0.5750604
Proportion of valid SMILES: 0.6465625
Sample trajectories:
BP(=O)(CCCl)NP(=O)(O)Oc1ccccc1Cl
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
BrCCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCc1ccccc1Br
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccsc12

 17 Training on 28473 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.568337
Reward: 4.616250
Trajectories with max counts:
51	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6855
Proportion of valid SMILES: 0.6251953735542357
Sample trajectories:
BP(=O)(OCC=C)N(NC(=O)OC(C)(C)N)c1nc2c(Br)c(Br)c(Br)c(Br)c2s1
Brc1cc(-c2ccnc3cc(NCCCN4CCOCC4)cc(Br)c23)cs1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Br)c(Nc2ncnc3ccsc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.49583533
Proportion of valid SMILES: 0.6680212566427008
Sample trajectories:
Bc1cc2c(Nc3ncnc4ccccc34)ccnc2s1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(-c4ccccc4Br)c3)c2c1
Brc1cc(N2CCCC2)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Nc2ncnc3sc(Br)cc23)cs1
Brc1cc(Nc2scnc2-c2ccccc2)c(Br)c2ccccc12
Fine tuning...
Mean value of predictions: 0.5769414
Proportion of valid SMILES: 0.6567584480600751
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(N4CCCCC4)c(Br)c3s2)cc1
BrCCNCCOc1ccccc1Nc1cc2c(Nc3ncc(Br)s3)ncnc2s1
BrCCSc1ccc(Nc2ncnc3ccccc23)cc1
BrCc1cncc2c(Nc3cccc(Br)c3)ncnc12
Brc1cc(-c2ccsc2)c2c(Br)ncnc2n1

 18 Training on 31284 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.837402
Reward: 4.494757
Trajectories with max counts:
124	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6608891
Proportion of valid SMILES: 0.5556597873671044
Sample trajectories:
BP(=O)(CCCC)C(=O)NCC=O
BP(=O)(NO)c1cc(Br)cc(Br)c1Br
BrBr
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2cc1Br
Policy gradient replay...
Mean value of predictions: 0.5364219
Proportion of valid SMILES: 0.6340625
Sample trajectories:
BP(=O)(NCCCCCCCCN)NS(=O)(=O)c1cc2cc(Br)cc(Br)c2s1
Bc1ccc(Nc2ncnc3ccc(Cl)cc23)cc1
Bc1cccc(Nc2ncnc3ccc(Br)c(Br)c23)c1
BrC=NNCCN1CCN(Cc2cc(Br)cc(Br)c2)CC1
BrCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.5846226
Proportion of valid SMILES: 0.6667708658955924
Sample trajectories:
BP(=O)(NCCO)c1c(Br)cc(Nc2ccc(Br)c(Br)c2N)nc1Br
BrCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1cccs1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1

 19 Training on 33954 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 37.985614
Reward: 4.748728
Trajectories with max counts:
75	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6513689
Proportion of valid SMILES: 0.6280087527352297
Sample trajectories:
BP(=O)(OCOc1ccc(Br)cc1)OP(=O)(O)Oc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Br
BrC(=Cc1ccc(Br)cc1)c1cccc(Br)c1
BrCNc1nc2cc(Nc3ccc(Br)cc3)cc(Br)c2s1
Policy gradient replay...
Mean value of predictions: 0.6691662
Proportion of valid SMILES: 0.6334375
Sample trajectories:
BP(=O)(NCCCCCNCCO)C(=O)NO
BP(=O)(NO)c1ccc(Br)cc1Br
BP(=O)(Oc1cccc(Br)c1)N(O)C=O
Bc1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1
BrCc1nc2c(Nc3cccc(Br)c3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.6205896
Proportion of valid SMILES: 0.6680212566427008
Sample trajectories:
BrCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(I)cc23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1

 20 Training on 37168 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 40.482297
Reward: 4.964150
Trajectories with max counts:
18	CS(=O)(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.65362805
Proportion of valid SMILES: 0.7507827175954915
Sample trajectories:
BrCc1cc2ncnc(Nc3ccc(Br)cc3)c2cn1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc2c(Nc3nc4c(Nc5ccc6ccccc6c5)ncnc4cc3I)ccnc2s1
Brc1cc2ncn(-c3ccccc3)c2nc1-c1cccc(Nc2ncnc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.6596843
Proportion of valid SMILES: 0.6540506725054739
Sample trajectories:
BOc1ccc2c(N)ncnc2c1Br
BP(=O)(OCC)C(F)(F)F
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
BrSc1ccc(Nc2ncnc3ccsc23)cc1
Fine tuning...
Mean value of predictions: 0.6611136
Proportion of valid SMILES: 0.690744215134459
Sample trajectories:
BP(=O)(ONC(=O)c1cc(Br)cc(Br)c1)OC(=O)c1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)c(n2cnnc2Nc2ccc(-c3ccccc3Br)nc2)c1
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br
Brc1cc(Br)c(Nc2ncnc3ccsc23)cc1Br
Brc1cc(Br)c2c(Nc3cc(Br)c4ncncc4c3)ncnc2c1

Trajectories with max counts:
103	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6178371
Proportion of valid SMILES: 0.6002126195985241
Mean Internal Similarity: 0.46281546921577105
Std Internal Similarity: 0.08956285805782478
Mean External Similarity: 0.41927972092684246
Std External Similarity: 0.06829744434520457
Mean MolWt: 418.0146467276566
Std MolWt: 99.76005811352795
Effect MolWt: -0.7962168091225502
Mean MolLogP: 4.893168907459538
Std MolLogP: 1.6063310335990024
Effect MolLogP: 0.1168472759936543
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.742170% (1342 / 1373)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 100, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 6188.268120288849, 'valid_fraction': 0.6002126195985241, 'active_fraction': 0.592206709731194, 'max_counts': 103, 'mean_internal_similarity': 0.46281546921577105, 'std_internal_similarity': 0.08956285805782478, 'mean_external_similarity': 0.41927972092684246, 'std_external_similarity': 0.06829744434520457, 'mean_MolWt': 418.0146467276566, 'std_MolWt': 99.76005811352795, 'effect_MolWt': -0.7962168091225502, 'mean_MolLogP': 4.893168907459538, 'std_MolLogP': 1.6063310335990024, 'effect_MolLogP': 0.1168472759936543, 'generated_scaffolds': 1373, 'novel_scaffolds': 1342, 'novel_fraction': 0.9774217042971595, 'save_path': '../logs/replay_combo_s2-6.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.912507
Reward: 1.000000
Mean value of predictions: 0.0012485369
Proportion of valid SMILES: 0.8029448621553885
Sample trajectories:
BP(=O)(NCc1ccc2c(c1)C=CC(=O)O2)P(=O)(O)O
BrCC(ON=C(c1ccccc1)c1ccc(CNc2nc(I)cs2)cc1)N1CCCCC1
Brc1ccc(CNc2ncccn2)nc1
C#CCC#Cc1cc(C(C#CCCCCCC)=CC(=O)OC)ccc1O
C#CCCC(NCc1ccccc1)C(OC(=O)NC(NC1CCCCC1)C(O)COC(=O)Cc1ccc(C(=O)NCCO)nc1)C(=O)NCC1CCCO1
Policy gradient replay...
Mean value of predictions: 0.00015791552
Proportion of valid SMILES: 0.7960402262727844
Sample trajectories:
Brc1ccc(-c2nn3ccccc3c2Br)cc1
Brc1cccc(Nc2nc3ccccc3nc2-c2ccccc2)c1
C#CC1OC(=O)C(CC)c2c(C(C)(C)C)cc(c3ccccc3)nc2SCC(=O)N1C
C#CC1c2ccccc2C2=CC(c3ccccc3)CC(=O)NC1O2
C#CCC(=O)OCP(=O)(NC(C)C(=O)OCC)c1ccc2c(c1)OCO2
Fine tuning...
Mean value of predictions: 0.0009592326
Proportion of valid SMILES: 0.7845719661335842
Sample trajectories:
Brc1ccc(-c2oc3ccc(Br)cc3c2C=C2Cc3ccccc3O2)cc1
Brc1cccc(-c2nccn2CC2CCCN2)n1
Brc1cncc(CN2CCCC2)n1
C#CC1(O)CCC2C3CCc4cc(O)ccc4C3CCC21C
C#CC1C(O)CC2C1(C)CC1(c3ccoc3)CCC(C(=C)C)C21C

  2 Training on 227 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.635497
Reward: 1.000000
Trajectories with max counts:
2	O=C(c1ccccc1)N1CCCC1
Mean value of predictions: 0.0012932675
Proportion of valid SMILES: 0.8236215538847118
Sample trajectories:
Brc1ccc(Oc2ncc(Br)c3ccccc23)cc1
Brc1ccc2oc3[nH]c(-c4ccccc4)nc3c2c1
Brc1ccccc1-c1nc2ncccn2c1-c1nc(Cc2cccnc2)c2ccccc2n1
C#CC(=O)N1CCN(S(=O)(=O)c2ccc(NC(C)=O)cc2C)CC1
C#CC(=O)c1ccc(C=CC2C3=C(CCCCC3)N2c2ccc(C)cc2)o1
Policy gradient replay...
Mean value of predictions: 0.0009984638
Proportion of valid SMILES: 0.8170693442108566
Sample trajectories:
Brc1ccc2nc(ccc2OCc2ccccc2)c1-c1ccc(-c2ccccc2)o1
Brc1ccnc(CNCCc2ccccc2)c1
C#CC(O)c1ccc(OCc2ccccc2)c(COC)c1
C#CC1COC=C(C(=O)N(C)CC#N)C1
C#CCCC1=CC=CCC=CC(C)c2cc(cc3ccccc23)OC(c2ccccc2)OC(=O)C1=O
Fine tuning...
Mean value of predictions: 0.0004571429
Proportion of valid SMILES: 0.8236586131157829
Sample trajectories:
Brc1cccc2c1N=C(c1ccccc1)c1ccccc1O2
Brc1ccccc1CN1CCN(Cc2nc3ccccc3[nH]2)CC1
Brc1cncc(C2CCCN2)c1
C#CC(O)C1CC(OC(CCCCCCCCCCCCCCC)C(O)C=C(C)C=C(C)CCC=C(C)CCC=C(C)CCC2C(=C)C(O)CC2=O)C1
C#CCCOc1cccc(Nc2nccc(C#N)n2)c1

  3 Training on 238 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.061738
Reward: 1.012655
Mean value of predictions: 0.0010387534
Proportion of valid SMILES: 0.7866121935889377
Sample trajectories:
BP(=O)(O)C(C(CCN)c1ccc(Br)cc1)P(=O)(O)O
BrCn1nc2ncnc(NCc3cccnc3)c2n1
Brc1ccccc1N=CN1CCNCC1
C#CC1(O)CC2CC3C(CCc4c[nH]c5ccccc45)C3CCC21C
C#CC1C2=C(C(O)CC1C)N(CC)C(=O)N(CCNC(C)C)C2=O
Policy gradient replay...
Mean value of predictions: 8.176615e-05
Proportion of valid SMILES: 0.7696664568911264
Sample trajectories:
Brc1ccc(-c2cnc3ccoc3c2)cc1
C#CC(C(C)=CC=Cc1ccc(OCCOCCO)c(OC)c1)c1cc(SC(C)=O)ccc1Br
C#CC(C)C1NC(=S)Nc2cccc(c2)N2CCCC2=C1O
C#CC=CC(OC(=O)CNC(C)(C)C)P(=O)(OCC)OCC
C#CCCCN(Cc1ccc(OC)c(OC)c1)c1ccncc1
Fine tuning...
Mean value of predictions: 0.0017857143
Proportion of valid SMILES: 0.7755744412968208
Sample trajectories:
Brc1ccc(C=NNc2nc(N3CCCNC3)nc(c3ccc4c(c3)CCCC4)n2)cc1
Brc1ccc(CN2CCSc3ccccc32)cc1
Brc1ccc(Cn2ccc3c2CCC3)cc1
C
C#CC(CC=NOC(=O)C(=O)O)CCCCC

  4 Training on 250 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.042286
Reward: 1.007316
Mean value of predictions: 0.0
Proportion of valid SMILES: 0.789423984891407
Sample trajectories:
BrC1=C(Oc2cccnc2)c2ccccc2N1c1ccccc1
Brc1c(-c2ccncc2)sc2nc(N3CCN(c4nc5ccccc5[nH]4)CC3)[nH]c12
C#CC(=Cc1ccc(N(=O)=O)c(C)c1)C(NC(=S)NCC1CCCC1)c1cccc(Cl)c1
C#CC(=O)CC(C)(Cc1c(F)c(F)c(F)c(F)c1F)P(C)C1CC1
C#CC(C)C(=C)CC(C)C
Policy gradient replay...
Mean value of predictions: 0.0014527845
Proportion of valid SMILES: 0.779245283018868
Sample trajectories:
Brc1cc2c(cc1OCc1ccccc1)CCNC2
Brc1ccc(-c2cc(-c3cccc(CN4CCCC4)c3)on2)cc1
Brc1ccc(-c2ncnc3c2c(OCC2CCOCC2)nn3Br)o1
Brc1ccc(N2CCC3(CC2)OCOC3COc2ccccc2)nn1
Brc1ccc(OCCc2nnc(CCC3CCNCC3)o2)cc1
Fine tuning...
Mean value of predictions: 0.00093896716
Proportion of valid SMILES: 0.8020081581424537
Sample trajectories:
Brc1cc(Br)cc(-n2cncc2Cc2ccc(Br)s2)c1
Brc1cc(C2(c3ccccc3)CCNCC2)c2[nH]c3ccccc3c(c1)c1c2CCCC1
Brc1ccc(-c2csc3[nH]cnc23)cc1
Brc1ccc2[nH]c(C3=NCCN3)nc2c1
Brc1cccc(Cc2cnc(-c3ccccc3)[nH]2)c1

  5 Training on 258 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.171329
Reward: 1.004351
Mean value of predictions: 0.0014780241
Proportion of valid SMILES: 0.8072213500784929
Sample trajectories:
Brc1ccc(C2=NCCN2)cc1
Brc1ccc2c(c1)CCC2CN1CCN(c2ccccc2)CC1
Brc1cccc2[nH]cc(CCN3CCCC3)c12
C#CC(C)C[N+]1([O-])OCCS1
C#CC(C1=CC2=C(CC1)CC(O)C2=C)C(O)C(=O)OCC
Policy gradient replay...
Mean value of predictions: 0.0012485369
Proportion of valid SMILES: 0.8037002195045468
Sample trajectories:
B[PH](=O)(=CC(=O)OCc1ccccc1)OCCOCCOCCOCn1c2ccc(Br)cc2c2ccc(Br)cc21
Brc1ccc(C#Cc2c[nH]c3ccccc23)s1
Brc1ccc(N2CCN(c3ccc(NC4=NCCN5CCCC4C5)cc3)CC2)cc1
Brc1ccc(Oc2ccc3c(C=C4CCNC4)c(-c4ccncc4)c3n2)cc1
Brc1cccc2ncnc(N3CCCCC3NCCN3CCOCC3)c12
Fine tuning...
Mean value of predictions: 0.0016587677
Proportion of valid SMILES: 0.7929846539304729
Sample trajectories:
BrSC1=Nc2sc3c(c2CCCC1CCn1ccnc1)CCCC3
Brc1ccc(-c2noc(N3CCN(c4ccc(Br)cc4Br)CC3)n2)cc1
Brc1ccc(Oc2ncccc2C2CCCNC2)c(Br)c1
Brc1cccc(C2CC(n3cc(-c4ccccc4)cn3)CN2)c1
Brc1ccccc1-c1nnc(C2CCNCC2)o1

  6 Training on 273 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.460850
Reward: 1.002588
Mean value of predictions: 0.0011862396
Proportion of valid SMILES: 0.7945334590009425
Sample trajectories:
BP(=O)(CCCCC(F)F)NC(=O)C(F)(F)F
Brc1ccc(C2=NC(=NCc3ccsc3)CN2)cc1
Brc1cccc(Nc2ncnc3cc[nH]c23)c1
Brc1ccccc1N1CCN(c2ncccn2)CC1
C#CC=CC(=O)OCC1CC(C#CCNCCCNc2c3c(nc4ccccc24)CCCC3)C2CCC(C)C12
Policy gradient replay...
Mean value of predictions: 0.0034345048
Proportion of valid SMILES: 0.7832342821395057
Sample trajectories:
Brc1ccc(-c2nn3ncccc3c2C2CN(Cc3ccccc3)C2Cc2ccccc2)cc1
Brc1ccc(CNc2nnc(-c3cccnc3)s2)cc1
Brc1ccc(N2CC3CC3(N3CCN(Cc4ccncc4)CC3)O2)nc1
Brc1ccc(OCc2ccccc2)cc1OCc1ccc(Cc2ccccc2)cc1
Brc1ccc2c(NC3CCCCC3)n[nH]c2c1
Fine tuning...
Mean value of predictions: 0.0011933175
Proportion of valid SMILES: 0.7871008140262993
Sample trajectories:
Brc1ccc(-c2ccc3c(c[nH]2)C#CC[n+]2ccccc2N=C3CCc2ccccc2)cc1
Brc1ccc(-c2noc(-c3cccc4ccccc34)n2)cc1
Brc1ccc(COc2ccc(NC=C3CCCCC3)c(Br)c2)c(Br)c1
C#CC#CC(NC(=O)c1ccc(C2OC(OC3=CC(=O)OC(OC4C(CO)OC4C(O)CO)C(O)C(O)C3O)C(O)C(O)C(O)C(O)CC(=O)OCCCCC2=O)c(C)c1)c1cc2ccccc12
C#CCC=C(Nc1ccc(C(=O)Nc2ccc(OC3CCN(Cc4ccc(O)cc4)CC3)cc2)cc1)c1ccc(CNC(=O)NC)cc1

  7 Training on 296 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.988449
Reward: 1.001539
Trajectories with max counts:
2	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
2	O=C1Nc2ccccc2C1=Cc1ccccc1
Mean value of predictions: 0.000929512
Proportion of valid SMILES: 0.8071272272585183
Sample trajectories:
BrCC1(N2CCCC2C#CC2CCCNC2)C=CCCC1
Brc1ccc(Cn2ccc3c(nc4ncccc43)n2)nc1
Brc1cccc(C2C3CCC4CCCC3C2(c2ccccc2)CO4)c1
C#CC1(O)CCC2C3c4ccccc4C3CCC21C
C#CC1C(C(=O)OCC)=C(C)CCC2(C)CCC(COC(C)=O)(c3ccc(Br)cc3)C12
Policy gradient replay...
Mean value of predictions: 0.0012688342
Proportion of valid SMILES: 0.789358372456964
Sample trajectories:
Brc1ccc2[nH]c3ncc(-c4ccccc4)cc3c2c1
Brc1ccc2c(c1)C(C1CCC1)C1(Br)C(O2)C1C1CCCc2ccccc2C1
Brc1cccc(Br)c1
Brc1ccccc1-c1noc(-c2ccc(NCc3ccccc3)nc2)n1
Brc1ccccc1C1=[SH]N(C2CCCCC2)CC1
Fine tuning...
Mean value of predictions: 0.00031397174
Proportion of valid SMILES: 0.7974960876369327
Sample trajectories:
BP(=O)(O)OCC(Br)(Br)Br
Brc1cccc(-c2ccc(-c3ccc4ccc5cnn(Cc6ccccc6)c5c4n3)nc2)c1
Brc1ccccc1-c1ccc(Nc2ncnc3nc(N4CCOCC4)oc23)cc1
C#CC#CCCOC(=O)C1CC(CC#N)C(=Nc2c[nH]c3ccccc23)C2=Nc(c1c1ccccc1)c1ccccc12
C#CC1(CCc2cc(OCCOCCOC)c3c(c2)OCC(=O)NC3C)COCCS1

  8 Training on 309 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.063611
Reward: 1.000915
Trajectories with max counts:
2	NC(=O)c1cccc2c1C(=O)c1ccccc1C2=O
Mean value of predictions: 0.0010534846
Proportion of valid SMILES: 0.7731829573934837
Sample trajectories:
Brc1ccc(-c2nc3cccnc3n3cccc23)cc1
Brc1ccc(C=C2N=C(OCc3ccccc3)C2OCc2ccccc2)cc1
Brc1cccs1
C#CC#CCC1C(N)CC(=O)N2CC1CC2C(=O)O
C#CC(C)(C)C(C)(C#N)C(=O)Nc1cc(C(C)C)c(Cl)n1C
Policy gradient replay...
Mean value of predictions: 0.0021496816
Proportion of valid SMILES: 0.7867209520826809
Sample trajectories:
BP(=O)(NO)C(CCCCN)c1ccc(O)cc1
Brc1ccc(OCc2ccc(C3=NCCN3)o2)cc1
Brc1ccc2c(-c3ccccc3)n[nH]c2c1
Brc1ccc2cc(-c3ccnc(-c4ncnc5cccnc45)n3)oc2c1
Brc1cncc(Nc2ccnc3cccc(Nc4ccnc5ccccc45)c23)c1
Fine tuning...
Mean value of predictions: 0.0017357002
Proportion of valid SMILES: 0.7934272300469484
Sample trajectories:
Brc1ccc(C2=CSC3C2CCN3CCc2ccccc2)cc1
Brc1ccc(OCCCCc2ccccc2)cn1
Brc1ccc2c(c1)CCN(CCc1c[nH]c3ccccc13)CC2
Brc1ccc2c(c1)[nH]c1c(Br)cc(Br)cc12
Brc1cccc(CNc2nc3ccccc3nc2-c2ccc(-c3ccccc3)cc2)c1

  9 Training on 331 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.157619
Reward: 1.000544
Trajectories with max counts:
2	O=C(O)CCN1C(=O)c2ccccc2C1=O
Mean value of predictions: 0.0012908431
Proportion of valid SMILES: 0.7761427676894177
Sample trajectories:
Brc1cccc(C2=NOC(C3CC3)C2)c1
Brc1ccccc1C=CCCN1CCCC1
Brc1ccccc1CN1CCN2CC(C1)C(c1ccccc1)C2
C#CCCC(=O)N1CCN(Cc2ccc(-c3cccnc3)c(CC)n2)CC1
C#CCCCC
Policy gradient replay...
Mean value of predictions: 0.0014634146
Proportion of valid SMILES: 0.7711598746081505
Sample trajectories:
BP1(=O)CCC2(C)CCC(COC(=O)c3ccc(Br)cc3)(C(=O)C(F)(F)F)C(C2)N1
BrC1=C(NCCc2ccccc2)CCC=C1
Brc1ccc(-c2nc3ccccc3s2)s1
Brc1ccc(C2=CC3C(C2)CC3N2CCNC2)cc1
Brc1ccc(N=Nc2nc3ncccc3c3ncccc23)cc1
Fine tuning...
Mean value of predictions: 0.00095465395
Proportion of valid SMILES: 0.7873473222674601
Sample trajectories:
Brc1ccc(-c2cnc3cnc(-c4ccc(Br)cc4)nc3c2)cc1
Brc1ccc(C(c2ccccc2)N2CCN(c3ccccc3)CC2)cc1
Brc1ccccc1-n1cnnc1
Brc1ccccc1N1CCN(CC2=CCCC2)CC1
Brc1ccccc1Nc1nnc(CCN2CC3CCCC32)n1CC1COc2cc(OCc3ccccc3)ccc21

 10 Training on 344 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.922194
Reward: 1.011702
Trajectories with max counts:
2	O=P(O)(O)C(F)(F)F
Mean value of predictions: 0.0015800416
Proportion of valid SMILES: 0.7539184952978056
Sample trajectories:
Brc1cc(N2CCOCC2)cc2cccnc12
Brc1ccc(-c2nc[nH]n2)s1
Brc1ccc(-n2c3ccccc3c3ccccc32)cc1
Brc1ccc(NC2CC3CNCC32)cn1
Brc1ccc2c(c1)CCC(c1ccc(Br)o1)O2
Policy gradient replay...
Mean value of predictions: 0.0031092437
Proportion of valid SMILES: 0.7444479199249296
Sample trajectories:
Brc1ccc(-n2cnnc2)o1
Brc1ccc2c(c1)C(SC1=NCCN1)=N2
Brc1cccc(OCc2cnn(-c3ccccc3)n2)c1
C#CC1=C(c2ccc(-c3ccccc3C)cc2)Oc2c(c(O)c(C#N)n2C)N1
C#CCCOc1cc(C(C)C)cc(F)c1C1=C(C)C(=O)c2c(nc(C)c(C)c2Cl)NC1=O
Fine tuning...
Mean value of predictions: 0.001972873
Proportion of valid SMILES: 0.7617407639323732
Sample trajectories:
Brc1ccc(CN(CN2CCCCC2)C2CCCC2)cc1
Brc1ccc(Oc2cncc(Br)n2)cc1
Brc1ccc2nc(-c3cc(OCc4cccnc4)on3)[nH]c2c1
Brc1cnc2c(Br)ccc(Br)c2c1
C#CC(=O)OC1OC2COC(C)C(OC2O)C(C)C(O)C(NC(C)=O)C(CC)OC(=O)C1C

 11 Training on 368 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.304049
Reward: 1.007066
Trajectories with max counts:
2	COc1ccc(N2C(=O)c3ccccc3C2=O)cc1
2	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0017857142
Proportion of valid SMILES: 0.7366113373003444
Sample trajectories:
Brc1ccc(-n2cnc3c(NCc4ccccc4)ncnc32)cc1
Brc1ccc2[nH]cnc2c1
Brc1ccc2nc(Nc3ccco3)nc(c1)n2
Brc1ccc2ncnc(Nc3ccncc3)c2c1
Brc1cccc(Nc2nn[nH]n2)c1
Policy gradient replay...
Mean value of predictions: 0.0031986532
Proportion of valid SMILES: 0.743894802755166
Sample trajectories:
Brc1c(COc2cccnc2)ccn2c(-c3cccnc3)ncc12
Brc1ccc(OCC2CCCCC2)cc1
Brc1cccc(-c2cc(Nc3ccnc(-c4ccccc4)n3)co2)c1
Brc1cccc(C2=NCCN2c2ccc(OCc3ccccc3Br)nc2)c1
Brc1cccnc1-n1cnc2cnc(NCC3CCCCC3)nc21
Fine tuning...
Mean value of predictions: 0.00075
Proportion of valid SMILES: 0.7514088916718847
Sample trajectories:
BrC1=C2CSC(=Nc3ccccc3)C12Br
Brc1cc(-c2nn[nH]n2)no1
Brc1cc(Nc2cs3cccc3ncn2)cc2cn[nH]c12
Brc1ccc(CN2CCN(c3ccncc3)CC2)cc1
Brc1ccc(OCc2ccc(N3CCOCC3)nc2)cc1

 12 Training on 389 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.236349
Reward: 1.004203
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0029813664
Proportion of valid SMILES: 0.755868544600939
Sample trajectories:
B[PH](=O)(NN=Cc1ccc(Br)cc1)(OP(=O)(O)O)P(=O)(O)O
Brc1ccc(-c2nc(CN3CCOCC3)oc2-c2ccncc2)cc1
Brc1ccc(NCc2ccccc2)o1
Brc1ccc(Oc2ccc(Br)cc2)cc1
Brc1ccc2[nH]cc(CCNC3CCN(Cc4ccccc4)CC3)c2c1
Policy gradient replay...
Mean value of predictions: 0.0015695993
Proportion of valid SMILES: 0.7589341692789968
Sample trajectories:
Brc1ccc(C#CCOc2nnc(-c3cccnc3)n2Cc2ccccc2)cc1
Brc1ccc(C2=C(C3CCCC3)SC3=NCNC3=N2)cc1
Brc1ccco1
Brc1nc(-c2nc(-c3ccccc3)cc(-n3nncc3CN3CCNC3)n2)cs1
C#CC(C(=O)Cc1nnc2c(-c3ccccc3)c1C=CC2=O)N1CCN(c2coc(-c3ccco3)n2)CC1
Fine tuning...
Mean value of predictions: 0.0025306123
Proportion of valid SMILES: 0.7665832290362954
Sample trajectories:
Brc1ccc(-n2nnnc2NCCc2ccco2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)cc1
Brc1ccc2[nH]c(-c3ccc(C=C4CCCO4)cc3)nc2c1
Brc1ccc2nc(N3CCOCC3)sc2c1

 13 Training on 412 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.532111
Reward: 1.037854
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0047264765
Proportion of valid SMILES: 0.7147325617766657
Sample trajectories:
Brc1ccc(NC2=C3C=NC4CCN=C4CC3=N2)cc1
Brc1ccc2c(c1)ncn2-c1cnc(Nc2ccc(OCc3ccccn3)cc2)nc1
Brc1cncc(-n2ccnc2)c1
C#CCC(=O)Oc1c(OC(=O)C=Cc2ccc(OCCOCC=C)cc2)cc(O)c2c1OC(C)=CC(=O)O2
C#Cc1cc(Nc2ncnc3ccc(OC)cc23)cc(OC)c1OC
Policy gradient replay...
Mean value of predictions: 0.0031069685
Proportion of valid SMILES: 0.7053850970569818
Sample trajectories:
Brc1ccc(-c2cccc3c(NC4CCCO4)cccc23)cc1
Brc1ccc(-c2cn3cc(-c4ccccn4)[nH]c3c2CCCN2CCCC2)cc1
Brc1ccc(C23CCN(CC2)c2nnnn2Cc2cc(Br)cnc2O3)cc1
Brc1ccc(Nc2ncnc3ccccc23)nc1
Brc1ccc(OCCOc2cnn3ccccc23)cc1Br
Fine tuning...
Mean value of predictions: 0.0032928945
Proportion of valid SMILES: 0.721475461081588
Sample trajectories:
BP(=O)(OCC1OC(=O)OC1c1cc(Br)cc(Br)c1)S(=O)(=O)O
BrC=C1CC(CC=Nc2ccccc2)c2ccccc21
BrCCCc1c[nH]c2ccccc12
Brc1ccc(-n2nnc3ccccc32)cc1
Brc1ccc2c(c1)OCCN2

 14 Training on 453 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.789127
Reward: 1.059530
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.005195989
Proportion of valid SMILES: 0.685625
Sample trajectories:
BrCCn1cnc2ccc(Br)cc21
Brc1ccc(OC2CCCN2)cc1
Brc1ccccc1-n1nnc2c(N3CCCC3)ncnc21
Brc1cnc2[nH]c(c3ccccc23)-c2cn[nH]c2c1
C#CC1(c2ccc(Oc3ccccc3)cc2)NC(=O)NC1=O
Policy gradient replay...
Mean value of predictions: 0.004436397
Proportion of valid SMILES: 0.6916092673763307
Sample trajectories:
Brc1ccc(Nc2nc3ccccc3s2)nc1
Brc1ccc2c(c1)C1CC(N3CCOCC3)CNC1CC2
Brc1cn(C2CN(Cc3cn(-c4cccc5c4CCCC5)nn3)Cn3c2nc2ccccc23)nn1
Brc1cnc(Nc2ccnc3c2Oc2ccccc2N3)c(Br)c1
Brc1cnc2sc(-c3ccc4[nH]c(-c5ccncc5)nc4c3)cc2n1
Fine tuning...
Mean value of predictions: 0.0076923077
Proportion of valid SMILES: 0.6912730685017203
Sample trajectories:
Brc1cc2n(c1Cc1nnnn1Cc1nnn[nH]1)CCC2
Brc1ccc(OCc2nnn[nH]2)cc1
Brc1ccc2[nH]c(-c3cc4ccccc4o3)nc2c1
Brc1ccc2c(NCCCOc3ccnc4cccn34)cccc2n1
Brc1ccc2nccc3c(n2)c2[nH]nc(c4ccncc14)c32

 15 Training on 514 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.951929
Reward: 1.089481
Trajectories with max counts:
4	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.011520303
Proportion of valid SMILES: 0.6622889305816135
Sample trajectories:
Brc1ccc2ncnc(Nc3ncnc4ccccc34)c2c1
Brc1cccc(Nc2ncnc3ccccc23)c1
Brc1cnc2nc(-c3nnn[nH]3)nc(-c3ccccc3)c2c1
Brc1ncnc2[nH]c3ccccc3c12
C#Cc1c(C)ncnc1NC1CCN(Cc2nc(-c3ccn(C)n3)no2)C1
Policy gradient replay...
Mean value of predictions: 0.012112675
Proportion of valid SMILES: 0.665833072835261
Sample trajectories:
Brc1ccc2[nH]c(-c3cnn(-c4cccc5cnc6ccccc6c45)c3)nc2c1
Brc1ccc2c(c1)C(Nc1cnccn1)CO2
Brc1ccc2ncnc(Nc3cccnc3)c2c1
Brc1cccc(Nc2nncn2Cc2ccccc2)c1
Brc1ccccc1-c1ncnc2nn(-c3cnc4ccc5cccnc5c4c3)nc12
Fine tuning...
Mean value of predictions: 0.010364683
Proportion of valid SMILES: 0.65125
Sample trajectories:
Brc1ccc2c(c1)C=C(N=Cc1ccc[nH]1)S2
Brc1ccncc1OC1CN(c2cncnc2)C1
Brc1cncn1Cc1ccc(Cn2ccnc2)cc1
Brc1nnn(-c2cc3nccn3c3ccccc23)c1-c1cccnc1
C#CC1(O)C(=O)C(C(=O)Nc2cnc3cccnc3c2)=CN1C1COC(OC2OC(C)C(OC(C)=O)C2O)C(O)C1O

 16 Training on 630 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.386393
Reward: 1.074480
Trajectories with max counts:
7	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.012762078
Proportion of valid SMILES: 0.6858393247889966
Sample trajectories:
BrC1=C(c2ccc3cccnc3c2)N2CC(Cc3cccnc3)c2N1
Brc1ccc(-n2nccn2)cc1
Brc1ccc2c(c1)CC(n1cc(Br)cn1)=Cc1cnnn1C2
Brc1ccc2nc(-c3cncc(-n4cnnn4)n3)[nH]c2c1
Brc1ccc2nc(N3CCN(Cc4ccccn4)CC3)sc2c1
Policy gradient replay...
Mean value of predictions: 0.009439252
Proportion of valid SMILES: 0.6689590497030322
Sample trajectories:
Brc1ccc2ncnc(Nc3ccccn3)c2n1
Brc1ccc2oc(-c3cccnc3)nc2c1
Brc1ccccc1Nc1ncnc2cncnc12
Brc1cccn2c(-c3ccncc3)nc(-c3ccnc4[nH]ncc34)c12
Brc1cn(-c2nc(NCc3cncnc3)c3nnnn3n2)cn1
Fine tuning...
Mean value of predictions: 0.012069779
Proportion of valid SMILES: 0.6630196936542669
Sample trajectories:
BrCc1cnc2sc3ccccc3n12
Brc1ccc(CN2CCN(c3ncnc4cc(Br)c(Br)cc34)CC2)cc1
Brc1ccc(Cc2c(Br)cnc3c2sN(Cc2nnnn2Cc2ccccn2)CC(c2ccccc2)=C3)cc1
Brc1ccc(N2c3ccnn3C2c2ccccn2)nc1
Brc1ccc(Nc2ncnc3ccsc23)cc1

 17 Training on 749 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.376453
Reward: 1.092409
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.012912348
Proportion of valid SMILES: 0.6633322913410441
Sample trajectories:
Brc1c(-c2cncnc2)cnn1-c1ccc2nccn2c1
Brc1cc2nc(Nc3ncnc4[nH]ncc34)cnc2cc1C1=CCOc2cn(-c3ccncc3)nc2-c2nnnn21
Brc1cc2nccnc2c(OCc2cccc3ccccc23)c1Br
Brc1ccc(-c2noc(-c3cccnc3)n2)cc1
Brc1ccc(Nc2ncnn2C2Cc3cccnc3N=C2COc2ccccc2)cc1
Policy gradient replay...
Mean value of predictions: 0.012392755
Proportion of valid SMILES: 0.6560350218886805
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(F)(F)P(=O)(O)O
Brc1cc(c2cncnc2)nc2ncnc(Nc3ncccn3)c12
Brc1ccc2[nH]c(-c3nnc(-c4ccc(-n5ccnc5)cc4)o3)nc2c1
Brc1ccc2[nH]cnc2c1
Brc1ccc2cc(-c3nnc(-c4cnoc4)o3)[nH]c2c1
Fine tuning...
Mean value of predictions: 0.011798289
Proportion of valid SMILES: 0.6570803376055018
Sample trajectories:
BP(=O)(OCc1ccccc1)c1ccc(Br)cc1
Brc1ccc(-c2cnc3ncnn3c2)o1
Brc1ccc(Br)nc1
Brc1ccc2nc(-c3noc(-c4cccnc4)n3)ncc2c1Br
Brc1ccccc1Cn1c(Br)cc2cccnc21

 18 Training on 883 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.195190
Reward: 1.168771
Trajectories with max counts:
12	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.03093415
Proportion of valid SMILES: 0.6123788683963739
Sample trajectories:
Bc1cc2cccc(-c3cccnc3)c2cn1
Brc1cc2ncn(Cc3cccnc3)c2c2ncccc12
Brc1ccc(C=CCn2cc(-c3ccc4ccccc4c3)c[n+]2Br)cc1
Brc1ccc2[nH]cc(COc3cccnc3)c2c1
Brc1ccc2ncc(Br)nc2c1
Policy gradient replay...
Mean value of predictions: 0.03071201
Proportion of valid SMILES: 0.588125
Sample trajectories:
BP(=O)(OCC)c1cccc(CP(=O)(O)O)c1
Brc1cc2nnnn2c(-c2ccccc2)c2ccccc2n1
Brc1ccc(Nc2ncnc3cc(Br)c(CN4CCSCN4)cc23)c(Br)c1
Brc1ccc2[nH]c3ccccc3c2c1
Brc1ccc2cnn2c2c3c1cc1ccccc1n32
Fine tuning...
Mean value of predictions: 0.034859523
Proportion of valid SMILES: 0.6008127539856205
Sample trajectories:
Brc1ccc(Nc2ncnc3ccc(Br)cc23)nc1
Brc1ccc2c(c1)C(c1ccc[nH]1)N2
Brc1cccc(-n2nnc3ccncc32)c1
Brc1cccc2oc(-c3ccncc3)nc12
Brc1cn2c(Nc3ccccc3)c(Br)cc2c(-c2cnc3ncnn3n2)n1

 19 Training on 1184 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.915702
Reward: 1.242742
Trajectories with max counts:
45	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.034591194
Proportion of valid SMILES: 0.59625
Sample trajectories:
Brc1ccc2c(N=Nc3ccccc3)c3ccccc3cc2n1
Brc1ccc2ncnn2c1
Brc1cn(-c2ccnc3c2Nc2ccccc2O3)c2c1NCC2
Brc1cnc2[nH]c3ccncc3c2c1
Brc1cnc2ccccc2n1
Policy gradient replay...
Mean value of predictions: 0.03155717
Proportion of valid SMILES: 0.6040625
Sample trajectories:
Brc1cc2ncnn2c(-n2cncn2)c2ccccc2n1
Brc1ccc2nc(-n3cnnn3)nc(Nc3ccccn3)c2c1
Brc1ccc2nc(Nc3cccc4ccccc34)cc(c1)n2
Brc1ccc2nc[nH]c2c1
Brc1ccc2ncnc(Nc3ccccc3)c2c1
Fine tuning...
Mean value of predictions: 0.034675255
Proportion of valid SMILES: 0.5821875
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCN(Cc2cnn3ccccc23)CC1
Brc1ccc(-c2noc(-n3cccn3)n2)cc1
Brc1ccc(Br)c(C=Cc2ccc3ncsc3c2)c1
Brc1ccc2c(Nc3ccccc3)ccnc2c1
Brc1ccc2ncsc2c1

 20 Training on 1502 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.688483
Reward: 1.326711
Trajectories with max counts:
53	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.037017167
Proportion of valid SMILES: 0.5826820881525476
Sample trajectories:
Brc1ccc2c(-c3ccc4ncncc4c3)cnn2c1
Brc1ccc2c(cc3N(N=Nc4ccncc4)CN32)c1
Brc1ccc2c(n1)-c1ccccc1O2
Brc1ccc2cc(Nc3ncnc4cc[nH]c34)ccc2n1
Brc1cccc2ncnc(Nc3cccc4ccccc34)c12
Policy gradient replay...
Mean value of predictions: 0.050626308
Proportion of valid SMILES: 0.59875
Sample trajectories:
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc2c(c1)OCO2
Brc1ccc2nc(N=Cc3ccnc4c3Nc3ccccc3O4)sc2c1
Brc1ccc2nccn2c1
Brc1ccc2nccn2n1
Fine tuning...
Mean value of predictions: 0.035725676
Proportion of valid SMILES: 0.5879962488277587
Sample trajectories:
Brc1cc2c(nn1)-c1ccccc1-2
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc2nc(-c3ccncn3)ncc2c1
Brc1ccc2nc(COc3cccc4ccccc34)ccc2c1
Brc1ccc2ncn(CCc3ccccc3)c2c1

Trajectories with max counts:
244	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.037001632
Proportion of valid SMILES: 0.5361920240030004
Mean Internal Similarity: 0.5020592761132208
Std Internal Similarity: 0.12426258722638917
Mean External Similarity: 0.43427631700646135
Std External Similarity: 0.09391282939997783
Mean MolWt: 355.6597407407409
Std MolWt: 70.13493348340589
Effect MolWt: -1.5264300208095296
Mean MolLogP: 4.145480987654324
Std MolLogP: 1.08398664379821
Effect MolLogP: -0.4349884368169223
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 91.608392% (131 / 143)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 0, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5480.504627466202, 'valid_fraction': 0.5361920240030004, 'active_fraction': 0.028328281650734435, 'max_counts': 244, 'mean_internal_similarity': 0.5020592761132208, 'std_internal_similarity': 0.12426258722638917, 'mean_external_similarity': 0.43427631700646135, 'std_external_similarity': 0.09391282939997783, 'mean_MolWt': 355.6597407407409, 'std_MolWt': 70.13493348340589, 'effect_MolWt': -1.5264300208095296, 'mean_MolLogP': 4.145480987654324, 'std_MolLogP': 1.08398664379821, 'effect_MolLogP': -0.4349884368169223, 'generated_scaffolds': 143, 'novel_scaffolds': 131, 'novel_fraction': 0.916083916083916, 'save_path': '../logs/replay_combo_s2-7.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.912507
Reward: 1.000000
Mean value of predictions: 0.0012485369
Proportion of valid SMILES: 0.8029448621553885
Sample trajectories:
BP(=O)(NCc1ccc2c(c1)C=CC(=O)O2)P(=O)(O)O
BrCC(ON=C(c1ccccc1)c1ccc(CNc2nc(I)cs2)cc1)N1CCCCC1
Brc1ccc(CNc2ncccn2)nc1
C#CCC#Cc1cc(C(C#CCCCCCC)=CC(=O)OC)ccc1O
C#CCCC(NCc1ccccc1)C(OC(=O)NC(NC1CCCCC1)C(O)COC(=O)Cc1ccc(C(=O)NCCO)nc1)C(=O)NCC1CCCO1
Policy gradient replay...
Mean value of predictions: 0.00015791552
Proportion of valid SMILES: 0.7960402262727844
Sample trajectories:
Brc1ccc(-c2nn3ccccc3c2Br)cc1
Brc1cccc(Nc2nc3ccccc3nc2-c2ccccc2)c1
C#CC1OC(=O)C(CC)c2c(C(C)(C)C)cc(c3ccccc3)nc2SCC(=O)N1C
C#CC1c2ccccc2C2=CC(c3ccccc3)CC(=O)NC1O2
C#CCC(=O)OCP(=O)(NC(C)C(=O)OCC)c1ccc2c(c1)OCO2
Fine tuning...
Mean value of predictions: 0.010570627
Proportion of valid SMILES: 0.6693800876643707
Sample trajectories:
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2nc(c3cccnc3)cnc2Nc2ccc3c(n2)-c2ccccc2N3c2cccnc2)cc1
Brc1cccc(C=NNc2ncnc3[nH]cnc23)c1
C#CCC1CC(CC(=O)C2=C(N3COCC3(C)C)CCCC2)C(C)(C)C1(C)O
C#CCCc1ccc(C(=O)NC(C(=O)OCC)c2ccc(Cl)c(S(N)(=O)=O)c2)cc1

  2 Training on 260 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.757332
Reward: 1.023917
Trajectories with max counts:
2	Cc1cccc(Nc2ncnc3cc(Cl)ccc23)c1
2	Fc1ccc(Nc2nc3ccccc3[nH]2)cc1
2	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.008456845
Proportion of valid SMILES: 0.7191222570532916
Sample trajectories:
BrCc1ccc(C(=Nc2ccc3ccccc3n2)c2cnc3ccccc3c2)cc1
Brc1ccc(-c2csc3ncnc(Nc4ccc5ccccc5c4)c23)cc1
Brc1ccc(Nc2csc(-c3cnn[nH]3)c2)cc1
Brc1ccc(Oc2ccc3c(Br)cccc3c2)cc1
Brc1ccc2ncn(-c3ccccc3Br)c2c1
Policy gradient replay...
Mean value of predictions: 0.009006623
Proportion of valid SMILES: 0.7086983729662077
Sample trajectories:
Brc1ccc(N2CCN(c3ncccn3)C2)c(NC2CCCC2)c1
Brc1ccc(Sc2ncnc3[nH]c(N4CCOCC4)nc23)c2ncccc12
Brc1ccccc1-c1cccc(-n2ccnc2)n1
Brc1cccs1
Brc1cnc(N2CCNc3cnnn3O2)nc1
Fine tuning...
Mean value of predictions: 0.016788322
Proportion of valid SMILES: 0.6873628096582001
Sample trajectories:
Brc1cc(Nc2cnc3ccccc3n2)on1
Brc1ccc(C(=NNc2nccs2)Nc2ccc3c(c2)OCO3)cc1
Brc1ccc(C=NN2CCN(Cc3ccccc3)CC2=Nc2ccccc2)cc1
Brc1ccc(Nc2cc(N3CCCC3)ccc2Br)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1

  3 Training on 382 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.323023
Reward: 1.049076
Trajectories with max counts:
2	O=C1c2cc(O)ccc2C(=O)c2c(O)cc(O)cc21
Mean value of predictions: 0.011538463
Proportion of valid SMILES: 0.6375353662370323
Sample trajectories:
Brc1ccc(Br)c(Nc2ncnc3cc(Br)c(Br)c(-c4cccnc4)c23)c1
Brc1ccc(Nc2nc3ccccc3nc2-n2cncn2)cc1
C#CC1C(=O)N(C)S1(=O)=O
C#CCNC(=O)S(=O)(=O)c1nc(NC(=O)c2ccccc2)c(N2CCOCC2)s1
C#Cc1ccc(N2nc3cccc(F)c3c(-c3nn(C)c4ncncc34)nc2c2cc(C)ccc2Cl)cc1
Policy gradient replay...
Mean value of predictions: 0.018033573
Proportion of valid SMILES: 0.6558666247247562
Sample trajectories:
BP(=O)(OCC(=O)Nc1ccc(F)cn1)P(=O)(Oc1ccccc1)Oc1ccccc1
BrC1=Nc2ccccc2SC(=Nc2ccncn2)C1
Brc1cc(Br)cc(CCNc2cc(-c3cncnc3)ncn2)c1
Brc1ccc(-c2cc(COc3cccc(Br)c3)nc(-c3ccccn3)n2)cc1
Brc1ccc(-c2ccn3cncc3n2)cc1
Fine tuning...
Mean value of predictions: 0.03131783
Proportion of valid SMILES: 0.6058234189104571
Sample trajectories:
BrC1=CN2C(N=C3c4ccccc4-c4ccccc43)=CSC2=C(CN2CCCC2)C=C1
BrCCNc1ncnc2ncnc(N3CCN(c4ccc5ncccc5c4)CC3)c12
Brc1cc2c(cc1CCN1CCCCC1)OCO2
Brc1ccc(-n2cc(-c3ccc(Br)cc3Br)nc2-c2cccnc2)cc1
Brc1ccc(-n2ccnc2CN=C(Nc2nnc(-c3cc4cnccc4nc3-c3cccc(Br)c3)s2)N2CCCCCC2)cc1

  4 Training on 573 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.822212
Reward: 1.145593
Trajectories with max counts:
15	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.038660713
Proportion of valid SMILES: 0.701095461658842
Sample trajectories:
Brc1ccc(-c2ccc3ccsc3c2)cc1
Brc1ccc(-c2nc3c(s2)-c2ccccc2-3)cc1
Brc1ccc(-c2nnc3cc(-c4cccnc4)ccn23)cc1
Brc1ccc(Br)cc1
Brc1ccc(N=Nc2cccc(Oc3ccccc3)c2)cc1-c1cccs1
Policy gradient replay...
Mean value of predictions: 0.035549525
Proportion of valid SMILES: 0.692018779342723
Sample trajectories:
BrC1(CCNc2ncnc3ccccc23)CCCCN1
Brc1ccc(-c2noc(-c3ccccc3Br)n2)o1
Brc1ccc(C=NNc2ccc(Nc3ccncc3)cc2)cc1
Brc1ccc(N=C(c2ccncc2)c2ccccc2Br)cc1
Brc1ccc(NN=C(c2ccc(Br)cc2)c2ccccc2Br)cc1
Fine tuning...
Mean value of predictions: 0.045386534
Proportion of valid SMILES: 0.6275430359937402
Sample trajectories:
BP(=O)(OCC1OC(Oc2ccccc2)SC(CO)C(O)C(O)C1O)OC(=O)Nc1cc(Br)cc(Br)c1
Brc1cc(Nc2cccnc2)nc2ccccc12
Brc1ccc(-n2cnc3c(Nc4ncncc4Br)ncnc32)cc1
Brc1ccc(NN=Cc2ccc3ccccc3c2)cc1
Brc1ccc(Nc2ccc3ncsc3n2)cc1

  5 Training on 950 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 21.498862
Reward: 1.735846
Trajectories with max counts:
25	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.10866389
Proportion of valid SMILES: 0.5991244527829893
Sample trajectories:
BP1(=O)CCN1CCN(=O)(O)CC(F)(F)F
Brc1cc(-c2ccccc2)c2ccccc2n1
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1ccc(-c2c[nH]c3ccccc23)c2ccccc12
Brc1ccc(-c2ncc(-c3ccccc3)c(Nc3cccc(-c4cc(Br)cc(Br)c4Br)n3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.101484895
Proportion of valid SMILES: 0.6105032822757112
Sample trajectories:
BP(=O)(NCCCCSS(N)(=O)=O)N(Cc1cccc(Cl)c1)c1nc(C(N)=O)c(N)c2cc(Br)ccc12
Brc1cc(Br)c2ncncc2nc2c1Oc1ccccc1-2
Brc1ccc(-c2ncc3ccccc3n2)cc1
Brc1ccc(-c2ncnc3sc(-c4cnco4)nc23)cc1
Brc1ccc(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.10425532
Proportion of valid SMILES: 0.588603631809643
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1OP(=O)(O)O)S(=O)(=O)c1ccc(Cl)cc1
Brc1cc(-c2ccccc2)c2ccccc2n1
Brc1cc(Br)c2c(Nc3ccc(I)cc3)ncnc2c1
Brc1cc(Nc2ncnc3ccccc23)cnc1-c1ccccc1
Brc1ccc(-c2cnc(Nc3ccc4c(n3)-c3ccccc3N4)cn2)cc1

  6 Training on 1805 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 21.405947
Reward: 2.054332
Trajectories with max counts:
51	Fc1ccc(Nc2ncnc3cc(F)ccc23)cc1
Mean value of predictions: 0.16686009
Proportion of valid SMILES: 0.4849906191369606
Sample trajectories:
Brc1c(I)cc(I)cc1-c1cnc2ncnc(Nc3ccccn3)c2c1
Brc1ccc(-c2ccccn2)c2cccnc12
Brc1ccc(-c2nc3ccccc3nc2Nc2nc(NCC3CCCCC3)c(Br)cc2Br)nc1
Brc1ccc(-c2ncnc3[nH]c(Br)cc23)c(Br)c1
Brc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.17050183
Proportion of valid SMILES: 0.5109443402126329
Sample trajectories:
BP(=O)(c1ccnc(N2CCOCC2)c1)C(O)C(F)(F)F
Brc1cc2ncnc(Sc3ccc(Br)c(Br)c3)n2n1
Brc1ccc(Br)c(Oc2c(Br)cc(Br)cc2Oc2ccc(Oc3cccnc3)nc2)c1
Brc1ccc(CSc2cnc3ncnc(Nc4ccc(Br)cc4)c3n2)cc1
Brc1ccc(N2CCC(Oc3ncncc3Nc3ccccc3)C2)s1
Fine tuning...
Mean value of predictions: 0.16747968
Proportion of valid SMILES: 0.5765625
Sample trajectories:
BP(=O)(Oc1ccc(Nc2cncc(Br)c2)nc1)OC(C)C
Br
BrC1=CC(c2ccccc2)=NC(c2ccsc2)=CN1
BrC=C1CCC2=CNC(Br)=C2CC1
Brc1c2ccccc2n2c1sc1ccccc12

  7 Training on 2945 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 21.205140
Reward: 2.659594
Trajectories with max counts:
169	Fc1ccc(Nc2ncnc3ccc(F)cc23)cc1
Mean value of predictions: 0.22447845
Proportion of valid SMILES: 0.449375
Sample trajectories:
BP(=O)(OC)N(CC(F)[PH](F)(F)F)C(=O)OC(Cl)CC
Br
BrCc1ccc(Nc2ncnc3c4ccccc4c23)cc1
Brc1cc(-c2ncnc3[nH]ccc23)n2ccncc12
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Policy gradient replay...
Mean value of predictions: 0.24040997
Proportion of valid SMILES: 0.42700844013754297
Sample trajectories:
Brc1cc(Br)c(Nc2nccnc2Nc2ccnc(Br)c2)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3cccc(Br)c23)c1
Brc1ccc(-c2c(Br)c(Br)c3c(-c4ccsc4)ncnn23)cc1
Brc1ccc(-c2cnc3c(n2)SCC2(CCN(c4ccc(Br)cc4)C2)c2ccccc2-3)cc1
Fine tuning...
Mean value of predictions: 0.2133989
Proportion of valid SMILES: 0.5085964363863708
Sample trajectories:
BP(=O)(NC(Cc1cccc(Br)c1)c1ccc(Br)cc1)C(=O)Oc1cc(Br)cc(Br)c1F
BP(=O)(NCc1cnc(Br)s1)OCCC(F)(F)F
BP(=O)(NO)n1cnc2c(N)ncnc21
BP(=O)(OCC)OC(=O)CCCCCCCCP(=O)(O)O
BrCc1cc(-c2ccc(Nc3ncnc4ccsc34)cc2)nc2c(Br)cncc12

  8 Training on 4084 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 21.170357
Reward: 4.064193
Trajectories with max counts:
485	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.24783656
Proportion of valid SMILES: 0.26
Sample trajectories:
BP(=O)(OCC)OC(=O)C=CC
Brc1[nH]cc(Nc2ncnc3cccnc23)c1Br
Brc1ccc(Nc2cncnc2)cc1
Brc1ccc(Nc2ncccc2n2ncnc2-c2ccc(Br)cc2)cc1
Brc1ccc(Nc2nccnc2-c2nc3ccccc3[nH]2)cc1
Policy gradient replay...
Mean value of predictions: 0.23674911
Proportion of valid SMILES: 0.2653125
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Brc1ccc(Nc2c(-c3cncs3)cnc3ccccc23)cc1
Brc1ccc(Nc2cc(Nc3cc4ccccc34)ncn2)cc1
Brc1ccc(Nc2nc3ccccc3s2)c(Br)c1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.2529321
Proportion of valid SMILES: 0.4052532833020638
Sample trajectories:
BP(=O)(NP(=O)(OP(=O)(O)O)c1ccc(Nc2nc3ccc(Br)cc3s2)cc1)OCC=CC
Brc1cc(-c2nc[nH]n2)c2ncnn2c1
Brc1ccc(-c2ncnc3occc23)c2cccnc12
Brc1ccc(Br)c(Nc2ncnc3cnc(Nc4cnc(Br)s4)cc23)c1
Brc1ccc(CSc2ncnc3cccnc23)cc1

  9 Training on 4911 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 23.159089
Reward: 5.251230
Trajectories with max counts:
201	Fc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.3553379
Proportion of valid SMILES: 0.3190625
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(N(c2ccccc2)c2ncnc3ccccc23)c(Br)c1
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)sc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.36592448
Proportion of valid SMILES: 0.3228125
Sample trajectories:
BP(=O)(CCCO)OCCC(=O)N(O)COP(=O)(O)OP(=O)(O)n1cnc2c(Nc3ccc(Br)cc3)ncnc21
Brc1cc(Br)cc(CNc2ccnc(-c3cncnc3)c2Nc2ncnc3ncnc(N4CCCC4)c23)c1
Brc1ccc(N2CCN(c3ncnc(Nc4cc(Br)c(Br)cc4Br)n3)CC2)cc1Br
Brc1ccc(Nc2cncc(Br)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)cnc23)cc1
Fine tuning...
Mean value of predictions: 0.3278459
Proportion of valid SMILES: 0.356875
Sample trajectories:
Brc1cc(Br)c(Br)s1
Brc1ccc(Br)c(-c2nccnc2SC2CCCC2)c1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2cnc3ccc(Br)nc3n2)cc1
Brc1ccc(Nc2nc3ccccc3s2)cc1

 10 Training on 6116 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 19.413221
Reward: 5.674921
Trajectories with max counts:
1211	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.2914956
Proportion of valid SMILES: 0.213125
Sample trajectories:
B[PH](=O)(Nc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O
Brc1ccc(NN=C2c3ccccc3CCc3ccccc32)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.3102041
Proportion of valid SMILES: 0.214375
Sample trajectories:
BrC(=Nc1cccc(Br)c1)c1ccc(Br)cc1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1ccc(-c2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Br)s1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.36444795
Proportion of valid SMILES: 0.3990625
Sample trajectories:
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c2c(Nc3csc(Br)n3)ncnc2c1
Brc1cc2ncnc(Nc3ncc(Br)c(Nc4ncnc5ncnc6ncnc4c56)n3)c2cc1Br
Brc1ccc(-c2ncnc3ccc(Br)cc23)c(Br)c1
Brc1ccc(-n2cnnc2Nc2cc(Br)ncn2)cc1

 11 Training on 7028 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 18.387916
Reward: 4.420758
Trajectories with max counts:
151	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.34653324
Proportion of valid SMILES: 0.4373241638011879
Sample trajectories:
BP(=O)(Br)OC
BP(=O)(OCC1NC(=N)O1)C(=O)OCCC
Brc1cc(-c2c(Br)cc(Br)c(Br)c2Br)[nH]c1Br
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Nc2ncnc3cccnc23)c(I)c1
Policy gradient replay...
Mean value of predictions: 0.3702683
Proportion of valid SMILES: 0.4309375
Sample trajectories:
BP1(=O)OCC2OC(=N)C(Cl)C(O2)C(N)=Nn2cnc3ncnc(cc(Br)cc(Br)c32)c2cc1ccc2Nc1cc(Br)c(Br)c(Br)c1O
BrC1=CC2c3cc(Br)ccc3OCC2N1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c(I)cc1Br
Brc1cc(Br)c(Nc2ncnc3ncnc(Nc4ccnc5cc(Br)sc45)c23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.34401223
Proportion of valid SMILES: 0.4098155673648015
Sample trajectories:
Brc1cc(Br)c(Br)s1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(I)cc23)c1
Brc1cc(Br)nc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc2c(s1)c(Br)cc1scnc12

 12 Training on 7822 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 16.469823
Reward: 3.992788
Trajectories with max counts:
615	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.35680473
Proportion of valid SMILES: 0.316875
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)N(O)C=O
BP(=O)(OCCS(=O)(=O)OC(Cl)(Cl)Cl)P(=O)(O)Oc1ccc(Br)cc1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Nc2ccc(Br)nc2)c1
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4cc(Br)cnc34)cc2)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.3349544
Proportion of valid SMILES: 0.3084375
Sample trajectories:
BrC1=Nc2cc(Br)ccc2O1
BrCCCCCCCn1nnnc1Nc1ccc(Br)cc1
BrSc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1cc(Br)c2c(c1)Nc1ncnn1CCC2
Brc1cc(Br)cc(CNc2ccc(Nc3ncnc4ccccc34)cc2)c1
Fine tuning...
Mean value of predictions: 0.36666667
Proportion of valid SMILES: 0.399375
Sample trajectories:
BP1(=O)NC(=O)OCC(Oc2ccc(Nc3cnc(Br)c(Br)n3)cc2)C(O)C(O)C1O
Bc1cc(Nc2ncnc3cc(Br)cc(Br)c23)cc(Br)c1Br
BrSc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(I)cc3)ncnc2c1

 13 Training on 8551 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 18.710470
Reward: 4.676478
Trajectories with max counts:
352	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.4223865
Proportion of valid SMILES: 0.2959375
Sample trajectories:
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Brc1c(Br)c(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(I)c1
Policy gradient replay...
Mean value of predictions: 0.43608248
Proportion of valid SMILES: 0.303125
Sample trajectories:
BrCc1nc2c(c(Nc3ccc(Br)cc3)n1)Nc1ncnc(Nc3ccc(Br)cc3)c1C(c1ccc(Br)o1)=N2
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4cc(Br)cc(Br)c34)cc2)c(Br)c1
Brc1cc(Br)c(Nc2nc(Br)cnc2Br)cn1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.41021895
Proportion of valid SMILES: 0.38555347091932457
Sample trajectories:
BP1(=O)OCC(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OC1=O
Bc1ccc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)cc1
Br
Brc1c(-c2ccc(Nc3ncnc4ccccc34)cc2)sc2ncncc12
Brc1cc(Br)c(Br)c(Br)c1Br

 14 Training on 9474 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 19.326524
Reward: 5.142766
Trajectories with max counts:
541	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.4526841
Proportion of valid SMILES: 0.25046904315197
Sample trajectories:
BP(=O)(NC(=O)CBr)C(F)(F)F
BP(=O)(O)OP(=O)(O)OP(=O)(O)P(=O)(O)O
BP(=O)(OCC1NC=C(OC[PH](=S)OCP(=O)(O)O)C(=O)O1)C(=O)O
BP(=O)(OCC1OC(OP(=O)(O)O)C(O)C1O)n1cnc(N)c1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)OP(=O)(O)O
Policy gradient replay...
Mean value of predictions: 0.4733656
Proportion of valid SMILES: 0.25820568927789933
Sample trajectories:
BP(=O)(Cc1ccc(Br)s1)P(=O)(OP(=O)(Oc1ccccc1)Oc1ccc(Br)cc1)c1cc(Br)c(Br)cc1Nc1ccc(Br)cc1
BP(=O)(OC(C)C)C(Cl)=CCl
BP(=O)(OCC1OC(=O)N(O)OCC=CN1C(=O)CF)c1ccc(F)cc1
BP(=O)(OCC1OC(Nc2ccc(Br)cc2)C(O)C(O)C1O)C(=O)OCC=C
BP(=O)(OCCCl)c1cccc(Nc2ncnc3c(Br)cnc(Br)c23)c1
Fine tuning...
Mean value of predictions: 0.44474447
Proportion of valid SMILES: 0.3240625
Sample trajectories:
BP(=O)(COP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O)N(O)COP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(NC(=O)C(CC(=O)O)NC(=O)Cc1cc(F)c(F)c(F)c1)OCC=CC(=O)OP(=O)(O)OP(=O)(O)O
BP(=O)(NC(=O)c1ccc(F)c(F)c1)Nc1cc(F)cc(F)c1F
BP(=O)(OC(F)(F)F)c1cc(Br)c(O)c(Br)c1
BrCc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Br

 15 Training on 10327 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 19.988956
Reward: 5.471231
Trajectories with max counts:
450	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.45328555
Proportion of valid SMILES: 0.26172607879924953
Sample trajectories:
BP(=O)(Nc1ccc(Nc2cc(Br)cnc2F)cc1)C(F)(F)F
BP(=O)(Nc1ccc(Nc2ncnc3cc(Br)cc(F)c23)c(Br)c1)c1ccc(F)cc1
BP(=O)(Nc1nc(Cl)c(Br)cc1F)C(=O)OCCl
BP(=O)(OC(=O)Cl)C(=O)OCC
BP1(=O)OCC(CC(O)COc2cc(F)cc(Br)c2F)c2cc(F)c(F)cc21
Policy gradient replay...
Mean value of predictions: 0.46469802
Proportion of valid SMILES: 0.2328125
Sample trajectories:
BC(=O)OCCS(=O)(=O)O
BP(=N)(N=O)c1ccc(Br)cc1
BP(=O)(N=[P+]([O-])Oc1ccc(Br)cc1)NCCCl
BP(=O)(OCC)OC(=O)CCCl
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)C(F)(F)P(=O)(O)CCl
Fine tuning...
Mean value of predictions: 0.4204757
Proportion of valid SMILES: 0.30228196311347294
Sample trajectories:
BP(=O)(COP(=O)(O)OCOP(=O)(O)O)OCCO
BP(=O)(OCCOS(=O)(=O)c1ccc(Br)cc1)Oc1ccc(Br)cc1
B[PH](=O)(Cl)(Cl)OCCl
Bc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Br
Bc1ccc(Br)cc1Nc1ncc(Br)s1

 16 Training on 11154 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.903712
Reward: 5.932201
Trajectories with max counts:
592	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.47097704
Proportion of valid SMILES: 0.21756798999687402
Sample trajectories:
BP(=O)(CBr)CC(=O)O
BP(=O)(CCl)NO
BP(=O)(N(CCCl)OP(=O)([O-])OP(=O)([O-])OCC[N+](C)(Br)P(=O)(O)O)n1cnc2c(N)ncnc21
BP(=O)(NO)c1cc(Br)cc(Br)c1Br
BP(=O)(Nc1cccc(F)c1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.47810814
Proportion of valid SMILES: 0.23125
Sample trajectories:
BP(=O)(Cl)CCC(F)F
BP(=O)(Cl)N(CC(=O)OC(COP(=O)(O)O)C(C)(F)F)C(=O)NC(C(F)F)C(F)(F)F
BP(=O)(NO)c1ccc(Br)cc1Br
BP(=O)(Nc1ccc(Br)cc1)P(=O)(Oc1ccc(Br)cc1)Oc1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cc1)[PH](Br)(Br)OP(=O)(O)O
Fine tuning...
Mean value of predictions: 0.45964915
Proportion of valid SMILES: 0.285
Sample trajectories:
BP(=O)(N1CCCC1)N1C(=O)Oc2ccccc21
BP(=O)(OC(C)Cl)P(=O)(O)O
BP(=O)(OCC1OC(=O)Nc2cc(Br)ccc21)c1cccc(Br)c1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)O
BP(=O)(OCCS)C(Br)C(Br)Br

 17 Training on 11976 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.025646
Reward: 6.516703
Trajectories with max counts:
484	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.5107652
Proportion of valid SMILES: 0.24108818011257035
Sample trajectories:
Bc1cc(Br)cc(Nc2ncnc3cc(F)c(Br)c(Br)c23)c1
Bc1cc(Nc2ncnc3cc(Br)ccc23)ccc1Br
BrCC=CC=NNc1cc(Br)cs1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Policy gradient replay...
Mean value of predictions: 0.5307895
Proportion of valid SMILES: 0.23764853033145716
Sample trajectories:
BP(=O)(NOC(=O)CBr)Oc1ccc(Br)cc1
BP(=O)(Nc1ccc(Nc2cc(Br)cc(Br)c2)cc1)C(=O)OCCl
BP(=O)(OCC)Oc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BP(=O)(c1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1F)N(O)C(F)(F)F
B[PH](=O)(=O)c1ccc(Nc2nc(Br)nc(NP(=O)(OCBr)P(=O)(O)OP(=O)(O)O)c2Cl)cc1
Fine tuning...
Mean value of predictions: 0.45902336
Proportion of valid SMILES: 0.294375
Sample trajectories:
BIc1ccc(Nc2ncnc3ccc(Nc4ccc(Br)cc4)cc23)cc1
BP(=O)(CCl)N(O)C(=O)OC(C)(C)C
BP(=O)(N1CCN(C(=O)O[PH](N)(=O)=O)CC1)N(=O)=O
BP(=O)(NO)c1cccc(F)c1F
BP(=O)(OC(=O)C[n+]1ccc(Br)cc1)P(=O)(OC(C)(C)O)C(F)(F)F

 18 Training on 12890 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.332298
Reward: 6.703996
Trajectories with max counts:
726	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.47389942
Proportion of valid SMILES: 0.19875
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c1
Brc1ccc(-c2ncnc3cnc(Br)cc23)c(Br)c1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ccc(Nc3ncnc4cc(Br)ccc34)cc2)c1
Policy gradient replay...
Mean value of predictions: 0.47238693
Proportion of valid SMILES: 0.2003125
Sample trajectories:
BP(=O)(OCC1OC(N2C=CC(N)=NC2=O)C(O)C1O)Oc1ccc(F)c(F)c1
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
BrCCNc1c2cnccc2cc2ncnc(Nc3ccc(Br)cc3)c12
BrCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Nc2ccnc3ccc(Br)cc23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.50242954
Proportion of valid SMILES: 0.32166301969365424
Sample trajectories:
BP(=O)(NCC(=O)OCCl)Oc1ccc(Nc2cc(Br)c(Br)c(Br)c2Br)cc1Br
BP(=O)(Nc1cc(Br)c(Br)cc1F)C(=O)Nc1cc(F)c(F)c(F)c1F
Bc1cc(Br)cc(Br)c1Br
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Brc1cc(Br)c(Br)c(Br)c1

 19 Training on 13710 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.363876
Reward: 6.381353
Trajectories with max counts:
638	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.33534744
Proportion of valid SMILES: 0.20693966864645202
Sample trajectories:
BP(=O)(Br)OP(=O)(Br)OP(=O)(O)OP(=O)(O)OP(B)(=O)Oc1ccc(Nc2cc(Br)ccn2)cc1
BP(=O)(CC(=O)O)Nc1ccc(Nc2cncc(Br)c2F)cc1
BP(=O)(CC(O)(C(F)(F)F)[PH](F)(F)P(=O)(O)O[PH](O)(F)F)NO
BP(=O)(N(CC(N)=O)Cc1ccc(Br)cc1)P(=O)(Oc1ccccc1)Oc1ccccc1
BP(=O)(NN=Cc1ccc(Br)cc1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.3267974
Proportion of valid SMILES: 0.19125
Sample trajectories:
BP(=O)(N=C(N)Oc1ccc(Br)cc1Br)OCC
BP(=O)(O)CCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BP(=O)(OCC)n1c(Nc2ncnc3c(Br)cc(Br)cc23)nc2ccccc21
BP(=O)(OCC1NC(=N)N=C(N)O1)c1ccccc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(F)C1O)OC(C(=O)c1cc(F)cc(Br)c1)C(F)(F)F
Fine tuning...
Mean value of predictions: 0.54524314
Proportion of valid SMILES: 0.2958098811757348
Sample trajectories:
BP(=O)(OC(C)(F)F)c1cc(Br)c2c(c1)NC=C(N1C=CC(=O)N1)C(=O)O2
BP(=O)(OCC)OC(=O)CCS
BP(=O)(OCC)Oc1ccc(Nc2ncnc3sc(Br)cc23)cc1
BP1(=O)NP(=O)(OCC)OC(CCP(=O)(O)O)C(n2cnc3c(N)ncnc32)O1
Bc1cc(Br)ccc1Br

 20 Training on 14396 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.887766
Reward: 6.972285
Trajectories with max counts:
999	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.5436567
Proportion of valid SMILES: 0.1675
Sample trajectories:
BP(=O)(Br)Br
BP(=O)(NO)c1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)c(Br)c1)c1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cc1)c1c(N)ncnc1Cl
BP(=O)(OC(Br)CBr)OP(=O)(O)OP(=O)(O)O
Policy gradient replay...
Mean value of predictions: 0.52404577
Proportion of valid SMILES: 0.16375
Sample trajectories:
BP(=O)(NO)P(=O)(NO)c1ccc(Br)c(Br)c1
BP(=O)(Nc1ccc(Cl)c(Br)c1)Oc1ccc(Br)cc1
BP(=O)(Nc1ccc(Nc2nc3ccc(Br)cc3s2)cc1)c1ccc(Br)cc1
BP(=O)(OCC)Oc1cc(N)nc(Br)c1
BP(=O)(OCC1Nc2c(Br)cc(Br)cc2OC(=O)O1)C(Cl)=NO
Fine tuning...
Mean value of predictions: 0.5084706
Proportion of valid SMILES: 0.265625
Sample trajectories:
BP(=O)(N(O)Cc1ccc(Br)c(Br)c1)P(=O)(Oc1ccc(F)cc1)Oc1ccc(Br)cc1
BP(=O)(OC(F)Cl)c1ccc(OP(=O)(O)O)cc1
BP(=O)(OCC(=O)Nc1ccc(Br)cc1)Oc1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1
B[PH](=O)(=NO)N(O)CSc1nc2c(F)c(F)cc(F)c2s1
B[PH](=O)(Nc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O

Trajectories with max counts:
1988	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.4224138
Proportion of valid SMILES: 0.19575
Mean Internal Similarity: 0.5176349676064322
Std Internal Similarity: 0.11773832938319559
Mean External Similarity: 0.4046284102103635
Std External Similarity: 0.08102665271586021
Mean MolWt: 413.43195104895113
Std MolWt: 96.88483736085078
Effect MolWt: -0.8834653856160407
Mean MolLogP: 5.1741681196581215
Std MolLogP: 1.3091747425570577
Effect MolLogP: 0.34186753123957814
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 93.922652% (340 / 362)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5608.050706148148, 'valid_fraction': 0.19575, 'active_fraction': 0.4109195402298851, 'max_counts': 1988, 'mean_internal_similarity': 0.5176349676064322, 'std_internal_similarity': 0.11773832938319559, 'mean_external_similarity': 0.4046284102103635, 'std_external_similarity': 0.08102665271586021, 'mean_MolWt': 413.43195104895113, 'std_MolWt': 96.88483736085078, 'effect_MolWt': -0.8834653856160407, 'mean_MolLogP': 5.1741681196581215, 'std_MolLogP': 1.3091747425570577, 'effect_MolLogP': 0.34186753123957814, 'generated_scaffolds': 362, 'novel_scaffolds': 340, 'novel_fraction': 0.9392265193370166, 'save_path': '../logs/replay_combo_s2-8.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.912507
Reward: 1.000000
Mean value of predictions: 0.0012485369
Proportion of valid SMILES: 0.8029448621553885
Sample trajectories:
BP(=O)(NCc1ccc2c(c1)C=CC(=O)O2)P(=O)(O)O
BrCC(ON=C(c1ccccc1)c1ccc(CNc2nc(I)cs2)cc1)N1CCCCC1
Brc1ccc(CNc2ncccn2)nc1
C#CCC#Cc1cc(C(C#CCCCCCC)=CC(=O)OC)ccc1O
C#CCCC(NCc1ccccc1)C(OC(=O)NC(NC1CCCCC1)C(O)COC(=O)Cc1ccc(C(=O)NCCO)nc1)C(=O)NCC1CCCO1
Policy gradient replay...
Mean value of predictions: 0.00015791552
Proportion of valid SMILES: 0.7960402262727844
Sample trajectories:
Brc1ccc(-c2nn3ccccc3c2Br)cc1
Brc1cccc(Nc2nc3ccccc3nc2-c2ccccc2)c1
C#CC1OC(=O)C(CC)c2c(C(C)(C)C)cc(c3ccccc3)nc2SCC(=O)N1C
C#CC1c2ccccc2C2=CC(c3ccccc3)CC(=O)NC1O2
C#CCC(=O)OCP(=O)(NC(C)C(=O)OCC)c1ccc2c(c1)OCO2
Fine tuning...
Mean value of predictions: 0.027381523
Proportion of valid SMILES: 0.6544486215538847
Sample trajectories:
Brc1ccc(-c2ccc3c(c2)Nc2ccccc2-3)cc1
Brc1ccc(CSc2nc[nH]n2)cc1Br
Brc1ccc(N=C2Oc3ccc(Br)cc32)cc1
Brc1ccc2c(c1)-c1cccnc1CN2Cc1ccc(I)cc1
Brc1ccc2c(c1)[nH]c1ncnc(-c3cnccn3)c12

  2 Training on 304 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.567367
Reward: 1.139302
Trajectories with max counts:
6	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.021773815
Proportion of valid SMILES: 0.6690737833594976
Sample trajectories:
BP(=O)(CCC(=NC(=O)OCc1ccc(Br)cc1)N(Cc1cc(Cl)ccc1Br)C(=O)OC(C)C)OCC
Brc1ccc(CNc2ccc(Nc3nccc(-c4ccccc4)n3)cc2)nc1
Brc1ccc(N2CCCc3cc(CCN4CCOCC4)ccc3O2)nc1
Brc1ccc(N2Cc3ccccc3N=C2c2ccccc2)cc1
Brc1ccc(Nc2cnc(Sc3ccccc3)nc2)cc1
Policy gradient replay...
Mean value of predictions: 0.024134617
Proportion of valid SMILES: 0.6520376175548589
Sample trajectories:
BrCCNc1nc2ccc(Br)cc2s1
Brc1c(-c2ccccc2)sc2ccccc12
Brc1ccc(Nc2c(-c3ccc4[nH]ccc4c3)cnc3ccccc23)cc1
Brc1ccc(Nc2nc3cc(Br)ccc3[nH]2)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.06691729
Proportion of valid SMILES: 0.5840652446675031
Sample trajectories:
BrC1=CC(N2CCCCC2)=NN=C1c1ccc(Br)cc1
Brc1cc(Br)nc(Nc2ccnc(Nc3cccnc3)c2)n1
Brc1ccc(Br)c(-c2ncnc3[nH]ccc23)c1
Brc1ccc(CN(Cc2ccsc2)C2Cc3ccccc3Nc3ccccc32)cc1
Brc1ccc(N2CCN(Cc3ccco3)CC2)cc1

  3 Training on 637 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.669138
Reward: 1.232604
Trajectories with max counts:
6	COc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.089651026
Proportion of valid SMILES: 0.52149356761845
Sample trajectories:
BP(=O)(OCC1OC(CO)C(O)C(O)C1O)OP(=O)(O)OP(=O)(O)OCC1OC(N(C2CCCCCC2)C(C(N)=O)C(OP(=O)(O)O)N(=O)=O)CC1O
Brc1cc2[nH]c(-c3ccnc4ccccc34)nc2s1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc4c5nccc(ccc23)CC(=N5)CO4)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.09522643
Proportion of valid SMILES: 0.5128688010043942
Sample trajectories:
Brc1ccc(Br)c(Nc2ncnc3sc4c(c3C23CCOC3)CCCC4)c1
Brc1ccc(N(C2CC2)C2CCN(c3nccs3)CC2)cc1
Brc1ccc(Nc2ccc(Nc3cscn3)cc2)cc1
Brc1ccc(Nc2cccnc2Nc2ncnc3cnc(Oc4ccc(Br)cc4)cc23)cc1
Brc1ccc(Nc2nc(-c3ccccc3)cs2)cc1
Fine tuning...
Mean value of predictions: 0.13930294
Proportion of valid SMILES: 0.5840901973066082
Sample trajectories:
Brc1cc2ncn(CC3=NCCN3)c2cc1Br
Brc1ccc(-c2ccncc2)c(CNc2ccc(NC3CC3)nc2)c1
Brc1ccc(C2=NN(c3nccs3)CC2)cc1
Brc1ccc(NN=C2c3ccccc3C2c2ccccc2)cc1
Brc1ccc(Nc2c3ccc(Br)cc3cc3cc(I)ccc23)cc1

  4 Training on 1433 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 23.758443
Reward: 1.639322
Trajectories with max counts:
15	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.21433447
Proportion of valid SMILES: 0.46156269691241336
Sample trajectories:
Brc1cc(Nc2ncnc3ccsc23)c2ccccc2n1
Brc1ccc(Nc2ccc3ncnc(Nc4ccc(Br)cc4)c3c2)cc1
Brc1ccc(Nc2ccncc2)nc1
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.20545194
Proportion of valid SMILES: 0.4385026737967914
Sample trajectories:
Brc1cc(Br)cc(Nc2nc3cc(Br)ccc3s2)c1
Brc1cc2c(cn1)OCO2
Brc1ccc(Nc2cc(Br)cc3ncnc(Nc4ccc(Br)o4)c23)cc1
Brc1ccc(Nc2cn3cc(-c4ccncc4)n3cn2)cn1
Brc1ccc(Nc2ncnc3[nH]c(-c4cnc5ccncc5c4)nc23)cc1
Fine tuning...
Mean value of predictions: 0.18487753
Proportion of valid SMILES: 0.5872420262664165
Sample trajectories:
BrSc1ccc(Nc2ncnc3ncsc23)cc1
Brc1cc(Br)c2nc(Nc3ccccc3Br)sc2c1
Brc1ccc(-c2ccccc2Br)cc1Br
Brc1ccc(C=NNc2ncnc3nsnc23)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4cccc(Br)c4)c3c2)cc1

  5 Training on 2691 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 24.729843
Reward: 2.640196
Trajectories with max counts:
169	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.20124687
Proportion of valid SMILES: 0.50125
Sample trajectories:
Brc1ccc(Nc2ccc(Nc3cccnc3)cc2)cc1
Brc1ccc(Nc2ccc3ncsc3c2)cc1
Brc1ccc(Nc2ccnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(Nc2ncnc3ccsc23)cn1
Policy gradient replay...
Mean value of predictions: 0.20773482
Proportion of valid SMILES: 0.5090625
Sample trajectories:
BP(=O)(NO)c1cccc(Br)c1
BrNc1cccc(Br)c1-c1ccc(Nc2ccc3ccccc3n2)cc1
Brc1ccc(Br)c(Nc2ccccc2Nc2ccccc2Br)c1
Brc1ccc(NN=Cc2cccs2)nc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.27565715
Proportion of valid SMILES: 0.54858934169279
Sample trajectories:
Brc1cc(Nc2c(Br)cnc3ccsc23)ncn1
Brc1cc(Nc2cncnc2)cc2ccccc12
Brc1cc(Nc2ncnc3sccc23)ccc1I
Brc1cc2c(cc1Nc1cccc(Nc3ncnc4ccccc34)c1)Sc1ccccc1N2
Brc1ccc(Br)c(Nc2nc3ccsc3nc2N2CCN(Cc3ccccc3)CC2)c1

  6 Training on 3988 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 26.047955
Reward: 3.463039
Trajectories with max counts:
356	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.43770885
Proportion of valid SMILES: 0.39305816135084426
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Oc2c(Br)cc(Nc3ncnc4scnc34)cc2Br)cc(Br)c1Br
Brc1cc2c(s1)-c1c(ccc(Br)c1Br)N2
Brc1ccc(Nc2ccc(Nc3ncnc4ccsc34)cc2)c(Br)c1
Brc1ccc(Nc2ccc(Nc3ncnc4cnc(Nc5ccccc5)cc34)nc2)cc1
Policy gradient replay...
Mean value of predictions: 0.4178093
Proportion of valid SMILES: 0.3968105065666041
Sample trajectories:
Brc1ccc(N(c2cncnc2)c2ncnc3ccsc3s2)cc1
Brc1ccc(Nc2cc3c(Nc4ccc(Br)cc4Br)ncnc3s2)cc1
Brc1ccc(Nc2ccc3nc(Nc4ccsc4)nc(Nc4ccc(Br)cc4)c3ncn2)cc1
Brc1ccc(Nc2nc(Nc3ccccc3)nc(Nc3ccc(Br)cc3)n2)cc1
Brc1ccc(Nc2ncc(Br)c(Nc3ccc(I)cc3Br)c(-c3ccc(Br)cc3)c3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.31936038
Proportion of valid SMILES: 0.5492471769134254
Sample trajectories:
Brc1cc(Nc2ncnc3cccc(Br)c23)cs1
Brc1cc2sc(Nc3ccccc3)ncc2c1Nc1ccc2ncnc(-c3ccccc3Oc3ccccc3)c2c1
Brc1cc2scnc2c2c1sc1ccccc12
Brc1ccc(Br)c(N2CCNc3ncnc(Nc4cccc5cc(Br)ccc45)c3C2)c1
Brc1ccc(Br)c(Nc2ncnc3scc(-c4ccccc4)c23)c1

  7 Training on 5765 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 27.673607
Reward: 4.512903
Trajectories with max counts:
228	Fc1ccc(Nc2ncnc3ccsc23)cc1F
Mean value of predictions: 0.4939248
Proportion of valid SMILES: 0.3242651657285804
Sample trajectories:
Bc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(Nc2ncnc3scc(-c4ccccc4)c23)ccc1CNc1ccccn1
Brc1cc(Nc2ncnc3sccc23)ccc2ncnc12
Brc1ccc(-c2nccnc2CNCc2cccs2)cc1
Policy gradient replay...
Mean value of predictions: 0.51268154
Proportion of valid SMILES: 0.32301438398999377
Sample trajectories:
Brc1cc(Br)c(Oc2ccc(Nc3ncnc4sc5ccccc5c34)nc2)c(Br)c1Br
Brc1cc(Br)cc(Nc2ncnc3ccc(Nc4cc(Br)ccc4Br)cc23)c1
Brc1ccc(Nc2ccnc3ccsc23)cc1
Brc1ccc(Nc2ncnc(Nc3ccc4ccccc4c3)n2)c(Br)c1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.3509123
Proportion of valid SMILES: 0.5311034698343232
Sample trajectories:
BP(=O)(NO)C(=O)Nc1cc(Br)sc1Br
BP(=O)(OCC)OC(=O)CC(F)(F)P(=O)(OCO)OC(=O)C(F)(F)F
BrN=CNc1ncnc2sc3ccccc3c12
Brc1cc(Br)c(Nc2ncnc3sccc23)s1
Brc1cc(Br)cc(Nc2cc(Br)cc(Br)c2Br)c1

  8 Training on 7425 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 24.744972
Reward: 4.917107
Trajectories with max counts:
773	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.5065963
Proportion of valid SMILES: 0.23694904657705532
Sample trajectories:
BP(=O)(Nc1cc(Br)ccc1Br)c1cc(Br)c(Br)c(Br)c1
Brc1cc(Nc2ncnc3sccc23)cnc1Nc1ncnc2sccc12
Brc1ccc(Nc2ncnc3ccc(-c4sccc4Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.5094945
Proportion of valid SMILES: 0.2535167239762426
Sample trajectories:
BrCCc1ncnc2sc3c(Br)cccc3c12
Brc1cc(Br)c(Nc2cc(Br)c(Br)cc2Nc2ncnc3cc(Br)ccc23)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3sccc23)cc1Br
Brc1cc(Br)nc(Nc2ncnc3sccc23)c1
Brc1cc(Nc2ncnc3sccc23)sc1Br
Fine tuning...
Mean value of predictions: 0.36193353
Proportion of valid SMILES: 0.5183213279047917
Sample trajectories:
BrI
Brc1cc(Br)cc(Nc2ccc(Br)c(Br)c2)c1
Brc1cc(Br)cc(Nc2nc3cccc(Br)c3s2)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Nc2ccccc2I)cc2cnccc12

  9 Training on 8755 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 23.277679
Reward: 5.146718
Trajectories with max counts:
780	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.5436451
Proportion of valid SMILES: 0.2607064707721163
Sample trajectories:
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1ccc(-c2sc3[nH]cnc3c2Br)s1
Brc1ccc(Nc2cc(Nc3cccc(Br)c3)ncn2)cc1
Brc1ccc(Nc2csc3ncnc(Nc4ccc(Br)cc4)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.54993516
Proportion of valid SMILES: 0.2411635908664373
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Nc4ccccc4Br)sc23)c(Br)c1
Brc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(Nc2ncnc3sc4ccccc4c23)cc1
Brc1ccc(Nc2ncnc3scc(-c4ccccc4)c23)cc1
Brc1ccc(Nc2ncnc3scc(-c4cccnc4)c23)cc1
Fine tuning...
Mean value of predictions: 0.4122209
Proportion of valid SMILES: 0.532540675844806
Sample trajectories:
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)cc(Nc2ncnc3scnc23)c1
Brc1cc(Nc2ccc(Nc3ncnc4csc(Br)c34)cc2)ncn1
Brc1cc(Nc2ccnc3ccc(Br)cc23)c2c(Nc3ccccc3)ncnc2c1
Brc1cc(Nc2ncnc3cccc(Br)c23)nc2ccccc12

 10 Training on 9867 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.236743
Reward: 5.529917
Trajectories with max counts:
799	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.51348317
Proportion of valid SMILES: 0.278125
Sample trajectories:
Brc1cc(Br)c2ncsc2c1
Brc1cc(Nc2ncnc3sc4ccccc4c23)nc(-c2cccnc2)c1
Brc1ccc(Nc2ccncc2)cc1
Brc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3sccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.5329584
Proportion of valid SMILES: 0.2778125
Sample trajectories:
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Nc2cc(Nc3ncnc4sccc34)ccc2Br)cc1Br
Brc1cc(Nc2ncnc3sc4ccccc4c23)nc2sccc12
Brc1ccc(-c2ccc(Nc3ncnc4sccc34)cc2)cc1
Brc1ccc(Nc2ncnc3sc(-c4ccccc4)cc23)cc1
Fine tuning...
Mean value of predictions: 0.450032
Proportion of valid SMILES: 0.4890488110137672
Sample trajectories:
BP(=O)(NC(c1ccc(Br)cc1F)[PH](=O)(Br)(Br)OCC(F)(F)F)Oc1ccc(F)cc1
BP(=O)(c1ccc(F)cc1F)N(CCCl)Nc1ccccc1F
Brc1ccc(Br)c(Nc2ccccc2Nc2ncnc3ccsc23)c1
Brc1ccc(NC(=Nc2ncnc3ccccc23)c2ccccc2)cc1
Brc1ccc(Nc2cccc3ncnc(-c4ccccc4)c23)cc1Br

 11 Training on 11051 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.153705
Reward: 5.321851
Trajectories with max counts:
312	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.6060206
Proportion of valid SMILES: 0.3321875
Sample trajectories:
Bc1cc(Br)cc(Br)c1Br
Brc1ccc(Nc2cc(Nc3ccsc3)ncn2)cc1
Brc1ccc(Nc2ncnc3c(Br)cc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccsc23)cc1Br
Brc1ccc(Nc2ncnc3sc(Br)cc23)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.57971835
Proportion of valid SMILES: 0.3329165364176305
Sample trajectories:
Brc1cc(I)ccc1Nc1ncnc2sccc12
Brc1ccc(Nc2cc3c(Nc4ccc(Br)cc4)ncnc3s2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(NC4=CCCNCC4)nc23)s1
Brc1ccc(Nc2ncnc3ccsc23)c(Br)c1
Brc1ccc(Nc2ncnc3ccsc23)cc1I
Fine tuning...
Mean value of predictions: 0.44578603
Proportion of valid SMILES: 0.5229759299781181
Sample trajectories:
BP(=O)(c1ccc(F)cc1Nc1ccc(Nc2cccc(F)c2F)cc1F)N1C=CC(N(=O)=O)=N1
BrCc1cccc2[nH]cc(-c3ccccc3)c12
Brc1cc(Br)c(Nc2ccnc(Nc3ncnc4sccc34)c2)c(Br)c1Br
Brc1cc(Br)cc(Nc2ncnc3sccc23)c1
Brc1cc(Nc2ncnc3cc(I)c(Br)cc23)c(Br)s1

 12 Training on 12512 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.948805
Reward: 5.380114
Trajectories with max counts:
460	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6274694
Proportion of valid SMILES: 0.3321875
Sample trajectories:
BP(=O)(NO)C(=O)c1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(I)cc(Nc2ncnc3ccsc23)c1
Policy gradient replay...
Mean value of predictions: 0.5996161
Proportion of valid SMILES: 0.3257267896217568
Sample trajectories:
Bc1ccc(Nc2ncnc3sccc23)cc1
Brc1cc(Br)c(Nc2ccc3ncnc(Nc4cccs4)c3n2)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(Br)cc(Nc2ncnc3scc(-c4ccccc4)c23)c1
Brc1cc(Nc2ncnc3ccsc23)ncn1
Fine tuning...
Mean value of predictions: 0.4472534
Proportion of valid SMILES: 0.5290625
Sample trajectories:
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)c2c(c1)Nc1ccccc1N2
Brc1cc(Br)c2ccc(Nc3ncnc4ccccc34)cc2c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1

 13 Training on 14020 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.418945
Reward: 5.796792
Trajectories with max counts:
298	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.66487557
Proportion of valid SMILES: 0.31416067521100344
Sample trajectories:
Brc1cc(Nc2ncnc3sc(Br)cc23)cs1
Brc1ccc(Br)c(Nc2ncnc3ccccc23)c1
Brc1ccc(Br)c(Nc2ncnc3ccsc23)c1
Brc1ccc(Nc2c(I)cc(Nc3ncnc4sccc34)cc2I)cc1
Brc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.6561049
Proportion of valid SMILES: 0.3096875
Sample trajectories:
Brc1c(Nc2ncnc3sc(Cc4ccsc4)cc23)sc2ccccc12
Brc1cc(Br)c(-c2ncccc2CN2CCN(Cc3ccco3)CC2)c(Br)c1Br
Brc1cc(Br)c(Nc2ncnc3sccc23)cn1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ncnc3cc(Br)c(Nc4ccsc4)cc23)cc1
Fine tuning...
Mean value of predictions: 0.48070842
Proportion of valid SMILES: 0.4940625
Sample trajectories:
Bc1ccc2c(Nc3ccc(I)cc3Cl)ncnc2c1
BrCCSc1ccc(Nc2ncnc3sccc23)cc1
Brc1cc(Br)c2ncnc(Nc3ccc(Nc4ccccc4Br)cc3)c2c1
Brc1cc(Br)cc(Nc2cccc(Br)c2Br)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1

 14 Training on 15608 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.569250
Reward: 6.078934
Trajectories with max counts:
689	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.5435443
Proportion of valid SMILES: 0.246875
Sample trajectories:
Brc1cc(-c2cccc3occc23)ccc1Nc1ncnc2ccccc12
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.55012286
Proportion of valid SMILES: 0.254375
Sample trajectories:
Brc1cc(Br)c(Nc2ccccc2Br)c(Br)c1
Brc1cc(Nc2ncnc3cscc23)cs1
Brc1cc(Nc2ncnc3sccc23)ccc1Nc1ncnc2sc3ccccc3c12
Brc1ccc(C=Nc2ncnc3sccc23)cc1
Brc1ccc(N2CCN(Cc3ccccc3Br)CC2)cc1
Fine tuning...
Mean value of predictions: 0.5034826
Proportion of valid SMILES: 0.5026570803376055
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCC(=O)Oc1ccc(Br)cc1Br
BP(=O)(OCCS(=O)(=O)OP(=O)(O)OP(=O)(O)P(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O)P(=O)(O)CCCl
B[PH](=O)(c1ccccc1)(c1cccc(Br)c1)N(O)C(Cc1ccc(Br)c(Br)c1)Nc1ccc(Br)cc1
Bc1cc(Br)cc2c1NOC(=O)N(C1CCCN(C(=O)c3cccc(Br)c3)C1)N=C2
Bc1ccc(Nc2ncnc3scc(-c4ccccc4)c23)cc1

 15 Training on 16868 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.541709
Reward: 5.903231
Trajectories with max counts:
225	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.6027027
Proportion of valid SMILES: 0.346875
Sample trajectories:
BP(=O)(COC(=O)c1ccc(Br)cc1)Nc1ccc(F)c(F)c1
B[PH](=O)(=NO)OC(C)=O
BrC(=Nc1ccc(Nc2ncnc3sccc23)cc1)c1cccc(Br)c1
BrC(I)=C(I)I
BrC=NN1C(c2cccs2)N1c1ncnc2sccc12
Policy gradient replay...
Mean value of predictions: 0.5880486
Proportion of valid SMILES: 0.3347921225382932
Sample trajectories:
BP(=O)(NO)S(=O)(=O)CCOP(=O)(Oc1ccccc1)Oc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3sccc23)cc1
BrC(Br)Br
BrCc1c(Nc2ccccc2Br)cc(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Br)c1Br
Fine tuning...
Mean value of predictions: 0.554768
Proportion of valid SMILES: 0.4856070087609512
Sample trajectories:
Br
Brc1cc(Br)c(Nc2ncnc3ccsc23)cc1Br
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(Br)cc(Nc2ncnc3sc(Br)cc23)c1
Brc1cc2c(Nc3cc(Br)c(Br)s3)ncnc2s1

 16 Training on 18533 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.682812
Reward: 5.866069
Trajectories with max counts:
785	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.42134568
Proportion of valid SMILES: 0.269375
Sample trajectories:
BP(=O)(CCCC)C(=O)NCC=Nc1ccc(Nc2ncnc3sc(-c4ccccc4)nc23)cc1
BP(=O)(Nc1ccccc1)P(=S)(c1ccccc1)c1ccccc1
BP(=O)(Nc1ccccc1Br)P(=O)(O)N(=O)=O
BP(=O)(Oc1ccccc1)N(CC=C)[PH](=O)(=O)Oc1ccccc1
BP(=O)(c1ccccc1Br)N(CC=C)c1ccccc1
Policy gradient replay...
Mean value of predictions: 0.42408255
Proportion of valid SMILES: 0.2725
Sample trajectories:
BP(=O)(Nc1cccc(Nc2ncnc3scnc23)c1)C1OC(N2C=CC(N)=NC2(F)F)C(O)C1O
BP(=O)(c1ncnc2sccc12)N1CCN(C(=O)c2ccccc2Br)CC1
Bc1ccc(Nc2ncnc3sccc23)cc1Br
Bc1cccc(Nc2ncnc3sccc23)c1
Bc1ccccc1Nc1ncnc2sccc12
Fine tuning...
Mean value of predictions: 0.5771706
Proportion of valid SMILES: 0.5151704723177979
Sample trajectories:
BP(=O)(NCCCCCCCN)C(=O)Nc1cccc2c(Br)cc(Br)cc12
BP(=O)(OCC)ON(=O)=O
Bc1ccc(N(c2ccccc2Cl)c2ncnc3ccsc23)c(F)c1
BrC(=NN=C1CCCCN1)c1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Br)c(Br)n1

 17 Training on 19812 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.777985
Reward: 6.285581
Trajectories with max counts:
387	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6335911
Proportion of valid SMILES: 0.2828125
Sample trajectories:
BP(=O)(COC(=O)c1ccccc1)Nc1ccc(Br)cc1Br
BP(=O)(NC(c1ccccc1)c1ccc(Br)cc1)P(=O)(Oc1ccc(F)cc1)Oc1cccc(F)c1F
BP(=O)(NN(=O)=O)C(=O)Oc1cc(Br)c(Br)c(Br)c1Br
BP(=O)(Nc1cc(Br)c(Br)cc1Br)OCc1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)c(Br)c1)c1cc(Br)cc(Br)c1N(=O)=O
Policy gradient replay...
Mean value of predictions: 0.63053095
Proportion of valid SMILES: 0.2825
Sample trajectories:
BP(=O)(Cl)P(=O)(C(Nc1cc(Br)oc1-c1ccc(F)c(F)c1)C(F)(F)F)P(F)(F)(F)F
BP(=O)(NC(=O)Oc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1)c1ccccc1F
BP(=O)(NCCCCO)C(=O)Nc1ccc(F)c(F)c1F
BP(=O)(NOC(=O)c1ccc(Br)s1)Oc1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cc1)P(=O)(O)Oc1cc(Br)c(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.57238096
Proportion of valid SMILES: 0.525328330206379
Sample trajectories:
BP(=O)(Nc1ccc(I)cc1)C(=O)Nc1cc(Br)c(Br)c(Nc2ncnc3c(F)c(F)c(F)c(F)c23)c1
Bc1ccc(Nc2ncnc3sccc23)cc1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(Br)cc(Nc2ncnc3sc(Nc4c(Br)cccc4Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3sccc23)c1

 18 Training on 21498 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.407076
Reward: 6.168269
Trajectories with max counts:
172	Fc1ccc(Nc2ncnc3sccc23)cc1F
Mean value of predictions: 0.7555133
Proportion of valid SMILES: 0.32875
Sample trajectories:
Brc1cc(Nc2ncnc3sc(Br)cc23)cs1
Brc1cc2c(Nc3cc(Br)n(Cc4sccc4Br)c3)ncnc2s1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ccc(Nc3ncnc4sc5ccccc5c34)cc2Br)cc1
Brc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.7269663
Proportion of valid SMILES: 0.33375
Sample trajectories:
Brc1cc(Nc2ncnc3ccsc23)cs1
Brc1cc(Nc2ncnc3cscc23)cs1
Brc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(Nc2ncnc3nsc(Nc4cccs4)c23)cc1I
Brc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.538115
Proportion of valid SMILES: 0.5275171982489055
Sample trajectories:
Bc1ccc(Nc2ncnc3sccc23)cc1Br
Br
BrBr
BrCc1ccc(Nc2ncnc3ccccc23)cc1
BrIc1cccc(Nc2cnc3ccccc3n2)c1

 19 Training on 23533 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.942904
Reward: 6.324195
Trajectories with max counts:
312	Fc1ccc(Nc2ncnc3sccc23)cc1F
Mean value of predictions: 0.58011174
Proportion of valid SMILES: 0.2796875
Sample trajectories:
Brc1cc(Nc2ncnc3sccc23)cs1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(Nc2ncnc3ccsc23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.56467944
Proportion of valid SMILES: 0.2778125
Sample trajectories:
BP(=O)(C=NNc1nc(N(=O)=O)nc2sccc12)N(=O)=O
Brc1cc(Nc2ncnc3ccc(Br)cc23)c2sccc2c1
Brc1cc(Nc2ncnc3sccc23)sc1Br
Brc1ccc(Nc2ccc(Br)s2)s1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.5878313
Proportion of valid SMILES: 0.5193992490613266
Sample trajectories:
BP(=O)(NCCO)c1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1cc(Nc2ncnc3ccsc23)cc(Br)c1Br
Bc1cccc(Nc2ncnc3sc(Br)cc23)c1
Brc1cc(Br)c(Br)c(C=NNc2ccc(Br)s2)c1
Brc1cc(Br)c(Nc2ncnc3ccsc23)cc1NCc1cccs1

 20 Training on 25010 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.030961
Reward: 6.091231
Trajectories with max counts:
311	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5370404
Proportion of valid SMILES: 0.3484375
Sample trajectories:
BS(=O)(=O)Nc1ccc2c(Nc3ccc(F)cc3F)cccc2c1
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrC#CCBr
BrC(Br)Br
BrCCC[N+]12Oc3ccccc3C1c1ccccc12
Policy gradient replay...
Mean value of predictions: 0.5607955
Proportion of valid SMILES: 0.33
Sample trajectories:
BP(=O)(Oc1cc(Nc2ncnc3cc(Br)c(Br)cc23)c2c(F)cc(F)cc2c1)OC(C)C
Bc1ccc(Nc2ncnc3ccc(NCP(=O)(O)O)cc23)cc1
BrC1=Nc2ccccc2Nc2ncnc(Nc3ccc(Br)cc3)c2N1
BrC=C1Oc2ccccc2-c2ncnc(Nc3cccc(Br)c3)c2Nc2ncccc21
Brc1c[nH]c2ccc(Nc3ccc4ccccc4c3)cc12
Fine tuning...
Mean value of predictions: 0.5870477
Proportion of valid SMILES: 0.4923413566739606
Sample trajectories:
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
Brc1cN(c2nc3c(-c4ccccc4Br)cccc3[nH]2)cc1
Brc1cc(Br)c(Nc2ncnc3cccc(Br)c23)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3ccsc23)c(Br)c1
Brc1cc(Br)c2cc(Nc3ncnc4ccc(Br)cc34)ccc2c1

Trajectories with max counts:
318	Fc1ccc(Nc2ncnc3sccc23)cc1F
Mean value of predictions: 0.5468377
Proportion of valid SMILES: 0.4141660415103776
Mean Internal Similarity: 0.4861321563050682
Std Internal Similarity: 0.10704532800605107
Mean External Similarity: 0.4037429283829737
Std External Similarity: 0.060367910681528233
Mean MolWt: 380.9031692174416
Std MolWt: 83.03174140630014
Effect MolWt: -1.1377697792858275
Mean MolLogP: 5.079348062373666
Std MolLogP: 1.320730238235205
Effect MolLogP: 0.27070089584904894
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.365854% (998 / 1025)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 100, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 6047.539115905762, 'valid_fraction': 0.4141660415103776, 'active_fraction': 0.5227169811320754, 'max_counts': 318, 'mean_internal_similarity': 0.4861321563050682, 'std_internal_similarity': 0.10704532800605107, 'mean_external_similarity': 0.4037429283829737, 'std_external_similarity': 0.060367910681528233, 'mean_MolWt': 380.9031692174416, 'std_MolWt': 83.03174140630014, 'effect_MolWt': -1.1377697792858275, 'mean_MolLogP': 5.079348062373666, 'std_MolLogP': 1.320730238235205, 'effect_MolLogP': 0.27070089584904894, 'generated_scaffolds': 1025, 'novel_scaffolds': 998, 'novel_fraction': 0.9736585365853658, 'save_path': '../logs/replay_combo_s2-9.smi'}
