starting log


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.447887
Reward: 1.000000
Trajectories with max counts:
2	OCC1OC(CO)C(O)C(O)C1O
Mean value of predictions: 0.0016032064
Proportion of valid SMILES: 0.7811521603005636
Sample trajectories:
BP(=O)(O)OP(=O)(O)SCC(N)CC(=O)O
Brc1ccc(-c2cc(-c3cccnc3)on2)cc1
Brc1ccc(C=Nn2cnnn2)cc1
Brc1ccc(CN2C=Nc3ccc4c(c3N=C2c2ccncc2)OCO4)cc1
Brc1cccc2c1-c1ncccc12
Policy gradient replay...
Mean value of predictions: 0.038512036
Proportion of valid SMILES: 0.5732204452806522
Sample trajectories:
Bc1ncc(-c2ccc(Cl)cc2)c(N2CCC(N3CCOCC3)C2)c1COc1ccc2c(c1)OCO2
Brc1ccc(-c2cnc3cnc4ccsc4n23)cn1
Brc1ccc(Nc2cnc3ncncc3n2)cc1
Brc1ccc(Nc2cnccn2)cc1
Brc1ccc(Nc2ncc3ccccn23)cc1
Fine tuning...
Mean value of predictions: 0.029725183
Proportion of valid SMILES: 0.5584090197306608
Sample trajectories:
Brc1c(-c2n[nH]c3ccccc23)[nH]c2ccccc12
Brc1ccc(NN=Cc2cccnc2)cc1
Brc1ccc(Nc2cc(-c3cccnc3)ncn2)cc1
Brc1ccc(Nc2ncnc3cccc4ncncc4Nc3n2)cc1
Brc1ccc(Nc2nn[nH]n2)nc1

  2 Training on 405 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.689601
Reward: 1.128571
Trajectories with max counts:
4	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.038339704
Proportion of valid SMILES: 0.5738013162018176
Sample trajectories:
BrC1=COc2cc(-c3ccncc3)ccc2N1
Brc1cc2nccc(-c3ccc(N4CCN(Cc5ccccc5)C4)cc3)c2cn1
Brc1ccc(C=NN2CCCCC2)cc1
Brc1ccc(N=Nc2ncnc3c2nnn3CCOCCOCCOCc2ccccc2)cc1
Brc1ccc(Nc2ccc3ncc(-c4cc5nc(NCc6ccccc6Br)sc5o4)nc3c2)cc1
Policy gradient replay...
Mean value of predictions: 0.056972113
Proportion of valid SMILES: 0.47388294524858404
Sample trajectories:
BrCCBr
Brc1c[nH]c2ncnc(N3CCCCC3)c12
Brc1cc(CSc2ncnc3cccnc23)n[nH]1
Brc1cc(NN=C2CCOCC2)cnn1
Brc1cc(NN=Cc2ccncc2)no1
Fine tuning...
Mean value of predictions: 0.0683931
Proportion of valid SMILES: 0.47418136020151136
Sample trajectories:
BP(=O)(OCC)C(=O)c1cnc(N)c(I)c1
Brc1cc2c(c(I)c1Br)-n1ncnc1-2
Brc1cc2ncn(CCNc3ccc(Br)c4nonc34)n2n1
Brc1cc2ncnc(Nc3cc(OCCOCCN4CCCC4)c3)c2s1
Brc1cc2nncnc2c2nnn(-c3ccccc3)c12

  3 Training on 778 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.233516
Reward: 1.165297
Trajectories with max counts:
3	COc1cc2ncnc(Nc3ccc(Cl)c(Cl)c3)c2cc1OC
Mean value of predictions: 0.070765205
Proportion of valid SMILES: 0.4797615312205836
Sample trajectories:
BrCCCCBr
Brc1cc(Br)cc(Nc2nc(-c3nn4ncnc4cc3Br)n3ncnc3n2)c1
Brc1ccc(-c2nn(-c3nc(Br)cs3)nc2-c2ccc(CN3CCCCC3)cc2)cc1
Brc1ccc(Br)c(-c2ccc(Nc3ncnc4[nH]cnc34)cc2)c1
Brc1ccc(NC(c2cncnc2)c2ccncn2)cc1
Policy gradient replay...
Mean value of predictions: 0.04576639
Proportion of valid SMILES: 0.7205882352941176
Sample trajectories:
BP(=O)(Oc1ccccc1O)c1ccccc1
BP1c2ncnc(Nc3cccc4ccccc34)c2-c2ccccc2c2ccccc21
Bc1ncccc1SCC(=O)Nc1ccccc1Br
BrCCN1CCC(c2ccccc2Nc2ccccc2Br)CC1
Brc1ccc(-c2c3ccccc3cc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.048618782
Proportion of valid SMILES: 0.7353125
Sample trajectories:
BP(=O)(CCl)Nc1cccc(Nc2ccccc2)c1
Bc1ccccc1Nc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1c2ccccc2cc2ccccc12
Brc1cc2c(Nc3ccccc3Br)ncnc2cc1Nc1ccccc1
Brc1ccc(-c2c3ccccc3nc3cn(Cc4ccccc4Br)cc23)cc1

  4 Training on 1239 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.481241
Reward: 1.263773
Trajectories with max counts:
11	COc1cccc(Nc2ncnc3ccccc23)c1
Mean value of predictions: 0.047168218
Proportion of valid SMILES: 0.739375
Sample trajectories:
Brc1cc(Nc2ccccc2)cc2ccccc12
Brc1ccc(Nc2ccc3ccccc3n2)cc1
Brc1ccc(Nc2ccccc2-c2ccccc2)cc1
Brc1ccc(Nc2ccccc2Br)cc1
Brc1ccc(Nc2ccncc2)cc1
Policy gradient replay...
Mean value of predictions: 0.10234741
Proportion of valid SMILES: 0.665625
Sample trajectories:
BrC1=C2Sc3ccccc3CCN12
Brc1[nH]c2ccc(Nc3ccccc3)cc2c1Br
Brc1cc2c(Nc3ncnc4ccccc34)cccc2n1-c1ccccc1
Brc1cc2c(s1)-c1ccccc1N2
Brc1cc2nc(-c3ccccc3Br)c(-c3ccccc3)nc2s1
Fine tuning...
Mean value of predictions: 0.11456766
Proportion of valid SMILES: 0.6652078774617067
Sample trajectories:
BP(=O)(NC(c1ccccc1)c1ccccc1)P(=O)(O)O
BP(=O)(OCCCCCCCCCCCCCCCC=C)C(=O)Nc1ccc(Br)cc1
BrC=CC=CC=Cc1ccccc1Oc1ccccc1
Brc1cc2c(Nc3ccc(Nc4ccccc4)cc3)ncnc2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Nc1ccccc1

  5 Training on 2030 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 16.838157
Reward: 1.232207
Trajectories with max counts:
40	Clc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.14974359
Proportion of valid SMILES: 0.609375
Sample trajectories:
BP(=O)(CP(=O)(O)O)OCCCc1ccccc1
BP(=O)(OCC=C)c1ccccc1
BrC=Cc1ccc2ccc(Nc3ncccn3)cc2c1
BrCCN1c(-c2ccccc2Nc2ccccc2)cc2c(Nc3ccccc3)ncnc21
BrCc1ccccc1-c1nc2ccccc2s1
Policy gradient replay...
Mean value of predictions: 0.2017017
Proportion of valid SMILES: 0.6245701781806815
Sample trajectories:
BP(=O)(NCCCCCCCC(=O)OC1CCCC1)Oc1ccc(Br)cc1
BrCCNc1nc2ccccc2nc1-c1ccccc1
Brc1[nH]c2ccccc2c1-c1ccccc1-c1ccccc1
Brc1ccc(Nc2ncnc3ccc(-c4ccccc4Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Nc4ccccc4)cc23)cc1
Fine tuning...
Mean value of predictions: 0.19880655
Proportion of valid SMILES: 0.6284375
Sample trajectories:
BP(=O)(OCC1OC(=O)C(C)(O)C1O)c1ccccc1
Bc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Bc1ccccc1-c1nc2ccc(Cl)cc2nc1Nc1ccccc1
Brc1ccc(-c2cccc(-c3cnc4[nH]ccc4c3)c2)cc1-c1ccccc1
Brc1ccc(-c2ccccc2-c2ccccc2)s1

  6 Training on 3605 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 20.495550
Reward: 1.716137
Trajectories with max counts:
35	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.24094409
Proportion of valid SMILES: 0.6096340319049108
Sample trajectories:
Bc1cc2ccccc2cc1-c1nc2ccc(Br)cc2s1
Brc1ccc(-c2ccccc2-c2ccccc2)c(Nc2ccccc2Br)c1
Brc1ccc(-c2ccccc2-c2nc3ccccc3nc2-c2ccccc2)c(Br)c1
Brc1ccc(-n2nnnc2Nc2c(Br)n(-c3nc4ccccc4n4nnnc34)c3ccccc23)cc1
Brc1ccc(Nc2ccc(Nc3ncnc4ncnc(Nc5ccccc5Br)c34)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.2627451
Proportion of valid SMILES: 0.6058143169740544
Sample trajectories:
BP(=O)(OCC)Oc1ccccc1-c1ccccc1
Brc1ccc(-c2ccc3ncnc(Nc4ccccc4)c3c2)cc1
Brc1ccc(-c2ccsc2)c2ccccc12
Brc1ccc(-c2ncnc3ccc(Br)cc3C(c3ccccc3)c3ccccc3N2)cc1
Brc1ccc(C=Nc2ncnc3sc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.25579968
Proportion of valid SMILES: 0.6115625
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Nc4ccccc4)cc23)cc1
BrCc1cccc2ncnc(Nc3ccccc3)c12
Brc1cc(Nc2ncnc3ccccc23)c2ccccc2n1
Brc1cc(Nc2ncnc3cccnc23)ncn1
Brc1cc2c(Nc3nc(-c4ccccc4Br)nc4c3CCC4)ncnc2[nH]1

  7 Training on 5316 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 20.315972
Reward: 1.899608
Trajectories with max counts:
30	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.30017212
Proportion of valid SMILES: 0.5448577680525164
Sample trajectories:
BP(=O)(OCC)C(F)(F)C(F)(F)F
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc2c(-c3ccc4c(Nc5ccccc5)ncnc4c3)c(Br)ccc2c(Br)c1Br
Brc1cc2c(Nc3cccc(CN4CCCC4)c3)ccnc2s1
Policy gradient replay...
Mean value of predictions: 0.3088634
Proportion of valid SMILES: 0.599375
Sample trajectories:
BP(=O)(N(NC(=O)c1ccc(Nc2ncnc3c(Nc4cccc(Br)c4)c4ccccc4sc3c2N)cc1)Nc1ccc(Nc2ccccc2)cc1)N(=O)=O
BP(=O)(OC)OCCCCF
BP(=O)(Oc1ccc(Br)cc1)N1CCOCC1
Bc1ccc(CNc2ncnc3c(Br)cccc23)cc1
Bc1cccc(Nc2ncnc3cc(-c4ccccc4)c23)c1
Fine tuning...
Mean value of predictions: 0.29716122
Proportion of valid SMILES: 0.5838023764853033
Sample trajectories:
BP(=O)(OCCCCCCCCCCC)C(=O)O
Bc1cccc(I)c1-c1ccccc1-c1ccccc1I
BrCc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1ccccc1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1

  8 Training on 7089 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 20.629904
Reward: 1.976837
Trajectories with max counts:
55	COc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.2426448
Proportion of valid SMILES: 0.610641627543036
Sample trajectories:
BP(=O)(NCCCOC(=O)C(CCCNc1ccc(Br)cc1)NC(=O)OCc1ccccc1)OCC
BP(=O)(OCCCc1ccccc1)N1C=CN(CC[PH](=O)c2ccccc2)c2ccccc21
BP(=O)(ON1CCOCC1)c1c(Br)cc(Oc2ccc(Br)cc2Br)nc1-c1ccccc1
BrCCN(c1ccccc1)c1ccc2scnc2c1
BrCN1CCCCC1
Policy gradient replay...
Mean value of predictions: 0.30177036
Proportion of valid SMILES: 0.6178125
Sample trajectories:
BP(=O)(CCN=C(N)N)Nc1ccc2c(Nc3cccc4ccccc34)cc(F)cc2c1
Bc1ccccc1-c1nc2ccccc2s1
Brc1cc2ncnc(Nc3ccccc3)c2c2ccccc12
Brc1ccc(Br)c(Nc2cc(Br)c3cc(Br)ccc3n2)c1
Brc1ccc(Nc2nc3ccccc3s2)cc1
Fine tuning...
Mean value of predictions: 0.278396
Proportion of valid SMILES: 0.6238273921200751
Sample trajectories:
BP(=O)(CCCN1CC(F)(F)C(F)(F)C1)NCC(F)(F)F
Bc1ccc2ccccc2c1Nc1ccccc1Oc1ccccc1
Bc1ccccc1Nc1ncnc2ncnc(Nc3ccccc3)c3ccccc3c12
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Brc1ccc(-c2ccccc2-c2cccc(Nc3ncnc4ccccc34)c2)c(Br)c1

  9 Training on 8698 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 21.833158
Reward: 2.396015
Trajectories with max counts:
89	COc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.26260245
Proportion of valid SMILES: 0.6107634543178974
Sample trajectories:
BP(=O)(OCCCOP(O)(O)=NP(=O)(O)O)[PH](O)(NC(=O)CCC=CCC=CCCCC=CCCC)SP(=O)(O)OP(=O)(O)O
BrC1=Nc2ncnc(Nc3ccccc3Br)c2Nc2ccccc21
Brc1cc2c(Nc3ccccc3-n3cccc3)ncnc2cc1-c1ccccc1
Brc1cc2ncnc(Nc3ccccc3)n2n1
Brc1ccc(Br)c(-c2ccccc2-c2ccccc2)c1
Policy gradient replay...
Mean value of predictions: 0.23895207
Proportion of valid SMILES: 0.6365638766519823
Sample trajectories:
BP(=O)(CP(=O)(O)O)N(O)CO
BP(=O)(OCCBr)OP(=O)(c1ccc(Br)cc1)N1CCOCC1
Brc1cc(Nc2ccccc2Br)c2cc(-c3ccccc3-c3ccccc3Br)cn2c1
Brc1ccc(Nc2nc(Nc3ccccc3)nc3ccccc23)cc1
Brc1ccc(Nc2ncnc(Nc3ccccc3-c3ccccc3Br)n2)cc1
Fine tuning...
Mean value of predictions: 0.25791505
Proportion of valid SMILES: 0.6519823788546255
Sample trajectories:
BC1=[N+](C(F)(F)F)[N-]O1
BP(=O)(NCCCCCCCN)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC1OC(Oc2ccccc2)C(O)C1F)C(=O)Nc1ccc(F)cc1F
Bc1ccccc1-c1ccccc1-c1ccc2ccccc2c1Nc1ccccc1-c1ccccc1S(N)(=O)=O
BrC1(Br)CCCN2CCCC=C2CC1

 10 Training on 10097 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 19.939046
Reward: 2.137145
Trajectories with max counts:
108	COc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.28881434
Proportion of valid SMILES: 0.5596244131455399
Sample trajectories:
BP(=O)(OC(C)Cl)P(=O)(O)O
Bc1ccccc1-c1ccccc1-c1cccc2ncnc(Nc3ccccc3)c12
Bc1ccccc1Nc1ccc2ncnc(Nc3ccc4ccccc4c3)c2c1
Br
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1ncnc2ncncc12
Policy gradient replay...
Mean value of predictions: 0.38635442
Proportion of valid SMILES: 0.6141338336460288
Sample trajectories:
BP(=O)(C=NP(=O)(OCOCOC(=O)OCC(F)F)Oc1c(F)cc(F)cc1F)OCC
BP(=O)(OC(C)=O)c1ccccc1
BrCc1cccc(Nc2ncnc3c2c(-c2ccccc2Br)cc2c(Nc4ccc(Br)cc4)ccnc23)c1
Brc1cc(-c2ccccc2)c2ncnc(N3CCOCC3)c2n1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.38908905
Proportion of valid SMILES: 0.6077572724429152
Sample trajectories:
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccccc12
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc2c(-c3c(Br)cccc3Oc3ccccc3Br)ncnc2s1
Brc1cc2c(Nc3ccccc3)cccc2nc1-c1ccccc1
Brc1cc2c(Nc3ccccc3)ccnc2cc1-c1ccccc1

 11 Training on 11378 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.370820
Reward: 2.313639
Trajectories with max counts:
45	COc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.34533855
Proportion of valid SMILES: 0.638871473354232
Sample trajectories:
BP(=O)(OCC1OC(N2C=C(Br)C(=O)NC2=O)C(O)C1O)Oc1ccccc1
Bc1ccc(Nc2cc(Nc3ccccc3)ccn2)cc1
BrCCN(CBr)c1ncnc2ncnc(Nc3ccc(Br)cc3Br)c12
Brc1ccc(-c2[nH]nc3ccccc23)cc1
Brc1ccc(-c2cc(Nc3ncnc4sccc34)ccc2-c2ccc3ncncc3n2)cc1
Policy gradient replay...
Mean value of predictions: 0.4092243
Proportion of valid SMILES: 0.5964363863707408
Sample trajectories:
BP(=O)(OCC1OC(CO)C(OC2C(=O)OC(OC(=O)Nc3ccc(Br)cn3)CC(=O)C=C2CO)C(O)C1O)OC(=O)CCNC(=O)CCl
Bc1ccccc1Nc1ccc2ncnc(-c3ccccc3)c2c1
BrC1=Cc2c(I)cc(Br)cc2O1
BrC=CBr
Brc1cc(Br)c2ncnc(Nc3cccc4ccccc34)c2c1
Fine tuning...
Mean value of predictions: 0.41337046
Proportion of valid SMILES: 0.5612883051907442
Sample trajectories:
Bc1ccc(Nc2cc(Nc3ccc(Br)cc3)ncn2)cc1Br
BrCC=NOCCCONc1ncnc2ccc(Br)cc12
BrCCNCCSc1ccccc1-c1ccc(Br)cc1
Br[n+]1ccccc1-c1ccccc1-c1ccccc1
Brc1cc(-c2ccc(-c3ccccc3)c(-c3ccccc3)c2)c2ncnc(Nc3ccccc3)c2c1

 12 Training on 12850 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.443370
Reward: 2.590406
Trajectories with max counts:
64	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.46394145
Proportion of valid SMILES: 0.5125078173858661
Sample trajectories:
BC=C1OCC(=O)Nc2ccc(Cl)cc21
BP(=O)(CC(=O)O)NO
BP(=O)(Nc1ccc(Cl)cc1)P(=O)(Oc1ccccc1)Oc1ccccc1
BP(=O)(OCC)C(=O)Nc1cc(Br)c(Br)cc1Br
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1OP(=O)(O)O)C(F)(F)P(=O)(O)O
Policy gradient replay...
Mean value of predictions: 0.43282774
Proportion of valid SMILES: 0.577958672510958
Sample trajectories:
BP(=O)(Nc1ccc(Br)c(Br)c1)N(=O)=O
BP(=O)(O)OP(C)(=O)O
Bc1ccc(NN=Nc2cc(Br)cc(Br)c2)cc1Br
Bc1nc(Nc2ncnc3sc(Br)cc23)nc(Br)c1Cl
BrBr
Fine tuning...
Mean value of predictions: 0.45
Proportion of valid SMILES: 0.5757196495619524
Sample trajectories:
BP(=O)(Nc1cccc(Nc2ccccc2Br)c1)C(=O)C=Cc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Bc1cccc(Nc2ncnc3c(Nc4ccccc4)cc23)c1
BrCc1nc2c(Nc3cc(Br)ccc3Br)ncnc2c2ccccc12
Brc1c(Nc2ncnc3ccccc23)cc2c(cc(Br)c3ncncc32)c1Br

 13 Training on 14537 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.312517
Reward: 2.693011
Trajectories with max counts:
62	Brc1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.44574207
Proportion of valid SMILES: 0.51375
Sample trajectories:
BCOc1cccc(Nc2ncnc3cc(Br)ccc23)n1
BP(=O)(Br)OCC
BP(=O)(CCC=CC(=O)NP(=O)(Cl)OCOC(=O)OC(C)Cl)OCCO
BP(=O)(CCP(=O)(O)O)NO
BP(=O)(CCl)NP(=O)(ONP(=O)(O)OP(=O)(O)O)N(=O)=O
Policy gradient replay...
Mean value of predictions: 0.46421224
Proportion of valid SMILES: 0.5782717595491547
Sample trajectories:
BP(=O)(CCOc1ccccc1-c1ccccc1)OCC
BP(=O)(OCC)C(CC(=O)NO)NS(=O)(=O)c1ccc(Br)cc1
BP(=O)(OCC)N1CC(N)=N[PH](=[PH](=O)(OCC)Oc2ccc(Br)cc2)N(CC)C1=O
BP(=O)(OCC1OC(N)(C=O)C(O)C1O)Oc1ccccc1
BP(=O)(OCC1OC(Oc2cccc(Br)c2)C(O)C1F)c1ccc(F)cc1
Fine tuning...
Mean value of predictions: 0.45704952
Proportion of valid SMILES: 0.5747809762202754
Sample trajectories:
BP(=O)(NCC1CCCCC1)C(=O)NC(CCCNC(N)=O)NC(=O)c1ccc(Br)cc1
Br
BrBr
BrCCNc1ccc2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(Br)c(Nc2ncnc3ccc(Nc4ccccc4Br)cc23)cc1Br

 14 Training on 16230 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.225427
Reward: 2.761925
Trajectories with max counts:
37	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.42239505
Proportion of valid SMILES: 0.6043233082706767
Sample trajectories:
Bc1ccccc1-c1ccc(Nc2ncnc3ccsc23)cc1
Br[n+]1cccc2ccccc21
Brc1cc(-c2ccc(Br)c3ccccc23)cc(Br)c1Br
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.4816196
Proportion of valid SMILES: 0.5867458580806502
Sample trajectories:
BP(=O)(OCC)C(F)C(F)(F)F
BrCc1cc2c(Nc3ccc(Br)cn3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6ccccc6Br)c5c4)cc23)c(Br)s1
Brc1cc(Nc2ncnc3cc(Br)sc23)cs1
Fine tuning...
Mean value of predictions: 0.49626514
Proportion of valid SMILES: 0.5946199562089459
Sample trajectories:
BP(=O)(NCCCO)N(=O)=O
Br
BrC=CCNc1ccc(Nc2ncnc3sc2-c2ccccc23)cc1
BrCN1CCC(=Nc2ccc(Br)s2)C1
Brc1cc(Br)c2c(Nc3cc(Br)c4nc(Nc5ccccc5)ncnc4s3)ncnc2c1

 15 Training on 18186 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.700436
Reward: 2.748125
Trajectories with max counts:
46	Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Mean value of predictions: 0.54753697
Proportion of valid SMILES: 0.5079762277134814
Sample trajectories:
BC=C1Oc2ncnc(Nc3cccc(F)c3)c2C(=O)Nc2ncnc(N)c21
BP(=O)(Br)OCC(Br)(Br)Br
BP(=O)(Cc1cc(Br)cc(Br)c1)Nc1cc2ncnc(Nc3cc(Br)cs3)c2s1
BP(=O)(OCC)n1c(N2CCN(c3ccc(Br)cc3)CC2)cc(Br)c1Br
Bc1cccc(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.5183921
Proportion of valid SMILES: 0.5682102628285357
Sample trajectories:
BP(=O)(NC(COCCN)P(=O)(O)O)P(=O)(O)O
BP(=O)(OC)OCCCF
BP(=O)(OCC)N(O)C(=O)NCCN
BP(=O)(OCC)Oc1c(Cl)c(Br)c(Br)c(N)c1Br
BP(=O)(OCOC(=O)CN1CCN(c2cccc(Br)c2)CC1)c1ccc(Br)c(Br)c1Br
Fine tuning...
Mean value of predictions: 0.50740933
Proportion of valid SMILES: 0.5954317897371715
Sample trajectories:
Bc1ccc(Nc2cc3c(-c4ccccc4Br)c3ncn2)cc1Cl
Bc1cccc(Nc2ncnc3c(Br)sc(-c4ccccc4Br)c23)c1
Br
BrC1=CC(Nc2ccc(Br)cc2)N=C1c1ccc(Br)cc1
BrCCNc1nc2ncnc(Nc3ccc(Br)s3)c2s1

 16 Training on 20321 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.940548
Reward: 3.107222
Trajectories with max counts:
40	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.53241456
Proportion of valid SMILES: 0.5672295184490307
Sample trajectories:
BP(=O)(NCCCCCCl)NC(=O)NC(CO)NC(=O)c1ccc2ncnc(Nc3ccc(Nc4cc(F)cc(F)c4)c(Cl)c3)c2c1
BP(=O)(NO)C(=O)Nc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
BP(=O)(NP(N)(=O)O)OCc1ccccc1
BP(=O)(OCC1NC(OP(=O)(O)O)=C(Br)C(=O)NN(N(=O)=O)C(=O)O1)C(N)=O
Bc1ccc(Nc2ncnc3ccc(Br)c(Br)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.5354949
Proportion of valid SMILES: 0.5500625782227785
Sample trajectories:
BP(=O)(CCl)NO
BP(=O)(OCCBr)C(Br)Br
BP(=O)(c1ccccc1Br)C(O)c1cc(Br)c(Br)c(Br)c1
Bc1cc(Br)c2ncnn2c1Br
Bc1ccc(N2CCOCC2)c(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c1
Fine tuning...
Mean value of predictions: 0.5265852
Proportion of valid SMILES: 0.5381966186599875
Sample trajectories:
BP(=O)(=O)(S)ON=C1OC(F)CNS1(=O)=O
BP(=O)(N=[PH](=O)(O)O)NCCCl
BP(=O)(NCCNc1cc2nnnn2c2cc(Cl)ccc12)c1cccc(Br)c1
BP(=O)(NCCO)c1ccc2ncnc(Nc3cc(Br)ccc3Cl)c2c1
Bc1ccc(Nc2c(Nc3ccccc3)ncnc2n2cnc(Nc3ccnc4ccccc34)c2)cc1

 17 Training on 22432 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.901256
Reward: 3.402123
Trajectories with max counts:
163	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.50832134
Proportion of valid SMILES: 0.4363067292644757
Sample trajectories:
BP(=O)(CCCCCl)P(=O)(N=Nc1ccc(Br)cc1)OCO
BP(=O)(CCOc1ccc(Br)c(Br)c1)OCC
BP(=O)(NC(=O)CN(CCCNC(=O)C(Nc1ccc(Br)c(Br)c1)P(=O)(O)O)NC(=O)CCCCCl)OCC
BP(=O)(NCCCCCCCCCO)Oc1cccc(Nc2ncnc3cc(Br)c(Br)cc23)c1
BP(=O)(NCCCNC(=O)c1ccc(Nc2cc(Nc3cccc(Br)c3)ncn2)cn1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.5474699
Proportion of valid SMILES: 0.5205393540294764
Sample trajectories:
BP(=O)(NCCO)c1ccc(N2CCOCC2)c(F)c1
BP(=O)(Nc1cc(Br)c(Br)cc1Nc1ccc(Br)c(Br)c1)C(=O)Nc1ccnc2c(Br)c(Br)[nH]c12
BP(=O)(c1cccc(Br)c1)N1CCN(C(=O)Nc2cc(Br)c(Br)c(Br)c2)CC1
Bc1ccc(Nc2ncnc3c(Br)sc(NCCN4CCCCC4)c3s2)cc1
BrC=CBr
Fine tuning...
Mean value of predictions: 0.5743076
Proportion of valid SMILES: 0.531641604010025
Sample trajectories:
BC(=O)COc1ccc2ncnc(Nc3ccccc3)c2c1
BP(=O)(NO)c1cc(Br)c(Br)c(Br)c1Br
BrC1=CN(C2CCCc3ccccc32)CN1
BrCc1sc2ncnc(Nc3ccc(Br)cc3)c2c1-c1ccccc1
Brc1c(Nc2ccccc2)nc2ncsc2c1Nc1scnc1-c1ccccc1

 18 Training on 24456 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.635671
Reward: 3.212947
Trajectories with max counts:
37	Fc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.5594514
Proportion of valid SMILES: 0.5242263207252267
Sample trajectories:
BP(=O)(C(=O)NO)N(O)Nc1ccc(Br)cc1
BP(=O)(NC(c1ccccc1)c1ccccc1)P(=O)(O)O
BP(=O)(NCCCCCN)P(=O)(O)O
BP(=O)(NCCO)NC(=O)CCCC(=O)NCC(=O)NCC12CC3CC(CC(C3)C1)C2
BP(=O)(Nc1ccc(Nc2ccccc2)cc1)N1CCOCC1
Policy gradient replay...
Mean value of predictions: 0.5644172
Proportion of valid SMILES: 0.5624215809284818
Sample trajectories:
BP(=O)(OCC)OC(=O)c1cc(Nc2cc(Br)cnc2-c2ccc(Br)c(Br)c2)ncn1
BP(=O)(OCC)Oc1ccc(Br)cc1-c1ccc2nc(Br)c(Br)c(Br)c2n1
BP(=O)(OCCOCCOCCOC)n1cc(-c2cc(Br)c(Br)c(N)n2)nc1-c1ccc(Br)cc1
Bc1cc(-c2ccccc2I)n(Cc2ccccc2n2cncn2)c1Br
Bc1cc(Br)ccc1Nc1ncnc2ccccc12
Fine tuning...
Mean value of predictions: 0.5568392
Proportion of valid SMILES: 0.5746946445349201
Sample trajectories:
BOc1ccccc1-c1cc2ncnc(Nc3ccnc4ccccc34)c2s1
BP(=O)(CCCCCC(F)F)C(F)(F)F
BP(=O)(OCC)n1cc(-c2cc(Br)c(Br)c(Br)c2Br)n2ncnc12
Bc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
Bc1ccccc1-c1ccc(Oc2ccccc2)cc1-c1ccc2ncncc2c1-c1ccc(Nc2ncnc3[nH]cc(Br)c23)cc1

 19 Training on 26705 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.516108
Reward: 3.261156
Trajectories with max counts:
17	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.57133067
Proportion of valid SMILES: 0.5482154038822793
Sample trajectories:
BP(=O)(C1CCC(Nc2ncnc3cc(Br)c(Cl)c(Cl)c23)CC1)N1CCCC(F)(F)CC1
BrC(=Cc1ccc2ncnc(Nc3cccc4ccc(Br)cc34)c2c1)c1ccc2ncnc(Nc3ccccc3Br)c2c1
BrCCNCc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cc1-c1cc2ccccc2nc1-c1ccc2c(Br)cccc2n1
BrCc1ccc2c(Nc3cccc(Br)c3)ncnc2c1
Brc1cc(Br)c(-c2cc3c(Nc4ccncn4)ncnc3s2)cc1Br
Policy gradient replay...
Mean value of predictions: 0.5375
Proportion of valid SMILES: 0.5365089313694766
Sample trajectories:
BP(=O)(CCCCC(F)(F)Cl)NO
BP(=O)(NCCCCCCCCO)C(=O)C(O)c1ccc(Br)cc1
BP(=O)(OCCC(F)(F)F)Oc1ccc(NP(=O)(O)CF)c(F)c1
B[PH](=O)(Nc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)Oc1ccc(Br)cc1
Bc1ccc(Nc2cc3c(Br)c(-c4ccccc4)ncnc3ncn2)cc1Br
Fine tuning...
Mean value of predictions: 0.5441089
Proportion of valid SMILES: 0.5301318267419962
Sample trajectories:
BP(=O)(CCCN)NCCCCN
BP(=O)(Nc1cc2c(Br)ncnc2s1)c1ccc(Br)cc1
BP(=O)(Nc1ccc(F)s1)N(CCl)CCCl
Bc1ccc(Nc2ncnc(Nc3cccc(Br)c3)n2)cc1
Br

 20 Training on 28888 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.803179
Reward: 3.719210
Trajectories with max counts:
76	Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Mean value of predictions: 0.48811996
Proportion of valid SMILES: 0.5439146800501882
Sample trajectories:
BP(=O)(CF)c1ccccc1-c1ccccc1
BP(=O)(Nc1ccc(Br)cc1)Nc1ccc(Br)cc1
BP1(=O)OCC(OC(=O)Nc2cc(Br)c(Br)cc2Br)c2cccc(Br)c2OC(=O)C(Br)=CC1=O
Bc1ccccc1-c1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
BrCCC=CC=CC=CC=CC=CC=CC=Nc1ccc(Br)cc1Br
Policy gradient replay...
Mean value of predictions: 0.296958
Proportion of valid SMILES: 0.6576691013020006
Sample trajectories:
BP(=O)(C=O)NO
BP(=O)(OCC)C(=O)O
Bc1cccc(Nc2ncnc3ccccc23)c1
Bc1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1
Bc1ccccc1-c1ccccc1-c1ccccc1Nc1ccc(Nc2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.30307913
Proportion of valid SMILES: 0.6695210910244211
Sample trajectories:
BP(=O)(CCCl)Nc1cccc(Br)c1Cl
BP(=O)(OCC)OCCCCCCCCOP(=O)(O)OP(=O)(O)OCCCl
Bc1ccc(Nc2ncnc3ccccc23)cc1Br
Bc1cccc(Nc2ncnc3ccccc23)c1
Bc1ccccc1-c1cc2ncnc(Nc3ccccc3)c2cc1Nc1ccccc1

Trajectories with max counts:
284	COc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.25964877
Proportion of valid SMILES: 0.5416375975137946
Mean Internal Similarity: 0.5213049843092313
Std Internal Similarity: 0.1183596602685552
Mean External Similarity: 0.4249781104839267
Std External Similarity: 0.07029783780216879
Mean MolWt: 483.88370177118253
Std MolWt: 154.2096502313015
Effect MolWt: -0.14741936975508413
Mean MolLogP: 7.831056179990425
Std MolLogP: 3.479379153648835
Effect MolLogP: 1.1423860877258054
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.898230% (885 / 904)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 0, 'seed': 3, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5567.294189691544, 'valid_fraction': 0.5416375975137946, 'active_fraction': 0.2446135831381733, 'max_counts': 284, 'mean_internal_similarity': 0.5213049843092313, 'std_internal_similarity': 0.1183596602685552, 'mean_external_similarity': 0.4249781104839267, 'std_external_similarity': 0.07029783780216879, 'mean_MolWt': 483.88370177118253, 'std_MolWt': 154.2096502313015, 'effect_MolWt': -0.14741936975508413, 'mean_MolLogP': 7.831056179990425, 'std_MolLogP': 3.479379153648835, 'effect_MolLogP': 1.1423860877258054, 'generated_scaffolds': 904, 'novel_scaffolds': 885, 'novel_fraction': 0.9789823008849557, 'save_path': '../logs/replay_combo_s3-1.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.447887
Reward: 1.000000
Trajectories with max counts:
2	OCC1OC(CO)C(O)C(O)C1O
Mean value of predictions: 0.0016032064
Proportion of valid SMILES: 0.7811521603005636
Sample trajectories:
BP(=O)(O)OP(=O)(O)SCC(N)CC(=O)O
Brc1ccc(-c2cc(-c3cccnc3)on2)cc1
Brc1ccc(C=Nn2cnnn2)cc1
Brc1ccc(CN2C=Nc3ccc4c(c3N=C2c2ccncc2)OCO4)cc1
Brc1cccc2c1-c1ncccc12
Policy gradient replay...
Mean value of predictions: 0.038512036
Proportion of valid SMILES: 0.5732204452806522
Sample trajectories:
Bc1ncc(-c2ccc(Cl)cc2)c(N2CCC(N3CCOCC3)C2)c1COc1ccc2c(c1)OCO2
Brc1ccc(-c2cnc3cnc4ccsc4n23)cn1
Brc1ccc(Nc2cnc3ncncc3n2)cc1
Brc1ccc(Nc2cnccn2)cc1
Brc1ccc(Nc2ncc3ccccn23)cc1
Fine tuning...
Mean value of predictions: 0.031982716
Proportion of valid SMILES: 0.5811616954474097
Sample trajectories:
BrCC(Br)Br
BrCCCCCCCCCCN1CCN(Cc2ccc3c(c2)OCO3)CC1
Brc1ccc(-c2nc(CN3CCOCC3)n(-c3ccccc3)c2Nc2ccccc2)cc1
Brc1ccc(-c2nnc(Nc3ccccc3)c3ccccc23)cc1
Brc1ccc(CN2N=CNc3c2c2ccccc32)cc1

  2 Training on 417 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.697485
Reward: 1.100000
Trajectories with max counts:
3	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.049021862
Proportion of valid SMILES: 0.5448275862068965
Sample trajectories:
Bc1cc(-c2ncc3[nH]c(-c4nn[nH]n4)nc3c2Br)nc(Nc2cccc(Br)c2)c1C#N
BrCCSc1ccc(N2CCC3COCCC3C2)cc1
Brc1[nH]ccc1-c1ccc2ncnn2c1
Brc1ccc(CN2N=Nc3ccccc32)cc1
Brc1ccc(NN=Cc2cc3ccccc3nc2NCCc2ccccc2)cc1
Policy gradient replay...
Mean value of predictions: 0.09307822
Proportion of valid SMILES: 0.557227971150831
Sample trajectories:
Brc1ccc(N2CCN(Cc3ncnc4ccccc34)CC2)c(CNc2cnccn2)c1
Brc1ccc(N2CCOCC2)c2c1Nc1ccccc1S2
Brc1ccc(Nc2cc(Br)ccc2Nc2ccnc3ccccc23)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4ccccc4)c3c2)cc1
Brc1ccc(Nc2ccncc2)c(Br)c1
Fine tuning...
Mean value of predictions: 0.10092167
Proportion of valid SMILES: 0.5448838669177652
Sample trajectories:
BP(=O)(OC)C(OC(=O)C(NC(=O)OC(C(C=O)OCOP(=O)(O)O)C(O)C(=O)O)N(=O)=O)C(=O)O
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)OP(=O)(O)OCP(=O)(O)O
Brc1cc(-c2nc3ccccc3n2Cc2cccc(-c3cccnc3)c2)cc(-c2ccccc2)n1
Brc1cc(N2CCc3ccccc32)c2ccccc2n1
Brc1ccc(Nc2c(Br)cnn2Cc2ccccn2)cc1

  3 Training on 1018 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.379292
Reward: 1.260951
Trajectories with max counts:
6	COc1cc2ncnc(Nc3ccc(Cl)c(Cl)c3)c2cc1OC
Mean value of predictions: 0.10806258
Proportion of valid SMILES: 0.5232997481108312
Sample trajectories:
BP(=O)(OCC)Oc1nc(Br)cc(C(=O)OP(=O)(O)OP(=O)(O)O)c1P(=O)(OCCSSCP(=O)(O)O)P(=O)(O)O
Brc1cc(-c2ncsc2Br)c2c(Br)ncnc2n1
Brc1cc(Nc2ncnc3oc(Br)cc23)no1
Brc1cc2ncnc(Nc3cc(Br)[nH]c3Br)n2n1
Brc1ccc(-c2ncnc(Nc3ccc(Br)c(Br)c3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.1538686
Proportion of valid SMILES: 0.4354736172917991
Sample trajectories:
BP(=O)(C(=O)O)N(CC(=O)OCC(Cl)(P(=O)(O)O)P(=O)(O)O)P(=O)(O)O
BP(=O)(OCC)OCC[N+](C)(C)C
Brc1cc(Nc2nc(Br)cnc2Br)ncn1
Brc1cc2ncnc(Nc3cc(Br)c(Br)cc3I)n2n1
Brc1cc2nncnc2s1
Fine tuning...
Mean value of predictions: 0.2001123
Proportion of valid SMILES: 0.5581322469445315
Sample trajectories:
Brc1cc(NN=Cc2ccco2)nc2ncnc(Nc3ccccc3)c12
Brc1ccc(-c2ncnc3ccsc23)c2sccc12
Brc1ccc(N=Nc2ccc(Br)o2)cc1
Brc1ccc(N=Nc2nc3nc4ccccc4cc3nc2SI)cc1
Brc1ccc(Nc2c(-c3cc(CCN4CCCCC4)ccc3Br)nc3cc(Br)c(Br)cn23)cc1

  4 Training on 2059 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 20.931498
Reward: 1.388374
Trajectories with max counts:
10	COc1cc2ncnc(Nc3ccc(Cl)c(Cl)c3)c2cc1OC
Mean value of predictions: 0.21222289
Proportion of valid SMILES: 0.5236899905867587
Sample trajectories:
BrC(Nc1ncnc2cncnc12)N1CCCCCCCC1
BrN=Cc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)n2c1
Brc1cc2ncnc(Nc3ccc(I)cc3Br)n2n1
Brc1cc2ncnc(Nc3ccc4c(c3)OCO4)c2nc1-c1ccnc2cccc(Br)c12
Policy gradient replay...
Mean value of predictions: 0.2636518
Proportion of valid SMILES: 0.5541979949874687
Sample trajectories:
BP(=O)(c1ccc2ccccc2c1)N(Cc1cccc(Br)c1)c1ccc(Br)cc1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c4ccccc34)c2c1
Brc1ccc(-c2ccc(Br)c(-c3ccc4ccncc4c3)c2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ncnc3sc(Nc4ccccc4Br)nc23)c1
Fine tuning...
Mean value of predictions: 0.2425486
Proportion of valid SMILES: 0.5798371947401377
Sample trajectories:
BP(=O)(OCCO)c1ccccc1
Br
BrCc1[nH]c(Nc2ncnc3[nH]cnc23)nc1Br
BrCc1ccc(Nc2ncccn2)c2ccccc12
Brc1cc(-c2ncnc(Nc3ccc(Br)s3)n2)c2cccnc2c1

  5 Training on 3578 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 20.591148
Reward: 1.526964
Trajectories with max counts:
27	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.268988
Proportion of valid SMILES: 0.5752738654147105
Sample trajectories:
BP(=O)(OCCC)c1ccnc(Nc2ccc(Cl)cc2)c1
Brc1c(Br)c2c1cccc(-c1ccccc1)Nc1nc3ccccc3-n12
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4ccccc34)cc2)cc1Br
Brc1cc(Br)cc(Nc2ncnc3[nH]cnc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.30321452
Proportion of valid SMILES: 0.5395774203721223
Sample trajectories:
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c(Nc2nc3cc(Br)cc(Br)c3s2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c4cc(Br)ccc34)ncnc2c1
Brc1cc(Br)cc(Nc2c(Br)cnc3ncnc(Nc4ccc(Br)c(Br)c4)c23)c1
Brc1cc(N2CCCCC2)c2cc1[nH]c1c(Br)ccc(Nc3ncnc4scc(-c5cccs5)c34)c21
Fine tuning...
Mean value of predictions: 0.27794263
Proportion of valid SMILES: 0.632665832290363
Sample trajectories:
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1cc(-n2ccc3cncc(Nc4ccccc4)c32)on1
Brc1cc(NCc2ccc3c(c2)OCO3)c(Nc2c(Br)cnc3ccccc23)cc1Br
Brc1cc2cc(Br)c(Br)cc2s1
Brc1ccc(-c2ccccc2Nc2ncnc3[nH]ccc23)s1

  6 Training on 5300 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 23.631742
Reward: 1.789943
Trajectories with max counts:
37	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.29984334
Proportion of valid SMILES: 0.5991864831038799
Sample trajectories:
Bc1ccc(Nc2cc(Br)cc(Br)c2Oc2cc(Br)ccc2I)cc1
BrC1=Cc2ccc3ncnc(Nc4ccc(Br)cc4)c3-c3cccnc3ccccc21
BrCc1ccc2c(Nc3ccc(Br)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2cc(Br)sc3c(sc4cc(Br)c(Br)c(Br)c43)c2c1
Policy gradient replay...
Mean value of predictions: 0.37682673
Proportion of valid SMILES: 0.6006269592476489
Sample trajectories:
BP(=O)(NP(=O)(O)OP(=O)(O)O)OCCCC(=O)O
Br
BrCCBr
BrCCCBr
BrCCCCCNc1c2cccc(Br)c2ncnc2ncnc(Nc3ccc(Br)cc3)c12
Fine tuning...
Mean value of predictions: 0.31036922
Proportion of valid SMILES: 0.6197492163009405
Sample trajectories:
Br
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3c(Br)c(Br)c(Br)c(Br)c(Br)n23)c1
Brc1cc(N2CCCCCCC(N3CCC(CN4CCOCC4)C3)C2)[nH]c1Br
Brc1cc2c(Nc3ccc(n4nccc4I)o3)ncnc2s1

  7 Training on 7283 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 22.455233
Reward: 2.018175
Trajectories with max counts:
54	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.327153
Proportion of valid SMILES: 0.6174538629965592
Sample trajectories:
BP(=O)(NC(CS(=O)(=O)c1ccccc1)P(=O)(Oc1ccccc1)Oc1ccccc1)Oc1ccc(Br)cc1
BP(=O)(NCCCl)N(=O)=O
BP(=O)(OCC)OC(=O)C(Cl)(SCCSS)P(=O)(O)P(=O)(O)O
B[PH](=O)(NO)(Oc1ccccc1)Oc1ccc(F)cc1
BrC(Br)(Br)Br
Policy gradient replay...
Mean value of predictions: 0.3813278
Proportion of valid SMILES: 0.6030653737879261
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCCCC)c1ccc(I)cc1
BrC(Br)C(Br)(Br)CCc1cc2ccccc2c2ccccc12
Brc1cc(Br)c(Nc2ncc3ccc(Nc4nc5ccccc45)cc3n2)cc1Br
Brc1cc(Br)cc(Nc2ncnc3c(Br)cccc23)c1
Fine tuning...
Mean value of predictions: 0.38124022
Proportion of valid SMILES: 0.5998749609252891
Sample trajectories:
BP1(=O)OCC(Oc2ccc(Br)cc2)OC(=O)COC(=O)N1Cc1ccc(Br)cc1
BrC=CBr
BrC=CC=CC=CC=CCN1CCCC1CC=NNc1cccc2cc(Br)ccc12
BrCCCC[n+]1ccc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1cc(Br)c2sc(N=Cc3sccc3Br)nc2c1

  8 Training on 9292 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 24.160052
Reward: 2.269662
Trajectories with max counts:
29	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.37734836
Proportion of valid SMILES: 0.5823694904657706
Sample trajectories:
Brc1cc(Br)c(Oc2cccc(Nc3ncnc4cccc(Br)c34)c2)c(Br)c1
Brc1cc(Br)c2cc(Br)c(-c3ccccc3)n2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(-c4nc5cc(Br)c(N6CCOCC6)cc5s4)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(s1)-c1c(ccc(Br)c1Br)N2
Policy gradient replay...
Mean value of predictions: 0.4039801
Proportion of valid SMILES: 0.6295020357031005
Sample trajectories:
BP(=O)(OCCS)N(=O)=O
BrCCOc1ccc2c(Nc3ccc(Br)cc3)ncnc2c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Nc2ncnc3ccccc23)c(Br)c1
Brc1cc2c(Nc3cc(Br)c4ccccc4n3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.4040394
Proportion of valid SMILES: 0.6361642118458164
Sample trajectories:
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)Oc1cc(Br)c(Br)c(Br)c1
B[PH](=O)(Br)(OCC#CC)c1ccc(Br)cc1
Bc1ccc(Nc2ncnc3[nH]c(Br)c(Br)c23)cc1
BrCCBr
BrCc1cc2c(Nc3csc(Br)n3)ncnc2s1

  9 Training on 11065 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 23.530912
Reward: 2.129841
Trajectories with max counts:
32	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.37724426
Proportion of valid SMILES: 0.600062637018478
Sample trajectories:
BP(=O)(CCOc1ccc2ncnc(Nc3ccc(F)c(Br)c3)c2n1)Nc1ccc(F)c(Cl)c1
BrCCCCCNc1cc2c(s1)c1ccccc1-2
Brc1cc(Br)c2ccc3cc(Br)c(Br)cc3c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.3727228
Proportion of valid SMILES: 0.6346875
Sample trajectories:
Bc1ccc(NC(=O)c2cc(Br)cn2C)cc1
BrC(=NNc1ccccc1)c1ccccc1
BrC=CC=CCCC=CCCBr
BrC=CC=CCCCCCBr
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.44138283
Proportion of valid SMILES: 0.6160350767303476
Sample trajectories:
Bc1cc(Nc2ncnc3ncnc(Nc4ccc(Br)c(Br)c4Br)sc23)cc(Br)c1O
BrC=C(Br)Br
BrCCOc1ccc(Br)cc1
Brc1cc(Br)c(Nc2ncnc3cc(Br)ccc23)c(Br)c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1

 10 Training on 12646 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.166657
Reward: 2.314108
Trajectories with max counts:
44	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.45630115
Proportion of valid SMILES: 0.573888541014402
Sample trajectories:
BP(=O)(N1CCC(F)(F)C(F)(F)C1)S(=O)(=O)c1cccc(Br)c1F
Brc1cc(Br)c(Br)c(Nc2ncnc3ccccc23)n1
Brc1cc(Br)c2c(Nc3cc(Br)c(Br)c4cc(Br)c(Br)c(Br)c34)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccncc4)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(I)cc3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.41958857
Proportion of valid SMILES: 0.7000626174076393
Sample trajectories:
BC(=O)Nc1sc2ncnc(Nc3cccc(Cl)c3)c2c1-c1ccccc1F
BP(=O)(NC(Cc1cccc(Br)c1)C(=O)CN(O)C(=O)Nc1ccc(Br)cc1)P(=O)(O)O
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)N1CCCCC1
Br
BrCc1cc(Nc2ncnc3cc(Br)sc23)ccc1Br
Fine tuning...
Mean value of predictions: 0.45068225
Proportion of valid SMILES: 0.6416510318949343
Sample trajectories:
BP(=O)(OCCOc1cc(Br)c(Br)c(Br)c1Br)N(=O)=O
BrCCCNc1ccc(Nc2ccnc3cc(Br)ccc23)cc1
BrCCN1CCN(CCc2cccs2)CC1
BrCc1ccc(Nc2ncnc3cc(Br)c(N4sc5ccccc54)cc23)cc1
Brc1cc(Br)c2ncnc(Nc3ccsc3)c2c1

 11 Training on 14569 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.518696
Reward: 2.462147
Trajectories with max counts:
28	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.49688795
Proportion of valid SMILES: 0.6040100250626567
Sample trajectories:
BP(=O)(NCCCC#CCCCC(=O)O)C(=O)NP(=O)(OCCF)Oc1cc2ncnc(Nc3cc(F)c(F)cc3F)c2s1
BP(=O)(OCC(=O)NO)Oc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BP(=O)(OCC)C(O)(O)COP(=O)(O)OP(=O)(O)OP(=O)(O)O
B[PH](=O)(C=NO)(OCC)c1cc(Br)cc(Br)c1O
BrC(Br)(Br)Br
Policy gradient replay...
Mean value of predictions: 0.5221778
Proportion of valid SMILES: 0.6264080100125157
Sample trajectories:
BP(=O)(C(N)=O)N1CCN(c2ccc(Br)cc2F)CC1
Br
BrCCN(CCBr)c1nc2c(Br)c(Br)c(Br)cc2s1
BrCCNc1ncnc2c1-c1cc(Br)ccc12
BrCCNc1ncnc2ncnc(Nc3ccc(Br)cc3)s12
Fine tuning...
Mean value of predictions: 0.46933728
Proportion of valid SMILES: 0.6328638497652582
Sample trajectories:
Br
BrCCNc1nc2ncnc(Nc3ccc(Br)cc3)c2s1
Brc1cc(Br)c(Br)c(Nc2ncc(Br)c(-c3ccccc3)n2)c1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Brc1cc(Br)c(Nc2ncnc3ccc(-c4nc(N5CC=CCCC5)c(Br)cc4Br)cc23)c(Br)c1

 12 Training on 16756 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.482887
Reward: 2.444571
Trajectories with max counts:
31	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.46467608
Proportion of valid SMILES: 0.6190476190476191
Sample trajectories:
BP(=O)(N(CC(=O)Nc1cccc(Br)c1)C(Cl)(Cl)Cl)N(=O)=O
BP(=O)(OCC#N)N(C(=O)OC)N(C(=O)OC(CO)CO)S(=O)(=O)c1ccc2c(Br)c(Br)c(Br)c(Br)c2c1
BrBr
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2cc1Br
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)cn23)c1
Policy gradient replay...
Mean value of predictions: 0.42701524
Proportion of valid SMILES: 0.7174116911534855
Sample trajectories:
BP(=O)(OCC)OCCCCN1CCCC1
BP(=O)(OCOC(=O)CN)c1ccc(Br)cc1
Brc1cc(Br)c(Nc2ncnc3c(Br)cccc23)cc1Br
Brc1cc(Br)c2ccccc2c1-c1ccc(NCc2ccccc2)cc1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.48616678
Proportion of valid SMILES: 0.6709375
Sample trajectories:
BrC(=Cc1ccc(Br)cc1)c1ncnc2scc(Br)c12
BrCCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3Br)ncnc2c1

 13 Training on 18919 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.348875
Reward: 2.661594
Trajectories with max counts:
28	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.52406055
Proportion of valid SMILES: 0.6405126602063145
Sample trajectories:
BrCCN(CCBr)c1cc2ncnc(Nc3cccc(Br)c3)c2cc1Br
BrSc1nc2ncnc(Br)c2s1
Brc1cc(Br)c2sc(-c3c(Br)ccnc3Br)nc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.45121494
Proportion of valid SMILES: 0.6693775414451048
Sample trajectories:
BN=C(Br)NOCC(=O)Nc1ccc(Br)cc1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Nc2ncnc3c(Br)c(Br)c(Br)c(Br)c23)cc1Br
Brc1cc(Br)c2c(NCCNc3ccc(I)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3cc(Br)c(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.5042308
Proportion of valid SMILES: 0.65
Sample trajectories:
BP(=O)(OCC)N(O)Nc1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CNC(=O)CCCCCCCl
BP(=O)(OCCO)C(Nc1cccc(Br)c1)Oc1ccccc1
Bc1cccc(Nc2ncnc3cc(Br)c(Br)cc23)c1F
Brc1c[nH]c2c1cc(Br)c1ncnN(c3ccccc3)c(N3CCCC3)c12

 14 Training on 21192 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.439945
Reward: 2.672620
Trajectories with max counts:
38	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4817444
Proportion of valid SMILES: 0.6164426383244764
Sample trajectories:
BP(=O)(CCC(=O)Nc1ccc2ncnc(Nc3cc(Br)c(Br)c(Br)c3O)c2n1)OCC
Bc1cc(F)cc(Nc2ncnc3c(Br)ccc(Br)c23)c1
Bc1ccc(Nc2ncnc3ccc(Br)c(Br)c23)cc1
BrC(Br)(Br)Br
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.52227837
Proportion of valid SMILES: 0.6805251641137856
Sample trajectories:
BP(=O)(OCCS)C(F)(F)F
BrCCOc1ccc2ncnc(Nc3cccc4cc(Br)ccc34)c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3cc(Br)c(Br)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.4943925
Proportion of valid SMILES: 0.6693775414451048
Sample trajectories:
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccccc23)ncn1
Brc1cc2c3ccnc(Br)c3cccs2c1Br

 15 Training on 23535 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.983564
Reward: 2.786460
Trajectories with max counts:
15	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
15	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5604519
Proportion of valid SMILES: 0.6109193598995921
Sample trajectories:
BP(=O)(N(CCCl)Nc1cc(F)c(F)c(F)c1)C(F)(F)F
BP(=O)(OCC)C(=O)Oc1cc2ncnc(Nc3cc(Br)c(O)c(Br)c3)c2s1
BP(=O)(OCC1OC(N2C=CC=C(Br)C(=O)NC2=O)C(O)C1O)Oc1ccc(Br)c(Br)c1
Bc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1
BrBr
Policy gradient replay...
Mean value of predictions: 0.53458387
Proportion of valid SMILES: 0.6502971535814827
Sample trajectories:
BP(=O)(NC(=O)C=Cc1ccc(Br)cc1)OCC(=O)NC(CO)C(=O)O
Bc1ccccc1N1CCN(CCOc2ncnc3ccsc23)CC1
BrCc1cc(Nc2ncnc3ccc(Br)c(Br)c23)ccc1Br
BrCc1ccc2c(Nc3ccc(Br)cc3)ncnc2c1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.531512
Proportion of valid SMILES: 0.6644757433489827
Sample trajectories:
BrCC(Nc1ccc(Br)cc1)c1ccc(Br)cc1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c23)cc1Br
Brc1cc(Br)c(Nc2ncnc3ccc(Br)c(N4CCCCCCC4)c23)cc1Br
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1

 16 Training on 26050 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.025148
Reward: 2.843240
Trajectories with max counts:
51	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.49279624
Proportion of valid SMILES: 0.6595811190997186
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Nc2ncnc3ccccc23)cc1
BrC=CCCBr
BrC=CCSc1ccccc1
Brc1cc(Br)c(-c2cncs2)c(-c2cccc(Nc3ncnc4ccccc34)c2)c1
Brc1cc(Br)c(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.5915325
Proportion of valid SMILES: 0.6794871794871795
Sample trajectories:
BP(=O)(NS(=O)(=O)Oc1cc2c(Br)cc(Br)cc2s1)OCCC(N)COP(=O)(O)OP(=O)(O)NO
BrCBr
Brc1cc(Br)c2c(-c3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(-c4cc(Br)c(Br)cc4Br)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.55311626
Proportion of valid SMILES: 0.6727158948685857
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Br)cc1Br
BrCC(Br)Br
BrCCBr
Brc1cc(Br)c(Nc2ccc(Nc3c4ccccc4nc4cc(Br)ccc34)cn2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ccnc2c1

 17 Training on 28738 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.365294
Reward: 3.116333
Trajectories with max counts:
93	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5461066
Proportion of valid SMILES: 0.61
Sample trajectories:
BP(=O)(OCC)OCC
Bc1ccc(Br)cc1Nc1ncnc2ncnc(Nc3ccc(Br)cc3)c12
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1F
Br
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.575989
Proportion of valid SMILES: 0.6795873710534542
Sample trajectories:
BP(=O)(OCOc1ccc(Br)cc1)c1ccc(Br)cc1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Nc2ncnc3ccc(Oc4ccccc4)c3n2)cc1Br
Brc1cc(Br)c(Nc2ncnc3ccccc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.5311819
Proportion of valid SMILES: 0.6877149109096593
Sample trajectories:
BP(=O)(NCCCCCCl)Oc1cc(F)c(Nc2cc(Nc3cncnc3)ncn2)c(Br)c1
BP(=O)(OCC1OC(n2cnc3c(N)nc(N)nc32)C(O)C1O)Oc1ccccc1
BrC1=CC2=C(Br)N(Cc3ccc(Br)cc3)c3cc(Br)ccc3C2=NC1
Brc1cc(Br)cc(Nc2ncnc3c(Br)cc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

 18 Training on 31406 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.762045
Reward: 3.485143
Trajectories with max counts:
51	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.54615766
Proportion of valid SMILES: 0.6386370740856517
Sample trajectories:
BP(=O)(NO)C(=O)Oc1cc2ncnc(Nc3cccc(Br)c3F)c2s1
BP(=O)(Nc1ccc(Cl)cc1)Nc1ccc(Nc2ncnc3ccc(F)c(F)c23)cc1
BP(=O)(OCC)Oc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Bc1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1Br
BrC#CC12c3ccccc3C1c1cc(Br)ccc12
Policy gradient replay...
Mean value of predictions: 0.6275122
Proportion of valid SMILES: 0.6426332288401254
Sample trajectories:
BP(=O)(OCC)OC(=O)CCl
BP(=O)(OCCCl)OC(=O)CNc1ccc(Br)c(Cl)c1
BrCCCC(Br)CI
BrCCN1CCOc2cnc(Nc3c(Br)cc(Br)c(Br)c3Br)c(Nc3ccc(Br)cc3Br)nN2CC1
BrCCNc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Fine tuning...
Mean value of predictions: 0.57337576
Proportion of valid SMILES: 0.6888888888888889
Sample trajectories:
Bc1ccnc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4nc5nccnc5s4)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3c(Br)cccc23)c1
Brc1cc(Nc2ncnc3scc(-c4ccccc4)c23)ncn1

 19 Training on 34257 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.504093
Reward: 3.414412
Trajectories with max counts:
11	Brc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Mean value of predictions: 0.6043302
Proportion of valid SMILES: 0.6954203262233375
Sample trajectories:
B[PH](=O)(Cl)(CCl)P(=O)(F)(F)(F)P(=O)(O)O
Bc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
BrCCCCCCCCCCCN1CCCC1CNc1ccc(I)cc1
BrCCCCCc1sc2ncnc(Nc3ccc(Br)cc3)c2c1Br
Brc1cc(Br)c(-c2nc3cc(Br)c(Br)c(Br)c3s2)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.5612115
Proportion of valid SMILES: 0.69125
Sample trajectories:
BP(=O)(Nc1nc2ccc(Br)cc2s1)N(=O)=O
BP(=O)(Oc1ccccc1)P(=O)(O)Nc1ccccc1
BrC(=CN1CCCCC1)c1ccc(Br)cc1
BrCCNc1nc2ccccc2nc1-c1ccccc1
BrCCc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1Br
Fine tuning...
Mean value of predictions: 0.61392343
Proportion of valid SMILES: 0.678448545511417
Sample trajectories:
BP(=O)(NC(=O)CCCl)OCC
BrC#CN1C=C2CCCN2CC1
BrCCBr
BrCCNc1ncnc2c1Nc1c(Br)cncc12
BrCc1ncnc2c(Nc3ccc(Br)cc3)ncnc12

 20 Training on 37378 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 33.400465
Reward: 3.868760
Trajectories with max counts:
39	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.63275194
Proportion of valid SMILES: 0.6460093896713615
Sample trajectories:
BrC(=NNc1ccc(Br)nc1)Nc1nc(Br)c(Br)nc1Br
BrC=CBr
BrCCCCOc1ccc(Nc2ncnc3ccsc23)cc1
BrCCOc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
BrCCc1cccc(Nc2ncnc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.602192
Proportion of valid SMILES: 0.7134813888020018
Sample trajectories:
BP(=O)(OCC)OC(=O)CSc1nc2c(Br)c(Br)c(Br)c(Br)c2s1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCc1nc2ncnc(Nc3cccc(Br)c3)c2s1
Brc1cc(Br)c(Nc2ncnc3sccc23)cc1Br
Brc1cc(Br)c2c(NCCN3CCNCC3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.61060196
Proportion of valid SMILES: 0.6958424507658644
Sample trajectories:
BP(=O)(NCc1ccc(Br)cc1)P(=O)(O)O
BP(=O)(OCC)Oc1ccc(Nc2ncnc3c(Br)cc(Br)c(Br)c23)cc1
Bc1ccc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)cc1Br
BrCCCCCCCCCCCNCCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1c2ncnc(Nc3ccc(Br)cc3)cc2N1Cc1ccccc1Br

Trajectories with max counts:
96	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5802485
Proportion of valid SMILES: 0.6040898005127885
Mean Internal Similarity: 0.4811106853625844
Std Internal Similarity: 0.09069586314353442
Mean External Similarity: 0.43929837390992543
Std External Similarity: 0.0824542958010098
Mean MolWt: 467.18610780115444
Std MolWt: 118.34159168543802
Effect MolWt: -0.31204175612084734
Mean MolLogP: 5.512875717743439
Std MolLogP: 1.721022554918941
Effect MolLogP: 0.4934539083411784
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.226502% (1262 / 1298)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 20, 'seed': 3, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5700.433534622192, 'valid_fraction': 0.6040898005127885, 'active_fraction': 0.5560041407867495, 'max_counts': 96, 'mean_internal_similarity': 0.4811106853625844, 'std_internal_similarity': 0.09069586314353442, 'mean_external_similarity': 0.43929837390992543, 'std_external_similarity': 0.0824542958010098, 'mean_MolWt': 467.18610780115444, 'std_MolWt': 118.34159168543802, 'effect_MolWt': -0.31204175612084734, 'mean_MolLogP': 5.512875717743439, 'std_MolLogP': 1.721022554918941, 'effect_MolLogP': 0.4934539083411784, 'generated_scaffolds': 1298, 'novel_scaffolds': 1262, 'novel_fraction': 0.9722650231124808, 'save_path': '../logs/replay_combo_s3-2.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.447887
Reward: 1.000000
Trajectories with max counts:
2	OCC1OC(CO)C(O)C(O)C1O
Mean value of predictions: 0.0016032064
Proportion of valid SMILES: 0.7811521603005636
Sample trajectories:
BP(=O)(O)OP(=O)(O)SCC(N)CC(=O)O
Brc1ccc(-c2cc(-c3cccnc3)on2)cc1
Brc1ccc(C=Nn2cnnn2)cc1
Brc1ccc(CN2C=Nc3ccc4c(c3N=C2c2ccncc2)OCO4)cc1
Brc1cccc2c1-c1ncccc12
Policy gradient replay...
Mean value of predictions: 0.038512036
Proportion of valid SMILES: 0.5732204452806522
Sample trajectories:
Bc1ncc(-c2ccc(Cl)cc2)c(N2CCC(N3CCOCC3)C2)c1COc1ccc2c(c1)OCO2
Brc1ccc(-c2cnc3cnc4ccsc4n23)cn1
Brc1ccc(Nc2cnc3ncncc3n2)cc1
Brc1ccc(Nc2cnccn2)cc1
Brc1ccc(Nc2ncc3ccccn23)cc1
Fine tuning...
Mean value of predictions: 0.053778924
Proportion of valid SMILES: 0.6101003764115432
Sample trajectories:
BP(=O)(N=O)OCC
BrC1=CC2=NC=CN12
Brc1cc(Nc2ncnc3ccc(Br)c(Br)c23)on1
Brc1ccc(-c2cc3ncnc(Nc4ccccc4Br)n3ccn2)cc1
Brc1ccc(-c2nc3c(-c4cccnc4)n[nH]c3c3cccnc23)cc1

  2 Training on 469 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.890026
Reward: 1.137500
Trajectories with max counts:
6	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.06298342
Proportion of valid SMILES: 0.5675760426465977
Sample trajectories:
BrCCCCNc1c2ccccc2nc2ccccc12
Brc1ccc(-c2cn[nH]c2)c(Nc2ccncn2)n1
Brc1ccc(Br)c(Br)c1
Brc1ccc(CNc2nccc(-c3cccnc3)n2)c2ncnc(Nc3ccccc3)c12
Brc1ccc(NN=C(COc2ccccc2Br)c2ccccc2)cc1
Policy gradient replay...
Mean value of predictions: 0.07098194
Proportion of valid SMILES: 0.7105757196495619
Sample trajectories:
BrC1=CCCCCC1c1ccccc1NC1CCN(c2ncccn2)CC1
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1ccc(C=NN=Cc2ccccc2)cc1
Brc1ccc(CN2c3ccccc3ccc3ccccc32)cc1
Brc1ccc(Cc2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.12357553
Proportion of valid SMILES: 0.5987480438184664
Sample trajectories:
BP(=O)(CCCS)OCCO
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)OP(=O)(O)OP(=O)(O)OCC1OC(CO)C(O)C1O
Bc1ccc(NS(=O)(=O)c2ccc(Nc3cc4nc(Sc5ccccc5)n[nH]c4ncn3)cc2)cc1
BrCCOc1ccccc1Nc1ccc(CBr)cc1
Brc1ccc(N2CCN(c3ccc(Nc4ncnc5cc(Br)ccc45)cc3)CC2)cc1

  3 Training on 1190 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 22.008377
Reward: 1.504324
Trajectories with max counts:
7	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
7	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.15245008
Proportion of valid SMILES: 0.5194846008799497
Sample trajectories:
BP(=O)(N=C(CCCCC)N(O)CC(N)=O)OCC
BrCCN(CCCCNc1ccc(Br)cc1)COCCOc1ccc(-c2cnc3ccccc3c2)cc1
Brc1c(Nc2ncnc3ccccc23)ncnc1-c1ccncc1
Brc1cc2c(s1)N=Nc1nc(Nc3cccnc3)nc2n1
Brc1ccc(-c2nc(Br)sc2Br)cc1
Policy gradient replay...
Mean value of predictions: 0.15501931
Proportion of valid SMILES: 0.6479049405878674
Sample trajectories:
BP(=O)(=N[PH](P(=O)(O)O)=[PH](=O)(O)O)(NO)Nc1cccc2ccccc12
BP(=O)(CC(N)Cc1cccc2ccccc12)NCCCN
BP(=O)(OCC)N1C=CC(=O)N(C)C1=O
BP1(=O)OCC2OC(OCCCN(C=C(Cl)C(=O)OCCl)C(=O)N=C(N)C1=O)C(O)(Br)C(O)C2O
BrCN1CCN(Cc2nc3ccccc3s2)CC1
Fine tuning...
Mean value of predictions: 0.18697765
Proportion of valid SMILES: 0.6443331246086412
Sample trajectories:
BrC1=CC2=CC3=CCCCC3=C2CO1
BrCCCN(CCBr)c1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(-c2ccccc2)n(-c2ccccc2)n1
Brc1cc2c(cc1-c1ccccc1)c1ccccc1N2
Brc1ccc(-c2ncnc3ncnc(Nc4cccc(Br)c4)c23)cc1

  4 Training on 2495 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 19.724497
Reward: 1.321683
Trajectories with max counts:
17	Fc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
Mean value of predictions: 0.22397508
Proportion of valid SMILES: 0.6033187226048842
Sample trajectories:
BrCCCCBr
Brc1cc2c(Nc3ccc(I)cc3)ncnc2cn1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1cc2ncnc(Nc3ccccc3)c2cc1Nc1ncnc2ncnc(N3CCCC3)c12
Brc1ccc(-c2c[nH]c3cccc(Br)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.29095528
Proportion of valid SMILES: 0.6165413533834586
Sample trajectories:
BP(=O)(OCC(=O)Nc1ccccc1Br)N1CCCCC1
Bc1cc(Br)cc(Nc2ncnc3sc(Br)c(Br)c23)c1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(CNc2ncnc3cccnc23)cc1
Brc1ccc(Nc2cc(Br)cc(-c3ccncc3)c2)cc1
Fine tuning...
Mean value of predictions: 0.29573104
Proportion of valid SMILES: 0.5870927318295739
Sample trajectories:
BP(=O)(OC(=O)COc1ccc(Br)cc1)P(=O)(O)CC(F)(F)F
BP(=O)(OCC1(O)COC(=O)C(C(=O)O)C(O)C1O)OP(=O)(O)O
BP(=O)(OCCCCCC)C(F)(F)F
BrCCSc1ccc2c(c1)N2c1ncncn1
Brc1cc(Nc2ncnc3ccccc23)ncn1

  5 Training on 4223 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 24.512684
Reward: 1.897730
Trajectories with max counts:
27	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.2681934
Proportion of valid SMILES: 0.6140625
Sample trajectories:
Brc1ccc(NN=Cc2ccc(Nc3ncnc4ncsc34)s2)cc1
Brc1ccc(Nc2ccnc3ccccc23)cc1
Brc1ccc(Nc2ccnc3nc(Nc4ccccc4)c(Br)cc23)cc1
Brc1ccc(Nc2ccncc2Br)cc1
Brc1ccc(Nc2nc3ncnc(Nc4ccc(Br)cc4)c3cc2Br)cc1
Policy gradient replay...
Mean value of predictions: 0.34562212
Proportion of valid SMILES: 0.6133793969849246
Sample trajectories:
BP(=O)(CC1N=CC(=NCc2cc(Br)c(Br)cc2O)O1)OCC
BP(=O)(NCCCCN)C(F)(F)F
BP(=O)(NO)c1ccc(Br)c(Br)c1
BrCCBr
BrCCCBr
Fine tuning...
Mean value of predictions: 0.30504245
Proportion of valid SMILES: 0.626720901126408
Sample trajectories:
BP(=O)(NCCCCCCCO)C(=O)Oc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1ccc(-c2ncc(-c3ccncc3)s2)cc1
Brc1ccc(-c2ncnc3[nH]ccc23)o1
Brc1ccc(CNCC23CCC4CC(CC(C4)C2)C3)cc1N1CCOCC1
Brc1ccc(Nc2cc(Br)cnc2Oc2ncccc2Br)cc1

  6 Training on 6119 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 24.484995
Reward: 2.008761
Trajectories with max counts:
33	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.37175024
Proportion of valid SMILES: 0.6117720726361928
Sample trajectories:
BP(=O)(OCC)ON=CC1=COC(O)(N2C=CC(=O)NC2=O)C(O)C1O
BP(=O)(OCC=C)c1ccc(NC(=O)CN(CCC=C)C(=O)c2cncn2C)cn1
BP(=O)(c1ccc(Br)cc1)N(c1ccccc1)c1ccc(Br)cc1
Bc1ccc(-c2ncnc3scnc23)c(I)c1
Brc1cc(Br)c2c(Nc3cccc(I)c3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.35835034
Proportion of valid SMILES: 0.6150955214531788
Sample trajectories:
BP(=O)(CCCCCC)CCCCCCCCCCCCCCN
BP(=O)(CCCCCCCCC(=O)Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(CCCCNC(=O)Oc1ccc(Cl)cc1Oc1ccc(Br)cc1)OCC
BP(=O)(OCC)OCCCCCN=C1C=C(Br)C(c2ccccc2)CC1
BP(=O)(OCCc1ccccc1)P(=O)(O)OCCCOP(=O)(Oc1ccccc1)Oc1ccc(F)cc1
Fine tuning...
Mean value of predictions: 0.3879028
Proportion of valid SMILES: 0.6312989045383411
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
BrC(=NNc1ccccc1)c1ccc(Br)cc1Br
BrC1=CCCCC1
Brc1ccc(Br)c(Nc2ncnc3c(N(c4ccccc4)c4ccccc4)ncnc23)c1
Brc1ccc(N2CCNCCCNc3ncnn32)cc1

  7 Training on 8308 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 25.406557
Reward: 2.357234
Trajectories with max counts:
29	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.385905
Proportion of valid SMILES: 0.5856785490931833
Sample trajectories:
BP(=O)(NCCCCCCCCCO)c1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BP(=O)(NCCCN)C(=O)Nc1cccc(Br)c1N
Brc1cc(Br)cc(Nc2ccc(Nc3ccccc3Br)cc2)c1
Brc1cc(Br)cc(Nc2ncnc3c2CN(Cc2ccccc2Br)CCCN3)c1
Brc1cc(Nc2ncnc3ccccc23)nc2ccccn2c1
Policy gradient replay...
Mean value of predictions: 0.31815767
Proportion of valid SMILES: 0.7058455767427321
Sample trajectories:
BP(=O)(CCNc1ccccc1Nc1cccc(Br)c1)NCCCCCCCCCCCCN
BP(=O)(Nc1cccc(Br)c1)P(=O)(O)Oc1ccc(F)c(F)c1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(Nc2c(Br)ncnc2N2CCOCC2)cc1
Brc1ccc(Nc2ccccc2-c2ccccc2-c2ccc(-c3ccccc3Br)cc2)cc1
Fine tuning...
Mean value of predictions: 0.41573033
Proportion of valid SMILES: 0.6400875547217011
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)nc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Nc4ccccc4)c3)c2cc1Br
Brc1ccc(-c2ncnc3ccc(Br)cc23)c(Br)c1

  8 Training on 10465 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 25.528358
Reward: 2.351443
Trajectories with max counts:
29	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.4591503
Proportion of valid SMILES: 0.575187969924812
Sample trajectories:
BP(=O)(NCCCCCCCCCCCCCCCN)Nc1ccc2ncnc(Nc3ccc(F)c(F)c3)c2c1
BP(=O)(OC)OCC
BP(=O)(OCC)C(=O)Nc1ccc(F)c(Nc2ncnc3c(Cl)cc(Cl)cc23)c1
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC)OC(F)(F)F
Policy gradient replay...
Mean value of predictions: 0.3898189
Proportion of valid SMILES: 0.6394366197183099
Sample trajectories:
BP(=O)(OCC)OC(=O)CNCCOCOCCOC(=O)CCCCCC(=C)COP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC)c1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Bc1ccc(Nc2ncnc3ncncc3s2)cc1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(-c4ccccc4Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.43896472
Proportion of valid SMILES: 0.6653099561678146
Sample trajectories:
BP(=O)(CCCC(=O)NO)C(=O)COC(=O)CS
Bc1cnc2ncnc(Nc3cccc(Br)c3)c2c1
BrCCCCCOc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrNc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1

  9 Training on 12264 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.958992
Reward: 2.414131
Trajectories with max counts:
22	Clc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4558767
Proportion of valid SMILES: 0.6538582677165354
Sample trajectories:
BP(=O)(NCCCN)C(N)C(=O)C(F)(F)F
BrCCCCCNc1ccc(Nc2ncnc3scnc23)cc1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1ccc(NN=Cc2ccc(Nc3ncnc4ccc(Br)cc34)cc2)cc1
Brc1ccc(Nc2cc(Nc3cncnc3)ccc2Br)cc1
Policy gradient replay...
Mean value of predictions: 0.4750374
Proportion of valid SMILES: 0.6277760400375352
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
Brc1cc(Nc2ncnc3c4ncnn(cnc23)[nH]4)cc(-c2ccccc2Br)c1
Brc1cc2c(Nc3cc(I)c(Br)cn3)ncnc2s1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ccnc2s1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.4772928
Proportion of valid SMILES: 0.6403894472361809
Sample trajectories:
BP(=O)(N=CC=CCC(=O)Nc1ccc(Br)cc1)OCC
BP(=O)(NCCCN)C(=O)Nc1ccc(Br)cc1
BP(=O)(Nc1cc(Br)c(Br)cc1Br)C(=O)O
BrCN1CCN(CCc2ccc(Nc3ncnc4ccccc34)nc2)O1
BrCc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1

 10 Training on 14321 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.881097
Reward: 2.606749
Trajectories with max counts:
29	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.50891924
Proportion of valid SMILES: 0.5997482693517936
Sample trajectories:
BC(=O)Nc1ccc(Nc2ncnc3sc(Cl)cc23)cc1
Bc1ccc(Nc2ncnc3c(Br)cnc(Nc4cccc(Br)c4)c23)cc1
BrCc1cc(Nc2ncnc3cc(Br)cc(Br)c23)[nH]n1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c2c(Nc3cc(Br)no3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.4628598
Proportion of valid SMILES: 0.6636392107735671
Sample trajectories:
Bc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Br
BrCCNc1ncnc2ncnc(Nc3ccc(Br)cc3)c12
BrCc1cc2ncnc(Nc3cccc(Br)c3)c2cc1Br
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1cc2ncnc(Nc3ccccc3)n2c1NC1CCCCC1
Fine tuning...
Mean value of predictions: 0.50068086
Proportion of valid SMILES: 0.6433041301627034
Sample trajectories:
BrCc1ccc(Nc2ncnc3cc(Br)ccc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ccnc2c1
Brc1ccc(CN2CCCCC2)cc1
Brc1ccc(Nc2c(Br)sc3ncnc(Nc4ccc(Br)s4)c23)cc1
Brc1ccc(Nc2cc3c(Nc4ccc(Br)cc4)ncnc3s2)cc1

 11 Training on 16505 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.066697
Reward: 2.487204
Trajectories with max counts:
31	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.48422113
Proportion of valid SMILES: 0.622848200312989
Sample trajectories:
BP(=O)(CC(O)=NO)NO
Bc1ccc(Nc2ncnc3ccsc23)cc1Br
BrCCNc1nc2ccc(Br)cc2s1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc2ncn(-c3ccccc3)c2nc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Policy gradient replay...
Mean value of predictions: 0.55462784
Proportion of valid SMILES: 0.6226119636705293
Sample trajectories:
BP(=O)(CCl)N=CNc1ccc(Br)cc1
BP(=O)(NC(Cl)(Cl)Cl)P(=O)(NO)C(=O)OCOc1ccc(Br)cc1
B[PH](=O)(=Nc1ccc(Br)c(N)c1)N(c1ccc(Br)cc1)c1ccc(Br)cc1
BrCCCCCCCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.49347112
Proportion of valid SMILES: 0.6659368157647795
Sample trajectories:
BP(=O)(NC(=O)CCCCCCCCCCCl)OP(=O)(O)OP(=O)(O)O
BP(=O)(c1ccc(Br)cc1)N(O)Cc1ccccc1
BrCc1sc2ncnc(Nc3ccc(Br)cc3)c2c1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1

 12 Training on 18763 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.544705
Reward: 2.919842
Trajectories with max counts:
18	Fc1ccc(Nc2ncnc3ccsc23)cc1
18	Nc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Mean value of predictions: 0.51013136
Proportion of valid SMILES: 0.6683385579937304
Sample trajectories:
BP(=O)(c1ccccc1)N(CCCN)Cc1ccc(O)cc1
BrCCc1ccc2c(c1-c1cccs1)Nc1ccccc1-2
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(OCc4cccc5ncnc(Nc6ncccc6Br)c45)c3)ncnc2c1
Brc1cc2c(Nc3cccs3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.5974516
Proportion of valid SMILES: 0.6630162703379224
Sample trajectories:
BP(=O)(NCC=C)OCCS
BrCCCCCCCCCCCCCCCCCCCN=CC1COc2ccccc2N=C2c3cc(Br)ccc3C21
BrCN1CCCCC1CNc1cc2ncnc(Nc3cccc(Br)c3)c2cc1Nc1ncnc2ccccc12
BrCc1cnc2c(Nc3ccc(Nc4nc5cncnc5cc4Br)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3cnc(Br)cn3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.4999104
Proportion of valid SMILES: 0.697718036886527
Sample trajectories:
BP(=O)(OCC)OC(=O)COc1ccc(Cl)cc1Br
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1
BrC=CC=Cc1ccccc1-c1ccc(Br)cc1
BrC=Cc1c(Br)sc2ncnc(Nc3ccc(Br)cc3)c12
BrCCCCc1ccc(Nc2ncnc3cc(Br)cc(-c4cccs4)c23)cc1

 13 Training on 21432 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.757443
Reward: 3.141809
Trajectories with max counts:
27	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.59027505
Proportion of valid SMILES: 0.6370463078848561
Sample trajectories:
BrCBr
BrCCCCCCCNc1cc2ncnc(Nc3cccc(Br)c3)c12
BrCN1CCOc2c(ncnc2Nc2ccc(Br)cc2)C1
Brc1c[nH]c(Nc2ncnc3c(Br)cc(Br)nc23)c1
Brc1cc(-c2ccsc2)c(Br)cn1
Policy gradient replay...
Mean value of predictions: 0.5823059
Proportion of valid SMILES: 0.6653099561678146
Sample trajectories:
BP(=O)(CC=CC(=O)O)N(O)CCCN
BP(=O)(OCC)C(F)(F)F
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCCCCCCCCCCCCCCCCCCCCCNc1ccc2ncnc(Nc3cccc(Br)c3)c2n1
BrCCNc1ccc(Nc2ncnc3c4ccccc4c23)cn1
Fine tuning...
Mean value of predictions: 0.5489945
Proportion of valid SMILES: 0.6878340144608613
Sample trajectories:
Brc1[nH]c2ccc(Nc3ccccc3)cc2c1Br
Brc1cc(Br)c(Nc2ccnc3ccc(I)cc23)cc1Br
Brc1cc(Nc2ncnc3cncnc23)ccc1-c1ccsc1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ccnc2s1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1

 14 Training on 24187 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.172637
Reward: 3.382142
Trajectories with max counts:
16	Brc1ccc(Nc2ncnc3ccsc23)cc1
16	Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
16	Cc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49681857
Proportion of valid SMILES: 0.729153605015674
Sample trajectories:
BC(=O)Nc1ccc(Nc2ncnc3cccc(Br)c23)cc1
BP(=O)(CSc1ccc(-c2ccc(Cl)cc2)c(Nc2ccc(Cl)cc2)c1)NCCCCN
BP(=O)(O)CN(c1ccc(Nc2ccc(Cl)cc2F)c(F)c1)C(C(=O)OCC)C(F)(F)F
BrCCN(CCN1CCN(c2cccc(Br)c2)CC1)C1CC1
BrCN1CCN(Cc2ccc3c(c2)OCO3)CC1
Policy gradient replay...
Mean value of predictions: 0.6124024
Proportion of valid SMILES: 0.6848065429380308
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Nc2cc3c(SCc4ccccc4)cc4ncncn4nc3cn2)ccn1
Brc1cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)ncn1
Brc1cc2c(Nc3ccc(Br)c(Nc4ccncc4)c3)ncnc2s1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cn1
Fine tuning...
Mean value of predictions: 0.56625855
Proportion of valid SMILES: 0.6901408450704225
Sample trajectories:
BrCCNc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1cc(Br)c(Nc2ncnc3c(Nc4ccccc4Br)ncnc23)cc1Br
Brc1cc(Nc2ncnc3ccsc23)ccc1-c1ccccc1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cn1
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1NCCCNc1ccnc2cccnc12

 15 Training on 27056 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.975327
Reward: 3.274779
Trajectories with max counts:
27	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5779565
Proportion of valid SMILES: 0.6624882481980571
Sample trajectories:
BP(=O)(NC(c1cccc(Cl)c1)P(=O)(O)Oc1ccc(Br)cc1)Oc1ccc(Br)c(Br)c1
BrBr
Brc1cc(Br)c2c(Nc3cc(Br)c4ccccc4n3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Nc4ccccc4Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3c(Br)cc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.60173285
Proportion of valid SMILES: 0.6885400313971742
Sample trajectories:
BP(=O)(CCCNS(=O)(=O)c1ccc(Nc2ncnc3ccsc23)cc1O)NCCCCN
BP(=O)(CCNCCNc1cc(Br)c(Br)c(Br)c1Br)NO
BrCC1CC(c2ccc3ncnc(Nc4ccc(Br)cc4)c3c2)=NO1
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.60890126
Proportion of valid SMILES: 0.6751173708920187
Sample trajectories:
BP(=O)(Nc1cccc(Cl)c1Nc1ccc(Br)cc1)OCCC
B[PH](=O)(Nc1ccc(Br)cn1)=[PH](=O)(Br)P(=O)(Oc1ccccc1)Oc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(I)cc23)c1

 16 Training on 30075 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.061032
Reward: 3.591562
Trajectories with max counts:
44	CS(=O)(=O)c1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5871894
Proportion of valid SMILES: 0.6543634657491398
Sample trajectories:
BrCN1CCOc2ccc(Br)cc2C1CNc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cn1
Brc1cc2ncnc(Nc3ccccc3)c2c(Nc2ccncn2)c1Br
Policy gradient replay...
Mean value of predictions: 0.57129127
Proportion of valid SMILES: 0.7078269324258629
Sample trajectories:
BP(=O)(NCCCCCCCCCCCCCCCCC(=O)O)NC(=O)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c2ncnc(Nc3cc(Br)c(Br)cn3)c2c1
Fine tuning...
Mean value of predictions: 0.6041237
Proportion of valid SMILES: 0.7098313713013045
Sample trajectories:
BP(=O)(C=O)N(O)CO
BP(=O)(NC(CCCC(=O)Nc1ccc(Br)c(Br)c1)C(=O)OCC(F)(F)F)C(=O)OCC(F)(F)F
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1

 17 Training on 32780 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.655567
Reward: 3.502796
Trajectories with max counts:
27	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.5175495
Proportion of valid SMILES: 0.6792866082603254
Sample trajectories:
BP(=O)(CCCl)OCC
BP(=O)(OCC)ON=C(C)c1ccc(Br)cc1
BP(=O)(Oc1ccc(Br)cc1)C(CCCNCCCCCl)NCc1ccc(Nc2ncnc3ccccc23)cc1
Bc1ccc(Nc2ncnc3sc4ccccc4c23)o1
BrCc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.56409186
Proportion of valid SMILES: 0.7538558388416745
Sample trajectories:
B[PH](=O)(Nc1ccc(F)cc1)(c1cccc(F)c1)c1ccc(F)c(F)c1
Bc1ccc(Nc2ncnc3c4ccccc4c23)cc1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc2ncnc(Nc3ccc(CN4CCOCC4)cc3)c2s1
Brc1ccc(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.59492064
Proportion of valid SMILES: 0.6899249061326659
Sample trajectories:
BrCC1cc2c(Nc3ccc(Nc4ccccc4Br)cc3)ncnc2ccc(c2cccnc2)O1
Brc1cc(Br)c2c(Nc3c(Br)cc(Br)c(Br)c3Br)ncnc2c1
Brc1cc2ncnc(Nc3cc(Br)c(Br)cc3Br)c2s1
Brc1cc2ncnc(Nc3ccc(Br)c(-c4ccccc4Br)c3)c2s1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1

 18 Training on 35688 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.563921
Reward: 3.879941
Trajectories with max counts:
28	Clc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.62025076
Proportion of valid SMILES: 0.6503606146127313
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1OCCCCCCN1CCCCCC1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Nc2ncnc3ccsc23)c1
Brc1cc(Br)c2c(Nc3cc(Nc4c(Br)ccc(Br)c4Br)cc(Nc4ncnc5cc(Br)c(Br)cc45)n3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.6356659
Proportion of valid SMILES: 0.6952291274325173
Sample trajectories:
BrCCBr
BrCc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Brc1cc(Br)c(Nc2ncnc3ccsc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.6225473
Proportion of valid SMILES: 0.7331230283911672
Sample trajectories:
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2cc1Br
Brc1cc(Br)cc(Nc2ncnc3c(Br)cccc23)c1
Brc1cc2nccc(Nc3ccc(Br)c(N4CCCCCCCCCCCCCC4)c3)c2cc1Br
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1ccc(-c2ccc3ncnc(Nc4cccc(CNc5ccccc5)c4)c3n2)cc1

 19 Training on 38977 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.070721
Reward: 4.002671
Trajectories with max counts:
19	Brc1ccc(Nc2ncnc3ccsc23)cc1
19	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.654514
Proportion of valid SMILES: 0.7243429286608261
Sample trajectories:
BP(=O)(NC(=O)CCCCC(=O)ONC(=O)c1cc(Br)cc(Br)c1)Oc1cc2c(N)ncnc2s1
BP(=O)(NC(c1ccc(Br)cc1)P(=O)(O)O)N1CCN(O)Cc2cc(F)c(F)cc2C1
BP(=O)(NCCCCCCl)c1cccc(Nc2ncnc3c2nc2c(Br)c(F)c(F)ccc23)c1
BP(=O)(NO)C(Br)=CBr
Bc1cc(Br)c2c(Nc3ccc(Br)cc3)ncnc2c1Br
Policy gradient replay...
Mean value of predictions: 0.6140195
Proportion of valid SMILES: 0.7092511013215859
Sample trajectories:
BrCCc1cc2ncnc(Nc3ccc(Br)cc3)c2nc1NCCCCCNc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cc(Nc2ncnc3ncnc(N4CCCCC4)c23)nc(-c2ccccc2Br)n1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(CNc2ccccc2Br)cc1Br
Fine tuning...
Mean value of predictions: 0.6737511
Proportion of valid SMILES: 0.6894176581089543
Sample trajectories:
BP(=O)(CS(=O)(=O)Nc1ccc(N)cc1)NCCCCCO
Brc1cc(Br)c2c(NCCCN3CCCC3)ncnc2c1Nc1ncnc2sccc12
Brc1cc(Br)c2c(Nc3ccc(-c4nc(Nc5ccc(Br)[nH]5)cn4-c4ccccc4Br)cc3I)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c4cc(Br)ccc34)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3c(Br)cc(Br)cc23)c1

 20 Training on 42554 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 37.963129
Reward: 4.123194
Trajectories with max counts:
17	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.697413
Proportion of valid SMILES: 0.7333987569512594
Sample trajectories:
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Nc1ccc2ncnc(Nc3cccnc3)c2c1
Brc1cc2ncnc(Nc3ccccc3)c2nc1Nc1ccc(Nc2ncnc3ccccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.65810007
Proportion of valid SMILES: 0.6930661577608143
Sample trajectories:
BP(=O)(Nc1ccc(Br)c(Br)c1)P(=O)(OCC)OCC
BrC=CC=CCCBr
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Nc2cccc3sc(Br)cc23)ccn1
Brc1cc(Nc2ncnc3ccsc23)ncn1
Fine tuning...
Mean value of predictions: 0.64394146
Proportion of valid SMILES: 0.7136522014570795
Sample trajectories:
BrCCCBr
Brc1cc2ncnc(Nc3ccc(Br)c(CN4CCCC4)c3)c2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCCCC1
Brc1ccc(-c2ccc(Nc3ncnc4ccsc34)cc2)cc1
Brc1ccc(-c2ccnc3ccc(Br)cc23)c(Nc2cccc(Nc3ncnc4[nH]cnc34)c2)c1

Trajectories with max counts:
81	Brc1ccc(Nc2ncnc3ccsc23)cc1
81	Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.614999
Proportion of valid SMILES: 0.6332228454869965
Mean Internal Similarity: 0.47375141529245707
Std Internal Similarity: 0.09466685357415378
Mean External Similarity: 0.4262522802218545
Std External Similarity: 0.07801161405987986
Mean MolWt: 457.3544011299436
Std MolWt: 173.79833086133416
Effect MolWt: -0.29711705469101457
Mean MolLogP: 6.059137697312105
Std MolLogP: 4.022298178119181
Effect MolLogP: 0.41032585733528343
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.714681% (1411 / 1444)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 100, 'seed': 3, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 6270.0396955013275, 'valid_fraction': 0.6332228454869965, 'active_fraction': 0.5879806724380914, 'max_counts': 81, 'mean_internal_similarity': 0.47375141529245707, 'std_internal_similarity': 0.09466685357415378, 'mean_external_similarity': 0.4262522802218545, 'std_external_similarity': 0.07801161405987986, 'mean_MolWt': 457.3544011299436, 'std_MolWt': 173.79833086133416, 'effect_MolWt': -0.29711705469101457, 'mean_MolLogP': 6.059137697312105, 'std_MolLogP': 4.022298178119181, 'effect_MolLogP': 0.41032585733528343, 'generated_scaffolds': 1444, 'novel_scaffolds': 1411, 'novel_fraction': 0.9771468144044322, 'save_path': '../logs/replay_combo_s3-3.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.757501
Reward: 1.000000
Trajectories with max counts:
2	O=C1CCCc2ccccc21
Mean value of predictions: 0.0014699877
Proportion of valid SMILES: 0.767471012221874
Sample trajectories:
Brc1ccc(CCN(c2cccc(Oc3ccccc3)c2)C(Br)Cc2ccncc2)o1
Brc1ccc(NCCN2CCOCC2)cn1
Brc1cncc(C=Cc2ccccc2)c1
C#CC1CC(c2ccc(N)cc2)C2CCC(O1)N2C
C#CC1CCCCC1N(CC)CC
Policy gradient replay...
Mean value of predictions: 0.015982952
Proportion of valid SMILES: 0.5911811023622047
Sample trajectories:
BrCc1c2cnc(-c3ccncn3)nc2nc2cccc(-c3ccc(Br)cc3)c12
Brc1c[nH]c2nc(Cc3c[nH]cn3)nc2c1
Brc1cc[nH]c1-c1cc2ncnc(Nc3ccccn3)n2n1
Brc1ccc(-c2nnc(NCc3ccccc3)o2)cc1
Brc1ccc(Nc2nc(-c3ccsc3)cs2)nc1
Fine tuning...
Mean value of predictions: 0.012678289
Proportion of valid SMILES: 0.5949088623507228
Sample trajectories:
Brc1c(-c2cccnc2)cnn1-c1cccnc1
Brc1cc(-c2nnc3[nH]cnc3n2)on1
Brc1ccc(-c2nnn3nc(-c4ccccc4)c(NC4CCCCC4)nc23)cc1
Brc1ccc(Br)o1
Brc1ccc(CNc2ncc(Br)c(N=CC=Cc3ccccn3)n2)cc1

  2 Training on 297 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.600280
Reward: 1.010890
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.015754476
Proportion of valid SMILES: 0.6142004398366321
Sample trajectories:
BrC(=NN1CC=C(Nc2nccc(-c3c[nH]cn3)n2)CC1)c1ccc(Br)cc1
BrCc1nnc2n1N=C(c1ncncc1Br)n1ncnc1-2
Br[n+]1cc2c(Nc3ccc(NC4CCCC4)nc3)ncnn2c1
Brc1cc2nnnc(-c3ccco3)n2n1
Brc1ccc(C2CNCCN2)o1
Policy gradient replay...
Mean value of predictions: 0.01684293
Proportion of valid SMILES: 0.7980618943419818
Sample trajectories:
BP(=O)(OCC1Oc2ccccc2-c2ccccc21)P(=O)(O)O
Bc1cccc(Nc2ccccc2F)c1
BrC(=NNc1ccccc1)c1ccccc1
Brc1cc(Oc2ccccc2)n(C2CCCCC2Nc2ccc(Br)c(Br)c2)c1
Brc1ccc(-c2ccccc2)c(CNc2ccccn2)c1
Fine tuning...
Mean value of predictions: 0.017154152
Proportion of valid SMILES: 0.790625
Sample trajectories:
BP(=O)(OCC1OC(C(=O)O)NC1=O)c1ccccc1
Bc1cc(Br)ccc1Nc1cccnc1
Brc1ccc(-c2ccc3ccccc3c2)cc1
Brc1ccc(-c2nc(Nc3ccccc3)c3ccccc3n2)cc1
Brc1ccc(C=Cc2ccccc2Br)cc1

  3 Training on 475 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.322688
Reward: 1.037389
Trajectories with max counts:
6	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.021382114
Proportion of valid SMILES: 0.76875
Sample trajectories:
Brc1ccc(-c2ccc3ncnc(Nc4ccc(OCCCN5CCCCCC5)cc4)c3c2)cc1
Brc1ccc(-c2nc(-c3ccccc3)c3ccccc3n2)cc1
Brc1ccc(-c2nc3ccc(Nc4ccccc4-c4nc[nH]n4)nc3[nH]2)cc1
Brc1ccc(N=Cc2ccc(Oc3ccccc3)cc2)cc1
Brc1ccc(Nc2cc(-c3ccccc3)ncn2)cc1
Policy gradient replay...
Mean value of predictions: 0.06859462
Proportion of valid SMILES: 0.619811320754717
Sample trajectories:
Bc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Nc1c2ccccc2cc2cccnc12
BrC1(Cc2ccc3ccccc3c2)CCCC1
BrCC(Br)CBr
BrCCc1cccnc1NCc1cncc2cc(Br)ccc12
Brc1c(N2Cc3ccc4ccccc4c3Nc3cncc(Nc4ccc(OCCCN5CCOCC5)nc4)c3O2)[nH]c2ccccc12
Fine tuning...
Mean value of predictions: 0.065890685
Proportion of valid SMILES: 0.6204081632653061
Sample trajectories:
BP(=O)(OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O)OP(=O)(O)OP(=O)(O)OP(=O)(Oc1ccccc1)C1(OCc2ccccc2)CCCCCC1
BrCCN=C(Nc1ccccc1)Nc1ccc(Br)cc1
Brc1cc(Br)c(Br)c(Nc2ccc(Br)nc2)c1
Brc1cc(Nc2ncnc3ccccc23)c2sccc2n1
Brc1cc2cc3cccnc3Nc2c1Br

  4 Training on 937 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 20.100891
Reward: 1.412753
Trajectories with max counts:
31	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.09075377
Proportion of valid SMILES: 0.6220693966864645
Sample trajectories:
BP(=O)(N(O)Cc1ccc(Br)cc1)P(=O)(O)O
BP(=O)(OCC)C1=Nc2ccccc2Oc2c(ccc(Br)c2Br)OC1=O
Bc1ccc(Nc2ncnc3cc(F)ccc23)cc1
BrC1=Nc2ccccc2Nc2ccccc21
Brc1cc2c(nc1Cc1cnc3ccccc3c1)-c1ccccc1CCCO2
Policy gradient replay...
Mean value of predictions: 0.08158971
Proportion of valid SMILES: 0.5487491982039769
Sample trajectories:
BP(=O)(O)CCCCCCCCCCCl
BP(=O)(O)N(O)CC(=O)Nc1cc(Br)cc(Br)c1Br
BP(=O)(OCCCCCCCCCCCCCCC)OCC1OC(N2C=CC(N)=NC2=O)CC1O
Br
BrCCCC=CCCCCCCCCCCCCCCCCCCBr
Fine tuning...
Mean value of predictions: 0.0862678
Proportion of valid SMILES: 0.5599872367581366
Sample trajectories:
BP(=O)(NC(=O)C(F)(F)F)OCC1OC(n2cnc3c(N)nc(Cl)nc32)C(O)C1O
BP(=O)(OCC)C(Nc1cc(Cl)c(F)cn1)C(=O)CCCCCCCC=CCl
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCBr
BrCCCOc1cc(Nc2ncnc3cc(Br)cc(Br)c23)cnc2ncncn12
Brc1c2c(c3ncnc13)C1CCCCCCCC21

  5 Training on 1616 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 20.521843
Reward: 1.466735
Trajectories with max counts:
5	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
5	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
5	Fc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
Mean value of predictions: 0.13162705
Proportion of valid SMILES: 0.5188112551375277
Sample trajectories:
Bc1cc(Br)c2ncnc(-c3ccc(Br)c(Br)c3)c2n1
BrCCN(c1ccccc1)c1nc2c(Nc3cccc(Br)c3)ncnc12
Brc1cc(Br)c(Nc2ncnc3nnc(Br)n23)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3oc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.17936231
Proportion of valid SMILES: 0.5422823011631562
Sample trajectories:
BP(=O)(OCC)C1=CC(=O)Oc2c1cc(Br)c(Br)c2Br
BrCc1nc2ccc3ncnc(Nc4ccc(Br)cn4)c3c2s1
Brc1[nH]cnc1-c1cccc2ccccc12
Brc1cc(Br)c2c(n1)c1[nH]nc(CCCCCCCC3CCCCCC3)c1O2
Brc1cc(Br)c2ncnn2c1
Fine tuning...
Mean value of predictions: 0.17773873
Proportion of valid SMILES: 0.5351097178683386
Sample trajectories:
BP(=O)(Br)OC(Br)CBr
BP(=O)(C(=O)OC(C)(Br)CBr)N(O)C(CO)CC(C)C
BP(=O)(OCC1CCCOC(=O)C1)C(=O)OCC=C
BrCCc1c2cc(Nc3cncnc3)ccc2c(-n2cncn2)n2nnnc12
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

  6 Training on 2733 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 25.003359
Reward: 1.920020
Trajectories with max counts:
23	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.21203098
Proportion of valid SMILES: 0.5281535073922617
Sample trajectories:
BP(=O)(OCC#C)C(=O)O
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)OCCCCCCCCCCCl
BrC(=NNc1nc2cncc(Br)c2s1)N1CCCCC1
BrCC(Br)(Br)Br
Brc1cc(-c2ccccc2)c2c(OCc3cncnc3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.24076295
Proportion of valid SMILES: 0.5768626218170386
Sample trajectories:
BP(=O)(NC(=O)OC(C)(C)C)OCCCc1ccc(Nc2cc(F)c(F)c(Br)c2)cc1
BrCC=CCC[n+]1cncc(Br)c1
BrCCBr
BrCCCN=C1Nc2ccc(Br)cc2C1=Nc1nc2ccc(Br)cc2s1
BrCCOc1ccc(CNc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.25071388
Proportion of valid SMILES: 0.5504558314995285
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)C(C)C
BP(=O)(c1ccc(Br)o1)[PH](=S)N(c1ccccc1)c1ccccc1
Brc1c[nH]c(N(c2ncnc3ccc(Br)cc23)N2CCCCC2)c1
Brc1cc(Br)c2c(c1)ncn2Cc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2cc1Br
Brc1cc(Br)c2ncnc(Nc3ccnc(Br)c3)c2c1

  7 Training on 4197 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 27.728111
Reward: 2.588004
Trajectories with max counts:
71	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.2817465
Proportion of valid SMILES: 0.5159574468085106
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(NC(=O)Nc4ccc(Br)cc4)ncnc32)C(O)C1O)C(=O)O
BP(=O)(OCCCCBr)n1c(Br)nc(Nc2ccc(Br)cc2)c1Br
B[PH](=O)(=NO)OCCOc1cccc(Cl)c1
B[PH](=O)(=O)Nc1cc(Br)c(Br)c(Br)c1
BrC(=NNc1cc(Br)ccc1Br)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.25837222
Proportion of valid SMILES: 0.6090937796021472
Sample trajectories:
BP(=O)(Oc1cc2cnc(Br)cc2nc1NC(=O)c1c(Br)ccc2ccccc12)P(=O)(O)O
BP(=O)(Oc1ccc(F)c(F)c1)c1cc(Br)c(Br)cc1F
BrCCCCCCCCCNCCCCN1CCN(CCCN=C(CC2CCCCC2)Nc2nccs2)CC1
BrCCCCCCNc1ccc2[nH]cnc2c1
BrCCCCCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.2514166
Proportion of valid SMILES: 0.6039290240811154
Sample trajectories:
BP(=O)(OCCCCCCCCCCCC)C(F)(F)Cl
BrC(Br)(Br)Br
BrC(Br)=Cc1ccc(Br)cn1
BrC1=Nc2ccc(Br)cc2O1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br

  8 Training on 5765 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 27.474596
Reward: 2.826474
Trajectories with max counts:
83	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
83	Fc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.3305075
Proportion of valid SMILES: 0.5257376020087885
Sample trajectories:
BP(=O)(OCC)C(=O)O
BP(=O)(OCCOCCOP(=O)(O)OP(=O)(O)OP(=O)(O)O)P(=O)(O)OP(=O)(O)O
BP(=O)(c1ccc(Br)cn1)N1CCC(N(C(=O)c2ccc(Br)cc2)c2cc(Br)c(Br)c(Br)c2)CC1
Bc1ccc(Nc2nc3ccc(Br)cc3s2)cc1
Br
Policy gradient replay...
Mean value of predictions: 0.32154194
Proportion of valid SMILES: 0.5522855353788353
Sample trajectories:
BP(=O)(OC)OCC(F)(F)F
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Br
Brc1c[nH]c(Nc2ncnc3ccc(-c4ccc(Br)cc4Br)cc23)c1
Brc1cc(Br)c(Br)s1
Fine tuning...
Mean value of predictions: 0.34093323
Proportion of valid SMILES: 0.5307210031347962
Sample trajectories:
BP(=O)(Nc1cc(Br)c(Br)cn1)N(O)Cc1csc(Nc2cc(Br)c(Br)nc2Br)n1
BP(=O)(OCC)OC(=O)C(F)(F)F
BP(=O)(OCC)Oc1cc(Br)c(Br)cc1Br
Bc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(Br)c1
BrC1=C(Oc2cc(Br)cc(Br)c2Br)CCCCC1

  9 Training on 7516 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 26.869899
Reward: 3.260751
Trajectories with max counts:
93	Fc1ccc(Nc2ncnc3cc(F)ccc23)cc1
Mean value of predictions: 0.37895447
Proportion of valid SMILES: 0.46088861076345433
Sample trajectories:
BP(=O)(COP(=O)(O)OP(=O)(O)OP(=O)(O)O)P(=O)(O)O
BrCC#CCOC1CCCCCCCCCCCCCCCCCCCCCCCC#CCNCCCc2ccc(Br)cc2N(c2nc3ccc(Br)cc3o2)CC1
BrCCN1CCCC1CNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1c(Br)c(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.33931777
Proportion of valid SMILES: 0.5241530740276035
Sample trajectories:
BP(=O)(OCC1OC(NC(=O)OCC(Cl)(Cl)Cl)c2cc(F)c(F)cc21)c1ccc(Br)cc1
BP1(=O)OCCOCOCCCC(CCCP(=O)(O)O)C(=O)C(=O)ON(Nc2cc(F)c(F)c(F)c2F)c2ncc(O1)C2=O
B[PH](=O)(Nc1cccc(F)c1)(P(=O)(O)O)[PH](=O)(O)(O)O
BrCCOc1ccc2cc1-c1c(cc1-c1ccccc1)CCO2
Brc1cc(Br)cc(Nc2ncnc3ncnc(Nc4ccccc4Br)c23)c1
Fine tuning...
Mean value of predictions: 0.33297873
Proportion of valid SMILES: 0.5302413036665622
Sample trajectories:
B[PH]1(=O)=CC(=O)N1CCCCCCCCC(CCl)C(=O)Nc1ccc(F)c(F)c1F
Brc1cc(-c2ncnc3ocnc23)c2ccccc2n1
Brc1cc(Br)c(Br)c(Nc2ncnc3ncnc(Br)c3o2)c1
Brc1cc(Br)c(Nc2c3ncnc3cc(Br)c3cncnc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ncnc(Br)n3)ncnc2c1

 10 Training on 9162 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 27.228407
Reward: 3.588791
Trajectories with max counts:
181	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.35463452
Proportion of valid SMILES: 0.41507663434469816
Sample trajectories:
BP(=O)(C(=O)CNC(=O)c1ccc2c(Nc3ccc(Br)cc3)ncnc2c1)N1CCOCC1
BP(=O)(NC(=O)OCC1(CCCl)OCCCCCCC1O)Oc1ccc(Nc2ncnc3c(Br)ccc(Br)c3s2)cc1
BP(=O)(OCCBr)c1cc2c(Br)cc(Br)cc2nc1N
BP(=O)(OCCC)n1cc(Br)c(Nc2cc(Br)cc(Br)c2F)c1
BP1(=O)OCC(Cl)(Cl)c2c(Nc3ccc(Br)cc3)ncnc2c2cc(Br)cc(Br)c21
Policy gradient replay...
Mean value of predictions: 0.39847687
Proportion of valid SMILES: 0.5351097178683386
Sample trajectories:
BP(=O)(CCl)NP(=O)(OCC)C(F)(F)F
BP(=O)(O)c1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BP(=O)(OCC1OC(=O)C(F)(F)C(O)C1(C)O)Oc1ccc(Br)cc1
Bc1cc(Nc2ncnc3ccsc23)c(Br)c(Br)c1Br
Bc1ccc2ncnc(Nc3ccc4ccc(Br)cc4c3)c2n1
Fine tuning...
Mean value of predictions: 0.4017857
Proportion of valid SMILES: 0.5259862241703194
Sample trajectories:
BP(=O)(O)CCC(F)(F)F
BP(=O)(O)Oc1ccc(Br)cc1Nc1cc2ncnc(Nc3cc(Br)c(Br)c(Br)c3)c2nc1Cl
BP(=O)(OCC)OC(=O)COc1ccc(Br)cc1
BP(=O)(OCCCCCCCCCCCCCCCCCCCCC(=O)O)N(=O)=O
B[PH](=O)(F)(F)C(F)(F)PS(=O)(=O)Oc1ccc2ncnc(-c3cc(F)c(F)c(Br)c3)c2c1

 11 Training on 10536 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 27.271092
Reward: 3.785859
Trajectories with max counts:
168	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.46858373
Proportion of valid SMILES: 0.43550407013149656
Sample trajectories:
BP(=O)(Cl)OCCCl
BP(=O)(Nc1ccc(Br)c(Br)c1)C(=O)c1cc(Br)c(Br)cc1CP(=O)(O)O
BP(=O)(OCC)OC(=O)CI
BP(=O)(OCC)Oc1c(Br)c(Br)c(Br)c(Br)c1Br
BP(=O)(OCC)Oc1c(Br)cc(Br)cc1Br
Policy gradient replay...
Mean value of predictions: 0.36763486
Proportion of valid SMILES: 0.5281778334376956
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)NCCc1ccc2ccccc2c1
BrBr
BrCC(=NNc1ccncc1)c1cn[nH]c1
BrCCBr
BrCc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)n1
Fine tuning...
Mean value of predictions: 0.38384798
Proportion of valid SMILES: 0.5275689223057645
Sample trajectories:
BP(=O)(CCl)NP(=O)(OCC)N1CC(Cl)CC(Cl)C1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)O
BP(=O)(OCCC)OC(=O)N1CCN(CC2CCCC2)CC1
BP(=O)(OCCCCCC)OC(=O)Oc1cc(Br)cnc1Nc1ccc(Br)cn1
BP(=O)(OCCCCCCC)OCCCCCCCCCCC

 12 Training on 11895 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.378787
Reward: 3.910013
Trajectories with max counts:
183	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.4430123
Proportion of valid SMILES: 0.43196746950265874
Sample trajectories:
BP(=O)(Br)Oc1cc(Br)c(Nc2ncnc3c(Br)c(F)c(Br)c(F)c23)c2ncnc(Cl)c12
BP(=O)(OCC)C(=O)Nc1cc(Br)c(Br)c(Br)c1
BP(=O)(OCC)OCCF
BP(=O)(OCC)Oc1cc(Cl)c(Br)c(Br)c1
BP(=O)(c1cc(Br)cc(Br)c1)P(=O)(Oc1ccc(Br)cc1)P(=O)(OCC)OCCCBr
Policy gradient replay...
Mean value of predictions: 0.44266
Proportion of valid SMILES: 0.4993734335839599
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1Cl)OCC1OC(CC(C)(F)F)C(F)(F)C(F)CNC1=O
BP(=O)(OCC1OC(=O)CC(C)(O)C1O)c1cc(Br)cc(Br)c1O
BP(=O)(OCCCC)OCCCC1OC(N2C=CC(=O)NC2=O)C(O)C1O
BP(=O)(OCCOCCn1cnc2c1Nc1cc(Br)ccc12)c1ccc(Br)cc1
BP(=O)(OCCOc1cccc(Br)c1)P(=O)(Oc1cccc(OCCCBr)c1)OP(=O)(O)O
Fine tuning...
Mean value of predictions: 0.4431209
Proportion of valid SMILES: 0.5251798561151079
Sample trajectories:
BC(=O)Nc1ccc(Nc2ncnc3scnc23)c(F)c1
BP(=O)(Nc1ccc(Br)cc1)Nc1cc2cc(Br)c(Br)cc2[nH]1
BP(=O)(Nc1ccc(Br)cn1)Oc1ccc(Br)cc1O
BP(=O)(Nc1ncc(Br)s1)C(F)(F)P(=O)(O)O
BP(=O)(OCC)OCCCCCCCCCCCCCCC1OC(N2C=CC(N)=NC2=O)C(O)C1O

 13 Training on 13353 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.565089
Reward: 4.249956
Trajectories with max counts:
180	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.5142857
Proportion of valid SMILES: 0.41859737006887915
Sample trajectories:
BP(=O)(Nc1cc2c(Br)cc(Br)cn2c1)c1ccc(Br)cc1
BP(=O)(OCC)C(C=O)(Nc1nc(Nc2ccc(Br)c(Br)c2)nc2cc(Br)ccc12)N1CCCCC1
BP(=O)(OCC)OC(=O)CCSCC(N)C(=O)O
BP(=O)(OCCCCC)C(Nc1cc(Br)c(Br)c(Br)c1Br)c1csc(N)n1
BP(=O)(OCCCl)C(=O)C(Br)Br
Policy gradient replay...
Mean value of predictions: 0.33189705
Proportion of valid SMILES: 0.5221875
Sample trajectories:
BP(=O)(O)C(=O)O
BP(=O)(Oc1cc(Br)cc(Br)c1)P(=O)(O)O
Bc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Bc1cc(F)ccc1C(=O)NCCCCCCCCCN1CCOCC1
BrNc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.33460397
Proportion of valid SMILES: 0.5246875
Sample trajectories:
BP(=O)(OCC)Oc1c(Br)cc2c(Nc3ccc(Cl)cc3)ncnc2c1Br
BP(=O)(OCCCCCCCCl)c1ccc(Br)cc1
BP(=O)(Oc1ccc(Nc2ccccc2)cc1)P(=O)(Oc1ccc(Br)cc1)c1ccc(Br)cc1
Bc1ccc(Br)cc1Br
Brc1cc(-c2cccs2)c2ccc(Nc3ncnc4cc(Br)ccc34)cc2c1

 14 Training on 14629 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.260993
Reward: 4.479117
Trajectories with max counts:
295	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.48102382
Proportion of valid SMILES: 0.3541731791184745
Sample trajectories:
BP(=O)(C=C(C)C1CCCCC1)NO
BP(=O)(CCl)Nc1nc2cc(Br)cnc2n1-c1ccc(Br)cc1
BP(=O)(NO)c1cc(Br)c(Br)cc1Nc1ccc(Br)cc1
BP(=O)(NO)c1cc(Br)cc(Br)c1
BP(=O)(Nc1ccc(Br)cc1)P(=O)(O)O
Policy gradient replay...
Mean value of predictions: 0.4094092
Proportion of valid SMILES: 0.5726817042606517
Sample trajectories:
BP(=O)(NC(=O)OCC=C)c1cc(Br)cc(Br)c1Br
BP(=O)(OCC(=O)Nc1ccc(Br)cc1)c1ccc(Br)o1
BP(=O)(OCC1OC(C2CCCCC2)OC(=O)C(C)(O)C(O)C1O)Oc1ccc(Br)cc1
BrC(Br)(Br)Br
BrCBr
Fine tuning...
Mean value of predictions: 0.42389867
Proportion of valid SMILES: 0.5694575101912825
Sample trajectories:
BP(=O)(OCC)C(Br)(Br)P(=O)(O)CCCCCCCCCCCCCCCCBr
BP(=O)(OCC)OC(=O)CNC(=O)C(O)(Br)P(=O)(O)O
BP(=O)(OCC)OCC=CCBr
Bc1cc(Br)c2oc(Nc3cc(Br)cc(Br)c3)cc2c1Br
Bc1cc(Br)cc(Br)c1Br

 15 Training on 16167 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.572627
Reward: 4.204702
Trajectories with max counts:
147	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4931865
Proportion of valid SMILES: 0.4544885830466062
Sample trajectories:
BP(=O)(O)CCCCCC(Br)Br
BP(=O)(O)O
BP(=O)(OCC)N(C(=O)OCC)C(=O)Oc1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CN(C(=O)OC)P(=O)(O)O
B[PH](=O)(Nc1ccc2c(c1)C(=O)c1ccccc1N2)(c1ccc(Br)cc1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.43762255
Proportion of valid SMILES: 0.515802781289507
Sample trajectories:
BP(=O)(Nc1cc2c(c(Br)c(Br)c(Br)n1)Nc1cc(Br)c(Br)c(Br)c12)OCc1csc(Br)c1
BP(=O)(OCC)N(O)S(=O)(=O)CC(NC(=O)OP(=O)(O)OP(=O)(O)O)C(=O)O
BP(=O)(OCC)Oc1ccc(Br)c(Br)c1
BP1(=O)NC(C(Br)(Br)C(Br)(Br)C(Br)=C(Br)Br)c2cc(Br)c(s1)c(N)n2
BrBr
Fine tuning...
Mean value of predictions: 0.44220626
Proportion of valid SMILES: 0.5276811135716546
Sample trajectories:
BP(=O)(OCC)Oc1c(Cl)c(Br)c(Br)c(Br)c1Br
Bc1ncnc(Nc2cc(Br)c(Br)s2)n1
BrCCCCCCCCCn1ccc(Nc2cc(Br)cc(Nc3ncnc4cc(Br)c(Br)c(Br)c34)n2)n1
BrCCN(c1ccc(Br)cc1)c1c[nH]cn1
BrCc1c(Br)c(Br)cc2ncnc(Nc3ccc(Br)cc3)c12

 16 Training on 17774 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.552734
Reward: 4.667716
Trajectories with max counts:
377	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.397791
Proportion of valid SMILES: 0.36815764779480764
Sample trajectories:
BP(=O)(CC(F)(F)Oc1cccc(F)c1Nc1ccc(F)c(F)c1)OCC
BP(=O)(Cl)OCCCl
BP(=O)(NC(CCl)CCCCCC(=O)OCCl)OCCCl
BP(=O)(Nc1ccc(Nc2ccc(Br)cc2)c(F)c1)S(=O)(=O)c1c(F)cc(F)cc1F
BP(=O)(OCC)OC(=O)C=CC(=O)Oc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.4515809
Proportion of valid SMILES: 0.5053258145363408
Sample trajectories:
BP(=O)(OCC=C)C(=O)NP(=O)(O)O
BP(=O)(OCOC(=O)CBr)OP(=O)(O)O
Bc1ccc(-c2ncnc3cc(Br)cc(Br)c23)cc1Br
Bc1ccc(Br)cc1Br
BrC=CC=CCBr
Fine tuning...
Mean value of predictions: 0.45824248
Proportion of valid SMILES: 0.48749218261413385
Sample trajectories:
BOc1ccc(Br)c(Br)c1Nc1cc(Br)cc(Br)c1Br
BP(=O)(O)CC(F)(F)P(=O)(O)O
Br
BrC(=NNc1ccc(Br)cc1)N1CCCCCCC1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1

 17 Training on 19203 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.075137
Reward: 4.820358
Trajectories with max counts:
321	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.4875
Proportion of valid SMILES: 0.3150984682713348
Sample trajectories:
BP(=O)(C(=O)C(Br)=C(Br)Br)N(Cc1ccc(NS(=O)(=O)CN2C(=O)C=CC(=O)c3cc(Br)c(Br)cc32)cc1)NS(=O)(=O)c1ccc(Br)cc1
BP(=O)(COc1ccc(Br)cc1)Nc1cc(Br)c(Br)cc1Br
BP(=O)(NO)P(=O)(NO)c1cc(Br)c(N)c(Br)c1
BP(=O)(NP(=O)(OC)OCCO)OCCC
BP(=O)(Nc1cc(Br)c(Br)c(Br)c1)N1CCN(C(=O)OC(C)(C)C)CC1
Policy gradient replay...
Mean value of predictions: 0.50278705
Proportion of valid SMILES: 0.5646836638338055
Sample trajectories:
BP(=O)(ONCCCCCCCCCCCO)n1c(Nc2ccc(Br)cc2Br)nc2ncnc(N)c21
B[PH](=O)(=NO)Nc1ccc(Br)cc1
Br
BrCC(Br)CBr
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.48849162
Proportion of valid SMILES: 0.5616567304675243
Sample trajectories:
BP(=O)(CCC=CCC=CCCCCCBr)OCCCCC
BP(=O)(N1CCCCC1)N1CCC(O)C1
BP(=O)(NC(=O)C(N)CCCC(=O)O)OCCO
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC)OCCCC[SH](N)(=O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OCCCCCCCCl

 18 Training on 20897 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.105576
Reward: 5.032096
Trajectories with max counts:
527	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.52589285
Proportion of valid SMILES: 0.35021888680425267
Sample trajectories:
BOc1ccc(Nc2ncnc3ccccc23)cc1Br
BP(=O)(CCC=CCC=CCCCCCCCCC(=O)NCCOP(=O)(O)OP(=O)(O)O)OCC
BP(=O)(OCC(=O)CBr)Oc1cc(Br)cc(Br)c1O
B[PH](=O)(=NC(=O)OCCCCCC)Oc1ccc(Br)cn1
Bc1cc(Br)c2cc(Nc3ncnc4ccccc34)ccc2c1
Policy gradient replay...
Mean value of predictions: 0.4963139
Proportion of valid SMILES: 0.5272727272727272
Sample trajectories:
BP(=O)(CCCC(=O)Nc1ccc(Br)cc1)OCC
BP(=O)(N=O)c1ccc(Br)cc1
BP(=O)(NO)c1cc2ccc(Br)cc2s1
BP(=O)(OCC)OCCCCCNC(=O)C=Cc1cc(Br)c(N)cc1Br
BP(=O)(OCC)Oc1cc(Br)c(Br)c(Br)c1N(=O)=O
Fine tuning...
Mean value of predictions: 0.5032407
Proportion of valid SMILES: 0.5410144020037571
Sample trajectories:
BP(=O)(CCCCC=CCC=C(O)OC)OCC
BP(=O)(NC(=O)OCCCNC(=O)c1cc(Br)c(Br)c(Br)c1)C(=O)O
BP(=O)(OCC)OC(=O)c1cc(Br)cc(Nc2ncnc3cc(Br)cc(Br)c23)c1
BP(=O)(OCC)Oc1cc(Br)cc(Br)c1Oc1cccc(Br)c1
BP(=O)(OCC1CCCN1)S(=O)(=O)c1ccc(Cl)cc1

 19 Training on 22667 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.109956
Reward: 5.674123
Trajectories with max counts:
665	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.48659286
Proportion of valid SMILES: 0.25422138836772984
Sample trajectories:
BP(=O)(Br)COc1ccc(Br)cc1
BP(=O)(Nc1cc(Br)cc(Br)c1)OCC1OCc2c(Br)cc(Br)cc21
BP(=O)(OCC(=O)Nc1ccc(Br)cc1)c1ccc(Br)c(Br)c1
BP(=O)(OCC(F)(F)F)c1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
BP(=O)(OCC)C(=O)Nc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.4460362
Proportion of valid SMILES: 0.449375
Sample trajectories:
BP(=O)(Br)C(=O)NO
BP(=O)(Br)CC(Br)=CBr
BP(=O)(Nc1ccc(Br)nc1)Nc1cccc(Br)c1
BP(=O)(OCC)Oc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1cc2ncnc(Nc3cccc(Br)c3)c2nc1Br
Fine tuning...
Mean value of predictions: 0.45429364
Proportion of valid SMILES: 0.45139105970615817
Sample trajectories:
BP(=O)(C(=O)Nc1ccc(Br)cc1)N(O)C(=O)c1ccc(Br)cc1
BP(=O)(O)Oc1ccc(F)c(Nc2c(F)ccc(F)c2F)c1
BP(=O)(OCCCCCCC)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCCCl)P(=O)(O)OP(=O)(O)OP(=O)(O)Br
Bc1ccc(Br)cc1Br

 20 Training on 23945 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.673036
Reward: 5.641995
Trajectories with max counts:
564	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.48009208
Proportion of valid SMILES: 0.2715625
Sample trajectories:
BP(=O)(Br)Oc1ccc(Br)cc1
BP(=O)(OCC1OC(=N)C(O)C(O)C1O)Oc1ccc2cc(Br)ccc2c1Br
BP(=O)(OCC1OC(=O)N(C(=O)Nc2c(Br)cc(Br)cc2Br)C(=O)N1c1ccc(F)c(Br)c1)N(=O)=O
BP(=O)(OCCCCNC(=O)OCCl)P(=O)(O)O
BP(=O)(ON1CCC2(O)CCCCCCCCCCCCC(C2)C12CCCCC2)N1C=CC(=O)NC1=O
Policy gradient replay...
Mean value of predictions: 0.5099886
Proportion of valid SMILES: 0.5514866979655713
Sample trajectories:
BP(=O)(Br)CC(Br)Br
BP(=O)(C(Br)=CCl)N(CCl)CCBr
BP(=O)(C=NP(=O)(OCC)OCCCCCCCCC)OCC
BrC(=Nc1ccc(Br)c(Nc2ncnc3cc(Br)cnc23)c1)c1ccc(Br)cc1
BrCCNc1ncnc2ncnc(Nc3ccc(Br)cc3Br)c12
Fine tuning...
Mean value of predictions: 0.5296703
Proportion of valid SMILES: 0.5423462986198243
Sample trajectories:
BC(=O)Nc1ncc(-c2ccnc(Nc3cc(F)c(Br)c(Br)c3)c2)nc1Cl
BP(=O)(OCC1CCCCC1)N1CCC(Br)(C(=O)O)P(=O)(O)O[PH](=O)(O)(P(=O)(O)O)NC(=O)O1
Br
BrCCC=CC=CC=CCC=CC=CCC=CC=CCN=CNc1ccc(Br)cc1
BrCCCc1ncc(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br

Trajectories with max counts:
306	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4729089
Proportion of valid SMILES: 0.47027873473222676
Mean Internal Similarity: 0.4778092219871378
Std Internal Similarity: 0.09419523920012406
Mean External Similarity: 0.41780450886310583
Std External Similarity: 0.07557859875658997
Mean MolWt: 459.8791971283279
Std MolWt: 127.24500130523667
Effect MolWt: -0.3727157341942435
Mean MolLogP: 5.3101396081364065
Std MolLogP: 1.6821150307233248
Effect MolLogP: 0.3884037202384041
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.058824% (990 / 1020)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 0, 'seed': 3, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5485.375762462616, 'valid_fraction': 0.47027873473222676, 'active_fraction': 0.4452583910495472, 'max_counts': 306, 'mean_internal_similarity': 0.4778092219871378, 'std_internal_similarity': 0.09419523920012406, 'mean_external_similarity': 0.41780450886310583, 'std_external_similarity': 0.07557859875658997, 'mean_MolWt': 459.8791971283279, 'std_MolWt': 127.24500130523667, 'effect_MolWt': -0.3727157341942435, 'mean_MolLogP': 5.3101396081364065, 'std_MolLogP': 1.6821150307233248, 'effect_MolLogP': 0.3884037202384041, 'generated_scaffolds': 1020, 'novel_scaffolds': 990, 'novel_fraction': 0.9705882352941176, 'save_path': '../logs/replay_combo_s3-4.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.757501
Reward: 1.000000
Trajectories with max counts:
2	O=C1CCCc2ccccc21
Mean value of predictions: 0.0014699877
Proportion of valid SMILES: 0.767471012221874
Sample trajectories:
Brc1ccc(CCN(c2cccc(Oc3ccccc3)c2)C(Br)Cc2ccncc2)o1
Brc1ccc(NCCN2CCOCC2)cn1
Brc1cncc(C=Cc2ccccc2)c1
C#CC1CC(c2ccc(N)cc2)C2CCC(O1)N2C
C#CC1CCCCC1N(CC)CC
Policy gradient replay...
Mean value of predictions: 0.015982952
Proportion of valid SMILES: 0.5911811023622047
Sample trajectories:
BrCc1c2cnc(-c3ccncn3)nc2nc2cccc(-c3ccc(Br)cc3)c12
Brc1c[nH]c2nc(Cc3c[nH]cn3)nc2c1
Brc1cc[nH]c1-c1cc2ncnc(Nc3ccccn3)n2n1
Brc1ccc(-c2nnc(NCc3ccccc3)o2)cc1
Brc1ccc(Nc2nc(-c3ccsc3)cs2)nc1
Fine tuning...
Mean value of predictions: 0.025646329
Proportion of valid SMILES: 0.6077938403519799
Sample trajectories:
BrCCCCC1CC1[N-][N+]1CC=CC(I)=CC1
BrCN(CC1CCCCC1)c1cccc(Br)c1
Brc1ccc(-c2ncnc3nc(-c4ccc(Br)o4)nn23)cc1
Brc1ccc(N2CCN(Cc3ccc(Br)cn3)CCC3OC(C=C3c3ccccc3)C2)cc1
Brc1ccc(Nc2ncc(-c3nc4ccccc4[nH]3)cc2-c2ccncc2)cc1

  2 Training on 336 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.516688
Reward: 1.191979
Trajectories with max counts:
6	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.028091602
Proportion of valid SMILES: 0.6146387238035659
Sample trajectories:
BP(=O)(OCC1OC(OP(=O)(O)O)C(O)C1N)n1cnc2c(N)ncnc21
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(O)C(N)=O
BP(=O)(Oc1ccc(Br)c(Br)c1)N(O)C=O
Br
BrC1=C(ON2CCOCC2)N=C(Nc2ccncc2)c2ccc(Br)cc2N(c2ccc(Br)cc2)N1
Policy gradient replay...
Mean value of predictions: 0.0716233
Proportion of valid SMILES: 0.5086668767727702
Sample trajectories:
BP(=O)(CSc1ccc(NC(=O)c2c(Br)c3c(N)ncnc3c3ccccc23)s1)C(=O)O
BP(=O)(N(Cc1ccc(Br)cc1)P(=O)(O)O)P(=O)(O)O
Brc1cc2ncnc(N3CCNCC3)c2cc1Nc1cccnc1
Brc1cc2ncnc(NCc3cccnc3)c2cc1Br
Brc1cc2ncnc(Nc3ccc(I)cc3)c2cc1N1CCCCC1
Fine tuning...
Mean value of predictions: 0.07281001
Proportion of valid SMILES: 0.5516159397552557
Sample trajectories:
BP(=O)(C(=O)COC(=O)c1cccc(Br)c1)N(CC)c1ccc(F)cc1
Bc1cc(Br)cc2ncnc(Nc3ccc4[nH]cnc4c3)c12
Brc1cc(N[SH](CSc2nc3ccccc3[nH]2)(=NCc2ccco2)N2CCCC2)ccc1-c1nc2ncnc(N3CCCCC3)c2s1
Brc1cc2c(cc1NCc1ccco1)c1cncnc1N2
Brc1cc2cnc(Nc3ccnc4ccccc34)cc2nc1Nc1ccc(CN2CCOCC2)cc1

  3 Training on 740 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.132956
Reward: 1.245905
Trajectories with max counts:
4	Cc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
4	Fc1ccc(Nc2ncnc3ccccc23)cc1
4	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.08992059
Proportion of valid SMILES: 0.5141331658291457
Sample trajectories:
Brc1cc(Nc2ccc3nncn3n2)Nc2nc3ccccc3nc2nc(Br)c1
Brc1cc(Nc2ncnc3cccnc23)cnc1-c1ncnc2[nH]ccc12
Brc1cc(Nc2ncnc3ncnc(Nc4ccccc4Br)c23)cs1
Brc1cc2c(Nc3ccccc3)ncc(-c3ccc(C4CC=C(c5ccccn5)CCC4)cc3Br)n2c1
Brc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.056886226
Proportion of valid SMILES: 0.7310819262038775
Sample trajectories:
BP(=O)(COC(=O)c1ccccc1)P(=O)(O)O
BP1(=O)OCC(OC(=O)OCC2OC(N3C=C(OC(=O)Nc4ccccc4)C(=O)OC3)C(O)C2O)O1
BrCCCC=CC=CC=CC=CC=CC=CCCC=Nc1ccc2ccccc2c1
BrCCNc1ccc(CNc2ncnc3ccc(Br)cc23)cc1
Brc1cc2ccccc2cc(-n2cnc(CN3CCCCC3)c2Br)cn1
Fine tuning...
Mean value of predictions: 0.13347504
Proportion of valid SMILES: 0.5888610763454318
Sample trajectories:
BrC=C(Br)Br
Brc1cc(Br)c2c(c1)C=NO2
Brc1ccc(-c2ccc3[nH]cnc3n2)o1
Brc1ccc(CN(c2cncc(Br)c2)C2CCOCC2)cc1
Brc1ccc(N2CCOCC2)c(Nc2ccccc2)c1

  4 Training on 1465 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 20.293875
Reward: 1.879325
Trajectories with max counts:
92	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.13111717
Proportion of valid SMILES: 0.5734375
Sample trajectories:
BP(=O)(NP(=O)(OCOc1ccccc1)P(=O)(O)OP(=O)(O)O)P(=O)(O)O
BP(=O)(OCC)OP(=O)(ON1CCC2(CC1)OO2)C(F)(F)Cl
BP(=O)(OCC1OC(=O)CC1O)Oc1ccccc1
BP(=O)(OCC1OC(n2cnc3c(Nc4ccc(Br)cc4)nc(Nc4ccccc4)nc32)C(O)C1O)C1CC1
Bc1ccc(Nc2ncnc3ccccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.10361703
Proportion of valid SMILES: 0.5876836511409815
Sample trajectories:
Bc1cccc(Br)c1Nc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc2ccccc2N=C1c1ccccc1Nc1ncnc2ccc(Br)cc12
Brc1cc2ccccc2nc1Nc1ccc2cnccc2c1
Brc1cc2ncnc(Nc3cccc4ccccc34)c2cc1Nc1ccccc1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.1734726
Proportion of valid SMILES: 0.5989990616202691
Sample trajectories:
BrC(Br)c1ccccc1
Brc1cc2c(Nc3ccccc3)ncnc2nc1NC1CCCCCC1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Nc1ccccc1
Brc1cc2ncnc(Nc3ccccc3I)c2cc1Br
Brc1cc2ncnc(Nc3ccccn3)c2cn1

  5 Training on 2529 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 20.618414
Reward: 2.119184
Trajectories with max counts:
140	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.25300613
Proportion of valid SMILES: 0.5096935584740463
Sample trajectories:
BP(=O)(OCC(F)(F)F)c1nc(-c2cncnc2)nc(-c2ccc(Nc3ccc(F)c(F)c3F)cc2F)n1
Bc1cnc(-c2cncnc2)c2cc(Nc3ccccn3)ccc12
BrC(=NN=C(N1CCCCC1)N1CCOCC1)N1CCc2sccc2C1
BrCc1ccc(Nc2nc3ccc(Br)c(Oc4ccccc4N=Nc4ccccc4)c3s2)cc1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.2790476
Proportion of valid SMILES: 0.5259862241703194
Sample trajectories:
BrCCCNc1ccncc1
BrCc1ccc2c(c1)c1ncnc(Nc3ccc(Br)cc3)c21
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c2nc3cnc(Nc4ncc(Br)s4)sc3c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.21322313
Proportion of valid SMILES: 0.6055677197372537
Sample trajectories:
BP(=O)(C=CC(F)(F)F)OCCC
BP(=O)(N(CC(=O)N(c1ccccc1)c1ccc(Br)cc1)c1ccc(Br)cc1)N(=O)=O
B[PH](=O)(=NP(=O)(NO)C(=O)Oc1ccc(Br)s1)OCCO
Brc1cc(-c2c[nH]c3ccccc23)nc2ccccc12
Brc1cc(Br)cc(Nc2nccc(-c3ccccc3Br)n2)c1

  6 Training on 3995 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 24.704885
Reward: 2.909596
Trajectories with max counts:
82	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.30980846
Proportion of valid SMILES: 0.5384375
Sample trajectories:
BP(=O)(NC(=O)C(F)(F)F)OCCCn1cnc2c1NC=NC2=O
BP(=O)(OC)OCC1OC(=O)N(Nc2ccc(Br)c(Br)c2)C1OC(=O)Nc1cccc(Br)c1
BP1(=O)OCC(=O)c2ccc(Br)cc2[PH](=S)(Br)(N=O)c2ncc(Br)cc21
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Policy gradient replay...
Mean value of predictions: 0.31109875
Proportion of valid SMILES: 0.5615408706545568
Sample trajectories:
BP(=O)(OCC)OC(=O)CN(C(=O)OP(O)OP(=O)(O)O)P(=O)(O)O
BP(=O)(OCC1C=CC(=O)N(O)C1=CC(=O)OCC(F)F)C(O)CO
BP(=O)(OCC=C(Br)Br)c1ccc(Br)cc1
BP(=O)(OCCCC)OC(=O)c1cc2cc(Br)cc(Br)c2s1
BrC1=Cc2cccnc2Nc2c1ncc(Br)c2Br
Fine tuning...
Mean value of predictions: 0.28344154
Proportion of valid SMILES: 0.5778611632270169
Sample trajectories:
BP(=O)(NC(=O)COc1cccc2cccc(F)c12)N(CC=C)P(B)(=O)Oc1cccc(F)c1
BP(=O)(Nc1cccc(Cl)c1)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCOC(=O)C(F)(F)F)Oc1c(F)cc(F)cc1F
BrBr
BrC=CC=CC=CC=CC=CC=CC=CCCBr

  7 Training on 5741 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 24.319599
Reward: 3.334291
Trajectories with max counts:
111	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.24607144
Proportion of valid SMILES: 0.525
Sample trajectories:
BP(=O)(OCC1OC(Oc2ccc(I)cc2)C(O)C1O)c1ccc(Br)cc1
BP(=O)(OCC1OC(Sc2ncnc(N)c2N(=O)=O)OC(N(=O)=O)C1O)c1ccccc1
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Bc1ccccc1-c1csc(Nc2ccccc2)n1
Bc1cnc(Nc2cccc(Br)c2)cc1Oc1ccccc1Nc1cccnc1
Policy gradient replay...
Mean value of predictions: 0.29191688
Proportion of valid SMILES: 0.5441407477222746
Sample trajectories:
BC(=O)Oc1cc(NS(=O)(=O)Nc2ncnc(F)c2F)cc(C(=O)O)c1F
BC=C1C(=O)Nc2cc(Br)nc(N3CCCC3)c21
BP(=O)(NOCC1OC(C(=O)OCC(Br)Br)C=CC1Br)C(Br)=CBr
BP(=O)(OCC=CC(=O)C(F)(F)F)Oc1c(F)c(F)c(F)c(F)c1F
BrCCBr
Fine tuning...
Mean value of predictions: 0.33782607
Proportion of valid SMILES: 0.5751797436698969
Sample trajectories:
BP(=O)(N(O)C=O)N(=O)=O
BP(=O)(OCCCCC=CCCN=C(N)NCc1cccc(F)c1)C(F)(F)F
Bc1ccc2ncnc(Nc3ccc(Br)cc3F)c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3Br)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(CN4CCCC4)nc3)ccnc2c1

  8 Training on 7287 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 25.018268
Reward: 3.942604
Trajectories with max counts:
88	Fc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4105482
Proportion of valid SMILES: 0.4503125
Sample trajectories:
Bc1ccc(Nc2ncnc3c(Br)cc(Br)c(Br)c23)c(Br)c1
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccccc3)ncnc2c1Br
Policy gradient replay...
Mean value of predictions: 0.2908726
Proportion of valid SMILES: 0.623709727869878
Sample trajectories:
BrCCCc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(-c4ccccc4Br)c(-c4ccccc4)nc23)c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Nc1ccccc1
Brc1ccc(-c2c(-c3ccccc3)c3ccc(Br)cc23)cc1
Brc1ccc(-c2ccc3c(Br)cccc3n2)cc1
Fine tuning...
Mean value of predictions: 0.32604057
Proportion of valid SMILES: 0.5859912445278299
Sample trajectories:
BP(=O)(NO)c1ccc(Br)cc1F
BP(=O)(OCC)C1CCCC1P(=O)(O)O
BP(=O)(OCOc1ccccc1)N(c1cc(Cl)cc(Br)c1)C(F)(F)F
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccsc23)ccc1-c1ccccc1

  9 Training on 9003 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 24.447610
Reward: 3.655326
Trajectories with max counts:
102	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.42495343
Proportion of valid SMILES: 0.5040675844806007
Sample trajectories:
BP(=O)(CC)OP(=O)(Nc1cc(F)cc(F)c1)c1ccc(F)c(F)c1
BP(=O)(OC(C)COP(=O)(O)O)P(=O)(O)O
BP(=O)(OCC(=O)Nc1ccc(Br)cc1)P(=O)(Oc1ccccc1)Oc1ccc(F)c(Cl)c1
BP(=O)(OCC)OC(=O)C(Cl)(Br)Br
BP(=O)(OCC)OC(=O)CN(C(=O)CBr)N(O)C(=O)c1cc(-c2ccccc2I)c(Br)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.29252136
Proportion of valid SMILES: 0.5855489521426337
Sample trajectories:
BP(=O)(OC(F)F)c1ccc(Nc2ccc(F)cc2F)cc1F
BrC=C1Oc2ccccc2C=C1c1ccc2ncccc2c1
BrCOc1ccc2ncnc(Nc3ccccc3)Nc3ccccc3-c2c1
BrCc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1c2ccccc2cc2c(Nc3ccccc3)ncnc12
Fine tuning...
Mean value of predictions: 0.37303126
Proportion of valid SMILES: 0.580463368816531
Sample trajectories:
Bc1ccc(Nc2ncnc3ccccc23)cc1F
Br
BrCc1ccc2sc(Nc3ccc(Br)s3)nc2c1
BrIc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1cc(Br)c(-c2ccc3ncncc3c2)c(Br)c1Br

 10 Training on 10182 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.218646
Reward: 3.501707
Trajectories with max counts:
86	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.37249863
Proportion of valid SMILES: 0.5840625
Sample trajectories:
BP(=O)(N(O)CP(=O)(O)O)[PH](O)(Br)OP(=O)(O)OP(=O)(O)CF
BP(=O)(Nc1cccc(Br)c1)Oc1ccc(Nc2ccc(Br)cc2)cc1
BP(=O)(OCCC)n1cc(Br)c(F)c1F
Bc1ccc2ncnc(Nc3ccc(CN4CCCC4)cc3)c2c1
BrBr
Policy gradient replay...
Mean value of predictions: 0.36024466
Proportion of valid SMILES: 0.613700344072568
Sample trajectories:
BrC=CC(I)=C(I)CCC(Br)Br
BrCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCNc1ccc2ncnc(Nc3ccc(Br)cc3Br)c2c1
BrSc1ccc(-c2ccc(Br)cc2)c(Br)c1
Brc1cc(Br)c(-c2ccc3cccc(Nc4ncnc5ccccc45)c3c2)cc1Br
Fine tuning...
Mean value of predictions: 0.3723283
Proportion of valid SMILES: 0.6055017192872773
Sample trajectories:
BC1=CC(O)C(=O)C(OC(=O)Nc2ccc(Nc3ccc4cccnc4c3)cc2Br)OCN1
BP(=O)(CCC(=O)Nc1ccc(Br)cn1)OCC
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC)OC(=O)CCCCCCCCCCN
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)S(=O)(=O)c1cccc(Br)c1

 11 Training on 11516 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.480183
Reward: 3.511032
Trajectories with max counts:
204	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.41501042
Proportion of valid SMILES: 0.4496875
Sample trajectories:
BP(=O)(Br)CI
BP(=O)(C(=O)Oc1ccccc1)N(O)Cc1cc(Br)cnc1Nc1ccccc1
BP(=O)(C(=O)c1ccc(Br)cc1)N(Cl)CCCl
BP(=O)(Nc1ccc(Br)cc1)Oc1cccc(F)c1
BP(=O)(O)C(F)(F)F
Policy gradient replay...
Mean value of predictions: 0.38200694
Proportion of valid SMILES: 0.5420443888715224
Sample trajectories:
BP(=O)(NS(=O)(=O)c1cccc2ncnc(Nc3ccccc3)c12)OCCC#N
BP(=O)(OCC1OC(N2C=CC(F)(F)C(F)(F)C(=O)NC2=O)C(O)C1O)C(F)(F)F
BP(F)(=[PH](c1ccccc1-c1ccccc1F)C(F)(F)C(F)(F)F)P(=O)(O)C(F)(F)F
Bc1ccc(Nc2ncnc3ccccc23)cc1
BrCCNc1ccc(Nc2ccccc2Nc2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.3872176
Proportion of valid SMILES: 0.5673648015004689
Sample trajectories:
BP(=O)(NO)c1ccc(Br)cc1Br
BP(=O)(OCC)C(=O)Nc1cccc(Br)c1Br
BP(=O)(OCCC)C(F)(F)F
Br
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc2n3c2ccccc2Br)c1

 12 Training on 12826 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.752329
Reward: 3.474015
Trajectories with max counts:
130	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.44875756
Proportion of valid SMILES: 0.4653125
Sample trajectories:
BP(=O)(C(=O)Nc1cccc(Br)c1)c1ccccc1Br
BP(=O)(NC(=O)OCCc1cc(Br)ccc1Br)P(=O)(O)O
BP(=O)(NC(CCCN)=NP1(=O)OCC(OC(=O)Nc2ccc(Br)cc2)C(O)C1O)C(=O)O
BP(=O)(NCCCCN)C(=O)Nc1cc2cc(Br)c(Br)cc2s1
BP(=O)(NO)c1ccccc1Br
Policy gradient replay...
Mean value of predictions: 0.44165668
Proportion of valid SMILES: 0.520625
Sample trajectories:
BP(=O)(OCC=C)P(=O)(O)OP(=O)(O)O
B[PH](=O)(CBr)(NC(=O)Oc1cc(Br)c(Br)c(Br)c1Br)OCC
BrC1CCN(Cc2ncnc3ccsc23)CCN1
BrCCNc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1-c1ccc(Br)cc1
BrCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Fine tuning...
Mean value of predictions: 0.4338665
Proportion of valid SMILES: 0.5670941507663434
Sample trajectories:
BP(=O)(OC(Cl)(Cl)P(=O)(OCC)OCC[SH](=O)(O)OP(=O)(O)O)C(F)F
BP(=O)(OCC(=O)NCC=CC(=O)O)P(=O)(O)O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Br
BrCC(Br)Br

 13 Training on 14341 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.094313
Reward: 3.891898
Trajectories with max counts:
220	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.5096172
Proportion of valid SMILES: 0.3364750235626767
Sample trajectories:
BBr
BP(=O)(C=COP(=O)(O)O)OCCO
BP(=O)(CN(c1cc(Br)cnc1F)c1c(F)c(F)c(F)c(F)c1F)OCCO
BP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCCCl)P(=O)(OCOC(=O)c1cc(F)c(F)cn1)c1cc(F)c(F)c(F)c1
Policy gradient replay...
Mean value of predictions: 0.38370198
Proportion of valid SMILES: 0.5370428258830885
Sample trajectories:
BP(=O)(NC(c1ccccc1-c1ccccc1)P(=O)(O)O)P(=O)(O)O
BP(=O)(OCCS)C(=O)NO
B[PH](=O)(=CP(=O)(O)OP(=O)(O)O)OC(F)F
Bc1ccc2ncncc2c1-c1ccccc1Br
Bc1cccc(Nc2ncnc3c4ccccc4c23)c1
Fine tuning...
Mean value of predictions: 0.44542935
Proportion of valid SMILES: 0.5642388246326977
Sample trajectories:
BP(=O)(NO)OCC=CBr
BP(=O)(Nc1ccc(Br)cc1)c1cc(Br)c(Br)cc1Br
BrC1=Nc2ccncc2Nc2nncn21
BrCCOc1ccc(Nc2ncnc3ccccc23)cc1Br
BrSc1sccc1-c1ccccc1

 14 Training on 15766 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.902095
Reward: 3.812288
Trajectories with max counts:
145	Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Mean value of predictions: 0.4922819
Proportion of valid SMILES: 0.3725
Sample trajectories:
BP(=O)(OC(Br)CBr)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OC)OC(=O)C(Br)Br
BP(=O)(OC1CCC(NC(=O)C(F)(F)F)N(CCc2ccc(F)cc2)CC1)C(=O)OC
BP(=O)(OCC)OC(=O)Nc1ccc2ncncc2n1
BP(=O)(OCC)OCC
Policy gradient replay...
Mean value of predictions: 0.25683996
Proportion of valid SMILES: 0.6194496560350219
Sample trajectories:
BP(=O)(NO)c1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cc1)c1ccccc1
BP(=O)(c1ccccc1F)C(F)F
Bc1cc2ncnc(Nc3ccccc3-c3ccccc3)c2cc1Br
Bc1ccc(N=Nc2ccc(Nc3ncnc4ccccc34)cc2)cc1C(F)(F)F
Fine tuning...
Mean value of predictions: 0.4488248
Proportion of valid SMILES: 0.5859154929577465
Sample trajectories:
BP(=O)(=O)(S)C(F)(F)F
BP(=O)(Oc1cc(Br)c(Br)c(Br)c1Br)ON(O)C=O
Br
BrC(=Nc1cccc(Br)c1)c1ccc(Nc2ncnc3ccsc23)cc1
BrCc1sc2ncnc(Nc3cccc(Br)c3)c2c1-c1ccc(Br)cc1

 15 Training on 17135 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.861607
Reward: 4.473367
Trajectories with max counts:
160	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.52975136
Proportion of valid SMILES: 0.38980931541106595
Sample trajectories:
Bc1cc(Br)c(Br)cc1Nc1ncnc2cc(Br)c(Br)cc12
Bc1cc(Br)cc2ncnc(Nc3ccc(Br)cc3Br)c12
Bc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Br
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1Cl
Bc1ccc(Nc2ncnc3cc(Br)sc23)cc1
Policy gradient replay...
Mean value of predictions: 0.37489498
Proportion of valid SMILES: 0.5951859956236324
Sample trajectories:
BP(=O)(NCc1ccccc1Br)C(=O)Nc1cc2c(Br)cncc2[nH]1
BP(=O)(OCC1OC(C(O)CP(=O)(O)OP(=O)(O)O)C(O)C1O)Oc1ccc2ccccc2c1Br
Bc1ccccc1-c1ccccc1-c1ccccc1NCc1ccccc1
Brc1cc(-c2ccccc2)c2cncnc2n1
Brc1cc(-c2ncnc3sccc23)c2ccccc2n1
Fine tuning...
Mean value of predictions: 0.47569725
Proportion of valid SMILES: 0.5490625
Sample trajectories:
BP(=O)(C=CC=CC=C)OCC
BP(=O)(OCC1OC(N2C=C(Br)C(=O)NC2=O)C(O)C1O)Oc1ccc(Br)cc1
BP(=O)(ONC(=O)Oc1ccc(Br)cc1Br)c1cnc(Br)c(Br)c1
B[PH](=O)(Nc1cc(F)c(Cl)c(Br)c1)(P(=O)(O)O)P(=O)(O)O
Bc1cc(Nc2ncnc3ccc(Br)cc23)c2ccc(Br)cc2c1

 16 Training on 18661 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.351310
Reward: 4.650285
Trajectories with max counts:
373	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5500901
Proportion of valid SMILES: 0.3469834323226008
Sample trajectories:
BP(=O)(C=CC=C)N(CCC(=O)Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(NCC=C)Oc1ccc(Br)cc1Br
BP(=O)(Nc1cccc(Br)c1)P(=O)(Oc1cccc(Br)c1)N1C(=S)Nc2cnc(Br)nc21
BP(=O)(Nc1cccc(Nc2ncnc3cnccc23)c1)OCC
BP(=O)(OCC1OC(=O)C(C)(C)C1O)c1cc(Br)c(O)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.4523446
Proportion of valid SMILES: 0.5733041575492341
Sample trajectories:
BP(=O)(NCCCO)c1ccc(Br)cc1
BrCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCNc1ncnc2ncnc(Nc3ccc(Br)cc3)c12
Brc1cc(-c2cncnc2)c2ncnc(Nc3cccs3)c2c1
Brc1cc(-c2nc3ccsc3cc2Br)c(Br)cn1
Fine tuning...
Mean value of predictions: 0.5142694
Proportion of valid SMILES: 0.5481852315394243
Sample trajectories:
BrCN1CCN(Cc2nccs2)CC1
Brc1cc(-c2ccccc2)c2cncnc2n1
Brc1cc(Br)c(-c2cc(Nc3ncnc4ccsc34)ccc2Br)cc1Br
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c(Br)c1

 17 Training on 20393 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.563389
Reward: 4.365450
Trajectories with max counts:
82	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.49908814
Proportion of valid SMILES: 0.5140625
Sample trajectories:
BP(=O)(CCl)Nc1cccc(Nc2ncnc3cc(Cl)ccc23)c1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)OP(=O)(O)O
B[PH](=O)(F)(F)P(=O)(O)N(O)C(=O)C(F)(F)F
Br
BrCCBr
Policy gradient replay...
Mean value of predictions: 0.46419755
Proportion of valid SMILES: 0.5573975602126994
Sample trajectories:
BP(=O)(OCC1C=CC(=O)N(C)C(=O)C1)C(=O)NO
BP(=O)(OCCCCCCCCCCI)C(F)(F)F
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
BrCCCCCCCCCCCCCCCNC1=NCCN1
BrCc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.4558036
Proportion of valid SMILES: 0.5603502188868043
Sample trajectories:
BP(=O)(NC(c1ccc(Br)cc1)P(=O)(O)O)C(F)(F)P(=O)(O)O
Br
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
BrSc1nc2ncnc(Nc3cccc(Br)c3)c2cc1Br
Brc1cc(-c2ccccc2-c2nc3ccccc3nc2-c2ccccc2)c2ccccc2n1

 18 Training on 22117 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.917579
Reward: 4.786169
Trajectories with max counts:
375	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.41083825
Proportion of valid SMILES: 0.3690625
Sample trajectories:
BP(=O)(C(F)(F)F)C(F)(F)C(F)(F)F
BP(=O)(C=C(Br)Br)OCC
BP(=O)(C=CC=CCC=C(NS(=O)(=O)N(CCCl)NC(=O)C(=O)CCCl)C(N)=O)OCC
BP(=O)(OC(C=CC=CBr)N(C)C=CBr)C(=O)c1c(Br)c(F)c(F)c(F)c1Br
Bc1cccc(Nc2ncnc(Nc3ccc(Cl)cc3)c2Nc2ccccn2)c1
Policy gradient replay...
Mean value of predictions: 0.47041485
Proportion of valid SMILES: 0.5725
Sample trajectories:
BP(=O)(CCCC(=O)Nc1ccc(Br)cc1Br)OCC
BP(=O)(NC(=O)c1ccc2ncnc(Nc3cccc(Br)c3)c2c1)C(F)(F)F
BP(=O)(NC(CS(=O)(=O)c1ccc2ncnc(Nc3ccccc3)c2c1)C(F)(F)F)C(F)(F)F
BP(=O)(NO)c1ccc(Nc2ncnc3cc(F)c(Br)cc23)cc1F
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)Oc1cc(Br)cc(Br)c1O
Fine tuning...
Mean value of predictions: 0.48875672
Proportion of valid SMILES: 0.578305720537668
Sample trajectories:
BP(=O)(OCC)OC(=O)c1cc2ncnc(-c3ccccc3)c2c(Br)c(Br)cn1
Bc1ccc(Nc2ncnc3cccc(Br)c23)cc1
BrCBr
BrCc1ccc2c(Nc3cccc(Br)c3)nc(-c3ccccc3)c3cccnc3n2c1
BrSc1ccccc1-c1ccccc1-c1ccccc1

 19 Training on 23750 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.893752
Reward: 4.888086
Trajectories with max counts:
416	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.4692641
Proportion of valid SMILES: 0.2888402625820569
Sample trajectories:
BP(=O)(NO)P(=O)(OCC)OC(=O)C=C(Br)Br
BP(=O)(OCCCC)C(=O)Nc1cccc(Br)c1O
BrC(=NNc1ccccc1Br)c1ccccc1Br
BrC(=Nc1cccc(Br)c1)c1ccc(Br)cc1
BrC(Br)(Br)Br
Policy gradient replay...
Mean value of predictions: 0.54365325
Proportion of valid SMILES: 0.5053191489361702
Sample trajectories:
BP(=O)(N(O)C(Cc1ccccc1)NP(=O)(Oc1ccc(Br)cc1)N(O)C=O)P(=O)(O)O
BP(=O)(NO)c1ccc(Br)c(Br)c1
BP(=O)(Nc1cc(Br)c(Br)cc1Br)c1ccc(Br)cc1F
Bc1cc(Br)c2nc(Nc3ccc(Br)cc3)sc2n1
BrCc1cc(Nc2ncnc3cc(-c4cc(Nc5ncnc6ccsc56)c(Br)cc4Br)sc23)ccc1Br
Fine tuning...
Mean value of predictions: 0.50394064
Proportion of valid SMILES: 0.5477009696590553
Sample trajectories:
BP(=O)(NP(=O)(OCC)OCCl)OCCCl
Br
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)cc1Br

 20 Training on 25321 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.961529
Reward: 4.265083
Trajectories with max counts:
68	Brc1ccc(Nc2ncnc3ccccc23)cc1
68	Brc1cccc(Nc2ncnc3ccccc23)c1
Mean value of predictions: 0.3424226
Proportion of valid SMILES: 0.5753125
Sample trajectories:
BBr
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Brc1cc(Br)cc(Nc2ccc(Br)c(Br)c2)c1
Brc1cc(Br)cc(Nc2ncnc3ncnc4ccccc24n3)c1
Brc1cc(Nc2ccccc2-c2ccccc2-c2ccccc2Br)ccc1Nc1ncnc2ccccc12
Policy gradient replay...
Mean value of predictions: 0.46403605
Proportion of valid SMILES: 0.554548296342607
Sample trajectories:
BP(=O)(OCC=CBr)C(F)(F)F
Bc1ccc(-c2cc(Nc3ccccc3)ncn2)c(Br)c1Br
Bc1ccc(Nc2ccnc3cccnc23)cc1Br
BrCCCCCCC=CC=CC=Cc1ccccc1
BrSc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.5170538
Proportion of valid SMILES: 0.551734917161613
Sample trajectories:
BP(=O)(CCl)Nc1ccc(-c2ccc(Br)cc2)c(F)c1
BrCc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
BrCc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)c(Oc2ccc(Br)nc2)c(I)c1
Brc1cc(Br)c2c(NCCN3CCN(c4ccccc4Br)CC3)ncnc2c1

Trajectories with max counts:
256	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.470601
Proportion of valid SMILES: 0.4631309024954656
Mean Internal Similarity: 0.49604510199049406
Std Internal Similarity: 0.10136222574709661
Mean External Similarity: 0.4210691581239039
Std External Similarity: 0.07872232695569259
Mean MolWt: 412.034439855291
Std MolWt: 93.70082549804704
Effect MolWt: -0.87340331430693
Mean MolLogP: 5.431773967440461
Std MolLogP: 1.5556083625999084
Effect MolLogP: 0.48373363628827726
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 96.114286% (841 / 875)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 20, 'seed': 3, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5605.057125091553, 'valid_fraction': 0.4631309024954656, 'active_fraction': 0.44794058068872383, 'max_counts': 256, 'mean_internal_similarity': 0.49604510199049406, 'std_internal_similarity': 0.10136222574709661, 'mean_external_similarity': 0.4210691581239039, 'std_external_similarity': 0.07872232695569259, 'mean_MolWt': 412.034439855291, 'std_MolWt': 93.70082549804704, 'effect_MolWt': -0.87340331430693, 'mean_MolLogP': 5.431773967440461, 'std_MolLogP': 1.5556083625999084, 'effect_MolLogP': 0.48373363628827726, 'generated_scaffolds': 875, 'novel_scaffolds': 841, 'novel_fraction': 0.9611428571428572, 'save_path': '../logs/replay_combo_s3-5.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.757501
Reward: 1.000000
Trajectories with max counts:
2	O=C1CCCc2ccccc21
Mean value of predictions: 0.0014699877
Proportion of valid SMILES: 0.767471012221874
Sample trajectories:
Brc1ccc(CCN(c2cccc(Oc3ccccc3)c2)C(Br)Cc2ccncc2)o1
Brc1ccc(NCCN2CCOCC2)cn1
Brc1cncc(C=Cc2ccccc2)c1
C#CC1CC(c2ccc(N)cc2)C2CCC(O1)N2C
C#CC1CCCCC1N(CC)CC
Policy gradient replay...
Mean value of predictions: 0.015982952
Proportion of valid SMILES: 0.5911811023622047
Sample trajectories:
BrCc1c2cnc(-c3ccncn3)nc2nc2cccc(-c3ccc(Br)cc3)c12
Brc1c[nH]c2nc(Cc3c[nH]cn3)nc2c1
Brc1cc[nH]c1-c1cc2ncnc(Nc3ccccn3)n2n1
Brc1ccc(-c2nnc(NCc3ccccc3)o2)cc1
Brc1ccc(Nc2nc(-c3ccsc3)cs2)nc1
Fine tuning...
Mean value of predictions: 0.040184047
Proportion of valid SMILES: 0.6129739893450329
Sample trajectories:
BrC(=NN1CCN(Cc2ccccc2)CC1)c1nc2ccccc2s1
BrCCCCCCCCCC12C=CC1CCC2
Brc1c[nH]c2c1-c1ccccc1-2
Brc1cc(Br)nc(Nc2ccccc2)c1
Brc1ccc(Nc2ccc(Nc3ncnc4ccccc34)cc2)cc1

  2 Training on 374 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.921138
Reward: 1.122292
Trajectories with max counts:
3	COc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
3	Clc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.036462884
Proportion of valid SMILES: 0.5748352682773769
Sample trajectories:
Brc1cc(Nc2ncnc3ccsc23)n[nH]1
Brc1cc2c(Nc3nc(Br)cs3)ncnc2cn1
Brc1cc2c3c(c(-c4ccccc4)c2c2ccc1n2)CCCC3
Brc1ccc(-c2ccccc2)nc1
Brc1ccc(-c2nc3ncc(-c4cncnc4)nc3s2)cc1
Policy gradient replay...
Mean value of predictions: 0.0025650992
Proportion of valid SMILES: 0.8040625
Sample trajectories:
BP1(=O)OCC(OC(=O)C2CC=C(C)C2O)O1
Bc1ccccc1Nc1ccc2ncncc2c1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccccc1-c1cc(-c2ccccc2)c2ccccc2n1
Brc1ccccc1-c1cccc(Nc2ncnc3ccccc23)c1
Fine tuning...
Mean value of predictions: 0.08969126
Proportion of valid SMILES: 0.599435382685069
Sample trajectories:
BP(=O)(CCN)NCCCCCCCCCN
BP(=O)(NC(Cc1ccccc1)C(=O)NO)c1ccccc1
BrC1=CN2C(Sc3ccccc3-c3cscn3)=CC2=N1
BrCN1CCN(C2CCN(C3CCc4ccccc43)CC2)CC1
Brc1cc(-c2ccccc2)c2ccccc2n1

  3 Training on 728 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.633898
Reward: 1.328983
Trajectories with max counts:
13	Cc1ccc(Nc2ncnc3ccccc23)cc1
13	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.09588263
Proportion of valid SMILES: 0.6611389236545682
Sample trajectories:
BP(=O)(NCCN)C(=O)Nc1cccc2ccccc12
Bc1c(NS(=O)(=O)c2ccc(Br)cc2)[nH]c2ccccc12
BrSc1ccc(Nc2ncnc3ccccc23)cc1
Brc1c[nH]c2c(Nc3cccc(Nc4ccccc4)c3)ncnc12
Brc1ccc(-c2cccc3ccnc(Nc4ccccc4)c23)c2ccccc12
Policy gradient replay...
Mean value of predictions: 0.19885057
Proportion of valid SMILES: 0.5475141598489616
Sample trajectories:
BP(=O)(Cn1cnc(Nc2cc(Cl)ccc2F)n1)C1CCCCC1
Brc1cc(-c2cnc(Nc3ccc(Br)s3)c(Br)c2)c2sccc2n1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)c2nsnc2n1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.17466253
Proportion of valid SMILES: 0.6041405269761606
Sample trajectories:
BP(=O)(N=C(Br)C(Br)=CBr)OCC(=NO)c1ccc(Br)s1
BrCCCCCCCCCC=CC(Br)Br
Brc1cc(Br)c2sc(-c3ccccc3Br)nc2c1
Brc1ccc(Br)c(Nc2nc(CNc3ncnc4cc(Br)ccc34)ccc2Br)c1
Brc1ccc(C=C2c3ccccc3CSc3ccccc32)cc1

  4 Training on 1919 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 21.948960
Reward: 2.024682
Trajectories with max counts:
347	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.1925354
Proportion of valid SMILES: 0.48577680525164113
Sample trajectories:
BP(=O)(c1nccn1-c1ccc(F)cc1)N1CCN(C(F)(F)F)CC1
BrC#Cc1cccc2c1OCO2
Brc1c(Nc2ncnc3ccccc23)ccc2cnnc(Nc3ccccc3)c2nc1Nc1ccncc1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)c2ncncc2c1-c1ccccc1
Policy gradient replay...
Mean value of predictions: 0.18630955
Proportion of valid SMILES: 0.6303939962476548
Sample trajectories:
BP(=O)(CC(=O)ON(O)C(=O)OCC)OCCO
BP(=O)(O)CC(=O)O
B[PH](=O)(Br)(Br)OCCCBr
BrCCCCBr
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.2528519
Proportion of valid SMILES: 0.5990595611285267
Sample trajectories:
BP(=O)(NC(c1ccc(Br)cc1)c1ccc(Br)cc1)C(=O)OC
BrCOc1ccc2ncncc2c1
Brc1cc(Br)c2c(Nc3cc[nH]c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4cncnc4)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ccc3ncc(Br)cc3c2)c1

  5 Training on 3379 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 24.771941
Reward: 2.391257
Trajectories with max counts:
60	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.33653304
Proportion of valid SMILES: 0.5539568345323741
Sample trajectories:
BP(=O)(C=O)OCC1OC(=O)C(F)(F)C(F)(F)C(F)(F)CC(O)C(O)C1O
BP(=O)(O)C(NC(C)=O)P(=O)(O)O
BP(=O)(OCC)OC(=O)CCCCCCl
BrCCCC=CC=CC=CC=CC=CC=CC=NNc1ncnc2cnc(Nc3cccc(Br)c3)cc12
Brc1cc(Br)c(Br)s1
Policy gradient replay...
Mean value of predictions: 0.21129033
Proportion of valid SMILES: 0.62
Sample trajectories:
Brc1cc(Br)c2ncncc2c1Br
Brc1cc(Nc2ncnc3ccccc23)ccc1NCc1ccccc1
Brc1cc2cccc(Br)c2c2ccccc12
Brc1cc2ncnc(Nc3cccc4ccccc34)c2cc1N1CCCC1
Brc1ccc(-c2ncnc3cc[nH]c23)cc1
Fine tuning...
Mean value of predictions: 0.30646968
Proportion of valid SMILES: 0.6143974960876369
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)OCCl
B[PH](=O)(=O)C(F)(F)F
Bc1ccccc1Nc1ncc2ncnc(Nc3ccccc3F)c2n1
BrC(=NNc1ccccc1Br)Nc1cc(Nc2nccc(Br)c2Nc2cc(Nc3ccccc3)ncn2)ccn1
BrC(Br)=NC(Br)=Nc1ccc(Br)s1

  6 Training on 5185 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 28.567575
Reward: 2.908802
Trajectories with max counts:
52	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.31566632
Proportion of valid SMILES: 0.6147592245153221
Sample trajectories:
BP(=O)(NC(=O)C(Br)CO)C(Br)P(=O)(O)OP(=O)(O)OC
BrC(=NNc1ccc(Br)cc1Br)c1ccc(Nc2ncnc3sccc23)cc1
BrC(=Nc1ccccc1)c1ccc(Nc2ncnc3cccc(Br)c23)cc1
Brc1cc(Br)c2nc(-c3ccccc3Br)c(Nc3ccccc3)n2c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2cn1
Policy gradient replay...
Mean value of predictions: 0.29125777
Proportion of valid SMILES: 0.5563225604016316
Sample trajectories:
BP(=O)(OCCCCCCC)OC(=O)OC
BP(=O)(OCCOCCOCCOCCOC(=O)CCl)P(=O)(OCOc1cccc(Br)c1)Oc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccccc23)cc1
BrC(=Cc1ccc(Nc2ccc(Br)cc2)cc1)c1ccccc1
BrC1=CNc2c(ncn2-c2ccccc2)C1
Fine tuning...
Mean value of predictions: 0.33221075
Proportion of valid SMILES: 0.5760325406758448
Sample trajectories:
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
BrCCc1cncnc1Nc1cccnc1-c1nnc(Nc2ccccc2-c2nc3ccccc3s2)s1
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br
Brc1cc(Br)c2ncnc(N3CCCN(c4ccc(Br)s4)CC3)c2n1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1

  7 Training on 7005 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 28.585841
Reward: 3.186136
Trajectories with max counts:
67	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.2762473
Proportion of valid SMILES: 0.5764301344170053
Sample trajectories:
BP(=O)(N1CCCCC1)N1CC1CC(=O)Nc1nc(Nc2ccc(F)cc2)sc1Cl
BP(=O)(NC(=O)c1ccc(Br)cc1)OCCO
BP(=O)(Nc1cccc(Br)c1)P(=O)(O)Oc1ccccc1
BP(=O)(Nc1ccccc1)Oc1ccc(F)cc1F
Bc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Policy gradient replay...
Mean value of predictions: 0.3914071
Proportion of valid SMILES: 0.582968065122104
Sample trajectories:
BP(=O)(C(=O)O)N(O)C=O
BP(=O)(NCCCCCCCCCCCCCCCN)P(=O)(NC(Cc1cc(Br)cc(Br)c1)C(=O)O)C(O)c1ccc(Br)cc1
Bc1ccsc1Br
BrC=CBr
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.3693305
Proportion of valid SMILES: 0.5792930872693149
Sample trajectories:
BP(=O)(CCNC(CO)C(Cc1cccc(Br)c1)(P(=O)(O)O)P(=O)(O)O)OCCO
BP(=O)(c1ccc(Nc2cccc(F)c2)cc1)N(O)CCC(F)(F)F
Brc1cc(Br)c2cnc3ncccc3c2c1
Brc1cc2c(Nc3ccccc3)ncnc2cc1Nc1cccnc1
Brc1ccc(-c2c[nH]c3ccc(Br)cc23)cc1

  8 Training on 8917 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 27.962134
Reward: 3.954629
Trajectories with max counts:
199	Fc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4750201
Proportion of valid SMILES: 0.38918412003751174
Sample trajectories:
BrCCn1ccc2ccccc2c2c(Nc3cc(Nc4ncnc5ccsc45)ccc3Br)ncnc21
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)c2c(Nc3ccc(I)cc3)ncnc2c1
Brc1cc(Br)cc(-c2ccc(Br)c(Br)c2)c1
Brc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.32924584
Proportion of valid SMILES: 0.5759375
Sample trajectories:
B[PH](=O)(=Nc1ccc2ccccc2c1)Nc1cccc(Br)c1
Brc1ccc(Br)c(-c2ccc(-c3ccccc3Br)cc2)c1
Brc1ccc(Br)c(Nc2ncnc3ncnc(Nc4ccccc4)c23)c1
Brc1ccc(Nc2cc(Nc3ccccc3)ncn2)cc1
Brc1ccc(Nc2ncnc3c(-c4ccccc4)cc23)cc1
Fine tuning...
Mean value of predictions: 0.44098976
Proportion of valid SMILES: 0.5813008130081301
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Br)cc1Nc1ccc(Br)cc1Br
BrC=CCC=CC=CC=CCN=C(Nc1cccnc1NC1CCCCC1)Nc1ncncc1Br
BrSc1ccc2ncnc(Nc3ccc(Br)cc3Br)c2c1
Brc1cc(Br)c2ccc(Br)c(Br)c2c1
Brc1cc(Nc2ncnc3ccccc23)ccc1Nc1c2ccccc2nc2ccccc12

  9 Training on 10587 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 27.532702
Reward: 3.598720
Trajectories with max counts:
57	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49275362
Proportion of valid SMILES: 0.5390625
Sample trajectories:
Brc1cc(Br)c(Br)c(-c2cc(Nc3ncnc4scc(Br)c34)ccc2Br)c1
Brc1cc(Br)c(Nc2ncnc3ccsc23)cc1Br
Brc1ccc(Br)c(Nc2ncnc3ccc(Nc4ccsc4Br)nc23)c1
Brc1ccc(Nc2ccc(Br)cn2)cc1
Brc1ccc(Nc2ccc3c(Nc4ccc(Br)cc4)ncnc3c2)cc1
Policy gradient replay...
Mean value of predictions: 0.46462977
Proportion of valid SMILES: 0.6049498746867168
Sample trajectories:
Brc1cc(-c2cccs2)c2c(Nc3ccccc3)ncnc2c1
Brc1cc(-c2ncncc2Br)c2c(Br)csc2n1
Brc1cc(Br)c(-c2ncncc2-c2ccccc2)c(Br)c1
Brc1cc(Br)c2c(I)cc3c(Br)c(Br)c(I)c(ncnc2c1Br)N3
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.44403765
Proportion of valid SMILES: 0.5980606818892712
Sample trajectories:
BP(=O)(CCS(=O)(=O)c1cccc(Br)c1)P(N)(F)(F)P(=O)(O)OP(=O)(O)OP(F)(F)(F)(F)(F)F
BP(=O)(OCC)OC(=O)C(Cl)(Cl)P(=O)(O)O[PH](N)(=O)(O)OP(N)(=O)(=O)OP(=O)(O)OP(=O)(O)CN(O)C(=O)OCCl
Brc1cc(-c2ccccc2)c2ccccc2n1
Brc1cc(I)cc2c(Nc3ccccc3)ncnc12
Brc1cc(Nc2ccc(Br)c(Br)c2)c(Br)s1

 10 Training on 12282 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.447330
Reward: 3.589196
Trajectories with max counts:
87	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4811905
Proportion of valid SMILES: 0.5259862241703194
Sample trajectories:
BP(=O)(OCC)Oc1cc(Br)c(O)c(Br)c1Br
B[PH](=O)(Cl)(Cl)OCCl
Bc1cc(Br)c2ncnc(Nc3ccc(Br)cc3)c2n1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
BrC(Br)(Br)Br
Policy gradient replay...
Mean value of predictions: 0.49815387
Proportion of valid SMILES: 0.6095654892153798
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(c1)c1c(ccc(Br)c1Br)O2
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1Br
Brc1cc(Br)cc(Nc2cccc(Br)n2)c1
Brc1cc2c(Nc3ccccc3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.43705583
Proportion of valid SMILES: 0.6165884194053208
Sample trajectories:
Bc1ccc(NC(=O)c2ccc(N3CCOCC3)c(I)c2)cc1
BrCC(Br)=Nc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(-c2ccccc2)c2sccc2n1
Brc1cc(Br)c2c(Br)c(Br)cnc2c1-c1ccccc1
Brc1cc(Br)c2c(Nc3ccc(I)c(Br)c3)ncnc2c1

 11 Training on 14130 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.834913
Reward: 3.604217
Trajectories with max counts:
284	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.4484722
Proportion of valid SMILES: 0.45056320400500627
Sample trajectories:
BP(=O)(Br)O[P+](c1cccc(Br)c1)(P(=O)(O)O)P(=O)(O)O
BP(=O)(Br)Oc1ccc(Br)c(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c1
BP(=O)(CC)CCc1cc(Nc2cccc(I)c2)nc2c(F)cc(F)c(F)c12
BP(=O)(CCC(=O)O)NS(=O)(=O)c1cccs1
BP(=O)(COCCN)NS(=O)(=O)c1cc(Br)cc(Br)n1
Policy gradient replay...
Mean value of predictions: 0.528821
Proportion of valid SMILES: 0.5732165206508135
Sample trajectories:
BP(=O)(NP(=O)(O)OP(=O)(O)OP(=O)(O)O)OCCCC=CCCCCCCCCCCC=C
Bc1c(Br)cc(Br)cc1Br
Bc1ccc(Nc2ncnc(Nc3c(F)cc(Br)cc3Br)c2-c2ncnc3sc4ccccc4c23)cc1
BrCCCCCCCCCCCCCCCCCCCCCCCC=Cc1ccccc1-c1csc2ncnc(Nc3cc(Br)ncn3)c12
BrSc1ccccc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.47779495
Proportion of valid SMILES: 0.6075117370892019
Sample trajectories:
BP(=O)(OCC)C(=O)N(O)N(C(=O)OCC)c1ccc(Br)cc1
BP(=O)(OCC)C(O)(P(=O)(O)O)P(=O)(O)O
BP(=O)(c1cc(Nc2ncnc3cc(F)c(Cl)cc23)ncn1)C(F)(F)F
Br
BrC1=CN(c2cc(Br)ccc2Br)c2c(Nc3ccc(Br)cc3)ncnc21

 12 Training on 15930 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.777889
Reward: 3.852257
Trajectories with max counts:
107	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5236988
Proportion of valid SMILES: 0.5647279549718575
Sample trajectories:
BP(=O)(NCC(F)F)c1c(O)cc(F)c(F)c1Br
BP(=O)(OC(C)(C)C)c1ccccc1
BP(=O)(OCC1OC(P(=O)(O)ONS(=O)(=O)O[PH](F)(F)F)C(F)C1O)Oc1cccc(N)c1
BrC#CC1Cc2c1c(Br)cc(Br)c2c1ccc(Br)cc1
BrSc1cc2ncnc(Nc3cccc(Br)c3)c2cn1
Policy gradient replay...
Mean value of predictions: 0.50950783
Proportion of valid SMILES: 0.559274319674695
Sample trajectories:
BP(=O)(N(O)C=P(B)(Br)Br)P(Br)Br
BP(=O)(NC(c1ccc(Br)cc1)P(=O)(O)O)S(N)(=O)=O
BP(=O)(OCC)C(=O)Nc1ccc(Br)cc1F
BP(=O)(OCC1ON=C(C(F)F)O1)S(=O)(=O)c1ccc(Nc2nc(N)nc(Br)n2)cc1
Bc1c(Br)ccc2ncnc(Nc3ccc(Br)c(Br)c3)c12
Fine tuning...
Mean value of predictions: 0.50499195
Proportion of valid SMILES: 0.5832811521603005
Sample trajectories:
BP(=O)(CC(=O)Nc1ccc(Br)c(Br)c1)NO
B[PH](=O)(=NN=Cc1c(Br)cc(Br)cc1Br)OCCS(=O)(=O)c1cccc(F)c1
Brc1cc(Br)c2c(Nc3cc(Br)c(Br)c(Br)n3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(CSc4n[nH]c(-c5ccccc5)n4)cc3)ccnc2c1

 13 Training on 17872 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.364095
Reward: 3.801985
Trajectories with max counts:
35	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.51663136
Proportion of valid SMILES: 0.590368980612883
Sample trajectories:
Bc1ccc(Nc2ncnc3ccccc23)cc1Br
BrCCCCCSc1ccc(Br)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccccc23)c1
Brc1cc(Br)c(Nc2ccnc3cc(Br)c(Br)c(Br)c23)c(Br)c1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.57114375
Proportion of valid SMILES: 0.5967438948027551
Sample trajectories:
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c(Br)c1Br
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Nc4ccccc4)nc23)cc1Br
Brc1cc(Br)c2c(Nc3cc(Br)c(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3cc(Br)c4ncnc(Nc5ccccc5)c4n3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.5140225
Proportion of valid SMILES: 0.6115805946791862
Sample trajectories:
BP(=O)(=Nc1ccc(Br)cc1)(Nc1ccc(F)cc1)c1ccc(F)cc1
BP(C)(=O)OCCN(Cc1ccc(Br)cc1)C(=O)C(CC(=O)NO)C(=O)NO
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1I
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(Br)c1

 14 Training on 20081 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.625789
Reward: 4.012045
Trajectories with max counts:
176	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5795342
Proportion of valid SMILES: 0.442950922163176
Sample trajectories:
BP(=O)(NCCCCCCNCCCl)c1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BP(=O)(NCCc1ccccc1)P(=O)(O)CC(=O)Oc1ccc(Cl)c(Cl)c1
BP(=O)(OC)OC(=O)C(CCCCN)NS(=O)(=O)Oc1ccc2ncnc(Nc3cccc(Br)c3)c2n1
BrC(Br)Br
Brc1cc(-c2ncc(Br)s2)c2sccc2n1
Policy gradient replay...
Mean value of predictions: 0.3927778
Proportion of valid SMILES: 0.675
Sample trajectories:
BP(=O)(CCl)NO
B[PH](=O)(Nc1ccc(Nc2ncnc3ccccc23)cc1)(C(F)(F)F)P(=O)(O)O
Bc1ccc(N2CCN(Cc3ncncc3Cl)CC2)cc1
Bc1ccc(Nc2ncnc3ccc(Br)c(Br)c23)cc1Br
BrC1CCN(c2ccccc2Nc2ncnc3ncnc(Nc4ccccc4)c23)CC1
Fine tuning...
Mean value of predictions: 0.54606307
Proportion of valid SMILES: 0.6355958711291836
Sample trajectories:
BP(=O)(OCCBr)C(Br)Br
Bc1ccc(Nc2ccc(Nc3ncnc4ccccc34)cc2)cc1
Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(n1)c1cncnc1N2

 15 Training on 22078 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.342033
Reward: 3.947222
Trajectories with max counts:
44	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.558297
Proportion of valid SMILES: 0.5725
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1Br
Br
BrC=NN1CCCCC1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1c[nH]c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.5724037
Proportion of valid SMILES: 0.6415389427588364
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCCCCCCNc1c2c(Br)c(Br)c(Br)cc2nc2c(Br)c(Br)c(Br)c(Br)c12
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c(Nc2ncnc3ccsc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.53720045
Proportion of valid SMILES: 0.6522826766729206
Sample trajectories:
BP1(=O)OCC2OC(=S)OC(C(O)C2O)N(C=CC(=O)NO)c2cc(Cl)c(F)cc2NC1=O
B[PH](=O)(=NO)Nc1cc(Br)c(Br)c(Br)c1
BrCc1scc2sc(CSC(=NNc3ccccc3Br)c3ccncc3)cc12
BrSc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Nc2cc(Br)cnc2Nc2ncnc3cc(Br)c(Br)cc23)c(Br)c1

 16 Training on 24487 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.619704
Reward: 4.052127
Trajectories with max counts:
48	Fc1ccc(Nc2ncnc3ccsc23)cc1F
Mean value of predictions: 0.6197911
Proportion of valid SMILES: 0.5693270735524256
Sample trajectories:
BP(=O)(OCC)C(O)(CC(Br)Br)OP(=O)(O)O[PH](=O)(O)(O)Br
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1
BrBr
BrC=Cc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2cc1Br
BrCN1CCCC1
Policy gradient replay...
Mean value of predictions: 0.5722435
Proportion of valid SMILES: 0.5704005006257822
Sample trajectories:
BP(=O)(CCl)Nc1cccc(Br)c1O
BP(=O)(Nc1cc(Br)c(Br)c(Br)c1)P(=O)(O)C(F)(F)F
BP(=O)(O)CNC(=O)c1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1cc(Br)cc(Br)c1Nc1nc2ccc(Br)cc2s1
BrCc1ccsc1-c1ccc(Nc2ncnc3ccsc23)cc1
Fine tuning...
Mean value of predictions: 0.57956725
Proportion of valid SMILES: 0.6215201751642164
Sample trajectories:
BP(=O)(OC(C)=O)c1ccc(Nc2ncnc3c(Br)c(Cl)cc(Br)c23)cc1
BP(=O)(OCC)Oc1cc(Br)c(Br)c(Br)c1Br
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(-c2ccccc2)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(Br)c(-c2cc(Nc3ncnc4cccc(Br)c34)ccc2Br)cc1Br

 17 Training on 26959 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.989795
Reward: 4.422372
Trajectories with max counts:
42	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.58002037
Proportion of valid SMILES: 0.6133166614567053
Sample trajectories:
BP(=O)(C=O)OCC
BrC=C(Br)Br
BrCCNc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cc(-c2ccccc2)c2c(Nc3ccccc3)ncnc2c1
Brc1cc(Br)c(-c2cc(-c3ccccc3Br)c(-c3ccc4ncncc4c3)s2)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.6168844
Proportion of valid SMILES: 0.621875
Sample trajectories:
Brc1c[nH]cn1
Brc1cc(-c2ncnc3scnc23)c2ccccc2n1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Brc1cc(Br)c(Nc2ncnc3scc(Br)c23)cn1
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.5482759
Proportion of valid SMILES: 0.634375
Sample trajectories:
BP(=O)(NO)c1ccc(Br)cc1Br
BrCc1ccc(Nc2ncnc3sc(-c4ccccc4Br)cc23)cc1
Brc1cc(Br)c(Nc2ncnc3ccccc23)c(Br)c1Br
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1

 18 Training on 29609 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.894272
Reward: 4.610873
Trajectories with max counts:
27	CS(=O)(=O)Nc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Mean value of predictions: 0.6902552
Proportion of valid SMILES: 0.5390869293308318
Sample trajectories:
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1cc(Br)c(Nc2ncnc3sc(Br)c(Br)c23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Policy gradient replay...
Mean value of predictions: 0.6555253
Proportion of valid SMILES: 0.5744215134459036
Sample trajectories:
BP(=O)(OCP(=O)(O)Oc1ccc(Br)c(Br)c1)Oc1cc(Br)c(Br)c(Br)c1
Brc1c(Nc2ncnc3ccsc23)cc2sccc2c1-c1ccsc1
Brc1cc(-c2ccc(Br)c(Br)c2)c2sccc2n1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Brc1cc(Br)c(Nc2ncnc3ccsc23)c(Br)c1Br
Fine tuning...
Mean value of predictions: 0.5469927
Proportion of valid SMILES: 0.6396621832968408
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Nc4ccccc4)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

 19 Training on 32330 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.501457
Reward: 4.973744
Trajectories with max counts:
139	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.6284856
Proportion of valid SMILES: 0.5208137715179969
Sample trajectories:
BP(=O)(OCCCCCCCCC=C)Oc1cc2ncnc(Nc3cc(Br)c(F)c(Br)c3)c2cc1Br
B[PH](=O)(OCCS(=O)(=O)Nc1cnc2ncnc(Nc3cccc(Br)c3)c2c1)(C(F)(F)F)C(F)(F)F
Bc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)cc1Br
Br
BrBr
Policy gradient replay...
Mean value of predictions: 0.63725394
Proportion of valid SMILES: 0.5871875
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1
BrBr
BrCc1cc(Br)c2c(Nc3cccc(Br)c3)ncnc2n1
Brc1cc(Br)c(-c2cc(Nc3ncnc4cc(Br)c(Br)c(Br)c34)ccc2Br)cc1Br
Fine tuning...
Mean value of predictions: 0.639438
Proportion of valid SMILES: 0.6230071897467959
Sample trajectories:
BrCCCc1nc2ccc(Br)cc2nc1Br
BrCc1cc2ncncc2cc1-c1ccc2ccccc2c1
BrCc1ncnc2c(Nc3cccc(Br)c3)ncnc12
Brc1cc(-c2ccc(-c3ccccc3OCc3cc4c(Nc5cc(Nc6cccc(I)c6)c5)ncnc4s3)cc2)ccc1Nc1ncnc2ccccc12
Brc1cc(Br)[nH]n1

 20 Training on 35106 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.685069
Reward: 4.932934
Trajectories with max counts:
32	Brc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Mean value of predictions: 0.69904155
Proportion of valid SMILES: 0.586875
Sample trajectories:
BP(=O)(N(O)CCc1ccc(I)cc1)P(NC(=O)OCP(=O)(O)O)C(F)(F)F
Bc1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1Br
BrCCOc1ccc(Br)c(Nc2ncc(Br)cn2)c1
BrCN1CCC(Cc2cc(Nc3ccc(Br)cc3)nc(Br)c2Br)CC1
Brc1cc(Br)c(-c2ccc(Nc3ncnc4ccsc34)cc2)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.6081915
Proportion of valid SMILES: 0.5887879736924523
Sample trajectories:
BP(=O)(CCCCCCCCCCC(=O)NO)S(=O)(=O)c1ccc(Br)cc1
Brc1cc(-c2nc3c(Nc4cc(Br)c(Br)c(Br)c4)ncnc3cc2Br)ccc1I
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)c(Nc2ncnc3c(Nc4ccc(Br)c(Br)c4)c(Br)c(Br)c(Br)c23)c1
Fine tuning...
Mean value of predictions: 0.6150267
Proportion of valid SMILES: 0.6446875
Sample trajectories:
BrCc1ccc2c(Nc3ccc(Br)cc3-c3ccc(Br)cc3)ncnc2c1
Brc1cc(Br)c(Br)c(Nc2ncnc3c(Br)cc(Br)cc23)c1
Brc1cc(Br)c(Nc2ncnc3c(Nc4ccccc4Br)ncnc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(c1)c1cc(Nc3ncnc4ccccc34)ccc1N2

Trajectories with max counts:
115	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.59046704
Proportion of valid SMILES: 0.5208229114557279
Mean Internal Similarity: 0.48027566140185124
Std Internal Similarity: 0.09163863953834102
Mean External Similarity: 0.41989896478111033
Std External Similarity: 0.0761095011720872
Mean MolWt: 417.15909825373456
Std MolWt: 98.53700456475696
Effect MolWt: -0.8096208047124899
Mean MolLogP: 4.86406797601515
Std MolLogP: 1.4942611850084018
Effect MolLogP: 0.10271666084114031
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.094658% (1036 / 1067)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 100, 'seed': 3, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 6199.127268075943, 'valid_fraction': 0.5208229114557279, 'active_fraction': 0.5706567415055829, 'max_counts': 115, 'mean_internal_similarity': 0.48027566140185124, 'std_internal_similarity': 0.09163863953834102, 'mean_external_similarity': 0.41989896478111033, 'std_external_similarity': 0.0761095011720872, 'mean_MolWt': 417.15909825373456, 'std_MolWt': 98.53700456475696, 'effect_MolWt': -0.8096208047124899, 'mean_MolLogP': 4.86406797601515, 'std_MolLogP': 1.4942611850084018, 'effect_MolLogP': 0.10271666084114031, 'generated_scaffolds': 1067, 'novel_scaffolds': 1036, 'novel_fraction': 0.9709465791940018, 'save_path': '../logs/replay_combo_s3-6.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.064314
Reward: 1.000000
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(COP(=O)(O)O)C(O)C1O
Mean value of predictions: 0.00080482895
Proportion of valid SMILES: 0.777291210509853
Sample trajectories:
Brc1ccc(C2=CC3=C(NCCC3)c3ccccc3O2)cc1
Brc1ccc(OCCOCCCCN2CCCC2)cc1
C#CC#CCNC(=O)C(=O)N(CCCN(C(=O)C(CC(C)C)N(C(C)=O)C(C)C)C(C)C(=O)OC)C(C)C
C#CC1(CO)C(=O)c2cc(OC)ccc21
C#CC1=CC(=O)OC1C(=O)O
Policy gradient replay...
Mean value of predictions: 0.001361634
Proportion of valid SMILES: 0.7810447294338442
Sample trajectories:
Brc1ccc(-c2nc(-c3ccnc4[nH]ccc34)no2)cc1
Brc1ccc(OCCN2CCCCCC2)cc1
Brc1ccccc1
C#CC=C(Cl)C(Oc1cc2ccccc2c2ccccc12)P(=O)(OC)OC
C#CCCOC(=O)c1c(OCCc2ccccc2)nc2c(c1O)C=CN2CCc1cccc(Cl)c1
Fine tuning...
Mean value of predictions: 0.0011354419
Proportion of valid SMILES: 0.7718309859154929
Sample trajectories:
Brc1ccc(-c2ccc3ncccc3c2)cc1
Brc1ccc(CNCCc2ccccc2)cc1
C#CCCN(C(=O)CN1C(c2ccc(C#N)cc2Br)CC(O)C1C)S(=O)(=O)c1ccc(Cl)c(Cl)c1
C#CCCOC(=S)Nc1ccc(N=Nc2ccc(Br)cc2)cc1
C#CCCOc1ccccc1CNC(=O)Cc1ccccc1

  2 Training on 231 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.132466
Reward: 1.000000
Trajectories with max counts:
2	COC(=O)C(O)=CC(=O)c1ccccc1O
2	COc1ccc(NC(C)=O)cc1
2	COc1ccccc1N1CCN(Cc2ccccc2)CC1
2	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
2	O=C1Oc2ccccc2C1=O
Mean value of predictions: 0.0021267892
Proportion of valid SMILES: 0.7647794807632156
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C(O)C1O)P(=O)(OCc1ccccc1)OCc1ccccc1
BP1(=O)OCC(O)Oc2c(cc(Br)c(O)c2Br)OC1=N
Brc1ccc(C2=NOC(c3ccccc3)C2)o1
Brc1ccc(N2CCN(CCOc3cccc(Br)c3)CC2)nc1
Brc1cccc(Nc2ccc(-c3ccccc3)cc2)c1
Policy gradient replay...
Mean value of predictions: 0.0022258863
Proportion of valid SMILES: 0.7595491546649968
Sample trajectories:
Brc1ccc(N2CCN(CCCOc3ccc(Br)s3)CC2)cc1
Brc1ccc2c(Br)ccc3NCCSCN3c2c1Br
Brc1ccc2c(c1)N1CCSC1=N2
Brc1cccc(Nc2nc(-c3ccccc3)cs2)c1
C#CC(O)CN(c1ncn(Cc2cncnc2F)n1)C(C)C
Fine tuning...
Mean value of predictions: 0.0019370461
Proportion of valid SMILES: 0.774859287054409
Sample trajectories:
Brc1ccc(-c2nnc3n2CCC3)cc1
Brc1ccc2[nH]nc(-c3ccc4c(c3)OCO4)c2c1
Brc1ccc2c(c1)C(c1ccccc1)=NOC(c1ccccc1)=C2
Brc1ccc2ccc3[nH]c(cc3-c3ccco3)c2c1
Brc1cccc(Sc2[nH]nc3ccc(COCc4cccc(CN5CCNCC5)c4)cc23)c1

  3 Training on 254 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.808018
Reward: 1.000000
Trajectories with max counts:
2	COc1ccc2c(c1)CC(=O)N2
2	COc1cccc2c1C(=O)C1=C2CCCC1
2	O=C(O)c1cccc(OCc2ccccc2)c1
2	O=C1NC(=O)C(=Cc2ccccc2)S1
Mean value of predictions: 0.0010136452
Proportion of valid SMILES: 0.8028169014084507
Sample trajectories:
Brc1ccc(C2=Nc3n[nH]c(n3)-c3ccccc32)o1
Brc1ccc2oc(-c3ccccn3)nc2c1
Brc1ccccc1
Brc1cn2ccnc2c(-c2ccccc2)n1
Brc1cncc(C=Cc2ccccc2)c1
Policy gradient replay...
Mean value of predictions: 0.0008789453
Proportion of valid SMILES: 0.7836568566061365
Sample trajectories:
B[P+](CCCc1ccccc1)P(=O)(O)O
Brc1ccc2[nH]c(-c3ccc(Br)o3)nc2c1
Brc1cccc(Nc2ccncc2)c1Br
C#CCN(CC)c1ccc(Cc2ccc3[nH]c(-c4cn5cc(Cl)ccc5n4)nc3c2)cc1
C#CCN(Cc1ccc(C#Cc2ccc(OC)cc2)cc1)S(=O)(=O)c1ccc(C)cc1
Fine tuning...
Mean value of predictions: 0.0014440432
Proportion of valid SMILES: 0.780281690140845
Sample trajectories:
Brc1ccc(Nc2ccnc3ccc(Br)cc23)cc1
Brc1ccc2ccccc2c1
Brc1cccc(Br)c1
Brc1cccc(N2CCN(c3ccc4c(ccc5ccccc54)c3)CC2)c1
C#CC=CC(C#N)C(=O)OCC

  4 Training on 266 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.468857
Reward: 1.030715
Trajectories with max counts:
4	Cc1cccc2ccccc12
Mean value of predictions: 0.002105263
Proportion of valid SMILES: 0.7421875
Sample trajectories:
Brc1ccc(N(CCNc2ccc3c(ccn3-c3cnc4cc(N5CCN(c6ccccc6)CC5)ccc4n3)c2)Cc2ccccc2)cc1
Brc1ccc2[nH]cc(-c3cc[n+](C=Cc4ccccc4)cc3)c2c1
Brc1ccc2[nH]cc(-c3cnc4[nH]ccc4c3)c2c1
Brc1ccc2c(c1)C=CC=C(c1c[nH]c3ccccc13)O2
Brc1ccccc1
Policy gradient replay...
Mean value of predictions: 0.00267335
Proportion of valid SMILES: 0.7483588621444202
Sample trajectories:
Brc1ccc(CN2CCN(Cc3ccoc3)C(OCc3ccccc3)C2)cc1
Brc1ccc2[nH]c(-c3nc4ccc(Br)cn4c3-c3cccc4[nH]ccc34)cc2c1
Brc1ccc2[nH]c3ccc(Br)cc3c2c1
Brc1cccc(N2Cc3ccccc3C3C(=NOCc4ccccc4)C4=C(N4)C32)c1
Brc1ccccc1Br
Fine tuning...
Mean value of predictions: 0.002854744
Proportion of valid SMILES: 0.7450735064122614
Sample trajectories:
Brc1ccc2c(c1)N1CCC13CN(CCN3)S2
Brc1ccc2c(c1)OC(=Cc1ccc3ccccc3n1)C2
Brc1cccc2ccccc12
Brc1ccccc1
Brc1ccccc1-c1nc2ccccc2s1

  5 Training on 294 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.828425
Reward: 1.032716
Trajectories with max counts:
3	Cn1c2ccccc2c2ccccc21
Mean value of predictions: 0.003109244
Proportion of valid SMILES: 0.74375
Sample trajectories:
BP(=O)(CC(F)(F)F)NC(CBr)P(=O)(F)OP(=O)(O)O
BrC(C=Cc1ccccc1)=CC1=CCC12Oc1ccc(Br)cc1O2
Brc1cc2c3cccc(c3-n1)OCO2
Brc1cc2ccccc2c2ccccc12
Brc1ccc(-c2cccnc2)cc1
Policy gradient replay...
Mean value of predictions: 0.0028704095
Proportion of valid SMILES: 0.7407754846779238
Sample trajectories:
Brc1ccc(-c2noc(-c3c[nH]c(C4CC5CCC(C4)O5)n3)n2)cc1
Brc1ccc(Br)c(C2CS2)c1
Brc1ccc(Nc2ccncc2)cc1
Brc1ccc(Oc2ccc(-n3ccnc3)cc2)cn1
Brc1ccc2c(c1)OCO2
Fine tuning...
Mean value of predictions: 0.0007556675
Proportion of valid SMILES: 0.7448405253283302
Sample trajectories:
Brc1ccc(-c2ccc3ccccc3n2)cc1
Brc1ccc(N2CCN(Cc3ccncc3)CC2)cc1
Brc1ccc(Oc2ccccc2)cc1Br
Brc1ccc2[nH]ccc2c1
Brc1ccc2oc3ncccc3c2c1

  6 Training on 321 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.177581
Reward: 1.020038
Trajectories with max counts:
4	COc1cccc2ccccc12
Mean value of predictions: 0.0031263204
Proportion of valid SMILES: 0.7396875
Sample trajectories:
Brc1ccc(NCc2ccco2)c2ccccc12
Brc1ccc2[nH]cc(-c3ccccc3)c2c1
Brc1ccc2c(I)cccc2c1
Brc1cccc(-n2c3ccccc3c3ccccc32)c1
Brc1cccc2c3c(ccc12)OCO3
Policy gradient replay...
Mean value of predictions: 0.002617138
Proportion of valid SMILES: 0.7407754846779238
Sample trajectories:
BP(=O)(NC(=O)c1cccs1)S(=O)(=O)N1CCOCC1
Brc1ccc(C=C2CCCc3cc(Br)ccc32)cc1
Brc1ccc(Nc2ncnc3nccc(Br)c23)cc1
Brc1ccc2[nH]cnc2c1-c1nc2ccccc2[nH]1
Brc1ccc2c(-c3ccccc3)n[nH]c2c1
Fine tuning...
Mean value of predictions: 0.0024855013
Proportion of valid SMILES: 0.7546108158799625
Sample trajectories:
Brc1ccc(Nc2nnnn2-c2ccc3ccccc3c2)cc1
Brc1ccc2cccc(Oc3cccc(C#CC[SH]4CCCCC4)c3)c2c1
Brc1ccc2ccccc2c1
Brc1ccc2cccnc2c1
Brc1cccc(Oc2ccc(Br)c(Br)c2)c1

  7 Training on 350 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.063935
Reward: 1.033500
Trajectories with max counts:
2	COc1cc(OC)c2oc(-c3ccccc3)nc2c1
2	COc1ccc2[nH]c3c(Br)cccc3c2c1
2	COc1cccc2c1C(c1ccccc1)=CC(=O)O2
2	Cc1nc2ccccc2[nH]1
2	N=C(N)Nc1ccccc1S(N)(=O)=O
2	Nc1nccc2ccccc12
2	O=C1CCc2ccccc21
2	O=C1Nc2ccccc2C1=O
2	O=C1Nc2ccccc2S1
Mean value of predictions: 0.0036989248
Proportion of valid SMILES: 0.726789621756799
Sample trajectories:
Brc1ccc(-n2ccnc2)nc1CCN1Cc2ccccc2Oc2ccccc2C1c1cccc(-c2ccccc2-c2nn[nH]n2)c1
Brc1ccc(C2=CSC3=NCCN23)cc1
Brc1ccc(CN2CCN(c3ccccn3)CC2)cc1
Brc1ccc2[nH]cnc2c1
Brc1ccc2c(c1)CC(N1CCc3ccccc3C1)=NC1CCNC2CC1
Policy gradient replay...
Mean value of predictions: 0.0042024013
Proportion of valid SMILES: 0.7296620775969962
Sample trajectories:
Brc1ccc(OCCCN2CCOCC2)cc1
Brc1ccc2nc(-c3nc4ccccc4[nH]3)[nH]c2c1
Brc1cccc2c1c1[nH]c3ccccc3c1O2
Brc1csc(N2CCn3c(nc4ccccc43)S2)n1
C#CCON1C(C)(C)c2ccccc2S1(=O)=O
Fine tuning...
Mean value of predictions: 0.004069264
Proportion of valid SMILES: 0.721875
Sample trajectories:
BP(=O)(OCC1OC(N2C=CC(N)=NC2=O)C(O)C(O)C1O)OP(=O)(O)O
Brc1ccc(-c2nnn3c2Cc2ccccc23)o1
Brc1ccc(C#CCC2(CCCN3CCOCC3)Cc3cc(Br)ccc32)cc1
Brc1ccc(Nc2nccc3ccncc23)cc1
Brc1ccc2[n+][nH]n2c1-c1ccccc1

  8 Training on 388 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.156466
Reward: 1.049286
Trajectories with max counts:
2	CC(=O)Nc1cccc2ccccc12
2	CN(O)C=O
2	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
2	COc1cccc(NS(=O)(=O)c2ccccc2)c1
2	Cc1n[nH]c(SCc2ccccc2)n1
2	Cc1nn(-c2ccccc2)c2c1NC(=O)N2
2	O=C(O)c1cccc2ccccc12
Mean value of predictions: 0.0038686462
Proportion of valid SMILES: 0.6946875
Sample trajectories:
Brc1ccc(C=Nn2cnnc2)cc1Br
Brc1ccc2[nH]cnc2c1
Brc1ccc2c(Br)c(Br)c(OCc3ccccc3)cc2n1
Brc1ccc2c(c1)CNCCN2c1ccc2[nH]ccc2c1
Brc1ccc2c(c1)ncn2-c1ccc2c(c1)ncn2Cc1cccnc1
Policy gradient replay...
Mean value of predictions: 0.0032243617
Proportion of valid SMILES: 0.6982489055659787
Sample trajectories:
BrC1=CN(Cc2ccccc2Br)C=CC2=CC=CC2=N1
Brc1cc2cn2c(-c2ccccc2)nn1-c1ccccc1
Brc1ccc(Br)o1
Brc1ccc(N=Cc2ccncc2Br)cc1
Brc1ccc(OC2=Cc3c([nH]c4ccccc34)C23CCN(Cc2ccccc2)C3)cc1
Fine tuning...
Mean value of predictions: 0.0023039433
Proportion of valid SMILES: 0.7057535959974984
Sample trajectories:
BP(=O)(OP(=O)(O)O)OP(=O)(O)OCC1OC(n2cnc3c(OC)nc(N)nc32)C(O)C1O
Brc1cc2c(ncn2-c2ccccc2)nc1CNc1ncccn1
Brc1ccc(OCc2ccc(-c3ccn(Cc4ccccc4)n3)cc2)cc1
Brc1ccc2[nH]cc(-c3ccnc(Br)c3)c2c1
Brc1ccc2[nH]nc(-c3cncnc3)c2c1

  9 Training on 427 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.853458
Reward: 1.043756
Trajectories with max counts:
3	Cc1nn[nH]n1
Mean value of predictions: 0.007800752
Proportion of valid SMILES: 0.6662492172824045
Sample trajectories:
BP(=O)(OCC)OC(=O)C=C(Br)Br
Brc1ccc(CSc2ncn3c(Br)cnc3n2)cc1
Brc1ccc(Cc2nc(N3CCOC3)nc3c(-c4ccc(Br)nc4)cnn23)cc1
Brc1ccc2[nH]ccc2c1
Brc1ccc2c(Nc3cccnc3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.0042095417
Proportion of valid SMILES: 0.6689612015018773
Sample trajectories:
Brc1cc(Cn2c(-n3cncc3Br)nc3ccccc32)cc2c1OCO2
Brc1ccc(CN2C=Nc3c(-c4cnc[nH]4)n[nH]c3-c3ccccc32)cc1
Brc1ccc(N2N=NS2)cc1
Brc1ccc2[nH]ncc2c1
Brc1ccc2c(Br)c[nH]c2c1
Fine tuning...
Mean value of predictions: 0.004817045
Proportion of valid SMILES: 0.675531914893617
Sample trajectories:
Brc1ccc2cc3c(cc2c1)OCO3
Brc1ccc2nc(-c3cccc4ccc(Br)cc34)sc2c1
Brc1ccccc1OC1CCN(Cc2ccnnc2OCc2ccccc2)CC1
Brc1cnc2cccc(N=C3SSC3=Nc3ccccc3Br)c2c1
Brc1cnc2sc(-c3ccccc3)nc2n1

 10 Training on 481 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.528786
Reward: 1.025861
Trajectories with max counts:
6	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.007320507
Proportion of valid SMILES: 0.6665624022521114
Sample trajectories:
Brc1ccc(-c2cccc3c2cnn3-c2ccc3ccccc3c2)cc1
Brc1ccc(-c2oc3ccccc3c2Br)cc1
Brc1ccc(-n2nnc3cnccc32)o1
Brc1ccc2[nH]cc(-c3ccccn3)c2c1
Brc1ccc2[nH]cnc2c1
Policy gradient replay...
Mean value of predictions: 0.0036984354
Proportion of valid SMILES: 0.6590625
Sample trajectories:
BrC1=C2CCN(CCc3cc4cccnc4[nH]3)OCCC3OC3C2O1
BrC1CC(c2nnnn2Sc2ccccc2)NN1Cc1ccccc1
Brc1ccc(-c2nc3ccccc3o2)s1
Brc1ccc(N2CC3=C(C2)c2cnc(CN4CCCC4)cc2CS3)cc1
Brc1ccc(NC2=C(Nc3ccccn3)C=C2)cc1
Fine tuning...
Mean value of predictions: 0.006299953
Proportion of valid SMILES: 0.6659361302442078
Sample trajectories:
BP(=O)(OC(=O)COCc1ccccc1)c1ccc(Br)cc1
Brc1ccc(C2=Nn3ncnc3S2)cc1
Brc1ccc(N=Nc2cccc3ncccc23)cc1
Brc1ccc2[nH]c3cc(OCc4ccccc4)ccc3c2c1
Brc1ccc2c(c1)-c1cccnc1N2

 11 Training on 540 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.705105
Reward: 1.026641
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0057368944
Proportion of valid SMILES: 0.6328638497652582
Sample trajectories:
Brc1c(-c2cn(Cc3ccco3)nn2)nc2scnc2c1Br
Brc1ccc(N2Cc3ccccc3N=C2c2ccccc2)c(Br)c1
Brc1ccc(Oc2cccc3cccnc23)cc1
Brc1ccc(Oc2ccccc2)c(Br)c1
Brc1ccc2[nH]c(-c3c[nH]cn3)nc2c1
Policy gradient replay...
Mean value of predictions: 0.0039506173
Proportion of valid SMILES: 0.6340012523481527
Sample trajectories:
Brc1cc2c(s1)N=Nc1nc(SN=Nc3cccs3)n2n1
Brc1ccc(-n2ccnc2-c2ccnn3cccc23)o1
Brc1ccc(Nc2ncnc3[nH]c(N4CCOCC4)nc23)cc1
Brc1cccc(OCc2cccnc2)c1
Brc1cccc(Oc2ccc(SCc3ccccc3)cc2)c1
Fine tuning...
Mean value of predictions: 0.004958678
Proportion of valid SMILES: 0.6436170212765957
Sample trajectories:
Brc1cc2c(-c3ccc4ccccc4c3)cn3cc(Br)c4ccccc4c3c(n1)Nc1cc(Br)c(Br)cc1C=C2
Brc1ccc(-c2cc[nH]n2)cn1
Brc1ccc(N=C2Sc3ccccc3N2CC2CC2)cc1
Brc1ccc2[nH]c(-c3ccccc3)cc2c1
Brc1ccc2[nH]cnc2c1

 12 Training on 593 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.178920
Reward: 1.038032
Trajectories with max counts:
2	O=N(=O)c1cccc2nsnc12
Mean value of predictions: 0.005652621
Proportion of valid SMILES: 0.6092673763306199
Sample trajectories:
Brc1cc2c(N=Nc3ccccc3)ncnc2cn1
Brc1ccc(-c2nn(-c3ccccc3)c3ccccc23)cc1
Brc1ccc(-c2nn3cc[nH]c3c2-c2cnn(C3=NNC=C3)c2)cc1
Brc1ccc(Br)cc1
Brc1ccc(N2CCN(Cc3nnc4onc(-c5ccccc5)n34)CC2)o1
Policy gradient replay...
Mean value of predictions: 0.005453592
Proportion of valid SMILES: 0.5964967156709415
Sample trajectories:
BP(=O)(OCC1OC(=O)c2ccccc21)c1ccc(O)cc1
BrC1=Nc2c(-c3cccs3)sc3cccc1c23
Brc1c[nH]c2nnnn12
Brc1cc2c(cc1C1(C3=CSC(c4cccs4)N3)N=CN1)OCO2
Brc1ccc(Nc2nc[nH]n2)cc1
Fine tuning...
Mean value of predictions: 0.009425785
Proportion of valid SMILES: 0.5781396805512058
Sample trajectories:
Brc1cc2c([nH]1)c1c3ccccc3oc21
Brc1ccc2[nH]c(-c3cc(-c4ccccc4)c[nH]3)nc2c1
Brc1ccc2[nH]c(-c3cnn(-c4ccnnc4)c3)nc2c1
Brc1ccc2c(n1)-c1ccccc1S2
Brc1ccc2oc3ccccc3c2n1

 13 Training on 653 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.182651
Reward: 1.050478
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0097694835
Proportion of valid SMILES: 0.5706232383338553
Sample trajectories:
Brc1cc2c(nn1)-c1ncccc1O2
Brc1ccc(C=NNc2ncnc3ncnn23)cc1
Brc1cn2cc(Cc3nnnn3Br)cc2o1
Brc1cnc2cncnc2n1
Brc1cncc(-c2nn[nH]n2)c1
Policy gradient replay...
Mean value of predictions: 0.0075865868
Proportion of valid SMILES: 0.5696836830566865
Sample trajectories:
Brc1c(Br)c(Br)n2ncc(Br)c2c1Br
Brc1cc2c(c3cccnc13)OCO2
Brc1ccc2c(Oc3cnc4[nH]cnc4c3)ncnc2c1
Brc1ccc2c(c1)[nH]c1ncncc12
Brc1ccc2nc(-c3ccn4c(N5CCOCC5)cnc4n3)[nH]c2c1
Fine tuning...
Mean value of predictions: 0.010726257
Proportion of valid SMILES: 0.5600750938673341
Sample trajectories:
Brc1cc(Br)c2c(c1)N(C1CCCS1)C(c1ccc3c(c1)OCO3)=N2
Brc1ccc2[nH]c3ccccc3c2c1
Brc1ccc2nc(SCc3ccccc3)nn2c1
Brc1ccc2ncn(-c3ccncc3)c2c1
Brc1cccc(Nc2nn[nH]n2)c1

 14 Training on 733 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.141325
Reward: 1.189174
Trajectories with max counts:
3	COc1cc2ncnc(Nc3ccc(N(=O)=O)cc3)c2cc1OC
3	Cn1cnnn1
Mean value of predictions: 0.007146725
Proportion of valid SMILES: 0.5775484677923702
Sample trajectories:
Bc1c(Br)c(Br)nc2c1ncn2C1OC(COP(=O)(O)OP(=O)(O)OP(=O)(O)O)C(O)C1O
Brc1cc(-c2nnnn2-c2ccnc3cccnc23)no1
Brc1ccc(N2C=Nc3nnnn3C2=Nc2cccnc2)nn1
Brc1ccc(N2CC3CN(c4cc(Br)n(-c5ccccn5)n4)C4=CC(C4)c4nc(no4)C3C2)cc1
Brc1ccc(Nc2nc(-c3cnnn3CSc3ccccn3)nc3ccccc23)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.012171972
Proportion of valid SMILES: 0.5602126994056928
Sample trajectories:
Bc1ccc2nc(ns2)N(C(C)C)S(=O)(=O)c2ccccc2n1
Brc1ccc2[nH]c(-c3cnc(-c4cnn(CN5CCCC5)c4)nc3N3CCOCC3)nc2c1
Brc1ccc2c(Br)cc3cnnn3c2c1
Brc1cn2c(-c3nc4c(-c5ccncc5)nnn4c4ccccc34)cnc2s1
Brc1cnc2c(Nc3cnc4ccccc4n3)nn12
Fine tuning...
Mean value of predictions: 0.010589519
Proportion of valid SMILES: 0.5728580362726704
Sample trajectories:
BP(=O)(OC(C#N)c1ccc(I)cc1)c1ccc(Cl)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc2ncn(-c3ccco3)c2n1
Brc1ccc2oc3c(CSc4ccccc4Br)c3cc2c1
Brc1ccc2ocnc2c1

 15 Training on 831 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.757820
Reward: 1.176701
Trajectories with max counts:
4	COc1cc2ncnc(Nc3ccc4c(c3)OCO4)c2cc1OC
Mean value of predictions: 0.01486175
Proportion of valid SMILES: 0.5426695842450766
Sample trajectories:
BP(=O)(OCC1OC(=O)c2ccccc2C1=O)c1ccc(I)cc1
Brc1cc2c(Nc3cccc4cccnc34)cc2nnn1-c1nnn[nH]1
Brc1cc2c3c(cccnc13)Sn1ncnc21
Brc1ccc2c(c1)[nH]c1ncc(Br)cc12
Brc1ccc2nc(-c3ccnc4[nH]cc(Br)c34)oc2c1
Policy gradient replay...
Mean value of predictions: 0.02034483
Proportion of valid SMILES: 0.5442602439787301
Sample trajectories:
Brc1cc(Br)c2ncnn2c1
Brc1cc2c(-c3ccc(-c4cn[nH]c4)cc3)c[nH]c2c2ncnn12
Brc1cc2c(cc1-n1ncc3c(OCc4cccnc4)cccc31)OCO2
Brc1ccc(-n2cccc2-c2nc3cc(Br)ccc3o2)cc1
Brc1ccc(CSc2nn3cc(Br)c(-c4ccccc4)c3o2)cc1
Fine tuning...
Mean value of predictions: 0.015563299
Proportion of valid SMILES: 0.5384615384615384
Sample trajectories:
Brc1cc2nc(N3CCOCC3)nc(Nc3cccnc3)c2cc1Br
Brc1cc2sc(SCc3ccc4c(c3)OCO4)c2n1
Brc1cc2sc3c(c2c2cncnc12)NCC3
Brc1ccc(-n2nnnc2-n2cnnc2-n2cc(n3ccnc3)nn2)cc1
Brc1ccc(C2=Nc3ncnn3-c3cccnc32)o1

 16 Training on 974 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.457536
Reward: 1.084734
Trajectories with max counts:
4	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.015311005
Proportion of valid SMILES: 0.5234815278647464
Sample trajectories:
BrC1=Cc2c3cc(Br)ccc3[nH]c2c2ccc2CCc2ccnn21
Brc1ccc2[nH]cnc2n1
Brc1ccc2nc(-c3nc4c(n3-c3ccccc3)SCC4)ncc2c1
Brc1ccc2ncnn2c1
Brc1ccnc2[nH]cnc12
Policy gradient replay...
Mean value of predictions: 0.017691858
Proportion of valid SMILES: 0.5339380669377541
Sample trajectories:
BP(=O)(OC(=O)c1cn2cc(Br)cnc2n1)OP(=O)(O)O
Bc1nn(C2CC(=Cc3ccccc3)S2)n1Cc1cccnc1
BrC1=NN2C(=Nc3ccc(Br)cc32)S1
Brc1cc2c(n1Cc1cccs1)C(=Cc1ccncc1)S2
Brc1cc2nnnn2c(-c2ccc3sccc3c2)n1
Fine tuning...
Mean value of predictions: 0.019075481
Proportion of valid SMILES: 0.5347309136420526
Sample trajectories:
Brc1cc2nc(-c3cc(Br)c(Br)c(Br)c3)[nH]c2cc1-c1cccs1
Brc1ccc(C=NNc2nncn2-c2ccc(Br)cc2)cc1
Brc1ccc2ncsc2c1
Brc1cn2c(N3Cc4ncccc4-n4nncc4C3)nn2c2cccn12
Brc1cnc2c(NCc3ccncn3)ncnc2n1

 17 Training on 1116 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.654223
Reward: 1.151397
Trajectories with max counts:
5	COc1cc2ncnc(Nc3ccc4c(c3)OCO4)c2cc1OC
Mean value of predictions: 0.030057143
Proportion of valid SMILES: 0.5470459518599562
Sample trajectories:
Brc1cc2nsnc2n1C1=Nc2nncn2C2=NOC(Cc3ccccc32)C1c1cccc2cnccc12
Brc1ccc2nc(-c3cnc4ccncc4c3)c(Nc3cncnc3)n2c1
Brc1ccc2nc(-c3noc(C4COc5cccnc5N4)n3)oc2c1
Brc1ccc2ncnn2c1
C#CCOc1cccc2c(OC)cc3nncnc3c12
Policy gradient replay...
Mean value of predictions: 0.0325856
Proportion of valid SMILES: 0.5297060662914321
Sample trajectories:
Brc1cc2cc(Oc3ccncc3)ccn2c1Br
Brc1cc2cccnc2s1
Brc1ccc2cnn(-c3ccccc3)c2c1
Brc1ccc2nc(-c3ccc4nc[nH]c4n3)[nH]c2c1
Brc1ccc2nc(-c3ccncc3)nc(N3CCOCC3)c2c1
Fine tuning...
Mean value of predictions: 0.027400611
Proportion of valid SMILES: 0.5109375
Sample trajectories:
Brc1cc2nnn[nH]c2c1-c1nc2ccncn2c1N1CCNCC1
Brc1ccc2[nH]c(-c3cnc4ccccc4c3)nc2c1
Brc1ccc2nc(SCc3cnccn3)[nH]c2c1
Brc1ccc2ncnc(Nc3ccccc3)c2n1
Brc1ccc2nsnc2n1

 18 Training on 1373 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.468046
Reward: 1.306931
Trajectories with max counts:
10	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.04401966
Proportion of valid SMILES: 0.5727244291523303
Sample trajectories:
BP1(=O)OCC(O)(Oc2ccccc2I)c2c(cc3C(=O)OC3=C2)O1
BrC1=C(Nc2cc3nncn3cc2Br)[nH]c2ccccc21
Brc1cc(Br)c2c(c1)C(SCc1ccccc1)=N2
Brc1cc2c(cc1N1CCOC1)C(c1ccccc1)=N2
Brc1cc2cncnc2s1
Policy gradient replay...
Mean value of predictions: 0.045434296
Proportion of valid SMILES: 0.5617766656240225
Sample trajectories:
Brc1cc2occc2n1-c1nnc2c1nc1cnccc12
Brc1ccc(Nc2ncnc3sc4c(Br)cncc4c23)cc1
Brc1ccc2[nH]cnc2c1-c1nnc2sccn12
Brc1ccc2c(Oc3ccccc3)ccnc2c1
Brc1ccc2c(c1)C=NN(c1cccnc1)C(c1ccnc3ccsc13)=N2
Fine tuning...
Mean value of predictions: 0.040023066
Proportion of valid SMILES: 0.541875
Sample trajectories:
Brc1c2ccccc2nc2nccnc12
Brc1cc2[nH]cnc2nc1-c1cc2ncccn2c1
Brc1cc2cccnc2c2nc(-c3ccncc3)[nH]c12
Brc1ccc(-c2nc3cccnc3[nH]2)nc1
Brc1ccc(OCc2nncn2-c2ccccn2)cc1

 19 Training on 1726 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.389826
Reward: 1.420651
Trajectories with max counts:
43	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.04150376
Proportion of valid SMILES: 0.6236323851203501
Sample trajectories:
Brc1cc2c(N=Nc3ccccc3)c3ccn2c3cc1Br
Brc1cc2c(nc1SCc1cnn3ccncc13)c1ccncc1O2
Brc1cc2ncnc2nc2ccccc12
Brc1ccc2[nH]cc(-c3ccc4ccccc4c3)c2c1
Brc1ccc2c(c1)-c1ccccc1O2
Policy gradient replay...
Mean value of predictions: 0.039856926
Proportion of valid SMILES: 0.6115625
Sample trajectories:
Brc1c2ncnc2ncn1-c1ccccc1
Brc1cc2ncnc(Nc3ccccc3I)n2n1
Brc1ccc2[nH]cnc2c1
Brc1ccc2nccn2c1
Brc1ccc2nccn2c1N1CCN(Cc2ccccn2)CC1
Fine tuning...
Mean value of predictions: 0.04156658
Proportion of valid SMILES: 0.5984375
Sample trajectories:
Brc1ccc(-c2nc3cccnc3n2CC2=NCCN2)cc1
Brc1ccc(-c2noc3cncnc23)cc1
Brc1ccc(Nc2cnc3cccnc3c2N2CCOCC2)cn1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc2[nH]cc(Br)c2c1

 20 Training on 2137 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.826078
Reward: 1.723027
Trajectories with max counts:
122	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.06817371
Proportion of valid SMILES: 0.5253125
Sample trajectories:
Brc1ccc(Br)c(Nc2ncnc3nncn23)c1
Brc1ccc2cccn2c1
Brc1ccc2ncnc(Nc3cccnc3)c2c1
Brc1ccc2ncnc(Nc3ccco3)c2c1
Brc1ccc2oc3ccc(COc4ccccc4)cccc3c2c1
Policy gradient replay...
Mean value of predictions: 0.08186715
Proportion of valid SMILES: 0.5221875
Sample trajectories:
Brc1c(N2CCOCC2)nc2ccnc(Nc3cccnc3)c2c1Nc1ccnc2cccnc12
Brc1cc2c(cn1)OCO2
Brc1cc2cc(c3ccnc4nnccc43)sc2nc1N1CCOC1
Brc1cc2ccccc2nc1SCc1cnn2ccncc12
Brc1cc2ncnc(Nc3ccccn3)c2cc1Br
Fine tuning...
Mean value of predictions: 0.059075147
Proportion of valid SMILES: 0.540625
Sample trajectories:
BP(=O)(NP(=O)(O)O)OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O
B[PH](=O)(=NO)OCOc1ccc2ccccc2c1
BrC1=C(Oc2ccccc2)c2ccccc2S1
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCOCC1
Brc1cc2ncnc(Sc3ccccc3Br)c2cn1

Trajectories with max counts:
539	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.05561152
Proportion of valid SMILES: 0.4839375
Mean Internal Similarity: 0.4878499961309399
Std Internal Similarity: 0.11823912074045768
Mean External Similarity: 0.4326495292802093
Std External Similarity: 0.08935623924808629
Mean MolWt: 356.05394545454556
Std MolWt: 81.39809765808833
Effect MolWt: -1.4896650631965922
Mean MolLogP: 4.194739424242426
Std MolLogP: 1.1995984838105043
Effect MolLogP: -0.3963736762664892
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 93.714286% (164 / 175)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 0, 'seed': 3, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5223.874439716339, 'valid_fraction': 0.4839375, 'active_fraction': 0.04261913986826811, 'max_counts': 539, 'mean_internal_similarity': 0.4878499961309399, 'std_internal_similarity': 0.11823912074045768, 'mean_external_similarity': 0.4326495292802093, 'std_external_similarity': 0.08935623924808629, 'mean_MolWt': 356.05394545454556, 'std_MolWt': 81.39809765808833, 'effect_MolWt': -1.4896650631965922, 'mean_MolLogP': 4.194739424242426, 'std_MolLogP': 1.1995984838105043, 'effect_MolLogP': -0.3963736762664892, 'generated_scaffolds': 175, 'novel_scaffolds': 164, 'novel_fraction': 0.9371428571428572, 'save_path': '../logs/replay_combo_s3-7.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.064314
Reward: 1.000000
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(COP(=O)(O)O)C(O)C1O
Mean value of predictions: 0.00080482895
Proportion of valid SMILES: 0.777291210509853
Sample trajectories:
Brc1ccc(C2=CC3=C(NCCC3)c3ccccc3O2)cc1
Brc1ccc(OCCOCCCCN2CCCC2)cc1
C#CC#CCNC(=O)C(=O)N(CCCN(C(=O)C(CC(C)C)N(C(C)=O)C(C)C)C(C)C(=O)OC)C(C)C
C#CC1(CO)C(=O)c2cc(OC)ccc21
C#CC1=CC(=O)OC1C(=O)O
Policy gradient replay...
Mean value of predictions: 0.001361634
Proportion of valid SMILES: 0.7810447294338442
Sample trajectories:
Brc1ccc(-c2nc(-c3ccnc4[nH]ccc34)no2)cc1
Brc1ccc(OCCN2CCCCCC2)cc1
Brc1ccccc1
C#CC=C(Cl)C(Oc1cc2ccccc2c2ccccc12)P(=O)(OC)OC
C#CCCOC(=O)c1c(OCCc2ccccc2)nc2c(c1O)C=CN2CCc1cccc(Cl)c1
Fine tuning...
Mean value of predictions: 0.006654676
Proportion of valid SMILES: 0.6965236454744754
Sample trajectories:
Bc1cc(Cl)ccc1Nc1ncnc(Nc2ccc(I)cc2Br)n1
Brc1ccc(Nc2nc(Nc3ccccn3)ncc2c2ncc3sccn32)cc1
Brc1ccc(Nc2nc3ccc(Br)cc3s2)cc1
Brc1ccc2c(c1)NC(N1CCCCCC1)CC2
Brc1ccc2ncnc(Nc3cccc4[nH]cnc34)c2c1

  2 Training on 246 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.375479
Reward: 1.027438
Trajectories with max counts:
6	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0133394245
Proportion of valid SMILES: 0.6851330203442879
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C(O)C1O)P(=O)(Oc1ccccc1)Oc1ccccc1
Brc1cc2c(cc1-c1cccnc1)OCO2
Brc1cccc(Br)c1
Brc1cccc(Nc2nc(Nc3ccccc3)ncc2-c2nn[nH]n2)c1
Brc1cccc(Nn2ccnc2)n1
Policy gradient replay...
Mean value of predictions: 0.008225508
Proportion of valid SMILES: 0.6768845792930873
Sample trajectories:
BrC(=NNC(Nc1ccc2ccccc2c1)Nc1ccncn1)c1c[nH]c2ccccc12
Brc1c(-c2ccc3oc(-c4ccccc4)nc3n2)oc2ccccc12
Brc1ccc(-c2nc3c(NCc4ccccc4)c(Br)cnc3[nH]2)cc1
Brc1ccc(Nc2nc3ccccc3nc2-c2cnn(CCN3CCCC3)n2)cc1
Brc1ccc2c(c1)-c1ccoc1CCN2
Fine tuning...
Mean value of predictions: 0.013371537
Proportion of valid SMILES: 0.6558095834638271
Sample trajectories:
Brc1ccc(-c2ccn(Cc3ccccc3)n2)c2cccnc12
Brc1ccc(-c2nccnc2CNc2ccc(Br)cn2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)nc1
Brc1ccc2[nH]c(CC3CCCC3)cc2c1
Brc1cccc(Nc2ncnc3ccccc23)c1

  3 Training on 370 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.184075
Reward: 1.070415
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.017023394
Proportion of valid SMILES: 0.630571249215317
Sample trajectories:
Brc1cc2c([nH]1)oc1ccccc12
Brc1ccc2oc(Br)c(Nc3ccco3)c2c1
Brc1cccc(CN2CCC(NCCc3cncnc3)CC2)c1
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Brc1csc(Nc2ccc(Nc3cnc4ccccc4n3)nc2)n1
Policy gradient replay...
Mean value of predictions: 0.015921377
Proportion of valid SMILES: 0.6411468178954002
Sample trajectories:
BP(=O)(OCC1CCCCC1)N(CCCl)CCCl
Brc1ccc(CN2CCC(c3c[nH]cn3)CC2)nc1
Brc1ccc2c(c1)C1C(CCn3ccnc3)=CC=CC(=C1c1ccccn1)N2
Brc1ccc2nc(-c3ccc4[nH]cnc4c3)[nH]c2c1
Brc1ccc2ncsc2c1
Fine tuning...
Mean value of predictions: 0.024518043
Proportion of valid SMILES: 0.6335734419041653
Sample trajectories:
Brc1ccc(-c2nc3cccc(-c4cnnc(N5CCCC5)n4)c3s2)cc1
Brc1ccc(Nc2nccc(Nc3cccc(Br)c3)n2)cc1
Brc1ccc2[nH]c(-c3ccccn3)nc2c1
Brc1cccc(Br)c1
Brc1cccc(Nc2cc(Oc3ccccc3)ccn2)c1

  4 Training on 552 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 20.238519
Reward: 1.269332
Trajectories with max counts:
6	Clc1ccc(Nc2ncnc3ccccc23)cc1
6	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.051079914
Proportion of valid SMILES: 0.5792930872693149
Sample trajectories:
Brc1c[nH]c2ccncc12
Brc1ccc(-c2ncnc3nc(-c4ccc(Nc5ccccn5)nc4)sc23)c(-c2ccccc2)c1
Brc1ccc(CN2CCN(Cc3ccccc3)C2)cn1
Brc1ccc(Nc2cc(Br)cnc2Br)cc1
Brc1ccc(Nc2cccnc2)cc1
Policy gradient replay...
Mean value of predictions: 0.04757333
Proportion of valid SMILES: 0.5877742946708464
Sample trajectories:
BrCc1sccc1-c1cnc2ccc(Nc3sccc3Br)nn12
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3nc(-c4ccccc4)cn23)cc1
Brc1ccc(Nc2ncnc3sc(Br)nc23)cc1
Fine tuning...
Mean value of predictions: 0.054565217
Proportion of valid SMILES: 0.5755395683453237
Sample trajectories:
BP(=O)(c1ccc(F)cc1F)N1CCC2(c3ccc4ccccc4c3F)OC(=O)c2c1F
Brc1ccc(Br)c(Nc2ncnc3sc(N4CCCNc5ccc(C=CCNc6nc(Br)cnc6COCc6ccccc6)cc5C5CCC54)nc23)c1
Brc1ccc(CCN2CCN(c3ncnc4cc(Br)ccc34)CC2)cc1
Brc1ccc(N2CCOCC2)c(NCc2ccncc2)c1
Brc1ccc(N=Nc2ccnc(Nc3ccc(Br)cc3)n2)cc1

  5 Training on 970 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 23.503430
Reward: 1.771709
Trajectories with max counts:
22	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.10251308
Proportion of valid SMILES: 0.5970615817442951
Sample trajectories:
BP(=O)(O)Cc1cc(Br)c(O)c(Br)c1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)O
BP(=O)(OCC1OC(n2cnc3c2NC=NC3=O)C(O)C1OP(=O)(O)O)n1cnc2c(N)ncnc21
BrCCc1ccc2c(c1)-c1cccnc1-2
Brc1cc(Br)c(-c2nc(Cn3cncn3)ncc2-n2nc(C3CCCN3)cc2Nc2cncnc2)cc1Br
Policy gradient replay...
Mean value of predictions: 0.100520834
Proportion of valid SMILES: 0.6
Sample trajectories:
Br
Brc1cc(Br)c2c(Nc3cccc4ccccc34)ncnc2c1
Brc1cc2sc(-c3cccc4ccccc34)nc2nc1-c1ccccc1
Brc1ccc(-c2ncnn2Cc2cncnc2)nn1
Brc1ccc(Nc2cccc3ncnc(Nc4cccc(Br)c4)c23)cc1
Fine tuning...
Mean value of predictions: 0.094844304
Proportion of valid SMILES: 0.6127619643415703
Sample trajectories:
BP(=O)(CCCl)OCCCl
BrC[n+]1cc(-c2cccs2)c2ccccc2n1
BrCc1ccnc2c1-c1cc(CN3C=CCCC3)ccc1-2
Brc1cc2scnc2c(-c2nc3ccccc3[nH]2)c1Br
Brc1ccc(-c2[nH]c(-c3ccncc3)nc2-c2cscc2Br)cc1

  6 Training on 1798 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 22.415562
Reward: 2.082028
Trajectories with max counts:
85	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.15714286
Proportion of valid SMILES: 0.6039387308533917
Sample trajectories:
Brc1ccc(-c2ccc(Nc3ncnc4ccccc34)cc2)cc1
Brc1ccc(-c2ccccc2)c2cc(ncn2)Nc2cc(Br)ccc2Nc2ccc(cc2)C2CCN(CC2)c2csc(c2)N1
Brc1ccc(-c2nc3ccccc3nc2-c2ccccc2)cc1
Brc1ccc(-c2nnnc3nc(-c4cccc(Br)c4)oc23)cc1
Brc1ccc(Br)c(-c2ccc3c(c2)Nc2ccccc2N3)c1
Policy gradient replay...
Mean value of predictions: 0.16290833
Proportion of valid SMILES: 0.593125
Sample trajectories:
BP(=O)(c1ccccc1)N(O)Cc1ccc(Br)cc1
Brc1c[nH]c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(N3CCOCC3)ncnc2cc1-c1ccc(Nc2ncccn2)nc1
Brc1cc2c(s1)-c1ccccc1N2
Brc1ccc(-c2nc3cccc(Br)c3s2)cc1
Fine tuning...
Mean value of predictions: 0.15632911
Proportion of valid SMILES: 0.5926852141294154
Sample trajectories:
BP(=O)(OCCOCCOC(=O)OP(=O)(O)O)P(=O)(O)O
Brc1ccc(-c2cnc3ccc(Nc4ccncc4)cnc23)cc1
Brc1ccc(Nc2nc(Br)nc3ccccc23)cc1
Brc1ccc(Nc2nc3ccc(Br)cc3o2)cc1
Brc1ccc(Nc2ncnc3c(-c4ccccc4)csc23)cc1

  7 Training on 3021 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 23.211577
Reward: 3.321687
Trajectories with max counts:
320	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.3092396
Proportion of valid SMILES: 0.38230697092841515
Sample trajectories:
Brc1cc(Nc2ncnc3ccsc23)nc2ccccc12
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(C4=Nc5c(Br)cc(Br)cc5C4=Nc4ccccc4)cc23)c(Br)c1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.27403682
Proportion of valid SMILES: 0.3732416380118787
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1ccc(N2CCN(c3ccccc3)CC=C(SC3CCCCCCC3)C2)cc1
Brc1ccc(Nc2ncccc2Br)cc1
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.2281671
Proportion of valid SMILES: 0.4649122807017544
Sample trajectories:
Brc1cc(Nc2ncnc3scnc23)sc1-c1ccccc1
Brc1cc2ncnc(Nc3cccnc3)n2n1
Brc1cc2sc3c(s1)N=Nc1ccc4c(Br)cccc4[nH]c1c23
Brc1ccc(CNc2ncnc3[nH]cnc23)cc1
Brc1ccc(Nc2ncnc3c2sc2ccc(Br)cc23)cc1

  8 Training on 4269 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 22.266090
Reward: 3.545991
Trajectories with max counts:
227	Fc1ccc(Nc2ncnc3ccc(F)cc23)cc1
Mean value of predictions: 0.29035535
Proportion of valid SMILES: 0.3694904657705533
Sample trajectories:
Brc1ccc(Nc2cc(Br)c3nc(Br)cc(Nc4ccc(Br)cc4)c3c2)cc1
Brc1ccc(Nc2cnc(Br)cn2)cc1
Brc1ccc(Nc2ncnc(Nc3ccc(Br)cc3)c2Nc2cccnc2)cc1
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.29086578
Proportion of valid SMILES: 0.39368355222013757
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Nc2ncnc3c[nH]cc23)ncc1-c1ccccc1
Brc1cc2c(nc3c(-c4ccccc4)c(-c4cccs4)c3c1Br)-c1ccccc1-2
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(CC2(c3ncccn3)c3ccc(Nc4ccccc4c(Nc4ccc(Br)cc4)ncn3)C2Br)cc1
Fine tuning...
Mean value of predictions: 0.24780822
Proportion of valid SMILES: 0.4563926226945921
Sample trajectories:
Brc1cc(Nc2ncnc3ccccc23)nc2cnccc12
Brc1cc2c(Nc3ccncc3)ncnc2cc1Nc1ccncc1
Brc1ccc(-n2c(-c3ccccc3Br)nc3c4ccccc4oc32)cc1
Brc1ccc(Br)c(Nc2ncnc3ccccc23)c1
Brc1ccc(Nc2ccncc2Br)nc1

  9 Training on 5473 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 23.064486
Reward: 4.351312
Trajectories with max counts:
531	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.260333
Proportion of valid SMILES: 0.3190625
Sample trajectories:
Brc1ccc(NNc2ncnc3ccccc23)cc1
Brc1ccc(Nc2cncc3ccccc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.27030787
Proportion of valid SMILES: 0.31478587058455765
Sample trajectories:
B[PH](=O)(CNc1ccc(F)cc1)=Nc1ccc(Nc2ncnc3ccccc23)nc1
Brc1ccc(-c2nc3ccccc3[nH]2)cc1
Brc1ccc(Nc2ccc(Br)c3[nH]cnc23)[nH]1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.27701613
Proportion of valid SMILES: 0.4652908067542214
Sample trajectories:
BP(=O)(NP(=O)(OCCCF)OP(=O)(O)OP(O)(F)(F)F)OC(C)CCCl
Brc1cc(Br)c2c(sc3ccccc32)c1Br
Brc1cc(Br)cc(-c2ccc(Nc3ncnc4cc(Br)ccc34)cc2Br)c1
Brc1ccc(Br)c(Nc2ccc(Nc3ncnc4ccc(Br)cc34)cc2)c1
Brc1ccc(C2Nc3ccccc3S2)o1

 10 Training on 6478 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 23.650021
Reward: 5.026856
Trajectories with max counts:
418	Fc1ccc(Nc2ncnc3cc(F)ccc23)cc1
Mean value of predictions: 0.36621007
Proportion of valid SMILES: 0.27375
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1ccc(Nc2ccc3ncnc(Nc4cccc(Br)c4)c3c2)cc1
Brc1ccc(Nc2ccncc2)cc1Br
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.3673516
Proportion of valid SMILES: 0.27383557361675526
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc(CCCc4ccc[nH]4)sc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.34216264
Proportion of valid SMILES: 0.3729290403251016
Sample trajectories:
Brc1ccc(Nc2ccnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ncccc4-c4ccc5ccccc5n4)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1

 11 Training on 7572 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 24.169546
Reward: 5.747348
Trajectories with max counts:
286	Fc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.41519275
Proportion of valid SMILES: 0.27588364091335627
Sample trajectories:
Brc1ccc(-c2csc(Nc3ncnc4ccccc34)c2)cc1
Brc1ccc(NN(c2ccccc2)c2ccc(Br)cc2)cc1
Brc1ccc(Nc2cncnc2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc(-c4cccs4)nc23)cc1
Policy gradient replay...
Mean value of predictions: 0.41123596
Proportion of valid SMILES: 0.2783859868626838
Sample trajectories:
Brc1cc2ncnc(Nc3cc(Br)c(Br)c(Br)n3)c2cc1Br
Brc1ccc(-c2ncnc3ccc(Br)cc23)cc1
Brc1ccc(-c2ncnc3ccccc23)c(Br)c1
Brc1ccc(Nc2ncnc(Nc3ccc(Br)cc3)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.31423423
Proportion of valid SMILES: 0.346875
Sample trajectories:
BP(=O)(OC(C)C(F)(F)F)C(F)(F)F
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2cc(Br)ccn2)nc1
Brc1ccc(Nc2ccc(-c3ncnc4ccc(Nc5cccc(Br)c5)cc34)cc2-c2ccnc3ccsc23)cc1
Brc1ccc(Nc2ccc(Br)cn2)cc1

 12 Training on 8287 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 20.399145
Reward: 4.183808
Trajectories with max counts:
126	Fc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.35932976
Proportion of valid SMILES: 0.4103125
Sample trajectories:
BP(=O)(OCC)OC(=O)C(Br)Br
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)OP(=O)(O)O
Brc1cc(-c2ncnc3ccc(Nc4ncccc4-c4ccc(Br)o4)cc23)ccn1
Brc1cc(Br)c(-c2ccc(Nc3ncnc4ccc(Br)cc34)cc2)c(Br)c1
Brc1cc(Br)c(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.34414688
Proportion of valid SMILES: 0.408692933083177
Sample trajectories:
BP(=O)(OCC(=O)Nc1cc(Br)cc(Br)c1)P(=O)(OC(C)Cl)C(=O)O
BrC(=NNc1cccc(Br)n1)Nc1ccc(Br)cc1
BrCCCSSc1nc2cc(Br)ccc2ncn1-c1ccccc1
BrCCSc1ncnc2ncnc(Nc3ccc(Br)cc3)c12
BrCc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.33956733
Proportion of valid SMILES: 0.4478125
Sample trajectories:
BrBr
BrCc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BrSc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1cc(Br)c2c(Nc3cncc(Br)n3)ncnc2c1
Brc1cc2c(Nc3ccncc3)ncnc2cn1

 13 Training on 9132 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.469015
Reward: 4.564465
Trajectories with max counts:
382	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.34391534
Proportion of valid SMILES: 0.2953125
Sample trajectories:
BP(=O)(C(=O)OCCl)N(Cc1ccccc1)c1cccc(Nc2ccccc2)c1
BP(=O)(NO)c1ccc(Nc2nccc(Br)n2)cc1
BP(=O)(Nc1ccc(Br)cc1)OCC(=O)Nc1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cc1F)OCC=C
BP(=O)(Nc1ccc(Nc2nccc(Br)n2)cc1)P(=O)(O)O
Policy gradient replay...
Mean value of predictions: 0.31360695
Proportion of valid SMILES: 0.289375
Sample trajectories:
BP(=O)(C#N)CCCCC
BP(=O)(COc1ccc(Nc2ncnc3cc(Br)ccc23)cc1)P1(=O)CCCCN(c2ccc(Br)cc2)C(=O)O1
BP(=O)(Cc1cccc(F)c1)Nc1ccc(Nc2ncnc3scc(-c4cccc(F)c4)c23)cc1
BP(=O)(N(Cc1ccccc1)Cc1ccc(Br)cc1)P(=O)(O)O
BP(=O)(N(O)Cc1cccc(Br)c1)P(=O)(Oc1ccccc1)Oc1cccc(Br)c1
Fine tuning...
Mean value of predictions: 0.39908257
Proportion of valid SMILES: 0.40887777430447014
Sample trajectories:
BP(=O)(N(CC#C)CC#N)N(CC(=O)Nc1cccc(Br)c1)P(=O)(O)O
BP(=O)(OCC=C)Oc1ccc(Br)cc1Br
BP1(=O)OCC2OC(C(O)C2O)N(C=CC(=O)Nc2ccc(Br)cc2)C(N)=C(c2cccc(Br)c2)C1=O
Bc1ccc(NS(=O)(=O)c2ccc(Br)cc2Br)c(Br)c1
Br

 14 Training on 9938 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.319571
Reward: 5.251864
Trajectories with max counts:
605	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.43645975
Proportion of valid SMILES: 0.2365625
Sample trajectories:
BP(=O)(Br)Oc1ccc(Nc2ncnc3c(Br)cc(Br)cc23)cc1
BP(=O)(C=[PH](=S)(Oc1ccc(F)cc1)c1ccccc1)OCC
BP(=O)(NP(=S)(C(=O)O)S(=O)(=O)Nc1cc(Br)cc(Br)c1)c1ccc(F)cc1F
BP(=O)(Nc1cc(Br)c(Br)cc1F)[PH](F)(F)P(=O)(O)O
BP(=O)(Nc1ccc(N)cc1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.41149732
Proportion of valid SMILES: 0.23375
Sample trajectories:
BP(=O)(NC(=O)Oc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(NC(CC(=O)OCc1ccccc1)C(=O)O)Oc1cc(Br)c(O)c(Br)c1
BP(=O)(Nc1cc(Br)c(Br)c(Br)c1)c1cc(Br)cc(Br)c1Br
BP(=O)(Nc1cccc(Br)c1)P(=O)(Oc1ccccc1)Oc1cccc(Br)c1
BP(=O)(OCC=C)OCC1OC(=O)C(C)(C)C(O)C(O)C1O
Fine tuning...
Mean value of predictions: 0.42062026
Proportion of valid SMILES: 0.3728125
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Br)cc1Br
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1OP(=O)(O)O)OS(=O)(=O)c1ccc(Cl)cc1
BP(=O)(c1ccc(Nc2nc(Br)cnc2Cl)cc1)N(O)C(F)(F)F
B[PH](=O)(NCCCCl)(c1cccc2ccccc12)N1CCCCC1
B[PH](=O)(Nc1ccc(Br)c(Br)c1)(P(=O)(O)O)P(=O)(O)O

 15 Training on 10801 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.170664
Reward: 6.052777
Trajectories with max counts:
653	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.46474162
Proportion of valid SMILES: 0.20575359599749843
Sample trajectories:
BP(=O)(N(O)Cc1ccc(Br)cc1)P(=O)(O)Oc1ccc(Br)cc1
BP(=O)(Nc1ccc(Nc2ncnn2-c2ccccc2F)cc1)c1ccc(F)cc1
BP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC)OC(=O)CCC(=O)Nc1ccc(F)cc1F
BP(=O)(OCC1OC(=O)OC1C(=O)OCC[P+](C)(C)O)c1ccc(Nc2ncnc3cc(F)cc(F)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.46860644
Proportion of valid SMILES: 0.2040625
Sample trajectories:
BP(=O)(N(O)CCNc1cc(Br)cc(Br)c1)P(=O)(O)O
BP(=O)(N1CCOCC1)P(=O)(O)O
BP(=O)(NC(Cc1cccc(F)c1)NP(=O)(O)O)P(=O)(O)O
BP(=O)(Nc1ccc(Br)cc1Br)OCCCCN1CCCC1
BP(=O)(OCC1OC(C(O)OC(=O)C(C)(C)C)C1O)OP(=O)(O)OCC1OC(C(=O)O)C(N)C(O)C1O
Fine tuning...
Mean value of predictions: 0.4642857
Proportion of valid SMILES: 0.37625
Sample trajectories:
BP(=O)(Nc1cccc(Nc2cccc(Br)c2)n1)N(=O)=O
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)NCN1CCCCC1
B[PH](=O)(Nc1cc(Br)c(Br)c(Br)c1)(P(=O)(O)O)P(=O)(O)O
Bc1cc(Br)c(Nc2ncnc3cc(Br)ccc23)cc1Br
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1Br

 16 Training on 11703 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.781403
Reward: 6.211286
Trajectories with max counts:
382	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.5364103
Proportion of valid SMILES: 0.3048780487804878
Sample trajectories:
BP(=O)(N(CCN)CCCl)P(=O)(Oc1cc(F)c(F)c(F)c1F)c1cc(F)c(F)c(F)c1F
BP(=O)(Nc1ccc(Br)c(Br)c1)[PH](P(=O)(O)O)=[PH](=O)(O)O
BP(=O)(OCC)C(=O)Nc1ccc(Nc2ncnc3c(Br)cc(Br)cc23)cc1
BP(=O)(OCC)OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O
BP(=O)(OCC1(Nc2ccc(Nc3cc(N)ncn3)cn2)CCCC1)c1ccc(F)c(F)c1
Policy gradient replay...
Mean value of predictions: 0.56572014
Proportion of valid SMILES: 0.3082213191622382
Sample trajectories:
BP(=O)(N=C(Br)c1cc(Br)c(N)c(Br)c1)OCC
BP(=O)(NCCCN)n1cnc2c(Nc3ccc(Br)c(Br)c3)ncnc21
BP(=O)(OC(Cl)P(=O)(O)OP(=O)(O)O)P(=O)(O)O
BP(=O)(OCC1OC(N(O)C(=O)c2c(F)cc(F)c(F)c2F)C(N)C(O)C1O)Oc1ccc(F)c(F)c1
BP(=O)(OCC1OC(Oc2cc(F)c(F)c(F)c2)C(C=Cn2cncn2)C1O)Oc1cc(F)c(F)c(F)c1
Fine tuning...
Mean value of predictions: 0.47959185
Proportion of valid SMILES: 0.336875
Sample trajectories:
BP(=O)(N(O)COc1ccc(Br)cc1)P(=O)(Oc1c(O)cc(Br)cc1F)OC(C)C
BP(=O)(OCC(=O)Oc1ccc(Br)cc1)P(=O)(Oc1ccc(F)cc1)c1ccc(F)cc1
BP(=O)(OCC1OC(O)C(O)C1O)Oc1cc(F)c(Br)c(Br)c1
BP(=O)(Oc1ccc(Br)cc1)OP(=O)(O)O
BP(=O)(SCn1ccc(Nc2ccc(F)cc2)n1)S(=O)Cc1cccc(F)c1

 17 Training on 12969 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.408801
Reward: 7.130339
Trajectories with max counts:
546	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.51952463
Proportion of valid SMILES: 0.1840625
Sample trajectories:
BP(=O)(NO)c1ccc(Br)cc1Br
BP(=O)(OC(C)=O)c1ccc(F)c(Nc2ncnc3c(F)cccc23)c1
BP(=O)(OCCCC)ON(O)c1ccc(Nc2ccc(Nc3ccc(I)cc3)cc2)cc1
BP(=O)(c1cc(F)cc(F)c1)N(C)CNc1ncncn1
Bc1cc(Br)cc(Nc2ncnc3c(Br)cc(Br)c(Br)c23)c1
Policy gradient replay...
Mean value of predictions: 0.47536233
Proportion of valid SMILES: 0.1940625
Sample trajectories:
BP(=O)(Br)OCC
BP(=O)(C=C(Br)Br)OCC
BP(=O)(CCl)Nc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BP(=O)(NO)c1ccc(F)cc1
BP(=O)(Nc1cc(Br)c(Br)cc1Br)c1cc(Br)cc(Br)c1
Fine tuning...
Mean value of predictions: 0.48900205
Proportion of valid SMILES: 0.306875
Sample trajectories:
BP(=O)(C(=O)Nc1ccc(Br)cn1)N(O)c1ccc(Br)cc1
BP(=O)(OCC1OC(n2cnc(Br)c2)C(O)C1O)c1csc(N)n1
Bc1cc(Br)cc(Br)c1Nc1ncnc2cc(Br)ccc12
Bc1ccc(Nc2ncnc3ccsc23)cc1Br
Bc1ccc(Nc2ncnc3scnc23)cc1

 18 Training on 13829 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.773336
Reward: 7.290319
Trajectories with max counts:
1445	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.3190965
Proportion of valid SMILES: 0.1521875
Sample trajectories:
BP(=O)(NC(=O)OCc1ccccc1)OCCOCCO
BP(=O)(OCC1OC([N-][N+]#N)C(O)C1O)OS(=O)(=O)c1ccccc1
BP(=O)(OCCc1ccc(Nc2cccc(Br)c2)cc1Br)C1=CCCCC1
BP(=O)(c1ccc(F)cc1)N1CCCN(C(=O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O)OP(=O)(O)OCC1Br
Bc1cc(Br)cc(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.33929312
Proportion of valid SMILES: 0.1503125
Sample trajectories:
BP(=O)(Cc1ccccc1)P(=O)(OCC=C)Oc1ccc(F)cc1
BP(=O)(Nc1ccc(Br)cn1)c1ccc(Nc2ncnc3ccccc23)cc1
B[PH](=O)(=Nc1cccc(NC(=O)c2cccnc2)c1)Nc1cccc(F)c1
Bc1ccc(Nc2ncnc3ccccc23)cc1
Bc1cccc(Nc2ncnc3ccccc23)c1
Fine tuning...
Mean value of predictions: 0.4395897
Proportion of valid SMILES: 0.3048780487804878
Sample trajectories:
BP(=O)(OCC1OC(OP(=O)(O)O)C(O)C1F)OP(=O)(O)O
BP1(=O)OCCC(CCBr)(OOP(=O)(O)O)O1
Bc1cc(Br)c2ncnc(Nc3cccnc3)c2c1
BrC1=NN2C(=NC(SCc3ccc(Br)cc3)=Nc3ccc(Br)cc32)S1
Brc1c(Nc2ncnc3sccc23)ccc2cncncn12

 19 Training on 14381 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.546980
Reward: 7.271416
Trajectories with max counts:
498	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.6148331
Proportion of valid SMILES: 0.2528125
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)O
Bc1cc(Nc2ncnc3ccc(Br)cc23)cc(Br)c1O
BrC(Br)=NNc1ccc(Br)cc1Nc1nccnc1Br
BrC1=Nc2ccc(Br)ncnc2O1
BrCCN(Br)C(CBr)c1sccc1Br
Policy gradient replay...
Mean value of predictions: 0.58385545
Proportion of valid SMILES: 0.259375
Sample trajectories:
B[PH](=O)(O)(Nc1ccc(Br)cc1)NS(=O)(=O)Oc1cc(Br)cc(Br)c1
Bc1cc(Nc2ccc(Br)c(Br)c2)nc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Bc1cc(Nc2ncnc3ccc(Br)cc23)cc(Br)c1Br
Bc1ccc(Nc2cc(Br)c(Br)nc2Br)cn1
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.5022654
Proportion of valid SMILES: 0.2896875
Sample trajectories:
BP(=O)(NCC(=O)O)OCCS
BP(=O)(Nc1ccc(F)c(F)c1)N1CCN(c2ccc(F)c(F)c2)CC1
BP(=O)(OCCC)OC(=O)COc1ccc(Nc2ncnc3scc(-c4ccc(Cl)cc4)c23)cc1
BP(=O)(c1ccc(Nc2ncnc3sc4cc(F)ccc4c23)nc1)C(F)(F)F
Bc1ccc(Nc2ncnc3ccc(Br)c(Br)c23)cc1F

 20 Training on 15561 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.386853
Reward: 7.499387
Trajectories with max counts:
731	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.5374532
Proportion of valid SMILES: 0.166875
Sample trajectories:
BP(=O)(C=C(O)Nc1ccc(Br)cc1Br)NO
BP(=O)(OC(C)O)c1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CCCCCCN1C=CC(=O)N(C)S1
BP(=O)(OCC)OCCCl
BP(=O)(OCC)Oc1ccc(Nc2ncnc3c(Br)ccc(Br)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.54758364
Proportion of valid SMILES: 0.168125
Sample trajectories:
BP(=O)(OCC)OC(=O)c1ccc(Br)c(Br)c1
BP(=O)(OCC)Oc1ccc(Br)cc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)OCP(=O)(O)Oc1ccncn1
BP(=O)(Oc1cccc(Nc2ncnc3ccc(F)cc23)c1)c1cccc(F)c1
B[PH](=O)(CCN1CCOCC1)(c1ccccc1)c1cccc(Nc2ncnc3c(Br)cccc23)c1
Fine tuning...
Mean value of predictions: 0.5140251
Proportion of valid SMILES: 0.2740625
Sample trajectories:
BP(=O)(OC(C)C)N(C(=O)OCc1ccc(Nc2cc(Br)ccc2F)cc1)N(=O)=O
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C(O)C1O)OP(=O)(O)O
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)OP(=O)(O)O
BP(=O)(Oc1ccc(Br)cc1)Oc1ccc(Br)cn1
BP(=O)(c1ccc(Nc2ncnc3c(F)ccc(Cl)c23)cc1)c1ccc(F)cc1F

Trajectories with max counts:
1736	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.45772332
Proportion of valid SMILES: 0.2092136517064633
Mean Internal Similarity: 0.5160592939768356
Std Internal Similarity: 0.11636622875297245
Mean External Similarity: 0.4018265483022468
Std External Similarity: 0.07813226293124986
Mean MolWt: 391.8008954334879
Std MolWt: 88.37549246530823
Effect MolWt: -1.0712293800071526
Mean MolLogP: 5.128575466578427
Std MolLogP: 1.258964983197145
Effect MolLogP: 0.3112489062121073
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 94.895592% (409 / 431)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 20, 'seed': 3, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5579.844041824341, 'valid_fraction': 0.2092136517064633, 'active_fraction': 0.4514490588586794, 'max_counts': 1736, 'mean_internal_similarity': 0.5160592939768356, 'std_internal_similarity': 0.11636622875297245, 'mean_external_similarity': 0.4018265483022468, 'std_external_similarity': 0.07813226293124986, 'mean_MolWt': 391.8008954334879, 'std_MolWt': 88.37549246530823, 'effect_MolWt': -1.0712293800071526, 'mean_MolLogP': 5.128575466578427, 'std_MolLogP': 1.258964983197145, 'effect_MolLogP': 0.3112489062121073, 'generated_scaffolds': 431, 'novel_scaffolds': 409, 'novel_fraction': 0.9489559164733179, 'save_path': '../logs/replay_combo_s3-8.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.064314
Reward: 1.000000
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(COP(=O)(O)O)C(O)C1O
Mean value of predictions: 0.00080482895
Proportion of valid SMILES: 0.777291210509853
Sample trajectories:
Brc1ccc(C2=CC3=C(NCCC3)c3ccccc3O2)cc1
Brc1ccc(OCCOCCCCN2CCCC2)cc1
C#CC#CCNC(=O)C(=O)N(CCCN(C(=O)C(CC(C)C)N(C(C)=O)C(C)C)C(C)C(=O)OC)C(C)C
C#CC1(CO)C(=O)c2cc(OC)ccc21
C#CC1=CC(=O)OC1C(=O)O
Policy gradient replay...
Mean value of predictions: 0.001361634
Proportion of valid SMILES: 0.7810447294338442
Sample trajectories:
Brc1ccc(-c2nc(-c3ccnc4[nH]ccc34)no2)cc1
Brc1ccc(OCCN2CCCCCC2)cc1
Brc1ccccc1
C#CC=C(Cl)C(Oc1cc2ccccc2c2ccccc12)P(=O)(OC)OC
C#CCCOC(=O)c1c(OCCc2ccccc2)nc2c(c1O)C=CN2CCc1cccc(Cl)c1
Fine tuning...
Mean value of predictions: 0.027811246
Proportion of valid SMILES: 0.6236693800876644
Sample trajectories:
Bc1cc(Cl)ccc1Nc1nc(Nc2ccc(F)cc2F)nc2ncnn12
BrCc1ccc2nc(Nc3cc4ccc3C4CCCN3CCOCC3)cnc2c1
Brc1ccc(Nc2nc(-c3ccccc3)nc(-c3ccsc3)n2)cc1
Brc1ccc(Nc2nc(Nc3ccc(Br)cc3)c3ccccc3n2)cc1
Brc1ccc(Nc2ncnc3ccc(Nc4cccc(Br)n4)cc23)cc1

  2 Training on 308 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.827587
Reward: 1.011378
Trajectories with max counts:
8	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.027649326
Proportion of valid SMILES: 0.6495619524405507
Sample trajectories:
Brc1ccc(-c2cccc(COc3ccccc3)c2)cc1
Brc1ccc(-c2ccccc2Sc2ccncn2)c(Br)c1
Brc1ccc(-c2nccs2)c(-c2cccs2)n1
Brc1ccc(-c2ncsc2CCCCCCCCn2ccnc2)cc1-c1nnc(-c2nnc3ccccc3n2)o1
Brc1ccc(Nc2ncnc3c(N=Nc4ccccc4)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.01972522
Proportion of valid SMILES: 0.6382712182900094
Sample trajectories:
Brc1ccc(-c2ccsc2)c(-c2ccccc2)c1
Brc1ccc(CN2CCc3noc(n3)CN(N3CCCCC3)CC2)cc1
Brc1ccc(N(CCSc2ncccn2)CCc2cccc(Br)c2)cc1
Brc1ccc(Nc2ccnc3c2NCCN3)cc1
Brc1ccc(Nc2ncc(Nc3cccc(Br)c3)o2)cc1
Fine tuning...
Mean value of predictions: 0.06461694
Proportion of valid SMILES: 0.6209702660406886
Sample trajectories:
Brc1ccc(-c2ccccc2)c2ccccc12
Brc1ccc(-c2nc(-c3nc4ccccc4n3Cc3ccccc3)nc3ccccc23)s1
Brc1ccc(CN(Cc2ccccc2)C2CCN(Cc3ccncc3)CC2)cc1
Brc1ccc(Cn2ccc3cnccc32)cc1
Brc1ccc(Nc2cccc(Nc3ncnc4ncsc34)c2)cc1

  3 Training on 636 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 20.893648
Reward: 1.312414
Trajectories with max counts:
12	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.07462165
Proportion of valid SMILES: 0.5387268736280966
Sample trajectories:
BP(=O)(OC)C(C=CBr)CCC
Brc1cc(Nc2cc3cc(Nc4ccccc4)nncc3ncn2)ncn1
Brc1cc2cncc(Br)c2s1
Brc1ccc(-c2cnc3c(NCc4cncnc4)ncnc3n2)cc1
Brc1ccc(-n2cc3ncnc(Nc4ccc5ccccc5n4)c3c2)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.07519334
Proportion of valid SMILES: 0.5274552871038595
Sample trajectories:
Brc1cc(Br)c2ncnc(Nc3cc(Nc4ncnc(Nc5ccc(N6CCNCC6)cc5)n4)c(Br)cn3)c2c1
Brc1ccc(-c2nccs2)c2cccn12
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2nnnc3cc(CNc4ccccn4)nc(-c4ccccc4)c23)c1
Brc1ccc(CSc2ncnc3scnc23)cc1
Fine tuning...
Mean value of predictions: 0.12091212
Proportion of valid SMILES: 0.5632832080200502
Sample trajectories:
BrC(=Cc1cccc(Br)c1)Cc1ccnc2ccccc12
Brc1cc2ncnc(Nc3ccccc3)c2cc1OCc1ccccc1
Brc1ccc(-n2c(Nc3ncnc(CN4CCOCC4)n3)nc3ccccc32)cc1
Brc1ccc(Br)c(Nc2ncsn2)c1
Brc1ccc(N2CCN(Cc3ccc(-c4ccccc4)c(Br)n3)CC2)cc1

  4 Training on 1316 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 24.164751
Reward: 2.055447
Trajectories with max counts:
87	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.192398
Proportion of valid SMILES: 0.5590625
Sample trajectories:
BrC=C(I)I
Brc1ccc(-c2cc(CNc3ncnc4sccc34)cc(Nc3ncnc4scnc34)c2)o1
Brc1ccc(Nc2cc3ccccc3c(Nc3ccc(Br)cc3)ncn2)cc1
Brc1ccc(Nc2cncnc2)cc1
Brc1ccc(Nc2nc(Br)nc3ccccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.18645775
Proportion of valid SMILES: 0.5584375
Sample trajectories:
B[PH](=O)(Nc1cccc(Br)c1)(P(=O)(O)O)P(=O)(O)O
Brc1cc2ncnc(Nc3ccc4ccccc4c3)c2cc1Nc1ccccc1
Brc1ccc(-c2ncccn2)c(Nc2ncnc3ccccc23)c1
Brc1ccc(Nc2cccc(Nc3ncnc4ccccc34)c2)cc1
Brc1ccc(Nc2ccccc2I)cc1I
Fine tuning...
Mean value of predictions: 0.17986436
Proportion of valid SMILES: 0.5990625
Sample trajectories:
BP(=O)(OCC)OCC1OC(COP(=O)(O)O)C(O)C(OP(=O)(O)O)OC(P(=O)(O)O)C(O)C1O
BP(=O)(OCC1OC(n2cnc3c2NC=NC3=O)C(O)C1O)C(=O)OCC[PH](=O)(=O)O
BP(=O)(c1ccc2c(Nc3ccccc3)ncnc2c1)[PH](F)(F)F
BrCCn1cc(Nc2ncnc3ccccc23)cc1Nc1ncnc2ccccc12
Brc1ccc(-c2ncccc2Nc2ncnc3ncnc(Nc4ccccc4)c23)s1

  5 Training on 2700 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 24.056951
Reward: 3.362487
Trajectories with max counts:
623	Fc1ccc(Nc2ncnc3ccccc23)cc1F
Mean value of predictions: 0.23136863
Proportion of valid SMILES: 0.3128125
Sample trajectories:
BP(=O)(OCCCC)N(O)C(=O)OC(C)(C)C
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Brc1cc(I)cc(Nc2ncnc3ccccc23)c1
Brc1cc(NN=Cc2ccccc2)nc2ccccc12
Brc1cc(Nc2ncnc3ccccc23)ccc1Nc1ncnc2ncncc12
Policy gradient replay...
Mean value of predictions: 0.24303535
Proportion of valid SMILES: 0.30071897467958736
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1cccs1
Brc1cc(Nc2ncnc3ccccc23)ccc1Nc1ncnc2ncnc(Nc3cccc4ccccc34)c12
Brc1cc2ncnc(Nc3cccc4ccccc34)c2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.2600229
Proportion of valid SMILES: 0.5468211713122455
Sample trajectories:
BP(=O)(OCC)OC(=O)C(Cc1ccc(Br)cc1)P(=O)(Oc1ccc(Br)cc1)Oc1ccc(Br)cc1
BP(=O)(OCCS)C(F)(F)F
BP1(=O)CCN1CCN(C(=O)Oc1ccc(F)c(F)c1F)c1ccc(Nc2cc(Nc3ccc(F)c(F)c3F)nc(Cl)n2)cc1F
Brc1cc(-c2ccc(Nc3ccc(C4CCCN4)cc3)cc2)ncn1
Brc1cc(Br)nc(-c2cccc(Nc3ncnc4ccccc34)n2)c1

  6 Training on 3753 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 27.244183
Reward: 3.354110
Trajectories with max counts:
89	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.39148074
Proportion of valid SMILES: 0.4623319787433573
Sample trajectories:
Bc1cc(Br)cc(Nc2ncnc3c(Nc4cccc(Br)c4)c(Br)c23)c1
Brc1cc(-c2ccncc2)c2scnc2n1
Brc1cc(-c2csc(Nc3ncnc4scnc34)c2)sc1Nc1ccncc1
Brc1cc(Nc2ccncc2)ccn1
Brc1cc(Nc2ncnc3ccccc23)ccc1N1CCN(CC2CCOC2)CC1
Policy gradient replay...
Mean value of predictions: 0.39253926
Proportion of valid SMILES: 0.47764926539543606
Sample trajectories:
B[PH](=O)(=O)CCS(=O)(=O)N1CCN(c2ncn3c(Cl)cc(Br)cc23)C(=O)C1
Brc1cc(Br)cc(Nc2ncnc3nc(-c4ccsc4)oc23)c1
Brc1cc(Nc2ncnc3ccccc23)cc(Br)c1Br
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Brc1ccc(NN(Cc2cccs2)c2cncc(Br)c2)cc1
Fine tuning...
Mean value of predictions: 0.2570643
Proportion of valid SMILES: 0.5687929956222639
Sample trajectories:
BrC1=Nc2ccc(Br)cc2Nc2ccc(Br)cc21
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Brc1cc(Nc2cccc(Nc3ncnc4ccccc34)c2)ncc1I
Brc1cc(Nc2ncnc3ccc(Br)cc23)cc(C2CC2)c1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1

  7 Training on 5501 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 27.009760
Reward: 4.905761
Trajectories with max counts:
446	Fc1ccc(Nc2ncnc3ccccc23)cc1F
Mean value of predictions: 0.40091157
Proportion of valid SMILES: 0.3428125
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Brc1cc(I)cc(Nc2ncnc3ccccc23)c1
Brc1cc(Nc2nncn2Cc2ccncc2)cc(Br)c1Br
Brc1ccc(Nc2nc(Nc3ccncc3)nc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc(Nc3cccc(Br)c3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.42240587
Proportion of valid SMILES: 0.34041888090028133
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Br)ccc23)c(I)c1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)nc1
Fine tuning...
Mean value of predictions: 0.31997705
Proportion of valid SMILES: 0.5445451703657392
Sample trajectories:
BrBr
BrCCOc1ccc(Nc2ncnc3sccc23)cc1
BrCN1CCc2ccc(Nc3ccncc3Br)cc2C1
BrCc1nc2cccnc2nc1N1CCCN(c2nc3ccccc3s2)CC1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1

  8 Training on 7018 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 29.011644
Reward: 5.096074
Trajectories with max counts:
209	Fc1ccc2ncnc(Nc3ccc(F)c(F)c3)c2c1
Mean value of predictions: 0.46402642
Proportion of valid SMILES: 0.3791054113231154
Sample trajectories:
Brc1c(Nc2ncnc3ccccc23)sc2ccsc12
Brc1cc(Br)cc(Nc2cc(Br)c3ccccc3c2)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Nc2ncnc3ccsc23)cs1
Brc1cc(Nc2ncnc3sccc23)cc(Br)c1Nc1ncnc2ccccc12
Policy gradient replay...
Mean value of predictions: 0.4645538
Proportion of valid SMILES: 0.3748046264457643
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ncsc23)c1
Brc1cc(Nc2ncnc3ccccc23)ccc1Nc1ncnc2ccccc12
Brc1cc(Nc2ncnc3scnc23)cc(Nc2ncnc3scc(-c4ccccc4)c23)c1
Brc1cc2c(Nc3cc(Br)c(Br)cn3)ncnc2s1
Brc1ccc(Nc2cc3sc(Nc4ccc(Br)cc4Br)ccc3ncn2)cc1
Fine tuning...
Mean value of predictions: 0.36769837
Proportion of valid SMILES: 0.5561815336463224
Sample trajectories:
BP(=O)(OCC)N1C=NN(c2ccc(F)cc2F)C(=O)C(F)=C1
BrC1=CN(Cc2nc3ncnc(Nc4ccc(Br)cc4)c3s2)C1
BrC1=CN=C(N2c3ncnc(Nc4ccc(Br)cc4)c32)Nc2ccc(Br)cc2Nc2c(Br)ccc(Br)c21
Brc1c(Br)c(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(-c2nc3cc(Br)ccc3[nH]2)c(-c2nc3ccccc3[nH]2)c1

  9 Training on 8796 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 26.611739
Reward: 5.178964
Trajectories with max counts:
517	Brc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.5329646
Proportion of valid SMILES: 0.2825
Sample trajectories:
BP(=O)(Br)Oc1ccccc1Br
BP(=O)(NO)c1ccc(F)c(F)c1
BP(=O)(Nc1cc(F)cc(F)c1)c1cccc(Nc2ncncc2F)c1
BP(=O)(Nc1ccc(Br)c(F)c1)c1cc(Nc2ccc(F)cc2F)c2ncnn2c1
BP(=O)(Nc1ccc(Br)cc1)Oc1ccccc1
Policy gradient replay...
Mean value of predictions: 0.5253363
Proportion of valid SMILES: 0.27875
Sample trajectories:
BBr
BP(=O)(=NO)(NO)Nc1ccc(F)cc1F
BP(=O)(CCCC)OCC(=O)ON
BP(=O)(Cc1ccc(F)cc1)OCBr
BP(=O)(N(O)C(F)F)P(=O)(O)C(F)(F)F
Fine tuning...
Mean value of predictions: 0.39060327
Proportion of valid SMILES: 0.5390869293308318
Sample trajectories:
BP(=O)(C(=O)NS(=O)(=O)c1ccc2ncnc(N)c2c1)N(O)CC(N)=O
BP(=O)(Nc1ccc(Nc2nc(F)ccc2F)cc1)c1ccc(F)cc1F
BP(=O)(O)C(F)(F)S(C)(=O)=O
B[PH](=O)(NC(c1ccc(F)cc1)c1ccc(F)cc1)=P(Br)(Br)[PH](=O)(SO)(P(=O)(O)O)P(=O)(O)O
BrC1=Nc2cc(I)ccc2N=CN1

 10 Training on 10021 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.756723
Reward: 4.598203
Trajectories with max counts:
499	Brc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.46982026
Proportion of valid SMILES: 0.3303125
Sample trajectories:
BP(=O)(C(=O)c1cc(F)c(F)c(F)c1)N(Cc1cc(Br)cs1)c1ccc(F)c(F)c1
BP(=O)(OCC(=O)Nc1cc(Br)cc(Br)c1)P(=O)(O)Oc1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CBr
BP(=O)(OCC1CC(F)(F)C(F)CN1)Oc1ccc2c(F)cccc2c1
BP(=O)(OCOC(=O)C=Cc1ccc(Br)cc1)P(=O)(O)Oc1ccccc1F
Policy gradient replay...
Mean value of predictions: 0.44942752
Proportion of valid SMILES: 0.3275
Sample trajectories:
BP(=O)(N(c1ccccc1)c1ccccc1)P(=O)(O)OP(=O)(O)O
BP(=O)(NC(c1cc(Br)cc(Br)c1)(P(=O)(O)O)P(=O)(O)O)P(=O)(O)O
BP(=O)(Nc1cncc(Br)c1)c1ccc(Br)cc1
BP(=O)(O)C(F)(F)F
BP(=O)(O)Cc1ccccc1N(=O)=O
Fine tuning...
Mean value of predictions: 0.37685186
Proportion of valid SMILES: 0.54
Sample trajectories:
BP(=O)(COP(=O)(O)Oc1ccc(F)c(F)c1)OCCO
BP(=O)(NC(=O)c1ccc(Nc2ncnc3c(F)cccc23)cc1)C(F)(F)F
BrBr
BrCCBr
Brc1cc(Br)c(Nc2ncnc3ncnc(Nc4cccc(Br)c4Br)c23)c(Br)c1

 11 Training on 11167 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.742291
Reward: 4.484935
Trajectories with max counts:
705	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.44715282
Proportion of valid SMILES: 0.3128125
Sample trajectories:
BP(=O)(CCCCC(N)COP(=O)(O)OP(=O)(O)O)OCCS
BP(=O)(CCCl)NP(=O)(OC)OP(=O)(Br)O[PH](=O)(O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(N(O)Cc1ccc(Br)cc1)P(=O)(O)OP(=O)(O)O
BP(=O)(NO)c1cc(Br)c(Br)cc1Br
BP(=O)(O)c1ccc(CNc2ccc(F)cc2Br)cc1F
Policy gradient replay...
Mean value of predictions: 0.4576098
Proportion of valid SMILES: 0.3059375
Sample trajectories:
BBr
BP(=O)(Cc1ccc(Br)cc1)NO
BP(=O)(NC(c1ccccc1)c1ccc(F)cc1)c1cccc(Br)c1
BP(=O)(NO)c1ccc(Nc2cc(Cl)cc(Cl)n2)cc1
BP(=O)(Nc1cc(Br)cc(Br)c1Br)C(=O)NO
Fine tuning...
Mean value of predictions: 0.4175165
Proportion of valid SMILES: 0.5209375
Sample trajectories:
BP(=O)(Nc1cc(Br)c(Br)cc1F)C(=O)O
BP(=O)(Nc1ncc2ccccc2c1F)c1cccc(Nc2ncnc3cc(F)ccc23)c1F
BP(=O)(OCC)c1ccc2ncccc2c1
Bc1ccc(-c2ncnc(Nc3ccc(Br)cc3Br)n2)c(F)c1
BrBr

 12 Training on 12282 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.588506
Reward: 4.673044
Trajectories with max counts:
325	Brc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.50325286
Proportion of valid SMILES: 0.2979055954985933
Sample trajectories:
BP(=O)(CC(CF)P(O)(F)(F)F)NO
BP(=O)(NCCCCN)C(F)(F)F
BP(=O)(NC[PH]1(=O)([O-])OCCO1)N(O)Cc1c(F)c(F)c(F)c(F)c1F
BP(=O)(Nc1ccc(Br)cc1Br)Oc1ccc(Br)cc1
BP(=O)(O)Oc1ccc(F)cc1Nc1ccc2ncnc(Nc3ccc(Br)cc3F)c2c1N1CCN(C)CC1
Policy gradient replay...
Mean value of predictions: 0.48602578
Proportion of valid SMILES: 0.3154110659581119
Sample trajectories:
BP(=O)(N=O)OCCSSCCCOP(=O)(O)c1ccccc1
BP(=O)(O)c1c(Br)cccc1Br
BP(=O)(OCC(=O)NCCC(F)(F)Cl)c1ccc(Br)cc1
BP(=O)(OCC1OC(=O)Cc2ccc(Br)cc2O1)c1ccc2ncnc(Nc3ccccc3)c2c1
BP(=O)(OCCOCCOP(=O)(O)O)P(=O)(O)O
Fine tuning...
Mean value of predictions: 0.4589404
Proportion of valid SMILES: 0.5667813575226776
Sample trajectories:
BP(=O)(OC)OC(=O)CBr
B[PH](=O)(NC(Cc1ccc(Br)cc1)NCP(=O)(O)Cc1cccc(Br)c1)(P(=O)(O)O)P(=O)(O)O
BrCBr
BrCC(Br)N1c2ccccc2Nc2ccc(Br)cc21
BrCCNc1cc2n(c1)-c1ccccc1N2

 13 Training on 13614 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.710175
Reward: 4.777603
Trajectories with max counts:
299	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.5907815
Proportion of valid SMILES: 0.311875
Sample trajectories:
Brc1cc(-c2ncnc3sccc23)c2sccc2n1
Brc1ccc(Nc2cc(Br)cnc2n2cncn2)cc1
Brc1ccc(Nc2ccc(Br)c(-c3cccc(Br)c3)n2)cc1
Brc1ccc(Nc2ccc(Nc3ncnc4ccc(Br)cc34)cc2)cc1
Brc1ccc(Nc2cnc(Nc3ncnc4ccccc34)s2)cc1
Policy gradient replay...
Mean value of predictions: 0.59315205
Proportion of valid SMILES: 0.3103125
Sample trajectories:
BP(=O)(N(O)CSc1cccc(Nc2ccccc2)c1)N(=O)=O
BrCc1cccc(Nc2ncnc3c2c2ncnc(Nc4ccccc4Br)c2N3)c1
Brc1cc(Nc2ncnc3sccc23)cc(Br)n1
Brc1ccc(N2CCC3(C2)c2cncnc23)s1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.44130042
Proportion of valid SMILES: 0.5193871169480926
Sample trajectories:
BP(=O)(NO)S(=O)(=O)Cc1ccc(F)cc1
BP(=O)(O)C(=O)Nc1cccc(Br)c1Br
BP(=O)(OCC)OC(=O)CS(=O)(=O)c1cccc(Br)c1
BP(=O)(OCCc1ccccc1)S(=O)(=O)c1c(O)ccc2c1[nH]c1cc(Br)c(Br)c(Br)c12
B[PH](=O)(N=C(NO)C(F)(F)F)(OP(=O)(O)OP(=O)(O)O)C(F)(F)F

 14 Training on 14981 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.126780
Reward: 5.204226
Trajectories with max counts:
345	Brc1ccc(Nc2ncnc3sc4ccccc4c23)cc1
Mean value of predictions: 0.60994035
Proportion of valid SMILES: 0.314375
Sample trajectories:
BP(=O)(C(=O)O)C(Cc1ccc(Br)cc1F)NP(=O)(O)O
BP(=O)(CCN)C(=O)Nc1ccc(F)c(F)c1
BP(=O)(Cc1scc(C(=O)O)c1N)NCCCN
BP(=O)(N=[PH](=O)(O)N1CCCC1)N(=O)=O
BP(=O)(NC(=O)c1ccc(O)c(Br)c1)Oc1cc(Nc2ccc(Br)cc2)nc(Nc2ccc(F)c(F)c2F)n1
Policy gradient replay...
Mean value of predictions: 0.6224122
Proportion of valid SMILES: 0.3290625
Sample trajectories:
BP(=O)(NCCO)NC(=O)C(Br)Br
BP(=O)(NO)c1c(Br)cc(Nc2ncnc3sccc23)cc1Br
BP(=O)(OC)OCC
Bc1cc(NN=C(Br)Br)cc(Br)c1Nc1ccc2ccccc2c1Br
Bc1cc(Nc2ncnc3sccc23)ccc1Br
Fine tuning...
Mean value of predictions: 0.4869765
Proportion of valid SMILES: 0.5451986237097278
Sample trajectories:
BP(=O)(CP(=O)(O)OP(=O)(O)NO)C(Br)Br
BP(=O)(NP(=O)(OCC1OC(C(O)CO)C1O)OS(=O)(=O)c1ccc(Br)cc1Br)C(=O)C(F)(F)F
BP(=O)(Nc1cccc(F)c1)P(=O)(Oc1ccccc1)Oc1cccc(F)c1F
Bc1cc(Nc2ncnc3cc(Br)ccc23)cc(Br)c1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1

 15 Training on 16694 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.912924
Reward: 5.745717
Trajectories with max counts:
385	Brc1ccc(Nc2ncnc3sccc23)cc1Br
Mean value of predictions: 0.5345455
Proportion of valid SMILES: 0.2578125
Sample trajectories:
BP(=O)(NC(=O)OC)OC(=O)CBr
BP(=O)(Nc1ccc(Nc2ncnc3ccccc23)cc1)c1cccc(F)c1F
BP(=O)(OCC)C(O)(c1ccccc1)c1ccc(Br)cc1
BP(=O)(OCC)S(=O)(=O)c1cccc2ncnc(Nc3ccc(Br)cc3)c12
BP(=O)(OCC)c1c(F)cccc1F
Policy gradient replay...
Mean value of predictions: 0.50786513
Proportion of valid SMILES: 0.2503125
Sample trajectories:
BP(=O)(Nc1ccc(F)cc1F)C(=O)Nc1ccc(F)cc1F
BP(=O)(Nc1ccc(Nc2ncnc3ccccc23)cc1)c1cccc(F)c1F
BP(=O)(Nc1sc2ccc(F)cc2c1N(=O)=O)Oc1cc2c(c(N)n1)OC(=O)C2
BP(=O)(OCC)Oc1ccc(Nc2ncnc3ccccc23)cc1
BP(=O)(OCC)c1c(F)cc(Nc2ncnc3c(F)cccc23)cc1F
Fine tuning...
Mean value of predictions: 0.47683892
Proportion of valid SMILES: 0.5140625
Sample trajectories:
BP(=O)(O)c1cc(Br)c(Br)cc1Br
BP(=O)(O)c1ccc(Br)cc1Br
BP(=O)(Oc1ccccc1Br)c1ccc(Br)cc1
Bc1ccc(Nc2ccc(Nc3ncnc4ccccc34)cc2F)c(F)c1
BrI

 16 Training on 17884 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.560824
Reward: 5.756786
Trajectories with max counts:
134	Brc1ccc(Nc2ncnc3sc4ccccc4c23)cc1
Mean value of predictions: 0.663388
Proportion of valid SMILES: 0.3432322600812754
Sample trajectories:
BP(=O)(NC(C(=O)OC(C)(C)C)N(=O)=O)Oc1cc(Br)c(Nc2ccc(Br)cc2N(=O)=O)cc1Br
BP(=O)(NCCCCl)c1c(F)cccc1F
BP(=O)(NCCCl)c1ccc(Br)cc1
BP(=O)(NO)c1ccc(Nc2ncnc3[nH]cnc23)cc1F
BP(=O)(Nc1nc(F)c(F)c(F)c1F)P(=O)(c1cccc(F)c1F)C(F)(F)F
Policy gradient replay...
Mean value of predictions: 0.6831735
Proportion of valid SMILES: 0.3587738504848295
Sample trajectories:
BP(=O)(Nc1cc(F)c(F)c(F)c1F)Oc1ccc(F)c(F)c1F
BP(=O)(OC(=O)c1ccc(Nc2ncnc3sc4ccccc4c23)cc1Br)C(F)(F)F
BP(=O)(OCC)Oc1cc(Br)c(Nc2c(Br)cc(Br)cc2Br)cc1Br
BP(=O)(OCCC)Oc1cc(I)c(Br)c(Br)c1Br
BP(=O)(ONC(=O)c1cc(Nc2ncnc3scc(Cl)c23)cc2c(Br)cc(Br)cc12)c1ccc(F)c(F)c1Br
Fine tuning...
Mean value of predictions: 0.51221776
Proportion of valid SMILES: 0.5398562050640825
Sample trajectories:
BP(=O)(C(=O)Nc1ccc(F)cc1F)N(O)Cc1ccccc1Nc1c(F)cc(F)cc1F
BP(=O)(Cc1ccc(F)cc1F)N1C(=O)Sc2ccccc21
Bc1cc(Nc2ncnc3ccccc23)ccc1I
BrCCCc1cccc2ncnc(Nc3cccc(Br)c3)c12
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1

 17 Training on 19861 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.741238
Reward: 6.049004
Trajectories with max counts:
402	Brc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.6838627
Proportion of valid SMILES: 0.24601437949359176
Sample trajectories:
BP(=O)(NO)c1ccc(Br)cc1Br
BP(=O)(NO)c1cccc(Br)c1
BP(=O)(Nc1cccc(F)c1)P1(F)(F)OC(=O)N[PH]1(N=O)c1ccc(F)c(F)c1F
BP(=O)(Oc1ccccc1)Oc1ccc2c(c1)C(=O)N2c1ccc(F)cc1F
Bc1cc(Nc2ncccc2Br)ccc1Br
Policy gradient replay...
Mean value of predictions: 0.6710257
Proportion of valid SMILES: 0.24375
Sample trajectories:
BP(=O)(Nc1ccc(F)c(F)c1F)C(=O)N1CCCC1
BP(=O)(Nc1ccc(Nc2ncnc3scc(-c4ccc(F)cc4)c23)cc1)c1ccccc1
BP(=O)(O)c1ccc(Br)cc1F
BP(=O)(OCC)OC(=O)C=CCN=C(Br)CNC(=O)C(Br)Br
Bc1cc(Br)cc(Nc2ncnc3sccc23)c1Br
Fine tuning...
Mean value of predictions: 0.5145248
Proportion of valid SMILES: 0.5228125
Sample trajectories:
BP(=O)(NP(=O)(Oc1ccccc1Br)c1ccc(Br)cc1)Oc1ccccc1
Bc1cc(Br)cc(Br)c1Nc1ncnc2sccc12
Bc1ccc(Nc2ncnc3sc(Nc4ccc(Br)cc4)cc23)cc1Br
Bc1ccc(Nc2ncnc3sccc23)cc1Br
BrC=C(I)I

 18 Training on 21449 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.696549
Reward: 6.062579
Trajectories with max counts:
231	Brc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.6915058
Proportion of valid SMILES: 0.3238512035010941
Sample trajectories:
BP(=O)(OCC)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC)c1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1Br
BP(=O)(OCC1OC(=O)NC12CCCO2)Oc1cc(Br)cc(Br)c1Br
B[PH](=O)(=NO)Nc1cc(Br)c(Br)c(Br)c1
B[PH](=O)(F)(F)Nc1ccc(Br)c(Nc2ncnc3sc(Cl)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.6750943
Proportion of valid SMILES: 0.33125
Sample trajectories:
BP(=O)(CC(=O)Nc1cc(Br)c(Br)cc1Br)NO
BP(=O)(CC(=O)OP(=O)(O)Nc1ccc(Br)cc1)OCOc1cc(Br)c(Br)c(Br)c1Br
BP(=O)(Nc1ccc(Br)nc1)Nc1cccc(Br)c1Br
BP(=O)(Oc1cc(Br)cc(Nc2ncnc3c(Br)cccc23)c1)[SH](=O)(O)CBr
BP(=O)(Oc1ccc(Nc2ncnc3sc4ccccc4c23)cc1)c1cc(Br)cc2c(F)cc(Br)cc12
Fine tuning...
Mean value of predictions: 0.52657473
Proportion of valid SMILES: 0.5209375
Sample trajectories:
BP(=O)(NCCCCCCCCCCO)C(=O)Nc1cccc(Br)c1Nc1c(F)cc(F)cc1F
BP(=O)(NO)C(=O)C(F)(F)F
Bc1ccc(Nc2ncnc3c(Br)c(Br)c(Br)nc23)cc1Br
BrCCCBr
BrCCN1CCc2c(c3ccc(Br)cc23)C1

 19 Training on 23340 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.578813
Reward: 6.091800
Trajectories with max counts:
294	Brc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Mean value of predictions: 0.6445759
Proportion of valid SMILES: 0.316875
Sample trajectories:
BP(=O)(Br)OCC=C(Br)Br
BP(=O)(CP(=O)(O)O)OCC
BP(=O)(NC(=O)c1cc(Br)cc(Br)c1Br)N(=O)=O
BP(=O)(Nc1cc(Br)cc(Br)c1N(=O)=O)N(=O)=O
BP(=O)(OCCBr)OP(B)(F)(F)F
Policy gradient replay...
Mean value of predictions: 0.6546154
Proportion of valid SMILES: 0.32510159424820256
Sample trajectories:
BP(=O)(c1ccc(F)c(Nc2ncnc3scc(C(F)(F)F)c23)c1)N(O)C=O
Bc1cc(Br)cc(Nc2ncnc3sc(Br)c(Br)c23)c1
Bc1ccc(Nc2ncnc3scc(Br)c23)cc1
Bc1ccc(Nc2ncnc3scc(Br)c23)cc1Br
BrCCBr
Fine tuning...
Mean value of predictions: 0.5258631
Proportion of valid SMILES: 0.5159375
Sample trajectories:
BP(=O)(Nc1cccc(F)c1F)C(=O)OCc1ccc(F)cc1F
Bc1cc(Nc2ncnc3ccc(Br)cc23)cc(Br)c1I
BrC=CCNC1=NCCCCCCC2CCCCCC[N+]2(c2ccccc2Br)C1
BrCCCCCN=C1CCSc2ccccc21
BrCCNc1c(Br)cc(Br)cc1Nc1ncncc1Nc1ccc(Br)cc1

 20 Training on 25129 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.961107
Reward: 6.022303
Trajectories with max counts:
407	Brc1ccc(Nc2ncnc3sccc23)cc1Br
Mean value of predictions: 0.64319247
Proportion of valid SMILES: 0.26625
Sample trajectories:
BP(=O)(OCC)OC(=O)c1cc(I)cc(Nc2ncnc3ccccc23)c1
BP(=O)(OCc1ccc(Br)cc1)Oc1ccc(Br)cc1
B[PH](=O)(F)(F)C(F)(F)C(F)(F)F
Bc1cc(Nc2ncnc3scc(Br)c23)ccc1Br
Bc1ccc(Nc2ncnc3c(Br)cccc23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.6401392
Proportion of valid SMILES: 0.269375
Sample trajectories:
BP(=O)(NO)c1ccc(F)cc1F
BP(=O)(OCC(F)(F)F)c1cc(Nc2ncnc3sc(F)cc23)ccc1F
BP(=O)(OCC)Oc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br
BP(=O)(OCC)c1cc2c(Nc3ccc(I)c(Br)c3)c(Br)cnc2s1
BP(=O)(ON(c1ccccc1)c1ccc(Br)cc1)c1ccccc1
Fine tuning...
Mean value of predictions: 0.5445963
Proportion of valid SMILES: 0.503125
Sample trajectories:
BP(=O)(CC(=O)Oc1ccc(Cl)c(NNc2ncnc3ccccc23)c1)NO
BP(=O)(CCl)NO
BP(=O)(OCC)OC(=O)C(Br)Br
BP(=O)(Oc1ccc(Cl)cc1)c1ccccc1
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1

Trajectories with max counts:
332	Brc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.49929664
Proportion of valid SMILES: 0.40882665499781207
Mean Internal Similarity: 0.48255605415764785
Std Internal Similarity: 0.10645831952335517
Mean External Similarity: 0.4104020911901939
Std External Similarity: 0.06881696428013628
Mean MolWt: 389.01829357506364
Std MolWt: 88.54371860926955
Effect MolWt: -1.0636916347600647
Mean MolLogP: 4.87861001590331
Std MolLogP: 1.276239167302789
Effect MolLogP: 0.12286330210736404
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.486339% (892 / 915)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 100, 'seed': 3, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 6025.82629609108, 'valid_fraction': 0.40882665499781207, 'active_fraction': 0.48073394495412847, 'max_counts': 332, 'mean_internal_similarity': 0.48255605415764785, 'std_internal_similarity': 0.10645831952335517, 'mean_external_similarity': 0.4104020911901939, 'std_external_similarity': 0.06881696428013628, 'mean_MolWt': 389.01829357506364, 'std_MolWt': 88.54371860926955, 'effect_MolWt': -1.0636916347600647, 'mean_MolLogP': 4.87861001590331, 'std_MolLogP': 1.276239167302789, 'effect_MolLogP': 0.12286330210736404, 'generated_scaffolds': 915, 'novel_scaffolds': 892, 'novel_fraction': 0.9748633879781421, 'save_path': '../logs/replay_combo_s3-9.smi'}
