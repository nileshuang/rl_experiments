starting log


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.747517
Reward: 1.000000
Mean value of predictions: 0.0012588513
Proportion of valid SMILES: 0.7956181533646323
Sample trajectories:
BP(=O)(O)COP(=O)(O)O
Brc1ccc(-n2cccc2-c2ccccn2)cc1
Brc1cnc(NCC2CCN(Cc3ncccn3)CC2)s1
C#CC(=O)C(C)=NNC(=O)C1C=C(S(=O)(=O)c2ccc3ccccc3c2)CC=CC1
C#CC1C(OCC)OC(c2ccncc2)=CC1c1ccc(C(=O)c2cccc(Cl)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.020521894
Proportion of valid SMILES: 0.7751114158381899
Sample trajectories:
Brc1ccc(C2=Nc3ncnn3CCCc3ccccc32)o1
Brc1ccc(Nc2ncnc3ccncc23)cc1
Brc1ccc2[nH]c3c(c2c1)CCCC3
Brc1ccc2c(c1)NCc1cccnc1N2
Brc1ccc2nc(NN=Cc3ccccc3SCCCCCCCCCCCCCN3CCOCC3)sc2c1
Fine tuning...
Mean value of predictions: 0.020882087
Proportion of valid SMILES: 0.763573883161512
Sample trajectories:
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc2ncnc(Nc3ccnc(Br)c3)c2c1
Brc1ccc2oc(-c3ccccc3)c(-c3ccccc3)c2c1
C#CCCCCCCCCCCCCCNC(=O)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCBr
C#CCCCCCCCCCCCCN(CCCCCCCCCCCCCCCCCCCCCCCCCCCCC)C(=O)CCCCCCCCCCCN(CCCC)CCCCCC

  2 Training on 354 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.352624
Reward: 1.090000
Trajectories with max counts:
5	Cc1ccc(Nc2ncnc3ccccc23)cc1
5	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.02397915
Proportion of valid SMILES: 0.7637690776376908
Sample trajectories:
BrC1CCC2(CCN(Cc3ccco3)CC2)O1
BrCCCN1CCCCC1
Brc1ccc2nc3cc(Br)c(Br)cc3c(CCN3CCCCC3)cnc2c1
Brc1cccc2c(Nc3ccc(I)cc3)ccnc12
Brc1cccc2c1NCCc1c(ccc3c1CCCO3)OCCCCO2
Policy gradient replay...
Mean value of predictions: 0.04266239
Proportion of valid SMILES: 0.7823086574654956
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCC1CCNCC1
Brc1ccc(Nc2ccccc2-c2ccccc2OCC2CNc3ncccc3-c3ccccc32)cc1
Brc1ccc(Nc2nccnc2-c2cnccn2)cc1
Brc1ccc(Nc2ncnc3c2CCC3)cc1
Brc1ccc2ccc(Nc3ccncc3)cc2c1
Fine tuning...
Mean value of predictions: 0.038571432
Proportion of valid SMILES: 0.7904642409033877
Sample trajectories:
Brc1cc2c(nc1CCCCCc1ccsc1)CCCCO2
Brc1ccc(-c2ccccc2)c2ncccc12
Brc1ccc(-c2cscc2Nc2ccccc2)cc1
Brc1ccc(-c2nc3ccccc3o2)c2ncnc(Nc3ccccc3)c12
Brc1ccc(Nc2ncnc3cccc(Br)c23)cn1

  3 Training on 729 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.310076
Reward: 1.123967
Trajectories with max counts:
21	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.044851445
Proportion of valid SMILES: 0.7685329996872068
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCN1CCCCCCCC1
BrCCCCCCCCN=C(Nc1ccccc1)c1ccccc1
Brc1ccc(-c2c3cccc2c2ccc4ccccc4-c32)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc2c(c1)-c1ccccc1O2
Policy gradient replay...
Mean value of predictions: 0.09591161
Proportion of valid SMILES: 0.7445495680789799
Sample trajectories:
BP(=O)(OCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)C(F)(F)F
BrCCN1CCCCC1
BrCc1nc(NC2=NCCCCCn(c3nc4ccccc4[nH]3)cc(Br)cn2)n[nH]1
Brc1ccc(NN=Cc2cncnc2)cc1
Brc1ccc(Nc2ncnc3scnc23)nc1
Fine tuning...
Mean value of predictions: 0.09847953
Proportion of valid SMILES: 0.7310816588285592
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3Br)n2n1
Brc1ccc(Nc2cc3ccncc3cn2)o1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc2cc1Nc1ncnc(cc(Nc3ccc4[nH]cnc4c3)ncn1)N2
Brc1ccc2ncnc(Nc3ccccn3)c2c1

  4 Training on 1360 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.229288
Reward: 1.193818
Trajectories with max counts:
10	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.0960396
Proportion of valid SMILES: 0.698961937716263
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC1CCCCCCC1NCCCCNc1ncnc2nsnc12
Brc1ccc(Nc2ncnc3cc(Br)ccc23)c(Br)c1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(NCc4ccccc4Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.15930736
Proportion of valid SMILES: 0.5986394557823129
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ncc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)nc23)cc1
Brc1ccc(Nc2ncnc3cncnc23)cc1
Fine tuning...
Mean value of predictions: 0.16413105
Proportion of valid SMILES: 0.5822825735531846
Sample trajectories:
Brc1cc2ncnc(N3CCCCCC3)c2cc1-c1nc2nc(Nc3ccncc3)ccc2s1
Brc1ccc(Nc2ncnc3c2c(I)cc2ncncc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)c(-c2ccsc2)c1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccsc23)cc1

  5 Training on 2450 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 18.001479
Reward: 1.168541
Trajectories with max counts:
16	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.16215031
Proportion of valid SMILES: 0.5911154345006485
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCCCCCCCCCCCCCCCCC)OCC
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)cnc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.21032807
Proportion of valid SMILES: 0.5258785942492012
Sample trajectories:
Brc1cc2c(c(Br)c1Br)CCCCCCCCCCCCCCN1CCCCCC21
Brc1cc2nc(Nc3ncccn3)sc2nc1N1CCN(CCc2ccncc2)CC1
Brc1cc2ncnc(-c3ccncn3)c2s1
Brc1cc2ncnc(Nc3ccnc4cnccc34)c2cn1
Brc1cc2nnc(Nc3ccccc3)nc(cs1)n2
Fine tuning...
Mean value of predictions: 0.19356348
Proportion of valid SMILES: 0.5461808884627677
Sample trajectories:
BP(=O)(C(=O)Nc1ccc(Br)cc1)N(O)CCCl
BP(=O)(OCC)C(=O)COC(=O)C(F)(F)F
BrC1C2C=C(c3cnccn3)C1CC2
BrCC(Br)(Br)CBr
Brc1cc(Br)c2ncnc(Nc3ccnc4ccccc34)c2n1

  6 Training on 3703 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 19.986287
Reward: 1.428418
Trajectories with max counts:
10	Nc1ncnc2ncnc(Nc3ccc(Cl)cc3)c12
10	Nc1ncnc2ncnc(Nc3ccc(F)cc3)c12
Mean value of predictions: 0.2507822
Proportion of valid SMILES: 0.5261158594491928
Sample trajectories:
B[PH](=O)(Nc1nc2nc(Cl)nc(N)c2nc1NC(=O)OCc1cc(Br)cc(Br)c1)(P(=O)(O)O)P(=O)(O)O
BrCc1ccc(Nc2ncnc3cc(Br)c(Nc4nnc5ncnc(Nc6ccc(Br)cc6)c5n4)cc23)cc1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1ccc(-c2cc(Nc3ccncc3)ncn2)c2ncnc(Nc3cccc4ccncc34)c12
Brc1ccc(Nc2ncnc3[nH]cnc23)cc1
Policy gradient replay...
Mean value of predictions: 0.32517081
Proportion of valid SMILES: 0.6008841174613199
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(NCc2ccnc3cncnc23)ccc1Nc1ncnc2ccccc12
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCN(CCN2CCCCC2)CC1
Brc1cc2ncnc(Nc3ccnc(Nc4ccccc4)c3)c2s1
Brc1ccc(-c2cc(Br)ccc2Br)c(COc2ccncc2)c1
Fine tuning...
Mean value of predictions: 0.3090625
Proportion of valid SMILES: 0.6066350710900474
Sample trajectories:
BP(=O)(Br)CCCCCCCCCCCNS(=O)(=O)c1cc(Br)c(NC(=O)CNC(=O)CN(C)S(=O)(=O)c2ccc(F)cc2)c2ccccc12
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNc1ccc(-c2nc3ccccc3[nH]2)c2ccccc12
BrCCCCNCCCCCCCCCCCNCCCCNc1cnc(Nc2cc(Br)c(Br)cn2)nc1
BrCCNCCCCCCCCNCCCCCNCCCCCNCCCCCCNc1ccnc2cnc3cnc[nH]c3c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1

  7 Training on 5531 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 21.425286
Reward: 1.769435
Trajectories with max counts:
37	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.35987023
Proportion of valid SMILES: 0.5958749597164035
Sample trajectories:
BP(=O)(OP(=O)(O)O)N(CCCCNS(=O)(=O)c1cc(F)c(Br)cc1Br)C(Cl)Cl
Bc1cc(Nc2ncnc3cc(F)ccc23)cnc1-c1cccs1
BrC=C(Br)c1ccc(-c2cc3ccnc(Nc4cccc5ccccc45)nc3n2)nc1
BrC=C=CCCC=CCCCCCCCCCCCCCNc1ncnc2ncnc(Nc3ccc(Br)cn3)c12
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC1CC=CCCCCCCCCCCCCCCCCCCCCCC1
Policy gradient replay...
Mean value of predictions: 0.33480346
Proportion of valid SMILES: 0.6553565818410305
Sample trajectories:
BP(=O)(OCC1CCCCC1)Oc1cc2ncnc(Nc3cccc(Br)c3)c2cc1Br
BrCCCNc1ccc(Nc2ccnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ncc23)c1
Brc1cc(Br)c(Br)c2c3c(c1)C3c1ccccc12
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2nc2ncnc2n1
Fine tuning...
Mean value of predictions: 0.3377778
Proportion of valid SMILES: 0.6636335111390022
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccccc3)ncnc2c1
Brc1cc2ncnc(Nc3cccc4ccccc34)c2cn1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccc2ncnc(Nc3ccsc3)c2c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCNCC1
Brc1cc2ncnc(Nc3ccccc3)c2cn1

  8 Training on 7649 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 22.199151
Reward: 1.761926
Trajectories with max counts:
26	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.22294243
Proportion of valid SMILES: 0.7465775230818211
Sample trajectories:
Brc1[nH]c(Br)c2c1sc1ccccc12
Brc1cc(Br)c2ncnc(Nc3ccc4ccccc4c3)c2c1
Brc1cc2c(Nc3ccccc3)ncnc2cc1N1CCCCC1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Brc1ccc(-c2ncnc3ccccc23)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.398053
Proportion of valid SMILES: 0.5785356695869838
Sample trajectories:
BP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)NP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O[PH](=O)(O)(O)OP(=O)(O)O
BP(=O)(OCC1(O)OCCN1CC(F)(F)F)C(F)(F)C(F)F
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCN=C1CCCCCC1
BrCCCNc1ccc(Nc2ncnc3ncnc(C4CCCCCCCC4)c23)cc1
BrCCN1CCCCC1CNc1ccccc1Nc1cccc(Br)c1
Fine tuning...
Mean value of predictions: 0.3899842
Proportion of valid SMILES: 0.5937402190923318
Sample trajectories:
Br
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)c(Nc2nc(-c3ccccc3)nc3cnccc23)c(Br)c1
Brc1cc(Br)cc(Nc2nc(-c3ccncc3Br)c3c(Nc4ccccc4)ncnc3n2)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

  9 Training on 9590 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 22.930321
Reward: 2.080327
Trajectories with max counts:
41	Fc1ccc(Nc2ncnc3cc(F)ccc23)cc1
Mean value of predictions: 0.42617255
Proportion of valid SMILES: 0.5408706545568431
Sample trajectories:
BP(=O)(NS(=O)(=O)c1ccc(Br)cc1)c1ccc2ncnc(N)c2n1
BP(=O)(OCC)Oc1c(Br)c(Br)c(Br)c(Br)c1Br
B[PH](=O)(NO)(Nc1cc(Br)c(Br)c(Br)c1)P(=O)(O)O
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCc1c[nH]c2ccccc12
Brc1cc(Br)c(Br)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.39098278
Proportion of valid SMILES: 0.6880446148483792
Sample trajectories:
BrCCCCCCCCCCCCNCCCCCCCNCCCCCCCCCCCCCCNCCCNCCCCCCNCCCN1CCCCCc2c(Nc3ccc(Br)cc3)nc21
BrCCCCCCCCCCNc1c2c(nCCCC2)nc2ncnc12
BrCCCNCC1CCCCCCCCCCCCCCCCCCCCCCCCC1CCCN1CCCCCCCCCCC1
Brc1cc(Nc2ncnc3cc(Br)c(Br)cc23)ccn1
Brc1cc2c(s1)c(Nc1nc3nnnnc3cc1Br)ncnc1cc(ncn1)N2
Fine tuning...
Mean value of predictions: 0.3734767
Proportion of valid SMILES: 0.6901060070671378
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCCCCCCC=CCCCF)OCC
BP(=O)(O)P(=O)(O)OP(=O)(O)OP(=O)(OCC)OCCCCCCCCCCCCCCCCCCCCCCCC=C
BrCCCNc1nc2ncnc(Nc3cccc(Br)c3)c2s1
BrCCNc1ncc2ncnc(Nc3ccccc3)c2n1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1

 10 Training on 11566 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 25.489784
Reward: 2.208421
Trajectories with max counts:
6	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4071321
Proportion of valid SMILES: 0.6737732656514382
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCCCNc1c2c(nCCCC2)nc2ncnc12
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCc1c(N2CCCCCC2)nc2ccccc2c1Br
BrCCN(CBr)c1cccc(Br)c1Nc1ncnc2ccc(Br)cc12
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.45958254
Proportion of valid SMILES: 0.6773778920308483
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC(N)=O)NCCCCCCCCCCCCCCCCCCCCCCCCCCCN
BP(=O)(NCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC(=O)Nc1cc(Br)c(Br)cc1Br)N1CCCCC1
BrC1=CC2=Nc3scc(Br)c3CCC2C1
BrC=CC=CC=Nc1cccc(Nc2cc3cc(Br)cnc3c3cccnc23)c1
BrCCCCOc1cc(Br)c(Br)cc1Nc1ncnc2ccc(Br)cc12
Fine tuning...
Mean value of predictions: 0.4460076
Proportion of valid SMILES: 0.6791478373143964
Sample trajectories:
BP(=O)(OCC)OCC=CCCC
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BrCCCCCCCCCCCCCCCCCCCNCCCSc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BrCCNc1nc2ncnc(Nc3cccc(Br)c3)c2s1
BrCCNc1ncnc2cc3c4ccccc4N=Nc(n1)c3s2

 11 Training on 13599 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.811623
Reward: 2.234413
Trajectories with max counts:
19	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.46557713
Proportion of valid SMILES: 0.6733149931224209
Sample trajectories:
BP(=O)(CCBr)C(F)(F)F
BP(=O)(CCCCCCCCCCCCCCCC)Nc1ncnc2ncnc(Nc3ccc(Br)cc3)c12
BP(=O)(Nc1ccc(Br)cc1)P(=O)(OCC)OCCCCCCCCF
BP(=O)(OCC)OCCC=CCCCCCCCCC(=O)c1ccc(Br)o1
BP(=O)(OCC1NC(=O)N(CCCl)C(=O)N(O)C(=O)O1)C(O)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
Policy gradient replay...
Mean value of predictions: 0.4919333
Proportion of valid SMILES: 0.7042208822596001
Sample trajectories:
BrCCNc1ncnc2ncnc(Nc3ccccc3)s12
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Brc1cc2c(cc1Nc1ncnc3ncnc(Nc4ccccc4)c13)Sc1ccccc1N2
Fine tuning...
Mean value of predictions: 0.48811567
Proportion of valid SMILES: 0.7034329307056579
Sample trajectories:
BC(=O)Oc1cc2ncnc(Nc3cc(Br)c4[nH]ccc4c3)c2s1
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCCCCCNc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Brc1cc(Br)c2ncnc(Nc3ccc4ccccc4c3)c2n1

 12 Training on 15928 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.917572
Reward: 2.136992
Trajectories with max counts:
12	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.44522044
Proportion of valid SMILES: 0.7526281208935611
Sample trajectories:
BP(=O)(OCCCCCCCCCCCCBr)P(=O)(O)OP(=O)(O)OP(=O)(O)NP(=O)(O)COCCCl
Bc1cnc(Nc2ccc(Br)cc2)c2sccc12
BrC1=CN(Cc2cccc(Br)c2)Nc2cc(Br)c(Br)cc2N1CCCCCCCCCCCCCNCC1CCCCCC1
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1Br
Brc1ccc(-c2ncnc3ccsc23)c2ccccc12
Policy gradient replay...
Mean value of predictions: 0.50818664
Proportion of valid SMILES: 0.7133437990580848
Sample trajectories:
BP(=O)(CCCCC)c1ccccc1O
BP(=O)(Nc1ncc(CC(=O)Nc2cc(F)c(F)cc2F)s1)C(F)(F)F
BrC(Br)Br
BrC=CCN1CCCC1CCCCCCCCCCCNc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BrCCCCCCCCCCCCNCCCCCCNc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.5202317
Proportion of valid SMILES: 0.7038895859473023
Sample trajectories:
BP(=O)(OCCCc1ccccc1)c1ccccc1
BrCCCCCC1CCCN1c1ncnc2ncnc(Nc3ccc(Br)cc3)c12
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(-c3ccccc3Br)ncnc2s1
Brc1cc2ncnc(Nc3cccc(Br)c3Br)c2cc1-c1ccccc1

 13 Training on 18514 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.757130
Reward: 2.416214
Trajectories with max counts:
16	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5055938
Proportion of valid SMILES: 0.7798657718120805
Sample trajectories:
BP1(=O)Nc2ncnc(Nc3ccc(Br)cc3)c2N1
Brc1cc(Br)c2c(Nc3ccc(Br)c4cncnc34)ncnc2c1
Brc1cc(Br)c2c(Nc3cnc(-c4ccccc4Br)s3)ncnc2c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1nc2ncnc(Nc3ccccc3)c2s1
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1Br
Policy gradient replay...
Mean value of predictions: 0.56349707
Proportion of valid SMILES: 0.7044444444444444
Sample trajectories:
Brc1cc(Br)c(Br)c(Nc2ncnc3scc(Br)c23)c1
Brc1cc(Br)c2ncnc(Br)c2c1Br
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c1
Brc1ccc(Br)c(-c2ccc3ncnc(Nc4cccc5ccccc45)c3c2)c1
Brc1ccc(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.5562528
Proportion of valid SMILES: 0.7096055979643766
Sample trajectories:
BP(=O)(c1cc2ccccc2c2c(Br)ccnc2c2ncnc(Nc3cc(Br)cs3)c12)P(=O)(Oc1ccccc1)O[PH](O)(F)F
Brc1cc(Br)c2cc(Br)c(Br)c(Br)c2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3c4ccccc4c23)cs1

 14 Training on 21439 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.469432
Reward: 3.066045
Trajectories with max counts:
9	CN(C)CCCNc1ncnc2ncnc(Nc3ccc(Br)cc3)c12
9	CN(C)CCCNc1ncnc2ncnc(Nc3cccc(Cl)c3)c12
Mean value of predictions: 0.5619658
Proportion of valid SMILES: 0.7398039835599115
Sample trajectories:
BP(=O)(N(CCC(=O)Nc1cc(Br)c(Br)cc1Br)c1ccccc1)P(=O)(O)O
BP(=O)(OCC)C(=O)Nc1ccc(Br)cn1
Br
Brc1cc2ncnc(Nc3cccc(Br)c3-c3ccncc3)c2c(Br)c1Br
Brc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.55279666
Proportion of valid SMILES: 0.752311125278929
Sample trajectories:
BP(=O)(Nc1ccc(Br)c2ncnc(Br)c12)c1ccc(Br)cc1
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCCCCCCNc1ccnc2ccc(Nc3ccc(Br)cc3)cc12
Brc1cc(Br)c2c(Nc3ncnc4ccc(Br)cc34)ccnc2c1
Brc1cc(Br)cc(Nc2ncnc3sc(Br)nc23)c1
Brc1cc(Nc2ncnc3cc(Br)c(Br)cc23)ccn1
Fine tuning...
Mean value of predictions: 0.5472326
Proportion of valid SMILES: 0.76479949077021
Sample trajectories:
BP(=O)(CNc1ccc(Br)cc1)c1ccc(CN(CCCl)CCCl)cc1
BP(=O)(N(O)C=O)P(=O)(O)c1ccccc1
BrBr
BrCCCCCCCCCCCCCCCCCCCCCCCCCNCCCCNCCCCCNCCCSc1cc2ncnc(Nc3ccccc3)c2c2ccccc12
BrCCN=C(Br)c1ccc2ncnc(Nc3cccc(Br)c3)c2c1

 15 Training on 24666 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.291933
Reward: 2.924920
Trajectories with max counts:
16	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5799649
Proportion of valid SMILES: 0.7130325814536341
Sample trajectories:
BP(=O)(OCC)OCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
Br
BrCCN(CCCNc1ccc2ncnc(Nc3cccc(Br)c3)c2n1)c1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BrCCc1cc2ncnc(Nc3cccc(Br)c3)c2s1
BrCN1CCCCCCCCCCC1CCNCc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.64101046
Proportion of valid SMILES: 0.7456828885400314
Sample trajectories:
BP(=O)(NC(CN)CCCCN)OCCCCCCCCCCCCCCCCNc1ccccc1Br
BP(=O)(OCC)S(=O)(=O)CCNC(=O)c1ccc(Br)cc1
BrC=CBr
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCN1CCCCC1CCCCc1ccccc1
BrCCCCCCCCCCCCCCCCCCCCCCCCCCNCCCNc1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.6355383
Proportion of valid SMILES: 0.7278162366268093
Sample trajectories:
Bc1cc(Br)c2ncnc(Nc3cccc(Br)c3)c2n1
BrC1CCCC1N1CCc2c1ncn2C1CCCCC1
BrCCCBr
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNCCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCN1CC=C(c2cc3ncnc(Nc4ccc5ccccc5c4)c3nc2-c2ccccc2)CC1

 16 Training on 28216 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.717413
Reward: 3.199688
Trajectories with max counts:
34	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.63237476
Proportion of valid SMILES: 0.6739606126914661
Sample trajectories:
BrCCCNc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BrCCNc1nc2ccc(Br)nc2s1
BrSc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2n1
Policy gradient replay...
Mean value of predictions: 0.6051865
Proportion of valid SMILES: 0.6873045653533458
Sample trajectories:
BP(=O)(NO)c1ccc2nc(Nc3ncnc4sc(Br)c(Br)c34)cc(Br)c2c1
Bc1ccccc1-c1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCc1cccc(Nc2ncnc3ccccc23)c1
BrCN1CCCCCNCC1
BrCc1ccc2ncnc(Nc3ccsc3)c2c1
Fine tuning...
Mean value of predictions: 0.5953551
Proportion of valid SMILES: 0.7007824726134585
Sample trajectories:
BP(=O)(N(CC#N)c1cccc(Br)c1)P(=O)(Oc1cccc(Br)c1)c1ccc(Br)cc1
BP(=O)(OCC)N1CCN(CCOc2cccc(Br)c2)CC1
BrC(Br)(Br)Br
BrSc1ccc(Nc2ncnc3sccc23)cc1
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(Br)c1

 17 Training on 31525 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 33.307371
Reward: 3.362458
Trajectories with max counts:
35	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5906508
Proportion of valid SMILES: 0.6837981823879662
Sample trajectories:
BP(=O)(Nc1ccccc1)OCC1CCCCC1
Bc1ccc2nc(Nc3ccc(I)cc3)ccc2c1
BrCCNc1nccc(Br)c1-c1cccc2ncncc12
Brc1cc(-c2nc3ccc(Br)nc3c3ccccc23)ccn1
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.5906559
Proportion of valid SMILES: 0.7279267495094833
Sample trajectories:
BP(=O)(OCCCC)C(F)(F)F
Bc1cc(Nc2ncnc3cc(Br)cc(Br)c23)c(Br)c(Br)c1Br
Bc1ccc(Br)cc1Br
BrCCCCCCCNc1ccc2c(c1)CN2Cc1ccccc1
BrCCCN(Nc1ncncn1)c1ccc(Br)c2c(Nc3ccc(Br)cc3Br)ncnc12
Fine tuning...
Mean value of predictions: 0.60587156
Proportion of valid SMILES: 0.7091737150292778
Sample trajectories:
BP(=O)(OCCO)c1ccc(Br)cc1
Bc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2c(N)n1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Nc2cc(-c3cc(Br)nc4ncncc34)ncn2)ccn1
Brc1cc(Nc2ncnc3c(Br)c(Br)nc(Nc4ccccc4)c23)cs1

 18 Training on 34741 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.374297
Reward: 3.643903
Trajectories with max counts:
29	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.62099344
Proportion of valid SMILES: 0.7010512483574245
Sample trajectories:
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(I)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.60212046
Proportion of valid SMILES: 0.737566468564279
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ccc(Br)c2n1
Brc1cc(Nc2nc3ccc(Br)c(Br)c3s2)ccn1
Brc1cc2ncnc(N3CCCCC3)Nc3cccc2nc2cnccc3c2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Brc1cc2ncnc(Nc3ccnc4cccnc34)c2cn1
Fine tuning...
Mean value of predictions: 0.6232519
Proportion of valid SMILES: 0.7428035043804756
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3cccc(Br)c23)c1
Brc1cc(Nc2ncnc3ccccc23)c2ncnc(Nc3ccncc3)c2n1
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCCCCCC1
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1cc2ncnc(Nc3ccccn3)sc2nc1Nc1ccccc1

 19 Training on 38285 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 39.144082
Reward: 3.904469
Trajectories with max counts:
20	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.63260317
Proportion of valid SMILES: 0.741875
Sample trajectories:
Brc1cc(Nc2ncnc3sccc23)ccc1-c1ccccc1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1ccc(-c2ncnc3ccc(NCCc4cccc5ncnc(Nc6ccccc6)c45)cc23)cc1Br
Brc1ccc(Nc2ccnc3cccnc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6ccccc6)c5c4)cc3[nH]2)cc1
Policy gradient replay...
Mean value of predictions: 0.685203
Proportion of valid SMILES: 0.7591119946984758
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccccc23)cs1
Brc1cc2c(Nc3ccc(Br)c(Br)c3Br)cccc2s1
Brc1cc2ncnc(Nc3cc(Br)c(Br)cn3)c2s1
Fine tuning...
Mean value of predictions: 0.6662437
Proportion of valid SMILES: 0.7791694133157547
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNc1cc2ncnc(Nc3ccccc3)c2s1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Nc2ncnc3ccsc23)cs1
Brc1cc(Nc2ncnc3ncsc23)nc2ccccc12
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCCCCC1

 20 Training on 42237 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 37.584382
Reward: 3.886337
Trajectories with max counts:
12	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.7010135
Proportion of valid SMILES: 0.7479469361970941
Sample trajectories:
Brc1cc(Br)c(Nc2ncnc3ccsc23)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc2c(c(Br)c1Br)CCc1c-2ccc(Br)c1Br
Brc1cc2ncnc(Nc3ccc4[nH]cnc4c3)c2s1
Policy gradient replay...
Mean value of predictions: 0.5815579
Proportion of valid SMILES: 0.7636655948553055
Sample trajectories:
BP(=O)(NCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)C(=O)Nc1ccccc1Br
BP(=O)(Nc1ccccc1Br)P(=O)(O)Oc1cc(Br)cc(Br)c1F
BP(=O)(OCC)C(CCCCCCCCCCCCCCCCCNC(=O)c1ccccc1)c1ccccc1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc2ncnc(Nc3cnc4ccccc4c3)c2s1
Fine tuning...
Mean value of predictions: 0.5888982
Proportion of valid SMILES: 0.7741518578352181
Sample trajectories:
BIc1ccccc1Nc1nc2ncnc(Nc3ccccc3)sc2cc1-c1ccccc1
Brc1cc(Br)c(-c2nccc3cncnc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)cc(CNc2ncnc3sc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

Trajectories with max counts:
50	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5779982
Proportion of valid SMILES: 0.7009075111025295
Mean Internal Similarity: 0.49060732184221056
Std Internal Similarity: 0.11024464010702348
Mean External Similarity: 0.4218304963322064
Std External Similarity: 0.07541109173249697
Mean MolWt: 532.3229030713343
Std MolWt: 248.2641820279887
Effect MolWt: 0.1559493894916026
Mean MolLogP: 8.268886114597096
Std MolLogP: 6.430093560260611
Effect MolLogP: 0.665841981968202
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.785349% (1148 / 1174)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 0, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5634.142457008362, 'valid_fraction': 0.7009075111025295, 'active_fraction': 0.5561065197428834, 'max_counts': 50, 'mean_internal_similarity': 0.49060732184221056, 'std_internal_similarity': 0.11024464010702348, 'mean_external_similarity': 0.4218304963322064, 'std_external_similarity': 0.07541109173249697, 'mean_MolWt': 532.3229030713343, 'std_MolWt': 248.2641820279887, 'effect_MolWt': 0.1559493894916026, 'mean_MolLogP': 8.268886114597096, 'std_MolLogP': 6.430093560260611, 'effect_MolLogP': 0.665841981968202, 'generated_scaffolds': 1174, 'novel_scaffolds': 1148, 'novel_fraction': 0.9778534923339012, 'save_path': '../logs/replay_ratio_s2-1.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.664134
Reward: 1.000000
Mean value of predictions: 0.0
Proportion of valid SMILES: 0.7974286610222641
Sample trajectories:
BP(=O)(OCCC(Cl)Cl)P(=O)(OC(Cl)Cl)C(=O)O
Brc1cc(C[P+](CCCCc2c[nH]c3ccccc23)C(Cc2ccccc2)=Nc2ccccc2)c2ccccc2n1
Brc1ccc(CN2CCN(Cc3ccc4c(c3)OCO4)CC2)cc1
C#CC1(O)CCC2C(O)C(O)C3CC(C)C1(C)CC23
C#CCC(=O)c1ccc(C)cc1O
Policy gradient replay...
Mean value of predictions: 0.017065557
Proportion of valid SMILES: 0.6038328620797989
Sample trajectories:
Brc1ccc(Cn2cnc(Nc3c4cc(Nc5ccccc5)c4nc4cccnc34)n2)c(Br)c1
Brc1ccc(Nc2ncnc3nc(-c4ccncn4)oc23)cc1
Brc1ccc(Nc2ncnc3nccnc23)cc1
Brc1ccc2nonc2c1-c1ncn[nH]1
C#CC=CCN(CC=C)CC(=O)Nc1ccc(F)c(CCC(=O)O)c1
Fine tuning...
Mean value of predictions: 0.022585753
Proportion of valid SMILES: 0.5953502984605717
Sample trajectories:
Brc1ccc(Br)cc1
Brc1ccc(CC2=C(c3ccncc3)N=C3C=CC=CC3=N2)cc1
Brc1ccc(Nc2nc(Nc3nc4cc(Br)c(NC5CCCCC5)cc4o3)ncc2Br)cc1
Brc1ccc(Nc2ncnc3nc(-c4ccc(CN5CCCC5)c(Br)c4)oc23)cc1
Brc1ccc2c(c1)NC1CCCCN21

  2 Training on 330 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.402154
Reward: 1.000000
Trajectories with max counts:
2	Nc1ncnc2c1ncn2C1CCCCC1
2	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.019820768
Proportion of valid SMILES: 0.594857322044528
Sample trajectories:
C#CC1CCC(COC(C)=O)CC1
C#CC1OC(=O)C(C)C2CCC(C)C12
C#CC=C(SCCO)N1Cc2nc3ncnc(Nc4cccc(OC)c4)c3cc21
C#CCCCCCCCCCCCC
C#CCCNc1nc(OC)cc2ncnc(Nc3ccc(Br)cc3F)c12
Policy gradient replay...
Mean value of predictions: 0.085566476
Proportion of valid SMILES: 0.6050078247261346
Sample trajectories:
BP(=O)(O)CCCC(N)C(=O)O
Brc1ccc(C2CNc3nc(-c4ccco4)ccc3N=N2)cc1
Brc1ccc(Cn2cnc3c(NCn4cccn4)ncnc32)cc1
Brc1ccc(Nc2cc(-c3ccccc3)nc3ccccc23)cc1
Brc1ccc(Nc2ccc(Oc3cnccn3)cn2)cc1
Fine tuning...
Mean value of predictions: 0.058922034
Proportion of valid SMILES: 0.5979349186483104
Sample trajectories:
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ccc3c(Br)ccc(-c4ccc(Br)s4)c3c2)s1
Brc1ccc(Nc2nccc3cccnc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
C#CC(=O)CCCCCCCC#CCC(O)C#C

  3 Training on 786 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.827446
Reward: 1.153183
Trajectories with max counts:
14	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.07376033
Proportion of valid SMILES: 0.6057571964956195
Sample trajectories:
BP(=O)(N(O)Cc1ccc(Br)cc1)P(=O)(Oc1ccc(F)cc1)Oc1ccc(F)cc1
Brc1ccc(Br)c(N2CCc3ccccc3S2)c1
Brc1ccc(N2CCC(CCc3ccccc3)N=C2c2cccnc2)c(Br)c1
Brc1ccc(Nc2cc(-c3ccccc3)ccn2)nc1
Brc1ccc(Nc2cc(-c3ccccc3)nc3ccccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.11380578
Proportion of valid SMILES: 0.5966176010021923
Sample trajectories:
Brc1cc(Br)c(Nc2ncc(-c3cccnc3)cn2)cc1Br
Brc1cc2c[nH]c3ccccc2c3c1Br
Brc1ccc(-c2cc(Br)ccc2CNc2cc(Nc3cccnc3)ccc2-c2ccccc2)cc1
Brc1ccc(Br)c(Nc2ccc3c(c2)c2c(Nc4cccs4)ncnc2N3)c1
Brc1ccc(N2CCN(c3ccc(Nc4cncc(Br)c4)cc3)CC2)cc1
Fine tuning...
Mean value of predictions: 0.110550694
Proportion of valid SMILES: 0.6089000313381385
Sample trajectories:
BrCc1ccc(Nc2ncnc3c(Nc4ccccc4Br)ncnc23)cc1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1ccc(Br)c(Nc2ncnc3cccnc23)c1
Brc1ccc(Nc2cnc3ccccc3c2)cc1
Brc1ccc(Nc2nc3ccccc3[nH]2)cc1

  4 Training on 1601 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 19.503876
Reward: 1.267726
Trajectories with max counts:
8	Clc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
Mean value of predictions: 0.12898877
Proportion of valid SMILES: 0.5595724614900974
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc2ncnc(Nc3ccc(NCc4ccnc(NC5CC5)n4)cc3)c2cc1Br
Brc1ccc(NCCCNc2ccc(Br)cc2)cc1
Brc1ccc(NN=Cc2ccnc(Nc3ccccc3)c2)cc1
Brc1ccc(Nc2ccc(Br)o2)cc1
Policy gradient replay...
Mean value of predictions: 0.16140905
Proportion of valid SMILES: 0.5954915466499687
Sample trajectories:
BrCC(=NNc1ccc(Nc2c(Br)cccc2Nc2cc(Br)ccc2Br)cc1)c1ccncn1
BrNc1cccc(Br)c1
Brc1ccc(-c2cc(Br)ccc2Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(-c2ccc(Br)cc2CNc2ccc(I)cc2)cc1
Brc1ccc(-c2nc3ccccc3s2)o1
Fine tuning...
Mean value of predictions: 0.1653105
Proportion of valid SMILES: 0.5846635367762129
Sample trajectories:
Brc1ccc(-c2ncnc3c(Br)c(Br)cc(Br)c23)cc1
Brc1ccc(-c2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Br)cc1
Brc1ccc(Nc2ccc(Br)nc2)cc1
Brc1ccc(Nc2ccc(Nc3ncnc4ccc(Br)cc34)cc2)cc1

  5 Training on 2663 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 21.887512
Reward: 1.533659
Trajectories with max counts:
48	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.14928718
Proportion of valid SMILES: 0.6139418568302595
Sample trajectories:
Brc1cc2c(nc1Nc1ccccc1)-c1ccccc1-2
Brc1cc2c3c(ccc2c(Br)c1Br)CCCC3
Brc1ccc(Br)c(N2CCN(CCOc3cccc4c3CCCC4)C2)c1
Brc1ccc(Br)c(Nc2nccc3ccc(Br)cc23)c1
Brc1ccc(C(Oc2ccccc2)c2ccccc2)cc1
Policy gradient replay...
Mean value of predictions: 0.15201554
Proportion of valid SMILES: 0.6434375
Sample trajectories:
Brc1ccc(Nc2cc(-c3ccco3)ccn2)cc1
Brc1ccc(Nc2ccc(-c3ccccc3Br)cc2)nc1
Brc1ccc(Nc2ccc(Nc3c(Br)ccc4ccccc34)cc2)cc1
Brc1ccc(Nc2ccc(Nc3ccncn3)c3ccccc23)cc1
Brc1ccc(Nc2ccccc2Br)cc1
Fine tuning...
Mean value of predictions: 0.15988594
Proportion of valid SMILES: 0.6579111944965603
Sample trajectories:
Brc1cc2ccccc2s1
Brc1ccc(Br)c(Nc2ncnc3ncnc(Nc4ccccc4Br)c23)c1
Brc1ccc(C2=Nc3ccccc3C(=Cc3ccccc3)S2)cc1
Brc1ccc(Nc2c3c(nc4ccncc24)CCCC3)cc1
Brc1ccc(Nc2cc3ncnc(Nc4ccccc4)c3cc2Br)cc1

  6 Training on 3718 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 21.548269
Reward: 1.728095
Trajectories with max counts:
26	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.18002827
Proportion of valid SMILES: 0.6636448890278211
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCCCOP(=O)(OC)OC(C)C
BrCCCNc1ccc(Nc2c3ccccc3nc3ccc(Br)cc23)cc1Nc1ccccc1
BrCCNc1ccc(Nc2ccccc2)cc1
BrCc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ccc(Nc3ccc(Br)cc3)c3cc(Br)ccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.2175772
Proportion of valid SMILES: 0.658018130665833
Sample trajectories:
BP(=O)(CCC(=O)O)OCC(=O)ON=C(N)N
B[PH](=O)(NO)(NO)c1ccc(Br)cc1
Brc1ccc(-c2ncnc3ncnc(Nc4cccc(Br)c4)c23)cc1
Brc1ccc(Br)c(Nc2c(Br)cccc2Br)c1
Brc1ccc(Br)c(Nc2nccc(-c3ccc4cc(Br)ccc4n3)n2)c1
Fine tuning...
Mean value of predictions: 0.22003874
Proportion of valid SMILES: 0.6470404008769183
Sample trajectories:
BrCCCCCCCCCCNc1ccc2c(c1)N2Cc1ccccc1
Brc1cc2cccnc2nc1Nc1ccc2cnccc2c1
Brc1ccc(Br)c(Nc2ccc3ncnc(Nc4cccs4)c3c2)c1
Brc1ccc(CNc2ncnc3ncnc(Nc4cccc(Br)c4)c23)cc1
Brc1ccc(NCc2ccc(Nc3ccccc3Br)c3cnnn23)cc1

  7 Training on 5030 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 20.847355
Reward: 1.630821
Trajectories with max counts:
16	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.14629948
Proportion of valid SMILES: 0.7267041901188243
Sample trajectories:
BrC(N=C1CC1c1ccccc1)c1ccc2ccccc2c1
BrC=CCC=CC=C=CCCCCCCCCCBr
Brc1ccc(-c2cc3c(Nc4ccccc4)ncnc3cc2Nc2ccccc2Br)cc1
Brc1ccc(Br)c(C=Cc2cccc(I)c2)c1
Brc1ccc(Br)c(Nc2ccc(Br)c(Br)c2)c1
Policy gradient replay...
Mean value of predictions: 0.31053957
Proportion of valid SMILES: 0.6196875
Sample trajectories:
BP(=O)(OC(=O)C(Br)Br)C(F)(F)F
BrCc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Br
Brc1cc(Nc2c(Br)cccc2Br)ccc1Nc1ncnc2ccccc12
Brc1cc2c(Nc3ccc[nH]3)ncnc2s1
Brc1ccc(-c2cc(Nc3ncnc4[nH]cnc34)ncn2)c(I)c1
Fine tuning...
Mean value of predictions: 0.305323
Proportion of valid SMILES: 0.6052549264935877
Sample trajectories:
BP(=O)(c1cccc(Nc2cc(F)cc(F)c2)c1F)N(O)C(F)(F)F
Brc1c(Br)c(Br)c2c(Br)cccc2c1Br
Brc1cc2ncnc(Nc3cccc(Nc4c(Br)cncc4-c4ccccc4)c3)c2s1
Brc1ccc(-c2cccnc2)c2ccccc12
Brc1ccc(-c2csc3ccccc23)c(Br)c1

  8 Training on 6562 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 24.892334
Reward: 2.432954
Trajectories with max counts:
51	Nc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.37956992
Proportion of valid SMILES: 0.5246004387339392
Sample trajectories:
Brc1cc(Br)c(Nc2c(Br)cccc2Br)cc1Br
Brc1cc(Br)c(Nc2ncnc3ncnc(Nc4ccccc4I)c23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(NCc4ccccc4)cc3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.36253688
Proportion of valid SMILES: 0.6358236949046577
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Br)cc23)c(Br)c1
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrC1=Nc2ccccc2Nc2ccc(Br)cc21
Brc1cc2ncnc(c(Br)s1)Nc1cccc2c1-c1ccccc1
Brc1ccc(Br)c(CSc2ncnc3c(Br)cc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.3661265
Proportion of valid SMILES: 0.5987480438184664
Sample trajectories:
BP(=O)(=O)(CCCON(=O)=O)OCC
BP(=O)(OC(Br)CCCCBr)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BrCCNc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1c[nH]c(Nc2ccc3ccccc3c2)n1
Brc1cc(Br)c2c(Nc3ccc(Br)c(CCc4ccccc4)c3)ccnc2c1

  9 Training on 8527 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 25.095280
Reward: 2.536437
Trajectories with max counts:
15	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.40082306
Proportion of valid SMILES: 0.6095954844778928
Sample trajectories:
BP(=O)(O)c1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrC(Br)=C(Br)Br
BrCCc1cc2ncnc(Nc3cccc(Br)c3)c2cn1
Brc1cc(Br)c2c(n1)N2c1ccccc1Br
Brc1cc2c(Nc3ccccc3)ncnc2nc1Br
Policy gradient replay...
Mean value of predictions: 0.4162906
Proportion of valid SMILES: 0.5687010954616588
Sample trajectories:
BP(=O)(CCCC(CC=C)c1cccc(Br)c1)OCC
BP(=O)(OCC)C(=O)O
BrCCN(c1cncnc1)c1ccc(Br)cc1Br
BrSc1sc(Nc2ccc(Br)cc2)cc1Nc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.42378083
Proportion of valid SMILES: 0.5719210278909432
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1ccc(Br)c(Br)c1
BP(=O)(OCC)Oc1ncnc2c(Br)c(Br)sc12
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c2c(Nc3cc(Br)c(Br)cc3Br)ncnc2c1
Brc1cc(Br)c2c(Nc3cc(Br)nc(-c4ccccc4)n3)ccnc2c1

 10 Training on 10244 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.526675
Reward: 2.679244
Trajectories with max counts:
108	Clc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.4282479
Proportion of valid SMILES: 0.524867062871442
Sample trajectories:
BrC(Br)=NNc1ccc(Br)c(Br)c1
Brc1cc(Br)c(Br)c(-c2cnc3cc(Br)ccc3n2)c1
Brc1cc(Br)c2c(Nc3ccc(NCc4nc5ccc(Br)cc5s4)cc3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ncc23)c1
Policy gradient replay...
Mean value of predictions: 0.43914127
Proportion of valid SMILES: 0.626720901126408
Sample trajectories:
BP(=O)(CS(=O)(=O)c1ccc(Nc2ccc(Br)cc2)cc1)C(N)=O
BrC=CC=CNc1ccccc1
Brc1cc(Br)cc(Nc2ccc(Nc3ncnc4ccc(Br)cc34)cc2)c1
Brc1ccc(-c2csc(Nc3ccccc3CC#Cc3ccsc3)c2)c2cncnc12
Brc1ccc(-c2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.4349951
Proportion of valid SMILES: 0.6401752190237797
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
B[PH](=O)(C=CC1=C(Br)C(=O)Oc2c(Cl)c(Br)cc(Br)c2O1)(CC=C)OCC
BrCC(=Nc1ccc(Br)cc1)c1cc(Br)cc(Nc2cccnc2)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(I)cc(Nc2ncnc3[nH]c(Br)cc23)c1

 11 Training on 12121 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.344932
Reward: 2.907657
Trajectories with max counts:
71	Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4661894
Proportion of valid SMILES: 0.5678549093183239
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCl
BrC=CC=CC=CC=CC=CC=CC=CCCCBr
BrCC(Br)(Br)CCCC(Br)C(Br)(Br)CBr
BrCc1ccc(Nc2nc(Nc3ccc(Br)cc3Br)nc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Nc2ccc(I)cc2)cc1Br
Policy gradient replay...
Mean value of predictions: 0.41329452
Proportion of valid SMILES: 0.6450704225352113
Sample trajectories:
BP(=O)(OCCCCC)OCCCCCC
BrC=CBr
BrCCBr
BrCCOc1ccc(Nc2ncnc3ccc(Br)cc23)s1
Brc1cc(Br)c2c(Nc3cccc(Br)n3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.41463178
Proportion of valid SMILES: 0.6468191789407709
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccccc4)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Nc4nc5cncc(Br)c5cc4Br)nc23)c1
Brc1cc(I)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc2cccc(Br)c2cc1Br

 12 Training on 14005 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.149400
Reward: 2.941228
Trajectories with max counts:
65	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4344111
Proportion of valid SMILES: 0.54125
Sample trajectories:
BP(=O)(NCc1ccc(Br)o1)P(=O)(Oc1ccccc1)Oc1cccc(Br)c1
BP(=O)(OCC=C)C(=O)Nc1cccc(Br)c1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCCNc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Policy gradient replay...
Mean value of predictions: 0.4198489
Proportion of valid SMILES: 0.621477770820288
Sample trajectories:
BP(=O)(N=CN1CCOCC1)OCC
BP(=O)(NO)c1ccc(Nc2ncnc3c(Br)ccc(Br)c23)c(Br)c1
Bc1cc(Nc2ncnc3c2[nH]c2ccc(Br)cc23)ccc1Br
Bc1ccc(Nc2ncnc3scnc23)cc1
Brc1cc2ncnc(Nc3cccc(Nc4ccccc4Br)c3)c2cc1Br
Fine tuning...
Mean value of predictions: 0.39582714
Proportion of valid SMILES: 0.6298498122653317
Sample trajectories:
BP(=O)(CCl)N=[PH](=O)(O)P(=O)(O)O
Brc1cc(Br)c2c(Br)cccc2c1
Brc1cc(NCc2c(Br)ccc3ncccc23)ccc1Nc1cccc2ccccc12
Brc1cc2nc[nH]c2c2ccc(CN3CCCC3)cc12
Brc1ccc(Br)c(Nc2ccc(Nc3ncnc4ccccc34)cc2)c1

 13 Training on 15824 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.006935
Reward: 3.002456
Trajectories with max counts:
55	Brc1ccc(Nc2ncnc3ccsc23)cc1
55	Cc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49724367
Proportion of valid SMILES: 0.5679398872886663
Sample trajectories:
BP(=O)(NCCCCCCCCCCCCO)c1ccc(Br)cc1
Bc1cc(Br)ccc1Nc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3sc(Br)cc23)cn1
Brc1cc(Nc2ccccc2Br)c(Nc2cc(Nc3ncnc4ccsc34)ccc2Br)cc1Br
Brc1ccc(-c2csc3ncnc(Nc4ccc(Br)nc4)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.52434784
Proportion of valid SMILES: 0.5760801502817783
Sample trajectories:
BP(=O)(OCC)Oc1c(Br)c(Br)c(Br)c(Br)[n+]1I
Brc1cc(Br)c2ncc(Br)c(Br)c2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1
Brc1cc(Br)cc(Nc2cnc3c(Nc4cccc(Br)n4)ncnc3cc(Br)c2)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Fine tuning...
Mean value of predictions: 0.5402639
Proportion of valid SMILES: 0.5938577248511439
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)OCCCC
BrC(=Cc1ccc2scc(-c3ccc(Br)c4ccccc34)cc12)c1ccc(Br)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)cn23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ccccc2)sc2c(Nc3ccc(Br)c4ccccc34)ncnc2c1

 14 Training on 18181 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.360959
Reward: 3.677690
Trajectories with max counts:
66	CC(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.53527063
Proportion of valid SMILES: 0.5486089402938418
Sample trajectories:
BP(=O)(OCC)OCC(Cl)(P(=O)(O)O)P(=O)(O)O
Br
BrBr
BrCCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Policy gradient replay...
Mean value of predictions: 0.581891
Proportion of valid SMILES: 0.6248827758674586
Sample trajectories:
Br
Brc1cc(Br)c(Nc2ncnc3ccc(Nc4ccccc4Br)nc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Nc4ccccc4Br)cc3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)n3)c2c1
Brc1ccc(-c2ncnc3ccsc23)cn1
Fine tuning...
Mean value of predictions: 0.5559857
Proportion of valid SMILES: 0.6138211382113821
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1c(Br)c(Br)c2c(Br)cccc2c1Br
Brc1cc(Br)c(Nc2nc3c(Br)cc(Nc4ncnc5cccc(Br)c45)cc3s2)c(Br)c1
Brc1cc(Nc2ncnc3ccc(Br)nc23)ccn1
Brc1ccc(Br)c(Br)c1

 15 Training on 20805 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.628659
Reward: 3.739513
Trajectories with max counts:
52	CC(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5553816
Proportion of valid SMILES: 0.6391494684177611
Sample trajectories:
BrC=CC=C(Br)Br
Brc1ccc(Nc2cc3c(Nc4ccc(Br)c(Br)c4)ncnc3cc2Br)cc1
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc[nH]c23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.58301
Proportion of valid SMILES: 0.5963101938711695
Sample trajectories:
Brc1c(Nc2ncnc3c(Br)ccc(Br)c23)ccc2ccccc12
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(N=CNc4ccc5ncnc(Nc6ccccc6)c5c4)cc3)ccnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.5854375
Proportion of valid SMILES: 0.5973091364205256
Sample trajectories:
BP(=O)(NN=Cc1cccc(Br)c1)N(=O)=O
BP(=O)(NO)c1cccc(Br)c1
Bc1ccc(Br)cc1Nc1ccc(Br)c(Br)c1
BrC(Br)=NNc1ccc(Nc2ncc3ccc(Br)cc3n2)cc1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1

 16 Training on 23653 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.357209
Reward: 3.774998
Trajectories with max counts:
57	C=CC(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6063245
Proportion of valid SMILES: 0.605081555834379
Sample trajectories:
BrC=CBr
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1ccc(-c2ncnc3ccsc23)c2ccsc12
Brc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.56824577
Proportion of valid SMILES: 0.6029504080351538
Sample trajectories:
BrCC=CCC=NNc1ccc(Nc2ncnc3ncnc(Br)c23)cc1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc(Nc2cs3cccc3ncn2)ncn1
Brc1cc(Nc2ncnc3ccc(Br)nc23)ccn1
Brc1ccc(-c2csc3ncnc(Nc4ccc(I)cc4)c23)cc1
Fine tuning...
Mean value of predictions: 0.58078516
Proportion of valid SMILES: 0.6067063616421184
Sample trajectories:
BP(=O)(OCC)OCCCCCCCCCCCCCCCCC
Brc1cc(Br)c2c(Br)c3cc(Br)ccc3nc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1
Brc1ccc(-c2cc(Nc3ccnc4ccccc34)ncn2)cc1
Brc1ccc(Nc2nc3c(Nc4ccc(Br)c(Br)c4)ncnc3cc2Br)cc1

 17 Training on 26495 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.197121
Reward: 4.083147
Trajectories with max counts:
74	CC(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5269081
Proportion of valid SMILES: 0.5611128477649265
Sample trajectories:
Brc1ccc(C=Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3c4ccccc4c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)c(Br)c1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)cnc23)cc1
Policy gradient replay...
Mean value of predictions: 0.6202871
Proportion of valid SMILES: 0.6772271016311167
Sample trajectories:
BrCc1ccc2ncnc(Nc3ccc(Br)cc3Br)c2c1
Brc1cc(Br)c2c(Nc3cncc(I)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Brc1ccc(Nc2cc(Nc3ncnc4ccc(Br)cc34)ccn2)cc1
Fine tuning...
Mean value of predictions: 0.5832803
Proportion of valid SMILES: 0.6899686520376176
Sample trajectories:
BP(=O)(OCCC)C(CCCC(=O)O)N1CCCCCCCCCCCCCCCCCC1CP(=O)(O)CCCCN
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2n1
Brc1cc(Br)c2c(Nc3ccccc3)ncnc2c1
Brc1cc(I)c2nc(Nc3ccccc3I)sc2c1

 18 Training on 29528 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.652759
Reward: 4.364235
Trajectories with max counts:
26	CC(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.66858214
Proportion of valid SMILES: 0.6642678347934918
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(NCc4ccccc4)cc3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(-c2cc3cccc(CCCCCNc4ccc5ncnc(Nc6ccccc6)c5c4)c3c3cc(Br)ccc23)cc1
Brc1ccc(NCCCCCCCCC=Nc2ccc3ncnc(Nc4cccc(Br)c4)c3n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.61748284
Proportion of valid SMILES: 0.6932106598984772
Sample trajectories:
BrC=Cc1cccc(Nc2ncnc3ccsc23)c1
Brc1cc(Br)c(-c2ccc(Nc3ncnc4cc(Br)cc(Br)c34)cc2)c(Br)c1
Brc1cc(Nc2ncnc3ccccc23)sc1Br
Brc1ccc(Nc2c(Br)ncnc2Nc2cccc(Br)c2)cc1
Brc1ccc(Nc2cnc3ccc(Br)cc3n2)cc1
Fine tuning...
Mean value of predictions: 0.5971275
Proportion of valid SMILES: 0.7075261987932677
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCBr
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(-c4ccccc4Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1

 19 Training on 33101 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.782979
Reward: 4.647022
Trajectories with max counts:
37	CC(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5547863
Proportion of valid SMILES: 0.7384032817923635
Sample trajectories:
BP(=O)(CCCCCCC=C(NO)c1cccc(Br)c1)OCC
Brc1cc(Br)cc(Nc2ncnc3cccc(Br)c23)c1
Brc1cc(Nc2ncnc3ccc(Nc4ccccc4I)cc23)c(Br)s1
Brc1ccc(-c2ncnc3cc(Nc4ccccc4Br)sc23)cc1Br
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.6334513
Proportion of valid SMILES: 0.6211550533584432
Sample trajectories:
BrC(=CC(Br)Br)Oc1ccc2nc3c(Nc4ccccc4Br)ncnc3nc2c1
Brc1cc(Br)cc(Nc2ncnc3c(Br)ncnc23)c1
Brc1ccc(-n2c3cncnc3ncnc(Nc3ccccc3Br)cc3ncnc(Nc4cccc(Br)c4)nc32)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2cccc(Nc3ncnc4cc(Br)c(Br)cc34)c2)c1
Fine tuning...
Mean value of predictions: 0.6377744
Proportion of valid SMILES: 0.6144918444165621
Sample trajectories:
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(Nc2cc(Br)ccc2Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Nc4ccccc4)c3)c2cc1Br
Brc1ccc(-c2ccc(Br)cc2-c2c(Br)cc(Br)cc2Br)cc1
Brc1ccc(Br)c(CNc2cccc3cnccc23)c1

 20 Training on 36371 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.932026
Reward: 4.816411
Trajectories with max counts:
123	CC(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.62239665
Proportion of valid SMILES: 0.5979349186483104
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(Br)c(-c2cc3ncnc(Nc4ccc(I)cc4)c3s2)cc1Br
Brc1cc2ncnc(Nc3cccc(I)c3)c2cc1Br
Brc1ccc(NC2C=CC(Nc3ccccc3)CCCC2)cc1
Brc1ccc(Nc2cc3c(Nc4ccccc4Br)ncnc3s2)cc1
Policy gradient replay...
Mean value of predictions: 0.66223276
Proportion of valid SMILES: 0.663828445285399
Sample trajectories:
BP(=O)(OCCCCCCCN)OCCCCCCCCCC
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCNc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BrCc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4cc(Br)cc(Br)c34)cc2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Nc4ccc(Nc5ncnc6cc7c(cc56)C=CCCCCCCCCCCCCCCO7)cc4)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.66
Proportion of valid SMILES: 0.6401766004415012
Sample trajectories:
BrCCCCCCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCNc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
BrCc1cc2c(Nc3cc(Br)c(Br)c(Br)c3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(Br)c4c(Nc5ccccc5)ncnc34)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)cn23)c1

Trajectories with max counts:
139	Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.63954127
Proportion of valid SMILES: 0.5718158890290038
Mean Internal Similarity: 0.48658753199173665
Std Internal Similarity: 0.10237574480537717
Mean External Similarity: 0.4200227326647997
Std External Similarity: 0.07321367114861307
Mean MolWt: 502.43180284479655
Std MolWt: 177.7522151529212
Effect MolWt: 0.00968994982549238
Mean MolLogP: 7.0009232625135045
Std MolLogP: 4.248571650193504
Effect MolLogP: 0.6477770848396724
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.464503% (961 / 986)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 10, 'n_policy_replay': 15, 'n_fine_tune': 0, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5532.51371884346, 'valid_fraction': 0.5718158890290038, 'active_fraction': 0.6124159223729188, 'max_counts': 139, 'mean_internal_similarity': 0.48658753199173665, 'std_internal_similarity': 0.10237574480537717, 'mean_external_similarity': 0.4200227326647997, 'std_external_similarity': 0.07321367114861307, 'mean_MolWt': 502.43180284479655, 'std_MolWt': 177.7522151529212, 'effect_MolWt': 0.00968994982549238, 'mean_MolLogP': 7.0009232625135045, 'std_MolLogP': 4.248571650193504, 'effect_MolLogP': 0.6477770848396724, 'generated_scaffolds': 986, 'novel_scaffolds': 961, 'novel_fraction': 0.9746450304259635, 'save_path': '../logs/replay_ratio_s2-2.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.698529
Reward: 1.000000
Mean value of predictions: 0.0011881188
Proportion of valid SMILES: 0.7910401002506265
Sample trajectories:
BP(=O)(OC(C)C)C1CCCC=CC1OC(=O)CCC(N)=O
Brc1cc(Br)c2c(c1Br)C1CC=CC(CO2)O1
Brc1ccc(-c2ccc3oc(N4CCCC4)nc3n2)cc1
Brc1ccco1
C#CCC#Cc1ccn(C(=O)N2CCN(c3ccccc3)CC2)c1
Policy gradient replay...
Mean value of predictions: 0.0028673834
Proportion of valid SMILES: 0.7846875
Sample trajectories:
Brc1ccc2c(c1)Sc1ccccc1O2
Brc1ccc2ccccc2c1
Brc1ccccc1
Brc1ccccc1-c1cc2ccccc2cc1-c1ccccc1
Brc1ccccc1-c1cc2ccccc2nc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.0018297534
Proportion of valid SMILES: 0.785625
Sample trajectories:
Brc1ccc2[nH]ccc2c1
Brc1ccc2ccccc2c1
Brc1ccc2ccccc2c1Cc1ccccc1
Brc1ccc2ccccc2n1
Brc1cccc(-c2ccccc2)c1

  2 Training on 239 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 15.047628
Reward: 1.000000
Trajectories with max counts:
31	O=C(Nc1ccccc1)c1ccccc1
Mean value of predictions: 0.0013524265
Proportion of valid SMILES: 0.7858705845576742
Sample trajectories:
BrCCc1ccccc1-c1ccccc1
Brc1ccc(-c2ccccc2-c2nc3ccccc3o2)cc1
Brc1ccc2ccccc2c1
Brc1ccc2ccccc2c1-c1ccc2ccccc2c1
Brc1cccc(Nc2c(-c3ccccc3)cnc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.026653882
Proportion of valid SMILES: 0.6543287327478042
Sample trajectories:
BrCI
Brc1cc2nc(-c3ncccc3Br)sc2c(Nc2ccccc2)n1
Brc1ccc(-n2cccc2)nc1
Brc1ccc(Nc2nc3ccccc3nc2-n2ccnc2)cc1
Brc1ccc(Nc2ncnc3cccc(-c4ccc(Br)cc4)c23)cc1
Fine tuning...
Mean value of predictions: 0.023438258
Proportion of valid SMILES: 0.6481481481481481
Sample trajectories:
BrCCNc1cccc(Br)c1
Brc1ccc(-c2nc3ncccc3s2)cc1
Brc1ccc(-n2ncnn2)s1
Brc1ccc(Br)cc1
Brc1ccc(C(=CCSc2ncncc2Br)n2ccnc2)cc1

  3 Training on 405 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.692166
Reward: 1.131414
Trajectories with max counts:
5	COc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.026808701
Proportion of valid SMILES: 0.7049436795994993
Sample trajectories:
BrC1=NN=C(c2ccccc2)Oc2ccccc21
BrCc1ccccc1-c1nccc(Nc2ccccc2)n1
Brc1ccc(-c2ccnc(Nc3ccc(CN4CCCC4)s3)c2)cc1
Brc1ccc(-c2nc3ccccc3s2)cc1
Brc1ccc(C2CCCCC2CCNCCN2CCCCC2)cc1
Policy gradient replay...
Mean value of predictions: 0.04448996
Proportion of valid SMILES: 0.7630509534229447
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(O)C=CC(=O)C(I)=CP(=O)(O)O
BP(=O)(OCCCCCCOCCO)C(O)CO
BrCCCOc1ccccc1Nc1ncnc2ccc(N3CCN(Cc4ccccc4)CC3)nc12
Brc1ccc(C=Cc2ccccc2)cc1
Brc1ccc(CN2CCN(Cc3ccccc3)CC2)cc1
Fine tuning...
Mean value of predictions: 0.045098037
Proportion of valid SMILES: 0.7657178604942133
Sample trajectories:
Brc1cc2ccccc2s1
Brc1ccc(N=NNc2ccc(Br)cc2)cc1
Brc1ccc(NN=Nc2ccc3ccccc3c2)cn1
Brc1ccc(Nc2nccc(-c3ccc4cnccc4c3)n2)cc1
Brc1ccc(Nc2ncnc3[nH]cc(Br)c23)cc1

  4 Training on 815 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.456278
Reward: 1.159296
Trajectories with max counts:
9	COc1ccccc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Mean value of predictions: 0.05230639
Proportion of valid SMILES: 0.7384375
Sample trajectories:
BrC1=Nc2sc(Br)cc2Nc2ccccc2Nc2ccccc2C(=Nc2ccccc2Br)S1
BrCCCCCCCc1ccc2c(n1)Nc1cnccc1-2
BrCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(-c2ncccn2)c2cccnc2c1
Brc1ccc(-c2cc(-c3ccncc3)c3ccnc(Nc4ccc5ccccc5n4)c3n2)cc1
Policy gradient replay...
Mean value of predictions: 0.09935484
Proportion of valid SMILES: 0.6783369803063457
Sample trajectories:
BP(=O)(c1ccc(Br)cc1)C(O)c1cccc(O)c1
Bc1ccccc1Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrC=CBr
BrCc1ccc[n+]2c1nc1cc(Nc3ccccc3)c(Br)cc1c(Nc1ccc(Br)s1)c1ccc(Br)cc1CC2
Brc1cc(-c2ncnc3cscc23)c2ccccc2n1
Fine tuning...
Mean value of predictions: 0.09921982
Proportion of valid SMILES: 0.6811503594873398
Sample trajectories:
BrCCCn1cncn1
BrI
Brc1cc(Nc2c(Br)cc3ccc4ncc(Br)c(Nc5ccccc5)nc4c23)ccn1
Brc1ccc(-c2ccc3[nH]cnc3n2)s1
Brc1ccc(-c2ccccc2)o1

  5 Training on 1629 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.351938
Reward: 1.445312
Trajectories with max counts:
20	COc1ccc(Nc2ncnc3ccccc23)cc1
20	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.11672224
Proportion of valid SMILES: 0.6563477173233271
Sample trajectories:
BrCCCCBr
BrCn1ccc(Nc2nc3ccccc3nc2-c2nc3ccccc3c3ccccc23)c1
Brc1cc(-c2ccccc2)c2cncnc2n1
Brc1ccc(C=Nc2ncnc3ncncc23)cc1
Brc1ccc(I)cc1Nc1ccc(I)cc1
Policy gradient replay...
Mean value of predictions: 0.12516432
Proportion of valid SMILES: 0.665625
Sample trajectories:
BrCCBr
BrCCCCCCCCCCCCCCCCCCC=Cc1ncccc1Nc1ccc2ncnc(Nc3ccccc3)c2c1
Br[n+]1ccccc1Nc1ccccc1
Brc1ccc(-c2ccccc2)c2sccc12
Brc1ccc(-c2cccnc2)c2ccncc12
Fine tuning...
Mean value of predictions: 0.12380507
Proportion of valid SMILES: 0.6670834635823695
Sample trajectories:
BP(=O)(N=C(N)c1ccc(O)cc1)OCC
BP(=O)(OCC1OC(=O)C=C(O)C1O)Oc1ccc(Br)c(Br)c1
BP(=O)(OCCC)c1cccc(Nc2ncnc3ccccc23)c1
BrBr
BrC(=NNc1nccs1)c1ccccc1

  6 Training on 2704 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 18.883114
Reward: 1.774113
Trajectories with max counts:
105	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.21289963
Proportion of valid SMILES: 0.5573616755236012
Sample trajectories:
BP(=O)(Br)Oc1ccc(-c2cc3c(cc2O)N=C(S)N3)cc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1OP(=O)(O)O)C(O)CP(=O)(O)O
BP(=O)(OCC=Nc1cncc(Nc2ccc(Br)cc2)n1)c1ccccc1
BP(=O)(Oc1ccccc1)Oc1ccccc1Cl
BrCNP1c2ccccc2-c2ccccc21
Policy gradient replay...
Mean value of predictions: 0.2546214
Proportion of valid SMILES: 0.5986245701781807
Sample trajectories:
BP(=O)(C=O)OCCOCCOCCOCCOCCOCCOCCOCCO
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccsc12
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)cc1Br
Brc1cc2ccccc2s1
Fine tuning...
Mean value of predictions: 0.25205916
Proportion of valid SMILES: 0.5924304035032844
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)NS(=O)(=O)c1ccccc1
Br
Brc1ccc(-c2c[nH]cn2)cc1
Brc1ccc(-c2ccc3ncnc(Nc4ccccc4Br)c3c2)s1
Brc1ccc(-c2cnc(Nc3ccc4ccccc4c3)nc2)cc1

  7 Training on 4270 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 21.370285
Reward: 2.543344
Trajectories with max counts:
198	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.18214494
Proportion of valid SMILES: 0.5390625
Sample trajectories:
BP(=O)(CCCCCC)OCC
BP(=O)(OCCOCCOCc1ccccc1)c1ccc(Br)cc1
Bc1ccccc1Nc1ncccn1
Brc1cc(Nc2ccccc2Nc2cccnc2-c2ccccc2Br)ccc1-c1ccccc1
Brc1ccc(-c2ccccc2Br)c(Nc2ccncc2)c1
Policy gradient replay...
Mean value of predictions: 0.31173974
Proportion of valid SMILES: 0.44549464398235666
Sample trajectories:
BP(=O)(OCC)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC1CCCCC1)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BrC=Cc1nc(Nc2ccc(Br)cc2)c2cc(Br)cc(Br)c2n1
Brc1cc(Br)c2c(Nc3cc(Br)ns3)ncnc2c1
Brc1cc(Br)c2c(Nc3ncc(Br)s3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.30613667
Proportion of valid SMILES: 0.4510852469329978
Sample trajectories:
BP(=O)(COP(=O)(O)O)OCC(=O)C(F)(F)F
BP(=O)(COP(=O)(O)Oc1cc(Br)cc(Br)c1O)OCCO
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(c1)-c1sc(Br)cc1N2

  8 Training on 5572 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 22.845565
Reward: 2.626472
Trajectories with max counts:
63	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.35457334
Proportion of valid SMILES: 0.5100187852222918
Sample trajectories:
BP(=O)(OCC)OC(=O)C(Cl)(Cl)Sc1ccc(Nc2nc3cc(Br)ccc3s2)cc1
BP(=O)(OCCCCCCC)OC(=O)NCC(=O)NCCO
BP(=O)(ONC(=O)CBr)Oc1c(F)cc(F)c(F)c1F
Bc1ccc(Nc2ncnc3ccnc(Br)c23)cc1
BrCCCCSc1ccnc(Nc2ncnc3ccc(Br)c(Br)c23)c1
Policy gradient replay...
Mean value of predictions: 0.2879552
Proportion of valid SMILES: 0.5604395604395604
Sample trajectories:
BP(=O)(NC(CCCN)C(=O)Nc1ccc(Oc2ccc(F)cc2F)cc1)C(F)(F)P(=O)(O)O
BrCCBr
BrCCCCCC[N+]12CCCCC1CC2
BrSc1ccc(Nc2ncnc3scc(-c4ccccc4)c23)cc1
Brc1cc(-c2ncnc3c2c(Br)cc2scnc23)cs1
Fine tuning...
Mean value of predictions: 0.30541596
Proportion of valid SMILES: 0.5614420062695925
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC(F)F)CC(=O)NCCC(=O)O
BP(=O)(OCCOCCCCOP(=O)(O)OP(=O)(O)CBr)OC(=O)CCC=CCCCCCCCCCC
B[PH](=O)(=Nc1ccc(F)c(F)c1F)Nc1ccc(Br)s1
Bc1ccc(Nc2ncnc3scc(Br)c23)cc1
BrCCOc1ccc(Nc2ncnc3sc(Nc4ccc(Br)s4)nc23)cc1

  9 Training on 7271 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 23.734783
Reward: 2.716735
Trajectories with max counts:
33	Nc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.32387656
Proportion of valid SMILES: 0.5773679274773367
Sample trajectories:
BP(=O)(OCCC1CCCCC1)Oc1ccc(Cl)cc1
B[PH](=O)(NC(c1ccccc1)c1ccccc1)(P(=O)(O)O)P(=O)(O)O
Bc1ccc(Nc2ncnc3sc4ccccc4c23)cn1
Bc1ccc(Nc2nnc(Nc3ccsc3)s2)cc1
BrCc1ccc(Nc2ccc3ncccn23)cc1
Policy gradient replay...
Mean value of predictions: 0.13371228
Proportion of valid SMILES: 0.7153581482639975
Sample trajectories:
BP(=O)(OCC)c1ccccc1
BP(=O)(c1ccccc1)N(c1ccccc1)c1ccccc1
BrC(=NNc1ccc(Nc2ncnc3cc(Br)c(Br)nc23)cc1)c1ccccc1
BrC=CCc1c(Nc2ccccc2Br)ncc2ccccc12
BrCCCNc1cccc(Nc2ncnc3ccccc23)c1
Fine tuning...
Mean value of predictions: 0.1337158
Proportion of valid SMILES: 0.708125
Sample trajectories:
BP(=O)(Oc1ccccc1)c1ccccc1
Bc1ccc(Nc2ccc(Cl)cc2)cc1
Bc1ccccc1Nc1ccc(Nc2ncnc(-c3cccs3)n2)cc1
BrBr
Brc1cc2cc(-c3ccccc3Br)[nH]c2cc1Br

 10 Training on 8409 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 25.157845
Reward: 3.095663
Trajectories with max counts:
103	Fc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.39681882
Proportion of valid SMILES: 0.45201625507971244
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)c1ncc(Br)s1
BP(=O)(OCC)Oc1ccc(Br)cc1
Bc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.21088746
Proportion of valid SMILES: 0.6833385432947796
Sample trajectories:
BP(=O)(OCC)c1cccc(Nc2ccccc2Br)c1
BrCCSc1ccccc1Nc1ccccc1Br
Brc1cc(Nc2ccccc2-c2ccccc2-c2ccccc2Br)ncc1Nc1ncnc2ccccc12
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Nc1c2ccccc2nc2ccccc12
Fine tuning...
Mean value of predictions: 0.20513977
Proportion of valid SMILES: 0.6933416692716474
Sample trajectories:
BP(=O)(OCC)Oc1ccc(I)cc1
BrCCCC=CC=CC=CC=CC=CC=Nc1ncnc2ncnc(Nc3ccccc3)c12
BrCCCCCCCSc1ccccc1
Brc1ccc(Br)c(-c2c(-c3ccccc3Br)sc3ccc(Br)cc23)c1
Brc1ccc(Br)c(Nc2ncnc3sc4c(c23)CCCCC4)c1

 11 Training on 9833 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 23.509788
Reward: 3.042830
Trajectories with max counts:
80	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.3964997
Proportion of valid SMILES: 0.5179743669896842
Sample trajectories:
BP(=O)(Br)C(Br)=C(Br)Br
BP(=O)(C=C(C)Cc1ccc(Br)cc1)NO
BP(=O)(C=CCC=CCC=CC=CCCC(=O)O)OCC
BP(=O)(NO)c1ccc(F)cc1F
BP(=O)(OCC)OC(=O)CCCCCCCCCCCN
Policy gradient replay...
Mean value of predictions: 0.22111002
Proportion of valid SMILES: 0.630625
Sample trajectories:
BP(=O)(C(=O)Nc1cc(Br)c(Br)s1)N(O)C=O
BP(=O)(OCC)OCCC=CCCCCCCC(=O)O
Bc1ccccc1Nc1ccccc1Nc1ccc(Br)cc1
BrC(Br)(Br)Br
BrP1c2ccccc2-c2ccccc21
Fine tuning...
Mean value of predictions: 0.22062376
Proportion of valid SMILES: 0.62125
Sample trajectories:
BP(=O)(Nc1ccc(Nc2ccccc2-c2ccccc2)cc1)OCC
BP(=O)(Nc1cccc(Br)c1)c1cccnc1
BP(=O)(OCC)N(C)C(=O)CSc1ccc(Nc2ncnc3sc(Cl)cc23)c(F)c1
BP(=O)(OCC)Oc1ccc(Nc2ncnc3sc(Br)cc23)cc1
BP(=O)(Oc1ccccc1)Oc1ccc(Br)cc1

 12 Training on 10855 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 22.980803
Reward: 3.303139
Trajectories with max counts:
242	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.42188495
Proportion of valid SMILES: 0.39137230384495153
Sample trajectories:
BP(=O)(NC(=O)OC(C)(C)C)OCCCSC(=O)C(CCC(=O)OCC)N(C=O)C(C)C
BP(=O)(NO)OCC
BP(=O)(Nc1sc2c(c1F)CCC2C(=O)O)N(=O)=O
BP(=O)(O)C(F)(F)F
BP(=O)(O)CCCCC=CCCC
Policy gradient replay...
Mean value of predictions: 0.37939528
Proportion of valid SMILES: 0.5586487331873632
Sample trajectories:
B[PH]1(=O)=C(N)NC(OC(=O)NC(=O)Nc2c(N(=O)=O)ncc(Br)c2Br)CO1
Bc1ccc(Nc2ncnc3sccc23)cc1
Br
BrC=CCCBr
Brc1cc(Br)cc(-c2ncncc2Nc2cccc(-c3cc(Br)[nH]c3Br)c2)c1
Fine tuning...
Mean value of predictions: 0.3736756
Proportion of valid SMILES: 0.5723663644889028
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCC(=O)Oc1c(Br)sc(Nc2ccc(Br)c(Br)c2)c1I
BP(=O)(OCCOc1ccc(Br)c(Br)c1)c1ccccc1
B[PH](P(=O)(Oc1ccccc1)Oc1ccccc1)=S(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)CCl
Bc1ccc(Br)cc1-c1ccccc1Br
Br

 13 Training on 12127 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.014028
Reward: 3.542591
Trajectories with max counts:
118	Brc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Mean value of predictions: 0.5001441
Proportion of valid SMILES: 0.434021263289556
Sample trajectories:
BP(=O)(C=C(Br)CBr)OCC
BP(=O)(CCS)CCCCCO
BP(=O)(Nc1cc(Br)c(Br)cc1Br)OCC
BP(=O)(OCC)OC(=O)C=CC(=O)Nc1ccc(Br)cc1
BP(=O)(OCC)OC(=O)Nc1cccc(Nc2ncnc3scc(Cl)c23)c1
Policy gradient replay...
Mean value of predictions: 0.26952142
Proportion of valid SMILES: 0.6203125
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
BP1(=O)OCC(O)(CC(=O)OCC2(C)CCC(C=C)C(=O)O2)C(C(=O)OCC(=O)O)O1
BrC=CC=CC=CBr
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(Br)c(-n2cnnc2Nc2ccc(-c3ccccc3Br)cc2)cc1Br
Fine tuning...
Mean value of predictions: 0.27041817
Proportion of valid SMILES: 0.6053125
Sample trajectories:
Bc1ccc(Nc2ncnc3ccccc23)cc1
Bc1ccccc1-c1cccc(Nc2ncnc3ncnc(Nc4ccccc4)c23)c1
BrC=CBr
BrCCOc1ccccc1Nc1ncc2ccccc2c1-c1ccc2ncnc(Nc3ccccc3)c2c1
BrSc1cccc(Nc2ncnc3cc(Br)ccc23)c1

 14 Training on 13341 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.706070
Reward: 4.049183
Trajectories with max counts:
272	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.45306122
Proportion of valid SMILES: 0.3215625
Sample trajectories:
B#Cc1ccc(Nc2ncnc3ccsc23)cc1
BP(=O)(CCCCC(=O)O)OCC
BP(=O)(Nc1ncc(Br)s1)c1cccc(Nc2ncncc2Br)c1
BP(=O)(O)C=C(Oc1ccccc1)c1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BP(=O)(OC(C)C)N(Cc1ccc2ccccc2c1)C(=O)OCc1cc(Br)c(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.43576685
Proportion of valid SMILES: 0.5523601125351673
Sample trajectories:
BP(=O)(C=C(Cl)Cl)OCC
BP(=O)(NC(CCCNS(=O)(=O)CC(N)C(=O)O)Oc1ccccc1)c1cccc(Nc2cc(Br)cnc2F)c1
BP(=O)(OCC)C(=O)Oc1cc(Br)c(Br)cc1Br
BP(=O)(OCC)C(=O)Oc1ccccc1-c1ccccc1
BrC=CC=CCCCCCCCCCBr
Fine tuning...
Mean value of predictions: 0.40726858
Proportion of valid SMILES: 0.5504845264145045
Sample trajectories:
BP(=O)(OCC)C(=O)Oc1ccc(Br)cc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)O
Bc1cc2c(s1)-c1c(ncnc1-c1ccccc1)N2
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2sc(Sc3ccccc3)cc12
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1

 15 Training on 14745 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.113399
Reward: 4.166184
Trajectories with max counts:
372	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.48469892
Proportion of valid SMILES: 0.3165625
Sample trajectories:
BP(=O)(OCC)OC(=O)CSCCCCCCCCCCCCCCCCCCCCCNC(=O)CCCCCCCCCCCCCCCCCC(Br)Br
BP(=O)(OCC)OCC
BP(=O)(OCC1OC(=O)C=C(Br)C1O)c1ccc(Br)cc1
BP(=O)(OCCO)c1ccc(Br)c(Br)c1
BP(=O)(Oc1ccccc1N(=O)=O)C(=O)OCCl
Policy gradient replay...
Mean value of predictions: 0.31564626
Proportion of valid SMILES: 0.6433260393873085
Sample trajectories:
BP(=O)(OCC(F)F)c1nc2ccccc2o1
BP(=O)(OCC)OC(=O)COC(=O)CCCCCCl
BrC=CC=CCCCBr
BrC=Cc1ccccc1Br
BrCCOc1ccccc1Nc1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.3107963
Proportion of valid SMILES: 0.6396875
Sample trajectories:
BP(=O)(Nc1cccc(Br)c1)C(=O)Nc1ccc(-c2ccncc2)c(-c2ccccc2)c1
Br
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccccc12
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Brc1cc(Nc2ncnc3sccc23)ccc1-c1ccccn1

 16 Training on 16010 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.422014
Reward: 4.611635
Trajectories with max counts:
473	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49410504
Proportion of valid SMILES: 0.2915625
Sample trajectories:
BP(=O)(NC(c1ccc(Br)c(Br)c1)(P(=O)(O)O)P(=O)(O)O)c1cccc(Nc2c(Br)ncc(Br)c2Br)c1
BP(=O)(Nc1ccccc1-c1ccccc1)OCC
BP(=O)(O)P(=O)(O)O
BP(=O)(OCC)OC(=O)c1ccc(Br)cc1
B[PH](=O)(NC(=O)OCc1ccccc1)(Nc1ccc(Nc2ccc(Br)cc2)cc1)c1cccc(N)c1
Policy gradient replay...
Mean value of predictions: 0.42942297
Proportion of valid SMILES: 0.49389289069840275
Sample trajectories:
BrC(Br)=NNc1ccc(Br)cc1
BrC(C=Cc1sc(Br)cc1Nc1ncnc2ccc(Br)cc12)=CC1=CCCC1
BrCCBr
BrSc1ccccc1-c1ccc(Nc2ncnc3sc(Sc4ccccc4Br)nc23)cc1
Brc1cc(Br)c(-c2c(Br)c(Br)cc3ncnc(Nc4cc(Br)c(Br)s4)c23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.43895423
Proportion of valid SMILES: 0.4788732394366197
Sample trajectories:
BP(=O)(OP(=O)(O)O)C(Br)CCCCCBr
BP1(=O)OCC2OC(C(O)C2O)N(C=C(Br)CBr)C(=O)NC1=O
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrC=C(Br)Br
BrCCNc1ccc(Nc2nc3ccccc3nc2-c2ccnc(Nc3ccc(Br)cc3)n2)cc1

 17 Training on 17337 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.349154
Reward: 4.732548
Trajectories with max counts:
365	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.47789854
Proportion of valid SMILES: 0.345
Sample trajectories:
BP(=O)(COCCCF)Nc1ccccc1-c1ccccc1
BP(=O)(Nc1ccc(F)cc1)OCC
BP(=O)(O)CCC(SSc1ccc(Br)cc1)C(=O)O
BP(=O)(O)OP(=O)(BO)Oc1ccc(Br)cc1
BP(=O)(OCC1OC(=O)C=CC1=O)Oc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.18081254
Proportion of valid SMILES: 0.6412429378531074
Sample trajectories:
BP(=O)(CCCl)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCl
BP(=O)(OC)OCCCCCCCCCCCO
BrCCNc1ccccc1-c1ccccc1Nc1ccccc1
BrCN1CCCCCC1Nc1ccccc1
Brc1cc(Br)c(-c2c(-c3ccccc3)sc3c2c(-c2ccccc2-c2ccccc2Br)nc2ccccc23)cn1
Fine tuning...
Mean value of predictions: 0.17896268
Proportion of valid SMILES: 0.6471141781681304
Sample trajectories:
BP(=O)(OCC1=C(Br)C(=O)c2c(F)cccc2-c2ccccc21)P(=O)(Oc1ccccc1)c1ccccc1
Bc1ccccc1Nc1ccccc1-c1ccccc1S(=O)(=O)c1ccccc1
BrCBr
BrCCCBr
BrCCCCCCCCCCNc1ccc(Nc2cccc3ncncc23)cc1

 18 Training on 18228 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.628276
Reward: 4.903655
Trajectories with max counts:
386	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.55581397
Proportion of valid SMILES: 0.3228026274632468
Sample trajectories:
BP(=O)(C(=O)Oc1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1)N(CCCl)SCCCCl
BP(=O)(NCCO)c1ccccc1
BP(=O)(O)CCCCCCCCCCCCCCCCCCCCC(C)Br
BP(=O)(OCC)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCC)c1ccccc1-c1cc(Br)nc(Nc2ncnc3ccsc23)c1
Policy gradient replay...
Mean value of predictions: 0.37013996
Proportion of valid SMILES: 0.605271415123941
Sample trajectories:
BrC#CC=CC#CCCCNc1ccccc1Nc1ccccc1
BrCCCCCCCC=CC=CCCC=CCC=C(Br)Br
BrCCCCCCCCCCCC=CC=CCCCCCCCCCCCCC=CC=CCC=CCCCCCCCCCCCCC=CCC=Cc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(Nc2ncnc3ccccc23)ccc1Nc1ccc(Nc2ncnc3ncnc(Nc4ccccc4)c23)nc1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.36389858
Proportion of valid SMILES: 0.6294209702660407
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCCCCCCCBr
Bc1cccc(Br)c1Nc1ccncc1
BrC=CC=Cc1ccccc1Nc1ccccc1-c1ccccc1SN1CCCC1
Brc1cc(Br)c2c(Nc3cccc(NN=Cc4nc5ccccc5nc4Nc4ccccc4I)c3)ncnc2c1
Brc1cc(I)cnc1Nc1ncnc2ccccc12

 19 Training on 19710 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.607100
Reward: 5.007267
Trajectories with max counts:
309	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.50929075
Proportion of valid SMILES: 0.31291028446389496
Sample trajectories:
BP(=O)(COP(=O)(O)Oc1ccccc1)Oc1ccccc1
BP(=O)(NC(c1ccccc1)c1ccc(Br)cc1)P(=O)(O)OP(=O)(O)O
BP(=O)(Nc1ccc(Br)cc1)Nc1ccc(Br)cc1
BP(=O)(Nc1ccncc1)c1ccc(Br)cc1
BP(=O)(OCC)C(=O)Nc1cc(Br)c(Br)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.3875
Proportion of valid SMILES: 0.5963044159098027
Sample trajectories:
BP(=O)(Nc1cccc(F)c1)c1ccc(Br)cc1
Bc1ccc(Nc2cc3c(Br)c(Nc4ccc(Br)cc4Br)c(Br)cc(Br)c3ncn2)cn1
Bc1ccccc1-c1ccccc1Nc1cccc2ccccc12
Brc1cc(Br)c2c(Nc3ccc(-c4cc(Br)c(Br)cc4Br)cc3)ncnc2c1
Brc1cc(Br)c2c3ncnc(ccc(Br)c2c1)c1c(Br)cccc1N3
Fine tuning...
Mean value of predictions: 0.3981595
Proportion of valid SMILES: 0.6120150187734669
Sample trajectories:
BP(=O)(C(Cl)(Cl)P(=O)(O)O)P(=O)(O)O
BP(=O)(OCC)Oc1ccc(Nc2nc(Br)nc3ccsc23)cc1
Bc1ccc(Nc2cc(Br)ccc2Br)cc1Br
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccccc12
Brc1cc(Br)c(Br)c(-c2ccccc2Br)c1

 20 Training on 21185 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.793370
Reward: 4.771473
Trajectories with max counts:
325	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5675019
Proportion of valid SMILES: 0.4119487019080388
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)NC1CCCCC1
BP(=O)(OCC1OC(Oc2ccc(Br)cc2)C(O)C1O)c1cc(Br)cc(Br)c1
Bc1cc(Br)c(Br)cc1Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Bc1ccc(Nc2ncnc3ccsc23)cc1Br
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.4947305
Proportion of valid SMILES: 0.516260162601626
Sample trajectories:
BP(=O)(CCC=C(F)F)OCC
BP(=O)(OCC)OCC=C
Bc1ccccc1-c1cc(Nc2ncnc3ccsc23)nc2scnc12
BrC#Cc1ccc(Nc2ncccn2)cc1-c1ccc(Br)cc1
BrCCBr
Fine tuning...
Mean value of predictions: 0.50704575
Proportion of valid SMILES: 0.5057830572053766
Sample trajectories:
BP(=O)(Oc1cccc(Nc2ncnc3scc(F)c23)c1)C(F)F
BrCc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCc1nc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
BrCc1ncnc2c(Nc3ccc(Br)cc3)ncnc12

Trajectories with max counts:
654	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.44852707
Proportion of valid SMILES: 0.4393522570964112
Mean Internal Similarity: 0.47010952226741626
Std Internal Similarity: 0.10010418580522204
Mean External Similarity: 0.4083171611582476
Std External Similarity: 0.06677504917284699
Mean MolWt: 412.4502013377928
Std MolWt: 106.07951212432222
Effect MolWt: -0.8307927363949068
Mean MolLogP: 5.661244652173915
Std MolLogP: 1.7638388849263549
Effect MolLogP: 0.5914652729195189
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.703549% (936 / 958)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 0, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5486.744393825531, 'valid_fraction': 0.4393522570964112, 'active_fraction': 0.4255016365447559, 'max_counts': 654, 'mean_internal_similarity': 0.47010952226741626, 'std_internal_similarity': 0.10010418580522204, 'mean_external_similarity': 0.4083171611582476, 'std_external_similarity': 0.06677504917284699, 'mean_MolWt': 412.4502013377928, 'std_MolWt': 106.07951212432222, 'effect_MolWt': -0.8307927363949068, 'mean_MolLogP': 5.661244652173915, 'std_MolLogP': 1.7638388849263549, 'effect_MolLogP': 0.5914652729195189, 'generated_scaffolds': 958, 'novel_scaffolds': 936, 'novel_fraction': 0.9770354906054279, 'save_path': '../logs/replay_ratio_s2-3.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.767311
Reward: 1.000000
Mean value of predictions: 0.0005568815
Proportion of valid SMILES: 0.7885821831869511
Sample trajectories:
Brc1ccc(-n2nnc3ccncc32)s1
Brc1ccc(CCCN2CCC3(CCCC3)CC2)cc1
Brc1ccc(COc2cccnc2Oc2ccccc2)cc1
Brc1ccc(CSc2nnc(C3CCCC3)n2C2CC2)cc1
C#CCC(Cc1ccc(O)cc1)NC(=O)NCCCNCCCCCCN
Policy gradient replay...
Mean value of predictions: 0.011343611
Proportion of valid SMILES: 0.568922305764411
Sample trajectories:
BP(=O)(Cc1ccccc1)P(=O)(O)O
Brc1ccc2cc1OC1(CC1)c1ncnc(c3ncncc3n1)N2
Brc1cnc2c(Br)cc(N3CCCC3)nc2c1
Brc1nccc2ncnc(NC3CC4COCCN4C3)c12
C#CC(CC)C1CCC2C3CC=C4CC(CO)C(O)CCC45CC(O)CCC5(C)C3CCC12C
Fine tuning...
Mean value of predictions: 0.009967498
Proportion of valid SMILES: 0.5783208020050126
Sample trajectories:
Brc1cc2c(s1)N(CC1CCn3nncc31)CCC2
Brc1ccc(N2CCCNc3ncnc(-c4cc5c(c4CCCCNc4c6cnccc6nc6ncccc46)c4c(nc6c4CCC6)CO5)c3N2)cc1
Brc1ccc2[nH]c3c(NCCN4CCCC4)nc(Nc4ccccn4)nc3c2c1
Brc1ccc2[nH]cnc2c1
C#CC#CCN(CC(=O)O)Cc1cnc2cc(Cl)ccc2c1

  2 Training on 278 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.972132
Reward: 1.010890
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.00745543
Proportion of valid SMILES: 0.5793427230046948
Sample trajectories:
Brc1ccc2[nH]cc(-c3cncnc3)c2c1
Brc1ccc2[nH]cnc2c1
Brc1cnc(NCCN=Cc2ccncc2)nc1C1CCCCN1
Brc1cnc2[nH]nc(-c3ccc4ncccn34)c2c1
Brc1cnc[nH]1
Policy gradient replay...
Mean value of predictions: 0.013953489
Proportion of valid SMILES: 0.725625
Sample trajectories:
BrC(=Cc1ccc(I)cc1)Cc1ccccc1
Brc1ccc(Nc2cc3ccccc3c3ccccc23)cc1
Brc1ccc(Nc2nc(-c3cscn3)nc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)c(Br)c1
Brc1ccc2[nH]cc(-c3ccccc3)c2c1
Fine tuning...
Mean value of predictions: 0.012131568
Proportion of valid SMILES: 0.7317911847452329
Sample trajectories:
BrCC1CC(c2ccc3c(c2)OCCO3)N(Cc2ccccc2)C1
BrCc1cccnc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Brc1ccc(Nc2ccc3ccncc3c2)cc1
Brc1ccc(Nc2ncnc3ccccc23)o1
Brc1ccc2c(Nc3ccc(OCc4ccccc4)cc3)nc2c1

  3 Training on 395 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.331731
Reward: 1.064325
Trajectories with max counts:
4	Nc1nc(Nc2ccccc2)c2ccccc2n1
4	O=C(Nc1ccccc1)c1ccccc1
4	O=C1Nc2ccccc2O1
4	Oc1cccc2ccccc12
Mean value of predictions: 0.017286085
Proportion of valid SMILES: 0.7233510472022507
Sample trajectories:
Brc1ccc(C(=Nc2cccnc2)c2cccnc2)cc1
Brc1ccc(C=NNc2nccn2-c2ccccc2)cc1
Brc1ccc(C=NNc2nnc3ccc(Br)cc3c2Br)nc1
Brc1ccc(Nc2cccnc2)cc1
Brc1ccc(Nc2ncnc3c4ccccc4c23)cc1
Policy gradient replay...
Mean value of predictions: 0.033027522
Proportion of valid SMILES: 0.5819723618090452
Sample trajectories:
Brc1ccc(-n2cnc3cc(OCc4ccc(Nc5ccncc5)ncnc4)ccc32)cc1
Brc1ccc(C2CC3=Nc2nc2ccc(I)cc23)cc1
Brc1ccc(Nc2nc(CN3CCN(c4cccs4)CC3)nc3ccccc23)cc1
Brc1ccc(Nc2nc(Nc3cccc(Br)c3)nc(N3CCNCC3)n2)cc1
Brc1ccc(Nc2ncc(Nc3ccc(Br)c(C=NN4CCCC4)c3)cn2)cc1
Fine tuning...
Mean value of predictions: 0.033117585
Proportion of valid SMILES: 0.582286432160804
Sample trajectories:
Brc1ccc(-c2ncnc(Nc3nc(Br)cs3)n2)cc1
Brc1ccc(C=NNc2cccc(I)c2)cc1
Brc1ccc(CNc2cncnc2)cc1
Brc1ccc(N=Nc2ccc(Br)cn2)cc1
Brc1ccc(NN=Cc2ccnc(Nc3cccnc3)c2)cc1

  4 Training on 640 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 20.812124
Reward: 1.387472
Trajectories with max counts:
4	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0814562
Proportion of valid SMILES: 0.5512699905926622
Sample trajectories:
Brc1cc2ncnc(Nc3ccc(Br)s3)n2n1
Brc1ccc2nc(-n3ncc4c3N=Nc3ccccc34)[nH]c2c1
Brc1ccc2nc(C3CCN(CC4CC4)CC3)[nH]c2c1
Brc1ccc2nnc(Sc3ncccc3-c3ccccc3)n2c1
Brc1cnc2c(c1)nc(N=Cc1nc3ccccc3s1)n2Cc1ccsc1
Policy gradient replay...
Mean value of predictions: 0.0953186
Proportion of valid SMILES: 0.4831919572730129
Sample trajectories:
BP(=O)(OCC)OC(=O)c1sc(Br)c(Br)c1Br
Brc1cc(Nc2[nH]c2-c2cccs2)c2cccnc2n1
Brc1cc2c(cc1I)N=C2NN=Cc1ccccc1
Brc1ccc(Nc2nc(-c3ccsc3)cs2)nc1
Brc1ccc(Nc2nc(-c3ncccn3)nc3ccncc23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.0786711
Proportion of valid SMILES: 0.4720828105395232
Sample trajectories:
BP(=O)(OCCCCC)C1=C(Br)P(=O)(O)OP(=O)(O)O1
Brc1cc2ncnc(Nc3ccccn3)c2cc1Br
Brc1ccc(C=C(c2ccc3ccccc3n2)n2ccnc2)cc1
Brc1ccc(Nc2ccc(OCc3ncc(Br)cn3)cc2)cc1
Brc1ccc(Nc2ccnc(N3CCCCC3)n2)cc1

  5 Training on 1231 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 21.469499
Reward: 1.447148
Trajectories with max counts:
4	COc1cc2ncnc(Nc3ccc(F)c(F)c3)c2cn1
4	COc1cc2ncnc(Nc3ccc(F)cc3)c2cc1OC
4	COc1cc2ncnc(Nc3ccc(F)cc3)n2n1
Mean value of predictions: 0.09592944
Proportion of valid SMILES: 0.4619241617047947
Sample trajectories:
Brc1cc(Br)c2c(c1)OCO2
Brc1cc2ncnc(Nc3ccsc3)c2cc1-c1ccncc1
Brc1ccc(-n2cnc3cc(Br)ncc32)cc1
Brc1ccc(NN=C2Nc3cc(Br)ccc32)cc1
Brc1ccc(Nc2nc3ccc(Br)cc3s2)cc1
Policy gradient replay...
Mean value of predictions: 0.09353575
Proportion of valid SMILES: 0.638125
Sample trajectories:
Brc1cc2cc(N=C3C=CCC3)ccc2s1
Brc1cc2ncnc(Nc3ccncc3)c2cc1Br
Brc1ccc(-n2ncc3ccc(Br)cc32)c(Br)c1
Brc1ccc(Nc2nc3ccccc3c3scnc23)cc1
Brc1ccc(Nc2nccc(Nc3ccccc3-c3ccccc3)n2)cc1
Fine tuning...
Mean value of predictions: 0.08204873
Proportion of valid SMILES: 0.6288305190744216
Sample trajectories:
Brc1ccc(-c2cc3ccc4ccccc4c3nc3ccccc3n2)nc1
Brc1ccc(Nc2ccccc2)c(-c2nc3ccc(Br)cc3c3ccccc23)c1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc2[nH]c(-c3ccc4cccnc4c3)nc2c1
Brc1ccc2c(Nc3ccccc3)ncnc2c1

  6 Training on 1918 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 22.018523
Reward: 1.842106
Trajectories with max counts:
23	COc1ccc2ncnc(Nc3ccc(F)cc3)c2c1
23	COc1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.15477273
Proportion of valid SMILES: 0.5501719287277275
Sample trajectories:
Brc1cc2ncnc(-c3ccncn3)c2s1
Brc1ccc(Nc2ccc(Nc3ncnc4ccc(I)cc4s3)cc2)cc1Br
Brc1ccc(Nc2cnccn2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(OCc4ccccc4)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ncc23)cc1
Policy gradient replay...
Mean value of predictions: 0.08078147
Proportion of valid SMILES: 0.6878125
Sample trajectories:
BP(=O)(NCCP(=O)(O)O)C(=O)O
BrCc1ccccc1N=Nc1ccccc1
Brc1cN(N=Nc2ccccc2)N1C1=Nc2ccc(Nc3nc4ccc(Br)cc4s3)cc21
Brc1ccc(-c2ncccn2)s1
Brc1ccc(Nc2cccc3ncccc23)cc1
Fine tuning...
Mean value of predictions: 0.07365567
Proportion of valid SMILES: 0.6915625
Sample trajectories:
BP(=O)(N(CC(=O)OC)P(=O)(O)O)P(=O)(O)O
BP(=O)(OCC1OC(n2c(Br)cnc2N)C(O)C1O)S(=O)(=O)Nc1ccccc1
Brc1ccc(Br)c(Nc2nccc(Nc3ccc4ccccc4c3)n2)c1
Brc1ccc(Nc2cc3c(Nc4ccccc4)ncnc3cn2)nc1
Brc1ccc(Nc2cccnc2)nc1

  7 Training on 2794 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 22.815569
Reward: 2.520575
Trajectories with max counts:
144	COc1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.15744439
Proportion of valid SMILES: 0.5478125
Sample trajectories:
BP(=O)(OCC)OC(=O)CP(=O)(O)O
BrC1=C(Oc2cncnc2)c2ccc(Br)cc2C1=Nc1ccnc(Nc2cccnc2)c1
Brc1ccc(Br)c(-c2ncccn2)c1
Brc1ccc(Nc2c(Br)ccc3nccnc23)cc1
Brc1ccc(Nc2cncnc2)cc1
Policy gradient replay...
Mean value of predictions: 0.09146522
Proportion of valid SMILES: 0.6334375
Sample trajectories:
Brc1c2ccccc2cc2ccccc12
Brc1cc2ncnc(Nc3ccccc3)n2c1
Brc1ccc(-c2cncnc2)c2ccccc12
Brc1ccc(-c2ncc(Nc3cccnc3)cc2-c2ccccc2Br)cc1
Brc1ccc(N=Nc2ccccc2Br)cc1
Fine tuning...
Mean value of predictions: 0.11000001
Proportion of valid SMILES: 0.6375
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1cc2ncnc(Nc3ccccc3)c2cc1Nc1ccccc1
Brc1ccc(Nc2ccccc2Br)cc1
Brc1ccc(Nc2cccnc2)cc1-c1cccc2ccnc(Nc3ccccc3)c12
Brc1ccc(Nc2ccnc(Nc3ccccc3)c2)cc1

  8 Training on 3798 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 21.618900
Reward: 2.693262
Trajectories with max counts:
81	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.2056513
Proportion of valid SMILES: 0.453579243513598
Sample trajectories:
BP(=O)(Oc1cc2cc(I)ccc2c2c(I)ncnc12)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
Brc1cc2c(Br)c(-c3ccccc3)c(-c3ccccc3)c2c1-c1cc2cnccc2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccc2ncnn2c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Policy gradient replay...
Mean value of predictions: 0.23309112
Proportion of valid SMILES: 0.5590625
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2c2c1N=CN(Cc1ccsc1)C=C2
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1ccc(Nc2ncnc3c(Br)ccc(Br)c23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)nc23)cc1
Fine tuning...
Mean value of predictions: 0.23683922
Proportion of valid SMILES: 0.5734375
Sample trajectories:
Brc1cc2ccccc2cc1Nc1ccncc1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Brc1ccc(-c2ccc3ncnc(Nc4ccccc4)c3c2)c(I)c1
Brc1ccc(-c2nc(-c3ccc(Br)cc3)c(-c3nccc4ccccc34)o2)cc1
Brc1ccc(-c2nc3nc(Nc4ccccn4)ncc3cc2-c2ccncc2Br)cc1

  9 Training on 5216 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 23.631619
Reward: 3.114674
Trajectories with max counts:
77	COc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.2403397
Proportion of valid SMILES: 0.58875
Sample trajectories:
BrCCSc1ccccc1-c1ccccc1
Brc1ccc(Br)c(Nc2ncnc3ccc(-c4ccccc4Br)cc23)c1
Brc1ccc(I)s1
Brc1ccc(Nc2ccc(-c3ccc(-c4ccccc4-c4ccccc4Br)c4cc(Nc5cccc(Br)c5)ccc34)cc2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.2918275
Proportion of valid SMILES: 0.5513141426783479
Sample trajectories:
BrCCBr
Brc1cc(Nc2ncnc3ccccc23)cc(Br)c1Br
Brc1ccc(-c2ccc3ncnc(Nc4ccccc4Br)c3c2)cc1
Brc1ccc(-c2ccncc2)c2scc(Nc3cccnc3)c12
Brc1ccc(-c2ncnc3ccccc23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.28483838
Proportion of valid SMILES: 0.560625
Sample trajectories:
Brc1cc(Br)c(Br)c(Nc2ncnc3ccccc23)c1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(I)c(-c2ccsc2)c1
Brc1ccc(Nc2cc3cc(Br)cc(Br)c3nc2-c2cncnc2)cc1
Brc1ccc(Nc2ncnc3cc(-c4ccccc4Br)sc23)cc1

 10 Training on 6824 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 23.216652
Reward: 3.681436
Trajectories with max counts:
224	COc1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.2594046
Proportion of valid SMILES: 0.461875
Sample trajectories:
BrC=C(Br)Br
Brc1cc2ccccc2nc1-c1ccncc1
Brc1ccc(-c2ccccc2Br)c(-c2ccc3ncnc(Nc4ccccc4)c3c2)c1
Brc1ccc(Br)c(-c2ccccc2)c1
Brc1ccc(NC2=C(c3ccccc3)N(CCN3CCOCC3)c3ccccc32)cc1
Policy gradient replay...
Mean value of predictions: 0.35419938
Proportion of valid SMILES: 0.5171875
Sample trajectories:
Brc1ccc(Nc2nc3ccc(Br)cc3s2)cc1
Brc1ccc(Nc2ncnc3cc(-c4n[nH]cc4-c4ccccc4Br)ncc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ncc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.32581425
Proportion of valid SMILES: 0.518125
Sample trajectories:
BP(=O)(OCCO)C(=O)O
BrC(c1ccc2ncnc(Nc3ccccc3)c2c1)N1CCN(c2cccc3ccncc23)CC1
Brc1ccc(-c2ccc3ncnc(Nc4cccnc4)c3c2)o1
Brc1ccc(-c2nc3ccccc3s2)c(Nc2cccnc2)c1
Brc1ccc(-n2c(Br)c3c(c(Nc4ccncc4)c4cncnc42)Nc2ccccc2O3)cc1

 11 Training on 8385 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 22.873776
Reward: 4.157455
Trajectories with max counts:
444	Fc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.32170686
Proportion of valid SMILES: 0.336875
Sample trajectories:
BrCN1c2ncccc2-c2ncnc(Nc3ccccc3)c2-c2ncnc(Nc3ccccc3)c21
Brc1cc2c(nc1-c1ccc3ncncc3c1)c1c-2ccc2ncccc21
Brc1cc2ncnc(Nc3ccccc3)c2c2ccccc12
Brc1cc2ncncc2nc1-c1cccs1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.31833628
Proportion of valid SMILES: 0.5334375
Sample trajectories:
Bc1cc2ncnc(Nc3cccc(Br)c3)c2cc1Br
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccc2noc(-c3ccccc3)c2c1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.33197674
Proportion of valid SMILES: 0.5375
Sample trajectories:
BP(=O)(CCCOc1cc2ncnc(Nc3ccccc3)c2s1)Nc1cc(Br)cnc1N
BP(=O)(OCC)C(=O)Oc1cc2ncnc(Nc3ccccc3)c2cc1Br
BrCc1cc2ncnc(Nc3ccccc3)c2nc1-c1ccccc1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1

 12 Training on 9729 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 24.342802
Reward: 4.047090
Trajectories with max counts:
106	Clc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.35227817
Proportion of valid SMILES: 0.52125
Sample trajectories:
Brc1cc2ccccc2nc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Brc1ccc(Nc2ncnc3ccc(-c4ccc(Br)cc4)nc3s2)cc1
Brc1ccc(Nc2ncnc3ccc(-c4ccccc4Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.4312575
Proportion of valid SMILES: 0.521875
Sample trajectories:
Bc1cc2ncnc(Nc3ccc(Br)cc3)c2nc1N
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1c2c(nc3cccnc13)-c1ccccc1O2
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.44379646
Proportion of valid SMILES: 0.5467333541731791
Sample trajectories:
Bc1ccc2ncnc(Nc3ccc(-c4ccc(Br)cc4)cc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(s1)c1c(-c3ccccc3Br)cc1ncnc(c1cccs1)N2
Brc1ccc(-c2cc3c(Nc4ccc(Br)cc4Br)ncnc3cc2Br)cc1
Brc1ccc(-c2cnc3cc(-c4ccccc4Br)n(-c4ccccc4)c3c2)cc1

 13 Training on 11324 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 23.909156
Reward: 4.080660
Trajectories with max counts:
79	Oc1ccc2ncnc(Nc3ccccc3)c2c1
Mean value of predictions: 0.46119592
Proportion of valid SMILES: 0.49125
Sample trajectories:
BP(=O)(OCC)C(O)C(F)(F)F
Bc1ccc(Nc2ncnc3c(Br)cccc23)cc1
Brc1[nH]c2ncnc(-c3ccccc3)c2c1-c1ccoc1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(-c4ccccc4Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.45853066
Proportion of valid SMILES: 0.5148483901219131
Sample trajectories:
BP(=O)(CCCl)NP(=O)(c1ccccc1)N(c1ccc(Br)cc1)C(Cl)(Cl)Br
BP(=O)(OCC)OC(=O)COc1cccc(Cl)c1
BrC(=NNc1ccccc1)c1ccc(Br)cc1
BrC1=C2CN(Cc3ccc(Br)nc3)CCCN12
Brc1cc(-c2ccc3ncnc(Nc4cccs4)c3c2)c(-c2ccccc2Br)cc1Br
Fine tuning...
Mean value of predictions: 0.46189326
Proportion of valid SMILES: 0.5151609878086902
Sample trajectories:
Brc1cc2ncnc(Nc3ccc4ccccc4c3)c2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Oc1ccccc1
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1cc2ncnc(Nc3ccccc3)n2c1-c1ccco1
Brc1cc2ncnc(Nc3cccnc3)c2s1

 14 Training on 12926 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.069041
Reward: 4.139728
Trajectories with max counts:
56	COc1cc2ncnc(Nc3ccccc3)c2s1
Mean value of predictions: 0.53067446
Proportion of valid SMILES: 0.4309375
Sample trajectories:
BP(=O)(OCC1OC(N2C=CC(=O)N(c3ccc(Br)cc3)N2)C(O)C1O)c1ccccc1
BP(=O)(Oc1ccccc1Br)OC(C)C
Bc1cc2ncnc(Nc3cccc(Br)c3)c2s1
Bc1cc2ncnc(Nc3ccccc3)c2cc1-c1cc2cncnc2s1
Bc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Policy gradient replay...
Mean value of predictions: 0.39941177
Proportion of valid SMILES: 0.53125
Sample trajectories:
BP(=O)(C=C)OCC
Brc1cc2ncnc(Nc3ccc(-c4ccccc4-c4ccccc4-c4ccccc4-c4ccccc4)cc3)c2s1
Brc1cc2ncnc(Nc3cccnc3)c2s1
Brc1ccc(-c2ccc(Br)cc2Nc2ncnc3cc(-c4ccccc4Br)c(-c4ccccc4)cc23)cc1
Brc1ccc(Br)c(-c2cc3ncnc(Nc4ccccc4)c3cc2-c2ccccc2)c1
Fine tuning...
Mean value of predictions: 0.40690047
Proportion of valid SMILES: 0.5525
Sample trajectories:
BP(=O)(Nc1ccc(F)c(F)c1)c1cccc(Nc2ncnc3sc(Br)cc23)c1
BP(=O)(OCCNC1=Nc2ccccc2C(=O)N1Cc1ccc(Br)cc1)c1ccc2c(n1)P(=O)(O)O2
BP(=O)(Oc1ccccc1)c1ccccc1
BP(=O)(c1ccc(F)cc1)N1CCCC(F)(F)C1
Brc1[nH]c2ncnc(Nc3cccc4ccccc34)c2c1-c1cccs1

 15 Training on 14502 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.193099
Reward: 4.368097
Trajectories with max counts:
161	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5284149
Proportion of valid SMILES: 0.4278125
Sample trajectories:
BP(=O)(OCC)OC(=O)C=C(O)C(N)COP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC=C)c1cc2ncnc(Nc3ccccc3)c2s1
BP(=O)(OCCC)N1C=C(Br)C(=O)Nc2cc(Br)ccc21
Bc1cc2nc(-c3ccccc3)sc2nc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Bc1cc2ncnc(Nc3ccccc3Br)c2cc1-c1ccsc1
Policy gradient replay...
Mean value of predictions: 0.4826698
Proportion of valid SMILES: 0.5342508601814201
Sample trajectories:
Brc1cc(Br)c[n+](Br)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Nc4ccccc4)c(-c4ccccc4)c23)c1
Brc1cc2c(Br)c[nH]c2nc1-c1ccc2ncnc(Nc3cc[nH]c3)c2c1
Brc1cc2c(s1)-c1ccccc1N2
Fine tuning...
Mean value of predictions: 0.5036777
Proportion of valid SMILES: 0.5353125
Sample trajectories:
BP(=O)(Nc1cc(Br)cc(Br)c1Br)OCOC(=O)Cn1c(Br)cc(Br)c1Br
BP(=O)(OCC)C(O)(NP(=O)(O)OCCCl)P(=O)(O)O
Bc1ccc(Br)cc1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

 16 Training on 16353 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.534008
Reward: 4.461865
Trajectories with max counts:
180	COc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Mean value of predictions: 0.6116169
Proportion of valid SMILES: 0.3982494529540481
Sample trajectories:
Bc1ccc2ncnc(Nc3cc(Br)cc(Br)c3)c2c1
BrC#CCOc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Brc1cc(Br)cc(-c2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.52908736
Proportion of valid SMILES: 0.4759375
Sample trajectories:
BrCCc1cc2ncnc(Nc3cccc(Br)c3)c2s1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3cccc(Br)c23)c1
Brc1cc2c(s1)-c1ccccc1-2
Brc1cc2ncnc(Nc3cccs3)c2s1
Fine tuning...
Mean value of predictions: 0.54066664
Proportion of valid SMILES: 0.46875
Sample trajectories:
BP(=O)(Nc1cc(Br)c(Br)c(Br)c1Oc1ccc(Br)cc1)N(O)C=O
BP(=O)(OCC)C(=O)NC(CC(C)P(=O)(O)O)NC(=O)C(CCC(N)=O)NC(=O)C(Br)Br
BP(=O)(OCC1C=CC=CC=CC(=O)OCC=CC=CC=CC(=O)C=C(C)C(=O)O1)OP(=O)(O)O
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(nc1-c1ccccc1)-c1ccccc1-2

 17 Training on 18286 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.026877
Reward: 4.999268
Trajectories with max counts:
286	COc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Mean value of predictions: 0.6197685
Proportion of valid SMILES: 0.3511569731081926
Sample trajectories:
BP(=O)(Nc1ccccc1)c1ccc(Br)cc1
Brc1cc(Br)c2ncncc2n1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc2ncnc(Nc3ccccc3)c2c2sccc12
Brc1cc2ncnc(Nc3ccccc3)c2nc1-c1cccs1
Policy gradient replay...
Mean value of predictions: 0.58884716
Proportion of valid SMILES: 0.49875
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(N)(=S)c1ccc(Br)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(NCc4ncnc5nsnc45)nc23)c1
Fine tuning...
Mean value of predictions: 0.5791283
Proportion of valid SMILES: 0.5090625
Sample trajectories:
BrCBr
Brc1cc(Br)cc(Nc2ncnc3cc(Cc4ncccn4)sc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(-c4ccccc4)nc23)c1
Brc1cc(Nc2ncnc3ccc(SN=Cc4ccsc4)cc23)ccn1
Brc1cc(OC2(CCN3CCc4cccc(-c5ccccn5)c4C3)CCCC2)ccc1N1CCCCC1

 18 Training on 20386 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.369124
Reward: 5.324454
Trajectories with max counts:
230	COc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Mean value of predictions: 0.6906805
Proportion of valid SMILES: 0.4226320725226633
Sample trajectories:
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6ccccc6)c5c4)cc23)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(-c4ccc5ncnc(Nc6cc(Br)sc6Br)c5c4)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Nc4ccc(Br)s4)nc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Nc4ccccc4)nc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.46498078
Proportion of valid SMILES: 0.5684375
Sample trajectories:
BP(=O)(OCC)C(=O)Oc1ccccc1-c1cccc(I)c1
B[PH](=O)(=O)c1ccccc1-c1ccccc1
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1cscn1
Brc1cc2ncnc(Nc3cc4ccccc4s3)c2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.468048
Proportion of valid SMILES: 0.5733041575492341
Sample trajectories:
Brc1cc(-c2csc(Nc3ccccc3Br)n2)c(-c2ccccc2Br)cc1Br
Brc1cc(Nc2ncnc3ccc(OCc4ccccc4)cc23)ccn1
Brc1cc2ncnc(Nc3ccc(I)c4ccccc34)c2cc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Brc1ccc(-c2ccc(-c3ccccc3)c(-c3ccccc3Br)c2)nc1
Brc1ccc(-c2ccc3ncnc(c4cccc(Nc5ncnc6cc(Br)ccc56)c4)Nc4cccc4c23)cc1

 19 Training on 22497 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.994918
Reward: 5.201321
Trajectories with max counts:
173	COc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Mean value of predictions: 0.7036144
Proportion of valid SMILES: 0.466875
Sample trajectories:
BrCOc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2nc1Br
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c3s2)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(-c4ccc5nncn5n4)nc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3scc(Br)c23)c1
Policy gradient replay...
Mean value of predictions: 0.35589203
Proportion of valid SMILES: 0.47692307692307695
Sample trajectories:
BP(=O)(NCc1ccccc1)c1ccccc1
BP(=O)(OCCO)c1ccccc1
Bc1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1cc2ncncc2s1
Bc1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.3522788
Proportion of valid SMILES: 0.4678582627783004
Sample trajectories:
BP(=O)(OCC)OC(=O)C[n+]1ccccc1
BP(=O)(Oc1ccc2ncnc(Nc3ccccc3-c3ccccc3)c2c1)c1ccccc1
Bc1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1ccccc1-c1[nH]cc2ccccc12
Brc1cc2ccccc2N1c1ncnc2cc(Br)c(-c3ccccc3)cc12
Brc1cc2nc[nH]c2c2ccccc12

 20 Training on 24125 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.554209
Reward: 5.376095
Trajectories with max counts:
161	Oc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Mean value of predictions: 0.72153485
Proportion of valid SMILES: 0.3990625
Sample trajectories:
BP(=O)(OCC)OC(=O)Oc1ccc2nc(Nc3ccc(Br)cc3)sc2c1
Bc1ccccc1-c1nc2ncnc(Nc3ccc(Br)cc3)c2s1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(-c4ccccc4Br)sc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.63234204
Proportion of valid SMILES: 0.504375
Sample trajectories:
Bc1ccc2ncnc(Nc3ccc(Br)s3)c2c1
BrBr
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)nc23)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cn1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Fine tuning...
Mean value of predictions: 0.6339926
Proportion of valid SMILES: 0.5057830572053766
Sample trajectories:
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(I)cc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Brc1cc(Br)c2cccnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2cnc3ncnc(Br)c3[nH]c2cc1Br

Trajectories with max counts:
365	Clc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Mean value of predictions: 0.59874296
Proportion of valid SMILES: 0.39775
Mean Internal Similarity: 0.49605723514616906
Std Internal Similarity: 0.09572785777891375
Mean External Similarity: 0.4356346873708188
Std External Similarity: 0.07165384453654652
Mean MolWt: 475.5514598740072
Std MolWt: 123.47947647934835
Effect MolWt: -0.23583734453395122
Mean MolLogP: 6.193586894001646
Std MolLogP: 1.8789943290195292
Effect MolLogP: 0.8335569416474312
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 98.252581% (1237 / 1259)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 20, 'n_policy_replay': 5, 'n_fine_tune': 0, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5479.0415987968445, 'valid_fraction': 0.39775, 'active_fraction': 0.5736957888120678, 'max_counts': 365, 'mean_internal_similarity': 0.49605723514616906, 'std_internal_similarity': 0.09572785777891375, 'mean_external_similarity': 0.4356346873708188, 'std_external_similarity': 0.07165384453654652, 'mean_MolWt': 475.5514598740072, 'std_MolWt': 123.47947647934835, 'effect_MolWt': -0.23583734453395122, 'mean_MolLogP': 6.193586894001646, 'std_MolLogP': 1.8789943290195292, 'effect_MolLogP': 0.8335569416474312, 'generated_scaffolds': 1259, 'novel_scaffolds': 1237, 'novel_fraction': 0.982525814138205, 'save_path': '../logs/replay_ratio_s2-4.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.912507
Reward: 1.000000
Mean value of predictions: 0.0012485369
Proportion of valid SMILES: 0.8029448621553885
Sample trajectories:
BP(=O)(NCc1ccc2c(c1)C=CC(=O)O2)P(=O)(O)O
BrCC(ON=C(c1ccccc1)c1ccc(CNc2nc(I)cs2)cc1)N1CCCCC1
Brc1ccc(CNc2ncccn2)nc1
C#CCC#Cc1cc(C(C#CCCCCCC)=CC(=O)OC)ccc1O
C#CCCC(NCc1ccccc1)C(OC(=O)NC(NC1CCCCC1)C(O)COC(=O)Cc1ccc(C(=O)NCCO)nc1)C(=O)NCC1CCCO1
Policy gradient replay...
Mean value of predictions: 0.00015791552
Proportion of valid SMILES: 0.7960402262727844
Sample trajectories:
Brc1ccc(-c2nn3ccccc3c2Br)cc1
Brc1cccc(Nc2nc3ccccc3nc2-c2ccccc2)c1
C#CC1OC(=O)C(CC)c2c(C(C)(C)C)cc(c3ccccc3)nc2SCC(=O)N1C
C#CC1c2ccccc2C2=CC(c3ccccc3)CC(=O)NC1O2
C#CCC(=O)OCP(=O)(NC(C)C(=O)OCC)c1ccc2c(c1)OCO2
Fine tuning...
Mean value of predictions: 0.0009592326
Proportion of valid SMILES: 0.7845719661335842
Sample trajectories:
Brc1ccc(-c2oc3ccc(Br)cc3c2C=C2Cc3ccccc3O2)cc1
Brc1cccc(-c2nccn2CC2CCCN2)n1
Brc1cncc(CN2CCCC2)n1
C#CC1(O)CCC2C3CCc4cc(O)ccc4C3CCC21C
C#CC1C(O)CC2C1(C)CC1(c3ccoc3)CCC(C(=C)C)C21C

  2 Training on 227 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.635497
Reward: 1.000000
Trajectories with max counts:
2	O=C(c1ccccc1)N1CCCC1
Mean value of predictions: 0.0012932675
Proportion of valid SMILES: 0.8236215538847118
Sample trajectories:
Brc1ccc(Oc2ncc(Br)c3ccccc23)cc1
Brc1ccc2oc3[nH]c(-c4ccccc4)nc3c2c1
Brc1ccccc1-c1nc2ncccn2c1-c1nc(Cc2cccnc2)c2ccccc2n1
C#CC(=O)N1CCN(S(=O)(=O)c2ccc(NC(C)=O)cc2C)CC1
C#CC(=O)c1ccc(C=CC2C3=C(CCCCC3)N2c2ccc(C)cc2)o1
Policy gradient replay...
Mean value of predictions: 0.0009984638
Proportion of valid SMILES: 0.8170693442108566
Sample trajectories:
Brc1ccc2nc(ccc2OCc2ccccc2)c1-c1ccc(-c2ccccc2)o1
Brc1ccnc(CNCCc2ccccc2)c1
C#CC(O)c1ccc(OCc2ccccc2)c(COC)c1
C#CC1COC=C(C(=O)N(C)CC#N)C1
C#CCCC1=CC=CCC=CC(C)c2cc(cc3ccccc23)OC(c2ccccc2)OC(=O)C1=O
Fine tuning...
Mean value of predictions: 0.0004571429
Proportion of valid SMILES: 0.8236586131157829
Sample trajectories:
Brc1cccc2c1N=C(c1ccccc1)c1ccccc1O2
Brc1ccccc1CN1CCN(Cc2nc3ccccc3[nH]2)CC1
Brc1cncc(C2CCCN2)c1
C#CC(O)C1CC(OC(CCCCCCCCCCCCCCC)C(O)C=C(C)C=C(C)CCC=C(C)CCC=C(C)CCC2C(=C)C(O)CC2=O)C1
C#CCCOc1cccc(Nc2nccc(C#N)n2)c1

  3 Training on 238 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.061738
Reward: 1.012655
Mean value of predictions: 0.0010387534
Proportion of valid SMILES: 0.7866121935889377
Sample trajectories:
BP(=O)(O)C(C(CCN)c1ccc(Br)cc1)P(=O)(O)O
BrCn1nc2ncnc(NCc3cccnc3)c2n1
Brc1ccccc1N=CN1CCNCC1
C#CC1(O)CC2CC3C(CCc4c[nH]c5ccccc45)C3CCC21C
C#CC1C2=C(C(O)CC1C)N(CC)C(=O)N(CCNC(C)C)C2=O
Policy gradient replay...
Mean value of predictions: 8.176615e-05
Proportion of valid SMILES: 0.7696664568911264
Sample trajectories:
Brc1ccc(-c2cnc3ccoc3c2)cc1
C#CC(C(C)=CC=Cc1ccc(OCCOCCO)c(OC)c1)c1cc(SC(C)=O)ccc1Br
C#CC(C)C1NC(=S)Nc2cccc(c2)N2CCCC2=C1O
C#CC=CC(OC(=O)CNC(C)(C)C)P(=O)(OCC)OCC
C#CCCCN(Cc1ccc(OC)c(OC)c1)c1ccncc1
Fine tuning...
Mean value of predictions: 0.0017857143
Proportion of valid SMILES: 0.7755744412968208
Sample trajectories:
Brc1ccc(C=NNc2nc(N3CCCNC3)nc(c3ccc4c(c3)CCCC4)n2)cc1
Brc1ccc(CN2CCSc3ccccc32)cc1
Brc1ccc(Cn2ccc3c2CCC3)cc1
C
C#CC(CC=NOC(=O)C(=O)O)CCCCC

  4 Training on 250 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.042286
Reward: 1.007316
Mean value of predictions: 0.0
Proportion of valid SMILES: 0.789423984891407
Sample trajectories:
BrC1=C(Oc2cccnc2)c2ccccc2N1c1ccccc1
Brc1c(-c2ccncc2)sc2nc(N3CCN(c4nc5ccccc5[nH]4)CC3)[nH]c12
C#CC(=Cc1ccc(N(=O)=O)c(C)c1)C(NC(=S)NCC1CCCC1)c1cccc(Cl)c1
C#CC(=O)CC(C)(Cc1c(F)c(F)c(F)c(F)c1F)P(C)C1CC1
C#CC(C)C(=C)CC(C)C
Policy gradient replay...
Mean value of predictions: 0.0014527845
Proportion of valid SMILES: 0.779245283018868
Sample trajectories:
Brc1cc2c(cc1OCc1ccccc1)CCNC2
Brc1ccc(-c2cc(-c3cccc(CN4CCCC4)c3)on2)cc1
Brc1ccc(-c2ncnc3c2c(OCC2CCOCC2)nn3Br)o1
Brc1ccc(N2CCC3(CC2)OCOC3COc2ccccc2)nn1
Brc1ccc(OCCc2nnc(CCC3CCNCC3)o2)cc1
Fine tuning...
Mean value of predictions: 0.00093896716
Proportion of valid SMILES: 0.8020081581424537
Sample trajectories:
Brc1cc(Br)cc(-n2cncc2Cc2ccc(Br)s2)c1
Brc1cc(C2(c3ccccc3)CCNCC2)c2[nH]c3ccccc3c(c1)c1c2CCCC1
Brc1ccc(-c2csc3[nH]cnc23)cc1
Brc1ccc2[nH]c(C3=NCCN3)nc2c1
Brc1cccc(Cc2cnc(-c3ccccc3)[nH]2)c1

  5 Training on 258 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.171329
Reward: 1.004351
Mean value of predictions: 0.0014780241
Proportion of valid SMILES: 0.8072213500784929
Sample trajectories:
Brc1ccc(C2=NCCN2)cc1
Brc1ccc2c(c1)CCC2CN1CCN(c2ccccc2)CC1
Brc1cccc2[nH]cc(CCN3CCCC3)c12
C#CC(C)C[N+]1([O-])OCCS1
C#CC(C1=CC2=C(CC1)CC(O)C2=C)C(O)C(=O)OCC
Policy gradient replay...
Mean value of predictions: 0.0012485369
Proportion of valid SMILES: 0.8037002195045468
Sample trajectories:
B[PH](=O)(=CC(=O)OCc1ccccc1)OCCOCCOCCOCn1c2ccc(Br)cc2c2ccc(Br)cc21
Brc1ccc(C#Cc2c[nH]c3ccccc23)s1
Brc1ccc(N2CCN(c3ccc(NC4=NCCN5CCCC4C5)cc3)CC2)cc1
Brc1ccc(Oc2ccc3c(C=C4CCNC4)c(-c4ccncc4)c3n2)cc1
Brc1cccc2ncnc(N3CCCCC3NCCN3CCOCC3)c12
Fine tuning...
Mean value of predictions: 0.0016587677
Proportion of valid SMILES: 0.7929846539304729
Sample trajectories:
BrSC1=Nc2sc3c(c2CCCC1CCn1ccnc1)CCCC3
Brc1ccc(-c2noc(N3CCN(c4ccc(Br)cc4Br)CC3)n2)cc1
Brc1ccc(Oc2ncccc2C2CCCNC2)c(Br)c1
Brc1cccc(C2CC(n3cc(-c4ccccc4)cn3)CN2)c1
Brc1ccccc1-c1nnc(C2CCNCC2)o1

  6 Training on 273 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.460850
Reward: 1.002588
Mean value of predictions: 0.0011862396
Proportion of valid SMILES: 0.7945334590009425
Sample trajectories:
BP(=O)(CCCCC(F)F)NC(=O)C(F)(F)F
Brc1ccc(C2=NC(=NCc3ccsc3)CN2)cc1
Brc1cccc(Nc2ncnc3cc[nH]c23)c1
Brc1ccccc1N1CCN(c2ncccn2)CC1
C#CC=CC(=O)OCC1CC(C#CCNCCCNc2c3c(nc4ccccc24)CCCC3)C2CCC(C)C12
Policy gradient replay...
Mean value of predictions: 0.0034345048
Proportion of valid SMILES: 0.7832342821395057
Sample trajectories:
Brc1ccc(-c2nn3ncccc3c2C2CN(Cc3ccccc3)C2Cc2ccccc2)cc1
Brc1ccc(CNc2nnc(-c3cccnc3)s2)cc1
Brc1ccc(N2CC3CC3(N3CCN(Cc4ccncc4)CC3)O2)nc1
Brc1ccc(OCc2ccccc2)cc1OCc1ccc(Cc2ccccc2)cc1
Brc1ccc2c(NC3CCCCC3)n[nH]c2c1
Fine tuning...
Mean value of predictions: 0.0011933175
Proportion of valid SMILES: 0.7871008140262993
Sample trajectories:
Brc1ccc(-c2ccc3c(c[nH]2)C#CC[n+]2ccccc2N=C3CCc2ccccc2)cc1
Brc1ccc(-c2noc(-c3cccc4ccccc34)n2)cc1
Brc1ccc(COc2ccc(NC=C3CCCCC3)c(Br)c2)c(Br)c1
C#CC#CC(NC(=O)c1ccc(C2OC(OC3=CC(=O)OC(OC4C(CO)OC4C(O)CO)C(O)C(O)C3O)C(O)C(O)C(O)C(O)CC(=O)OCCCCC2=O)c(C)c1)c1cc2ccccc12
C#CCC=C(Nc1ccc(C(=O)Nc2ccc(OC3CCN(Cc4ccc(O)cc4)CC3)cc2)cc1)c1ccc(CNC(=O)NC)cc1

  7 Training on 296 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.988449
Reward: 1.001539
Trajectories with max counts:
2	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
2	O=C1Nc2ccccc2C1=Cc1ccccc1
Mean value of predictions: 0.000929512
Proportion of valid SMILES: 0.8071272272585183
Sample trajectories:
BrCC1(N2CCCC2C#CC2CCCNC2)C=CCCC1
Brc1ccc(Cn2ccc3c(nc4ncccc43)n2)nc1
Brc1cccc(C2C3CCC4CCCC3C2(c2ccccc2)CO4)c1
C#CC1(O)CCC2C3c4ccccc4C3CCC21C
C#CC1C(C(=O)OCC)=C(C)CCC2(C)CCC(COC(C)=O)(c3ccc(Br)cc3)C12
Policy gradient replay...
Mean value of predictions: 0.0012688342
Proportion of valid SMILES: 0.789358372456964
Sample trajectories:
Brc1ccc2[nH]c3ncc(-c4ccccc4)cc3c2c1
Brc1ccc2c(c1)C(C1CCC1)C1(Br)C(O2)C1C1CCCc2ccccc2C1
Brc1cccc(Br)c1
Brc1ccccc1-c1noc(-c2ccc(NCc3ccccc3)nc2)n1
Brc1ccccc1C1=[SH]N(C2CCCCC2)CC1
Fine tuning...
Mean value of predictions: 0.00031397174
Proportion of valid SMILES: 0.7974960876369327
Sample trajectories:
BP(=O)(O)OCC(Br)(Br)Br
Brc1cccc(-c2ccc(-c3ccc4ccc5cnn(Cc6ccccc6)c5c4n3)nc2)c1
Brc1ccccc1-c1ccc(Nc2ncnc3nc(N4CCOCC4)oc23)cc1
C#CC#CCCOC(=O)C1CC(CC#N)C(=Nc2c[nH]c3ccccc23)C2=Nc(c1c1ccccc1)c1ccccc12
C#CC1(CCc2cc(OCCOCCOC)c3c(c2)OCC(=O)NC3C)COCCS1

  8 Training on 309 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.063611
Reward: 1.000915
Trajectories with max counts:
2	NC(=O)c1cccc2c1C(=O)c1ccccc1C2=O
Mean value of predictions: 0.0010534846
Proportion of valid SMILES: 0.7731829573934837
Sample trajectories:
Brc1ccc(-c2nc3cccnc3n3cccc23)cc1
Brc1ccc(C=C2N=C(OCc3ccccc3)C2OCc2ccccc2)cc1
Brc1cccs1
C#CC#CCC1C(N)CC(=O)N2CC1CC2C(=O)O
C#CC(C)(C)C(C)(C#N)C(=O)Nc1cc(C(C)C)c(Cl)n1C
Policy gradient replay...
Mean value of predictions: 0.0021496816
Proportion of valid SMILES: 0.7867209520826809
Sample trajectories:
BP(=O)(NO)C(CCCCN)c1ccc(O)cc1
Brc1ccc(OCc2ccc(C3=NCCN3)o2)cc1
Brc1ccc2c(-c3ccccc3)n[nH]c2c1
Brc1ccc2cc(-c3ccnc(-c4ncnc5cccnc45)n3)oc2c1
Brc1cncc(Nc2ccnc3cccc(Nc4ccnc5ccccc45)c23)c1
Fine tuning...
Mean value of predictions: 0.0017357002
Proportion of valid SMILES: 0.7934272300469484
Sample trajectories:
Brc1ccc(C2=CSC3C2CCN3CCc2ccccc2)cc1
Brc1ccc(OCCCCc2ccccc2)cn1
Brc1ccc2c(c1)CCN(CCc1c[nH]c3ccccc13)CC2
Brc1ccc2c(c1)[nH]c1c(Br)cc(Br)cc12
Brc1cccc(CNc2nc3ccccc3nc2-c2ccc(-c3ccccc3)cc2)c1

  9 Training on 331 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.157619
Reward: 1.000544
Trajectories with max counts:
2	O=C(O)CCN1C(=O)c2ccccc2C1=O
Mean value of predictions: 0.0012908431
Proportion of valid SMILES: 0.7761427676894177
Sample trajectories:
Brc1cccc(C2=NOC(C3CC3)C2)c1
Brc1ccccc1C=CCCN1CCCC1
Brc1ccccc1CN1CCN2CC(C1)C(c1ccccc1)C2
C#CCCC(=O)N1CCN(Cc2ccc(-c3cccnc3)c(CC)n2)CC1
C#CCCCC
Policy gradient replay...
Mean value of predictions: 0.0014634146
Proportion of valid SMILES: 0.7711598746081505
Sample trajectories:
BP1(=O)CCC2(C)CCC(COC(=O)c3ccc(Br)cc3)(C(=O)C(F)(F)F)C(C2)N1
BrC1=C(NCCc2ccccc2)CCC=C1
Brc1ccc(-c2nc3ccccc3s2)s1
Brc1ccc(C2=CC3C(C2)CC3N2CCNC2)cc1
Brc1ccc(N=Nc2nc3ncccc3c3ncccc23)cc1
Fine tuning...
Mean value of predictions: 0.00095465395
Proportion of valid SMILES: 0.7873473222674601
Sample trajectories:
Brc1ccc(-c2cnc3cnc(-c4ccc(Br)cc4)nc3c2)cc1
Brc1ccc(C(c2ccccc2)N2CCN(c3ccccc3)CC2)cc1
Brc1ccccc1-n1cnnc1
Brc1ccccc1N1CCN(CC2=CCCC2)CC1
Brc1ccccc1Nc1nnc(CCN2CC3CCCC32)n1CC1COc2cc(OCc3ccccc3)ccc21

 10 Training on 344 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.922194
Reward: 1.011702
Trajectories with max counts:
2	O=P(O)(O)C(F)(F)F
Mean value of predictions: 0.0015800416
Proportion of valid SMILES: 0.7539184952978056
Sample trajectories:
Brc1cc(N2CCOCC2)cc2cccnc12
Brc1ccc(-c2nc[nH]n2)s1
Brc1ccc(-n2c3ccccc3c3ccccc32)cc1
Brc1ccc(NC2CC3CNCC32)cn1
Brc1ccc2c(c1)CCC(c1ccc(Br)o1)O2
Policy gradient replay...
Mean value of predictions: 0.0031092437
Proportion of valid SMILES: 0.7444479199249296
Sample trajectories:
Brc1ccc(-n2cnnc2)o1
Brc1ccc2c(c1)C(SC1=NCCN1)=N2
Brc1cccc(OCc2cnn(-c3ccccc3)n2)c1
C#CC1=C(c2ccc(-c3ccccc3C)cc2)Oc2c(c(O)c(C#N)n2C)N1
C#CCCOc1cc(C(C)C)cc(F)c1C1=C(C)C(=O)c2c(nc(C)c(C)c2Cl)NC1=O
Fine tuning...
Mean value of predictions: 0.001972873
Proportion of valid SMILES: 0.7617407639323732
Sample trajectories:
Brc1ccc(CN(CN2CCCCC2)C2CCCC2)cc1
Brc1ccc(Oc2cncc(Br)n2)cc1
Brc1ccc2nc(-c3cc(OCc4cccnc4)on3)[nH]c2c1
Brc1cnc2c(Br)ccc(Br)c2c1
C#CC(=O)OC1OC2COC(C)C(OC2O)C(C)C(O)C(NC(C)=O)C(CC)OC(=O)C1C

 11 Training on 368 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.304049
Reward: 1.007066
Trajectories with max counts:
2	COc1ccc(N2C(=O)c3ccccc3C2=O)cc1
2	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0017857142
Proportion of valid SMILES: 0.7366113373003444
Sample trajectories:
Brc1ccc(-n2cnc3c(NCc4ccccc4)ncnc32)cc1
Brc1ccc2[nH]cnc2c1
Brc1ccc2nc(Nc3ccco3)nc(c1)n2
Brc1ccc2ncnc(Nc3ccncc3)c2c1
Brc1cccc(Nc2nn[nH]n2)c1
Policy gradient replay...
Mean value of predictions: 0.0031986532
Proportion of valid SMILES: 0.743894802755166
Sample trajectories:
Brc1c(COc2cccnc2)ccn2c(-c3cccnc3)ncc12
Brc1ccc(OCC2CCCCC2)cc1
Brc1cccc(-c2cc(Nc3ccnc(-c4ccccc4)n3)co2)c1
Brc1cccc(C2=NCCN2c2ccc(OCc3ccccc3Br)nc2)c1
Brc1cccnc1-n1cnc2cnc(NCC3CCCCC3)nc21
Fine tuning...
Mean value of predictions: 0.00075
Proportion of valid SMILES: 0.7514088916718847
Sample trajectories:
BrC1=C2CSC(=Nc3ccccc3)C12Br
Brc1cc(-c2nn[nH]n2)no1
Brc1cc(Nc2cs3cccc3ncn2)cc2cn[nH]c12
Brc1ccc(CN2CCN(c3ccncc3)CC2)cc1
Brc1ccc(OCc2ccc(N3CCOCC3)nc2)cc1

 12 Training on 389 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.236349
Reward: 1.004203
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0029813664
Proportion of valid SMILES: 0.755868544600939
Sample trajectories:
B[PH](=O)(NN=Cc1ccc(Br)cc1)(OP(=O)(O)O)P(=O)(O)O
Brc1ccc(-c2nc(CN3CCOCC3)oc2-c2ccncc2)cc1
Brc1ccc(NCc2ccccc2)o1
Brc1ccc(Oc2ccc(Br)cc2)cc1
Brc1ccc2[nH]cc(CCNC3CCN(Cc4ccccc4)CC3)c2c1
Policy gradient replay...
Mean value of predictions: 0.0015695993
Proportion of valid SMILES: 0.7589341692789968
Sample trajectories:
Brc1ccc(C#CCOc2nnc(-c3cccnc3)n2Cc2ccccc2)cc1
Brc1ccc(C2=C(C3CCCC3)SC3=NCNC3=N2)cc1
Brc1ccco1
Brc1nc(-c2nc(-c3ccccc3)cc(-n3nncc3CN3CCNC3)n2)cs1
C#CC(C(=O)Cc1nnc2c(-c3ccccc3)c1C=CC2=O)N1CCN(c2coc(-c3ccco3)n2)CC1
Fine tuning...
Mean value of predictions: 0.0025306123
Proportion of valid SMILES: 0.7665832290362954
Sample trajectories:
Brc1ccc(-n2nnnc2NCCc2ccco2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)cc1
Brc1ccc2[nH]c(-c3ccc(C=C4CCCO4)cc3)nc2c1
Brc1ccc2nc(N3CCOCC3)sc2c1

 13 Training on 412 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.532111
Reward: 1.037854
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0047264765
Proportion of valid SMILES: 0.7147325617766657
Sample trajectories:
Brc1ccc(NC2=C3C=NC4CCN=C4CC3=N2)cc1
Brc1ccc2c(c1)ncn2-c1cnc(Nc2ccc(OCc3ccccn3)cc2)nc1
Brc1cncc(-n2ccnc2)c1
C#CCC(=O)Oc1c(OC(=O)C=Cc2ccc(OCCOCC=C)cc2)cc(O)c2c1OC(C)=CC(=O)O2
C#Cc1cc(Nc2ncnc3ccc(OC)cc23)cc(OC)c1OC
Policy gradient replay...
Mean value of predictions: 0.0031069685
Proportion of valid SMILES: 0.7053850970569818
Sample trajectories:
Brc1ccc(-c2cccc3c(NC4CCCO4)cccc23)cc1
Brc1ccc(-c2cn3cc(-c4ccccn4)[nH]c3c2CCCN2CCCC2)cc1
Brc1ccc(C23CCN(CC2)c2nnnn2Cc2cc(Br)cnc2O3)cc1
Brc1ccc(Nc2ncnc3ccccc23)nc1
Brc1ccc(OCCOc2cnn3ccccc23)cc1Br
Fine tuning...
Mean value of predictions: 0.0032928945
Proportion of valid SMILES: 0.721475461081588
Sample trajectories:
BP(=O)(OCC1OC(=O)OC1c1cc(Br)cc(Br)c1)S(=O)(=O)O
BrC=C1CC(CC=Nc2ccccc2)c2ccccc21
BrCCCc1c[nH]c2ccccc12
Brc1ccc(-n2nnc3ccccc32)cc1
Brc1ccc2c(c1)OCCN2

 14 Training on 453 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.789127
Reward: 1.059530
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.005195989
Proportion of valid SMILES: 0.685625
Sample trajectories:
BrCCn1cnc2ccc(Br)cc21
Brc1ccc(OC2CCCN2)cc1
Brc1ccccc1-n1nnc2c(N3CCCC3)ncnc21
Brc1cnc2[nH]c(c3ccccc23)-c2cn[nH]c2c1
C#CC1(c2ccc(Oc3ccccc3)cc2)NC(=O)NC1=O
Policy gradient replay...
Mean value of predictions: 0.004436397
Proportion of valid SMILES: 0.6916092673763307
Sample trajectories:
Brc1ccc(Nc2nc3ccccc3s2)nc1
Brc1ccc2c(c1)C1CC(N3CCOCC3)CNC1CC2
Brc1cn(C2CN(Cc3cn(-c4cccc5c4CCCC5)nn3)Cn3c2nc2ccccc23)nn1
Brc1cnc(Nc2ccnc3c2Oc2ccccc2N3)c(Br)c1
Brc1cnc2sc(-c3ccc4[nH]c(-c5ccncc5)nc4c3)cc2n1
Fine tuning...
Mean value of predictions: 0.0076923077
Proportion of valid SMILES: 0.6912730685017203
Sample trajectories:
Brc1cc2n(c1Cc1nnnn1Cc1nnn[nH]1)CCC2
Brc1ccc(OCc2nnn[nH]2)cc1
Brc1ccc2[nH]c(-c3cc4ccccc4o3)nc2c1
Brc1ccc2c(NCCCOc3ccnc4cccn34)cccc2n1
Brc1ccc2nccc3c(n2)c2[nH]nc(c4ccncc14)c32

 15 Training on 514 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.951929
Reward: 1.089481
Trajectories with max counts:
4	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.011520303
Proportion of valid SMILES: 0.6622889305816135
Sample trajectories:
Brc1ccc2ncnc(Nc3ncnc4ccccc34)c2c1
Brc1cccc(Nc2ncnc3ccccc23)c1
Brc1cnc2nc(-c3nnn[nH]3)nc(-c3ccccc3)c2c1
Brc1ncnc2[nH]c3ccccc3c12
C#Cc1c(C)ncnc1NC1CCN(Cc2nc(-c3ccn(C)n3)no2)C1
Policy gradient replay...
Mean value of predictions: 0.012112675
Proportion of valid SMILES: 0.665833072835261
Sample trajectories:
Brc1ccc2[nH]c(-c3cnn(-c4cccc5cnc6ccccc6c45)c3)nc2c1
Brc1ccc2c(c1)C(Nc1cnccn1)CO2
Brc1ccc2ncnc(Nc3cccnc3)c2c1
Brc1cccc(Nc2nncn2Cc2ccccc2)c1
Brc1ccccc1-c1ncnc2nn(-c3cnc4ccc5cccnc5c4c3)nc12
Fine tuning...
Mean value of predictions: 0.010364683
Proportion of valid SMILES: 0.65125
Sample trajectories:
Brc1ccc2c(c1)C=C(N=Cc1ccc[nH]1)S2
Brc1ccncc1OC1CN(c2cncnc2)C1
Brc1cncn1Cc1ccc(Cn2ccnc2)cc1
Brc1nnn(-c2cc3nccn3c3ccccc23)c1-c1cccnc1
C#CC1(O)C(=O)C(C(=O)Nc2cnc3cccnc3c2)=CN1C1COC(OC2OC(C)C(OC(C)=O)C2O)C(O)C1O

 16 Training on 630 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.386393
Reward: 1.074480
Trajectories with max counts:
7	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.012762078
Proportion of valid SMILES: 0.6858393247889966
Sample trajectories:
BrC1=C(c2ccc3cccnc3c2)N2CC(Cc3cccnc3)c2N1
Brc1ccc(-n2nccn2)cc1
Brc1ccc2c(c1)CC(n1cc(Br)cn1)=Cc1cnnn1C2
Brc1ccc2nc(-c3cncc(-n4cnnn4)n3)[nH]c2c1
Brc1ccc2nc(N3CCN(Cc4ccccn4)CC3)sc2c1
Policy gradient replay...
Mean value of predictions: 0.009439252
Proportion of valid SMILES: 0.6689590497030322
Sample trajectories:
Brc1ccc2ncnc(Nc3ccccn3)c2n1
Brc1ccc2oc(-c3cccnc3)nc2c1
Brc1ccccc1Nc1ncnc2cncnc12
Brc1cccn2c(-c3ccncc3)nc(-c3ccnc4[nH]ncc34)c12
Brc1cn(-c2nc(NCc3cncnc3)c3nnnn3n2)cn1
Fine tuning...
Mean value of predictions: 0.012069779
Proportion of valid SMILES: 0.6630196936542669
Sample trajectories:
BrCc1cnc2sc3ccccc3n12
Brc1ccc(CN2CCN(c3ncnc4cc(Br)c(Br)cc34)CC2)cc1
Brc1ccc(Cc2c(Br)cnc3c2sN(Cc2nnnn2Cc2ccccn2)CC(c2ccccc2)=C3)cc1
Brc1ccc(N2c3ccnn3C2c2ccccn2)nc1
Brc1ccc(Nc2ncnc3ccsc23)cc1

 17 Training on 749 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.376453
Reward: 1.092409
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.012912348
Proportion of valid SMILES: 0.6633322913410441
Sample trajectories:
Brc1c(-c2cncnc2)cnn1-c1ccc2nccn2c1
Brc1cc2nc(Nc3ncnc4[nH]ncc34)cnc2cc1C1=CCOc2cn(-c3ccncc3)nc2-c2nnnn21
Brc1cc2nccnc2c(OCc2cccc3ccccc23)c1Br
Brc1ccc(-c2noc(-c3cccnc3)n2)cc1
Brc1ccc(Nc2ncnn2C2Cc3cccnc3N=C2COc2ccccc2)cc1
Policy gradient replay...
Mean value of predictions: 0.012392755
Proportion of valid SMILES: 0.6560350218886805
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(F)(F)P(=O)(O)O
Brc1cc(c2cncnc2)nc2ncnc(Nc3ncccn3)c12
Brc1ccc2[nH]c(-c3nnc(-c4ccc(-n5ccnc5)cc4)o3)nc2c1
Brc1ccc2[nH]cnc2c1
Brc1ccc2cc(-c3nnc(-c4cnoc4)o3)[nH]c2c1
Fine tuning...
Mean value of predictions: 0.011798289
Proportion of valid SMILES: 0.6570803376055018
Sample trajectories:
BP(=O)(OCc1ccccc1)c1ccc(Br)cc1
Brc1ccc(-c2cnc3ncnn3c2)o1
Brc1ccc(Br)nc1
Brc1ccc2nc(-c3noc(-c4cccnc4)n3)ncc2c1Br
Brc1ccccc1Cn1c(Br)cc2cccnc21

 18 Training on 883 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.195190
Reward: 1.168771
Trajectories with max counts:
12	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.03093415
Proportion of valid SMILES: 0.6123788683963739
Sample trajectories:
Bc1cc2cccc(-c3cccnc3)c2cn1
Brc1cc2ncn(Cc3cccnc3)c2c2ncccc12
Brc1ccc(C=CCn2cc(-c3ccc4ccccc4c3)c[n+]2Br)cc1
Brc1ccc2[nH]cc(COc3cccnc3)c2c1
Brc1ccc2ncc(Br)nc2c1
Policy gradient replay...
Mean value of predictions: 0.03071201
Proportion of valid SMILES: 0.588125
Sample trajectories:
BP(=O)(OCC)c1cccc(CP(=O)(O)O)c1
Brc1cc2nnnn2c(-c2ccccc2)c2ccccc2n1
Brc1ccc(Nc2ncnc3cc(Br)c(CN4CCSCN4)cc23)c(Br)c1
Brc1ccc2[nH]c3ccccc3c2c1
Brc1ccc2cnn2c2c3c1cc1ccccc1n32
Fine tuning...
Mean value of predictions: 0.034859523
Proportion of valid SMILES: 0.6008127539856205
Sample trajectories:
Brc1ccc(Nc2ncnc3ccc(Br)cc23)nc1
Brc1ccc2c(c1)C(c1ccc[nH]1)N2
Brc1cccc(-n2nnc3ccncc32)c1
Brc1cccc2oc(-c3ccncc3)nc12
Brc1cn2c(Nc3ccccc3)c(Br)cc2c(-c2cnc3ncnn3n2)n1

 19 Training on 1184 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.915702
Reward: 1.242742
Trajectories with max counts:
45	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.034591194
Proportion of valid SMILES: 0.59625
Sample trajectories:
Brc1ccc2c(N=Nc3ccccc3)c3ccccc3cc2n1
Brc1ccc2ncnn2c1
Brc1cn(-c2ccnc3c2Nc2ccccc2O3)c2c1NCC2
Brc1cnc2[nH]c3ccncc3c2c1
Brc1cnc2ccccc2n1
Policy gradient replay...
Mean value of predictions: 0.03155717
Proportion of valid SMILES: 0.6040625
Sample trajectories:
Brc1cc2ncnn2c(-n2cncn2)c2ccccc2n1
Brc1ccc2nc(-n3cnnn3)nc(Nc3ccccn3)c2c1
Brc1ccc2nc(Nc3cccc4ccccc34)cc(c1)n2
Brc1ccc2nc[nH]c2c1
Brc1ccc2ncnc(Nc3ccccc3)c2c1
Fine tuning...
Mean value of predictions: 0.034675255
Proportion of valid SMILES: 0.5821875
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCN(Cc2cnn3ccccc23)CC1
Brc1ccc(-c2noc(-n3cccn3)n2)cc1
Brc1ccc(Br)c(C=Cc2ccc3ncsc3c2)c1
Brc1ccc2c(Nc3ccccc3)ccnc2c1
Brc1ccc2ncsc2c1

 20 Training on 1502 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.688483
Reward: 1.326711
Trajectories with max counts:
53	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.037017167
Proportion of valid SMILES: 0.5826820881525476
Sample trajectories:
Brc1ccc2c(-c3ccc4ncncc4c3)cnn2c1
Brc1ccc2c(cc3N(N=Nc4ccncc4)CN32)c1
Brc1ccc2c(n1)-c1ccccc1O2
Brc1ccc2cc(Nc3ncnc4cc[nH]c34)ccc2n1
Brc1cccc2ncnc(Nc3cccc4ccccc34)c12
Policy gradient replay...
Mean value of predictions: 0.050626308
Proportion of valid SMILES: 0.59875
Sample trajectories:
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc2c(c1)OCO2
Brc1ccc2nc(N=Cc3ccnc4c3Nc3ccccc3O4)sc2c1
Brc1ccc2nccn2c1
Brc1ccc2nccn2n1
Fine tuning...
Mean value of predictions: 0.035725676
Proportion of valid SMILES: 0.5879962488277587
Sample trajectories:
Brc1cc2c(nn1)-c1ccccc1-2
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc2nc(-c3ccncn3)ncc2c1
Brc1ccc2nc(COc3cccc4ccccc34)ccc2c1
Brc1ccc2ncn(CCc3ccccc3)c2c1

Trajectories with max counts:
244	COc1cc2ncnc(Nc3ccccc3)c2cc1OC
Mean value of predictions: 0.037001632
Proportion of valid SMILES: 0.5361920240030004
Mean Internal Similarity: 0.5020592761132208
Std Internal Similarity: 0.12426258722638917
Mean External Similarity: 0.43427631700646135
Std External Similarity: 0.09391282939997783
Mean MolWt: 355.6597407407409
Std MolWt: 70.13493348340589
Effect MolWt: -1.5264300208095296
Mean MolLogP: 4.145480987654324
Std MolLogP: 1.08398664379821
Effect MolLogP: -0.4349884368169223
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 91.608392% (131 / 143)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 0, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5485.749741077423, 'valid_fraction': 0.5361920240030004, 'active_fraction': 0.028328281650734435, 'max_counts': 244, 'mean_internal_similarity': 0.5020592761132208, 'std_internal_similarity': 0.12426258722638917, 'mean_external_similarity': 0.43427631700646135, 'std_external_similarity': 0.09391282939997783, 'mean_MolWt': 355.6597407407409, 'std_MolWt': 70.13493348340589, 'effect_MolWt': -1.5264300208095296, 'mean_MolLogP': 4.145480987654324, 'std_MolLogP': 1.08398664379821, 'effect_MolLogP': -0.4349884368169223, 'generated_scaffolds': 143, 'novel_scaffolds': 131, 'novel_fraction': 0.916083916083916, 'save_path': '../logs/replay_ratio_s2-5.smi'}
