starting log


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.548579
Reward: 1.000000
Mean value of predictions: 0.0012553942
Proportion of valid SMILES: 0.7983088005010961
Sample trajectories:
Brc1ccccc1-c1nc2cccnc2[nH]1
Brc1ccccc1C=Nc1nc(-c2ccccc2)cs1
Brc1cnc(-c2cnc3ncnc(Nc4ccncc4)n23)nc1
C#CC(=CCC=CCCC)CCCC(=O)O
C#CCCN(Cc1ccccc1OCCO)c1cccc(COC(Cn2cncn2)c2ccc3ccccn23)c1
Policy gradient replay...
Mean value of predictions: 0.010656754
Proportion of valid SMILES: 0.7565625
Sample trajectories:
BP(=O)(NCCCCOc1ccccc1Cl)S(=O)(=O)Nc1nccs1
BrCC1Cc2cccn2C1
Brc1ccc(Nc2ccccc2)cc1
Brc1ccc2c(Nc3ccccc3)ncnc2c1
Brc1ccc2ccc(CN(Cc3ccncc3)Cc3ccccn3)cc2c1
Fine tuning...
Mean value of predictions: 0.03267974
Proportion of valid SMILES: 0.6225352112676056
Sample trajectories:
Brc1cc2c(-c3ccccc3)ncnc2cn1
Brc1ccc(Nc2ccnc(Nc3ccc(Br)cc3)n2)cc1
Brc1ccc(Nc2ccnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3sc(C=CCN4CCOC4)cc23)cc1

  2 Training on 364 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.779687
Reward: 1.016071
Trajectories with max counts:
3	COc1ccc(Nc2ncnc3ccccc23)cc1
3	Cc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.03174442
Proportion of valid SMILES: 0.6172143974960876
Sample trajectories:
BP(=O)(OCc1ccccc1)OCC1OC(N2C=CC(=N)OC2=O)C(O)C1O
Brc1cc(-c2ccccc2)c2ncnc(Nc3ccccc3)c2c1
Brc1cc2c(-c3ccccc3)c(-c3ccccc3)c2cn1
Brc1ccc(Br)cc1
Brc1ccc(Nc2ccc(-c3ccc(-c4ccccc4)c4ccccc34)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.089852214
Proportion of valid SMILES: 0.6345733041575492
Sample trajectories:
BrC1=NC2(CCCCC2)C1Cc1cccc(Br)c1
BrCCN1CCC(Cc2ccc3ccccc3c2)CC1
Brc1cc(-c2cccc3ccccc23)c2ccccc2n1
Brc1ccc(-c2nc3ccccc3o2)c2ccccc12
Brc1ccc(Nc2ccnn2Cc2ccc(Nc3ncnc4ccccc34)cc2)cc1
Fine tuning...
Mean value of predictions: 0.09249875
Proportion of valid SMILES: 0.6298498122653317
Sample trajectories:
BP(=O)(OCC)OC(=O)COc1ccccc1
Brc1c(-c2ccc(Oc3ccccc3)cc2)c2c3cccnc3c3cccc(cc-2c2ccccc12)-c1ccccc1-3
Brc1cCc2ccccc2CCc(Nc2ncnc3cccnc23)ccc1
Brc1ccc(-c2ccc3ccncc3c2)c2ccncc12
Brc1ccc(-c2ccc3oc(-c4ccc(Br)cc4)nc3c2)cc1

  3 Training on 978 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.592878
Reward: 1.258641
Trajectories with max counts:
23	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.10744403
Proportion of valid SMILES: 0.6006879299562227
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)CCl
Brc1ccc(-c2ccccc2)c2ccccc12
Brc1ccc(-c2cccnc2)cc1
Brc1ccc(Br)c(Nc2cccc(Nc3ccccc3Br)n3ncnc3n2)c1
Brc1ccc(C2CCNC2)cc1
Policy gradient replay...
Mean value of predictions: 0.030624995
Proportion of valid SMILES: 0.7
Sample trajectories:
BP(=O)(O)CN(C(=O)c1ccc(Br)cc1)c1ccccc1
Brc1ccc(Nc2ncnc3ncnc(Nc4ccccc4Br)c23)cc1
Brc1ccc2ccccc2c1-c1ccc2ncnc(Nc3ccccc3)c2c1
Brc1ccc2ccccc2c1-c1cccc2ccccc12
Brc1ccc2ccccc2c1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.18188368
Proportion of valid SMILES: 0.607567229518449
Sample trajectories:
BP(=O)(NCCCCCCCN)c1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Bc1ccccc1I
BrC(=NN1CCCCCC1)c1ccccc1
BrC(=Nc1ncnc2ccccc12)c1ccccc1
BrC1=CN(Cc2ccc(Br)cc2Br)C=Nc2cc(Br)ccc21

  4 Training on 1854 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 20.140313
Reward: 1.470333
Trajectories with max counts:
37	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.19436765
Proportion of valid SMILES: 0.6106941838649156
Sample trajectories:
Bc1ccccc1Nc1ncnc(Nc2ncnc3cc(Br)ccc23)c1C#N
BrC1=CC2=Nc3ccccc3c2c1
Brc1cc(Br)c(Nc2ccc(N3CCOCC3)c(Br)c2)c(Br)c1
Brc1cc(Nc2ncnc3ccccc23)ccc1Oc1ccccc1
Brc1ccc(-c2cccnc2)c2sccc12
Policy gradient replay...
Mean value of predictions: 0.25630602
Proportion of valid SMILES: 0.5236306729264476
Sample trajectories:
BP(=O)(N1CCC(F)C1)N(=O)=O
BP(=O)(OCC1OC(N2C=CC(=O)NN=[SH]2)C(O)C(O)C1O)c1ccc(Br)cc1
BrCN1c2ccc(Br)cc2Sc2ccc(Br)cc2C1Cc1ccc2ccccc2c1
BrCc1nc2ncnc(Nc3ccc(Br)cc3)c2o1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.24366198
Proportion of valid SMILES: 0.62125
Sample trajectories:
BP(=O)(CC(=O)O)N(O)Cc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BP(=O)(NCCOC(F)(F)CF)Nc1cnc(Nc2ccccc2F)cn1
BP(=O)(Nc1ccc(Br)cc1)N1CCOCC1
BrC1CCc2ccc3NCCN3c3cccc1c23
BrCCNc1ncc2ncnc(-c3ccccc3)c2n1

  5 Training on 3464 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 20.450738
Reward: 1.578016
Trajectories with max counts:
48	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.20362976
Proportion of valid SMILES: 0.6904761904761905
Sample trajectories:
BP(=O)(NCCCCCCC)C(=O)Nc1ccc(Cl)c([N-]C(=O)NO)c1
BP(=O)(OCCC)C(F)(F)F
BP(=O)(c1nc2c(F)cc(F)cc2s1)N1CCOCC1
Bc1cccc(Nc2ncnc3ccccc23)c1
BrCC1(Br)CCCN(c2ccc(Nc3ncnc4ccccc34)cc2)CC1
Policy gradient replay...
Mean value of predictions: 0.31394958
Proportion of valid SMILES: 0.5595611285266457
Sample trajectories:
BP(=O)(NCCCO)Nc1nc2c(Br)c(Br)[nH]c2nc1N(=O)=O
BP(=O)(Nc1nc(Nc2ccc(Br)cc2)c2ncn(C(CBr)C(=O)Br)c2n1)OCC
B[PH](=O)(CC(=O)Nc1ccc(O)c2c1c1cc(Br)c(Br)cc1N2)=Nc1cccs1
BrC1=Nc2cc(Br)ccc2Nc2ccc(Br)cc21
BrC=C=CC=CC=CCC=CCC=CCCCCCCCCCCCCBr
Fine tuning...
Mean value of predictions: 0.28403118
Proportion of valid SMILES: 0.641875
Sample trajectories:
BP(=O)(CC(=O)O)c1cccc(Br)c1
Brc1cc(Nc2ccncc2)c2ccccc2n1
Brc1cc2c(-c3ccncc3Oc3ccccc3Br)ncnc2[nH]1
Brc1cc2cc3ccccc3nc2nc1Sc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(-c2ccc(Nc3ncnc4ccsc34)cc2)cc1

  6 Training on 5227 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 25.613978
Reward: 2.115659
Trajectories with max counts:
27	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.31855565
Proportion of valid SMILES: 0.623319787433573
Sample trajectories:
BP(=O)(Oc1cc(NN=Cc2ccnc(Nc3cccnc3)c2)ccc1O)OP(=O)(O)CCNC(C)=O
Brc1ccc(-n2cnnc2Nc2cccc(-c3ccccc3Br)c2)cc1
Brc1ccc(Br)c(Nc2cc(Nc3ccnc4ccc(Br)cc34)ncn2)c1
Brc1ccc(Br)s1
Brc1ccc(N2CCOCC2)c2cccnc12
Policy gradient replay...
Mean value of predictions: 0.41156012
Proportion of valid SMILES: 0.6109375
Sample trajectories:
BP(=O)(NCCCl)SC(Cl)Br
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1
BrCCn1cnc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2cnc(NC3=NCC3)nc2s1
Fine tuning...
Mean value of predictions: 0.37206933
Proportion of valid SMILES: 0.6135084427767354
Sample trajectories:
BP(=O)(NCCCCCCCCCOc1cccc2c(OC(F)(F)F)cc(O)cc12)C(=O)O
BP(=O)(OCC)OC(=O)CCCCCCCCCCl
BP(=O)(OCCS(=O)(=O)OCC(Br)C(Br)Br)OC(=O)CBr
B[PH](=O)(=Nc1cc(Br)c(Br)c(Br)c1F)OCOc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)c(Br)c1

  7 Training on 7446 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 23.564168
Reward: 2.058709
Trajectories with max counts:
42	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.36520842
Proportion of valid SMILES: 0.6071875
Sample trajectories:
B[PH](=O)(=Nc1ccc(Br)cc1)Nc1ccc(Br)c(Br)c1
Brc1cc(Nc2cc(-c3csc(Br)n3)ncn2)cc(Nc2ccccc2)n1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Brc1cc2ncnc(Nc3ccc(Br)s3)c2cn1
Brc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.35257226
Proportion of valid SMILES: 0.6384110103221771
Sample trajectories:
BP(=O)(CCCCCC=C(Br)Br)OCC
BrCCCNc1cccc(Nc2ncnc3ccccc23)c1
BrCCNc1cccc(Nc2ncnc3cc2c2ccccc2c2c4ccccc4nc4ccccc4c32)n1
Brc1cc2cc3sc(-c4ccccc4Br)nc3cc2s1
Brc1ccc(-c2ccccc2)c2c(CCN3CCCCC3)ncnc12
Fine tuning...
Mean value of predictions: 0.38887787
Proportion of valid SMILES: 0.6297686053783614
Sample trajectories:
B[PH](=O)(=CC)NC(C(=O)c1ccc(N)cc1)N1CCOCC1
BrSc1sc2ccccc2c1-c1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(Br)c2c(c1)Nc1ccc(Br)cc12
Brc1cc2c(Nc3ccccc3-c3ccccc3Br)ncnc2s1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1

  8 Training on 9556 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 25.084673
Reward: 2.284486
Trajectories with max counts:
33	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.40392157
Proportion of valid SMILES: 0.6058143169740544
Sample trajectories:
BP(=O)(CCF)CCCCC
BrCCN1CCN(c2cccc3c(Nc4ccccc4Br)ncnc23)CC1
BrCN1CCN(CCNc2ccccc2)CC1
Brc1cc(Nc2ncnc3ccccc23)cc(Br)c1Oc1ccccc1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.4452848
Proportion of valid SMILES: 0.6536295369211514
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccncc23)c1
Brc1cc(Nc2ccncc2)cc2ccccc12
Brc1cc(Nc2ncnc3scc(-c4ccccc4)c23)cc(Br)c1OCCCN1CCCCC1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Brc1cc2oc(Br)c(Br)c2cc1Br
Fine tuning...
Mean value of predictions: 0.4483342
Proportion of valid SMILES: 0.6290272130121989
Sample trajectories:
BP(=O)(CCCCl)Nc1ccc(Nc2ccnc(Nc3c(Cl)cccc3Cl)c2)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(-c2ccncc2)cnc1Nc1cccc2ncccc12
Brc1cc(Br)c(Br)cn1
Brc1cc(Br)c2sc3c(Nc4ccccc4Br)ncnc3c2c1

  9 Training on 11797 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 27.051723
Reward: 2.510308
Trajectories with max counts:
21	CS(=O)(=O)Nc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.40413517
Proportion of valid SMILES: 0.6200750469043153
Sample trajectories:
BP(=O)(OCCS)C(=O)Nc1cccc(Br)c1
BrC1=CN2C(C=Nc3ccccc3)=Nc3ccc(Br)cc3N=CN=C2S1
BrCCNc1nc(Nc2ccc3ccccc3n2)c2sccc2n1
BrCCOc1cccc(Nc2ncnc3scnc23)c1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Policy gradient replay...
Mean value of predictions: 0.5230769
Proportion of valid SMILES: 0.6176930290715849
Sample trajectories:
BP(=O)(NCC(=O)Nc1ccc(Br)cc1)C(=O)Oc1ccc(F)cc1F
Brc1cc(Br)c2cc(-c3ccccc3Br)sc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(-c4ccc(Br)cc4Br)sc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccc(Br)c(Br)c23)c2ccccc2c1
Fine tuning...
Mean value of predictions: 0.49652433
Proportion of valid SMILES: 0.6295717411691153
Sample trajectories:
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1
BrC(=NNc1ccccc1)c1ccc2ncnc(Nc3ccc(Br)cc3Br)c2c1
BrCCCCCNCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCNCCCNc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cc1Br
BrNc1ccc(Br)cc1

 10 Training on 13872 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.758358
Reward: 2.506320
Trajectories with max counts:
37	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.50609356
Proportion of valid SMILES: 0.574375
Sample trajectories:
BP(=O)(Nc1ccccc1)N(c1ccccc1)c1ccccc1
BP(=O)(OCC=C)C(=O)Nc1cccc(F)c1F
Bc1ccccc1P(=O)(Nc1ccccc1)c1ccc(Br)cc1
BrC=CC=CBr
BrCc1ccccc1Nc1ncnc2c3ccc(Br)c2c(ncn3)s1
Policy gradient replay...
Mean value of predictions: 0.5307022
Proportion of valid SMILES: 0.6114070824193043
Sample trajectories:
BP(=O)(OCC)c1cc(Nc2cc(Cl)cc(Cl)c2)cc(Br)c1O
BrCCCCCCCCNc1ccnc(Nc2ccccc2)c1
Brc1cc(Br)c(Nc2ncnc(Nc3cccc4cc(Br)ccc34)n2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(I)cc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.4693698
Proportion of valid SMILES: 0.6398874648327603
Sample trajectories:
BP(=O)(NCCCCCC(=O)O)C(=O)OCF
BP(=O)(OCC(=O)NO)C(=O)NCCCN
BP(=O)(OCC1OC(N2C=C(Br)C(=O)NC2=O)C(O)C1O)Oc1cccc(Br)c1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1cccc(Nc2ncnc3cc(Br)ccc23)c1

 11 Training on 16077 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.402956
Reward: 2.646939
Trajectories with max counts:
89	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.49345893
Proportion of valid SMILES: 0.5977478886456052
Sample trajectories:
BP(=O)(COc1cc(Br)c(Br)cc1Br)Nc1ccc(Br)cc1
BP(=O)(NCCCCl)N(=O)=O
Bc1ccc(NC(=O)c2nnc(Nc3ccc(Br)cc3F)s2)cc1
BrC1=CN=C2Nc3ccc(Br)cc3Nc3ccc(Br)cc3CN12
BrCCCCBr
Policy gradient replay...
Mean value of predictions: 0.5372116
Proportion of valid SMILES: 0.623709727869878
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Nc2ncnc3ccccc23)cc1
BP(=O)(Oc1ccc(Br)cc1)Oc1ccc(Br)cc1
B[PH](C(=O)O)=[PH](=O)(O)CCCN
Brc1cc(Br)c2c(Nc3cccc(C=Cc4ccc(Nc5ncnc6ccccc56)cc4)c3)ncnc2c1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Fine tuning...
Mean value of predictions: 0.54636645
Proportion of valid SMILES: 0.6111979981232405
Sample trajectories:
BP(=O)(CCCCC(NC(=O)Nc1ccc(F)c(F)c1)c1ccc(Nc2ncnc3c(N)cccc23)cc1)C(F)(F)F
BP(=O)(NCCCCCCCCCCCCCl)NS(=O)(=O)c1ccc2ncnc(N3CCCCC3)c2c1
BrCC(=NNc1ccc(Br)c(Br)c1)c1cccnc1
BrCCCCCC=C(Br)Br
BrCCN1sc2ncnc(Nc3cccc(Br)c3)c21

 12 Training on 18492 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.584022
Reward: 2.964296
Trajectories with max counts:
72	CS(=O)(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49935693
Proportion of valid SMILES: 0.583125
Sample trajectories:
BrCCCBr
BrCN1C=Cc2c(Nc3ccccc3)ncnc21
BrCN1CC=C(Nc2ccccc2Nc2cccc(Nc3ncnc4ccccc34)c2)CC1
Brc1cc(Br)nc(Nc2ncnc3cc(Nc4ccccc4Br)sc23)c1
Brc1cc2ncnc(Nc3ccc(-c4ccccc4Br)cc3)c2s1
Policy gradient replay...
Mean value of predictions: 0.51758796
Proportion of valid SMILES: 0.621875
Sample trajectories:
BrC=CC=CBr
BrSc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(Br)c2nc(Nc3ccccc3Br)sc2c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.5504715
Proportion of valid SMILES: 0.6298843388558925
Sample trajectories:
B[PH](=O)(Nc1cccc(Nc2ncnc3cc(Br)ccc23)c1)(P(=O)(O)O)[PH](O)(F)P(=O)(O)O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2cn1
Brc1cc2c(Nc3ccccc3)ncnc2s1

 13 Training on 20837 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.289870
Reward: 3.236532
Trajectories with max counts:
134	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5681449
Proportion of valid SMILES: 0.5003125
Sample trajectories:
BP(=O)(Nc1ccccc1)P(=O)(O)Oc1ccc(Br)cc1
BP(=O)(OCC)c1ccc(Nc2ncnc3sc(Br)cc23)cc1
BP(=O)(OCCBr)n1cnc2c(Br)c3c(Br)cc(Br)ccc3c21
Bc1ccc(Br)cc1Nc1cc2ncnc(Nc3ccc(Br)s3)c2cc1Br
Bc1ccc(Br)cc1Nc1ccc(Br)cc1Nc1cc(Br)cc(Br)c1
Policy gradient replay...
Mean value of predictions: 0.56681436
Proportion of valid SMILES: 0.6356807511737089
Sample trajectories:
BP(=O)(SCCS)C(=O)O
BrCC1(c2ccccc2Nc2ncnc3c(Nc4ccncc4)cc(Br)nc23)CCC1
BrCCCCCCCNc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCCCCOCCc1cc2c(Nc3ccc(Br)c4ccsc34)ncnc2s1
Brc1cc(Br)nc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.5860442
Proportion of valid SMILES: 0.6226945920600188
Sample trajectories:
BP(=O)(Nc1cccc(Nc2ncnc3c(Br)ccc(Br)c23)c1)c1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cn1
Bc1ccc(Nc2ncnc3cccc(Br)c23)cc1
Brc1cc(Br)c(Nc2ncnc3c(Br)cccc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(N4CCCCCC4)c3)ncnc2c1

 14 Training on 23447 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.724532
Reward: 3.271108
Trajectories with max counts:
38	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
38	CS(=O)(=O)Nc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.60480094
Proportion of valid SMILES: 0.573170731707317
Sample trajectories:
BP(=O)(O)C(=O)Nc1cc(Br)c(Br)c2c1C(=O)P2(=O)O
Bc1cc2cncnc2c(Nc2cccc(Br)c2)c1-c1ccc(Br)cc1Br
Bc1ccc(Nc2ncnc3scnc23)cc1Br
BrCCCCBr
Brc1c(-c2ccc(N=CN3CCOCC3)c3ncsc23)cnc2ccccc12
Policy gradient replay...
Mean value of predictions: 0.5994673
Proportion of valid SMILES: 0.5871129183609634
Sample trajectories:
BP(=O)(COC(=O)CN(O)C=O)OCCCCO
BrCc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.5577114
Proportion of valid SMILES: 0.6283213504220069
Sample trajectories:
BP(=O)(N=C(C)Nc1ccc(Br)cc1)OCC
BP(=O)(O)Cc1csc(NN=Cc2ccccc2Br)c1
BrC=C=CCOc1ccc(Nc2ncnc3cccnc23)cc1
BrC=CBr
BrC=CC=CBr

 15 Training on 26177 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.729852
Reward: 3.543273
Trajectories with max counts:
12	CS(=O)(=O)c1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5220859
Proportion of valid SMILES: 0.784192439862543
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCNc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc(Nc2ccc(Br)c(Br)c2)c2cc(Br)sc2n1
Brc1cc2c(Nc3ccccc3)ccnc2nc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Brc1cc2c(Nc3ccccc3Br)cccc2s1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Policy gradient replay...
Mean value of predictions: 0.5949612
Proportion of valid SMILES: 0.6523388116308471
Sample trajectories:
BrSC1(c2cc(Br)cc(Nc3ncnc4scc(Br)c34)c2I)Nc2ccccc2S1
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4ccc5ccsc5c34)cc2)cc1Br
Brc1cc(Br)c2c(Nc3ccc(I)cc3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2n1
Fine tuning...
Mean value of predictions: 0.59330744
Proportion of valid SMILES: 0.6457876605073598
Sample trajectories:
BrC(=Nc1ccc(Br)s1)c1ccnc(Nc2ncnc3ccc(Br)cc23)n1
BrC=CC=CCBr
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ccccc2Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccc(Br)nc23)ccn1

 16 Training on 29156 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.831076
Reward: 3.886192
Trajectories with max counts:
47	CS(=O)(=O)Nc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.6084656
Proportion of valid SMILES: 0.5913642052565707
Sample trajectories:
BrCN1CCCN(Cc2c(Br)cccc2-c2ccc(Br)cc2)CC1
Brc1cc(Br)c(CNc2ccc(Br)o2)c(Br)c1
Brc1cc(Br)c2cc(Br)nc(Nc3ccc(I)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.60513633
Proportion of valid SMILES: 0.5964363863707408
Sample trajectories:
BP(=O)(CC)Nc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1O
Bc1cccc(Nc2ncnc3cc(Br)ccc23)c1
BrCCCN=C1Nc2c(Br)cc(Nc3ncnc4ccccc34)cc2O1
BrCCSc1ccccc1-c1ccccc1Nc1ccc(CNc2ncnc3cccc(Br)c23)cc1
Fine tuning...
Mean value of predictions: 0.6041016
Proportion of valid SMILES: 0.64
Sample trajectories:
BP(=O)(O)CNC(=O)C(CC(=O)NCP(=O)(O)O)C(=O)O
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1
BrCc1ccc2ncnc(Nc3cccc(Nc4ccccc4Br)c3)c2c1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2sccc12
Brc1cc(Br)cc(Nc2ncnc3ccc(Nc4ccccc4Br)nc23)c1

 17 Training on 32044 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.975317
Reward: 4.115313
Trajectories with max counts:
163	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6455927
Proportion of valid SMILES: 0.514866979655712
Sample trajectories:
BP(=O)(CCNC(=O)Nc1ccc(Br)cn1)OCC
BP(=O)(NCCCCCCN)c1ccc(Br)c(Nc2ncnc3sc(Br)cc23)c1
B[PH](=O)(Nc1ccc(Nc2ncnc3sc(Cl)cc23)cc1)(C(=O)O)c1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
BrC=CBr
BrC=CC=CCC=CC=CCC=CC=CC=CCCCC=CCC=CCC=CC=CCC=C(Br)Br
Policy gradient replay...
Mean value of predictions: 0.586649
Proportion of valid SMILES: 0.7174119885823026
Sample trajectories:
BrCCCCCCCCCCCCCCCCCNCCSc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Nc4ccccc4Br)sc23)c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(CNc2ncnc3c(Nc4ccccc4Br)ncnc23)cc1
Fine tuning...
Mean value of predictions: 0.6140009
Proportion of valid SMILES: 0.6765997490589711
Sample trajectories:
BrC(=Nc1ccc(Br)cc1Nc1ncnc2cc(Br)sc12)C1CC1
BrCCCCCCCCNc1cc(Nc2ncnc3c(Br)cccc23)ccc1Br
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Nc2ccc(Br)c(Br)c2)c2c(Br)sc(Br)c2n1
Brc1cc2c(s1)-c1ccccc1N2

 18 Training on 35150 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.338539
Reward: 3.866268
Trajectories with max counts:
35	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.60259616
Proportion of valid SMILES: 0.6742732103782432
Sample trajectories:
Bc1cccc(Nc2ncnc3cc(Br)c(Br)cc23)c1
BrC=CC=Nc1cccc(Nc2ncnc3ccccc23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(Nc2ncnc3c(Br)csc23)ccc1I
Policy gradient replay...
Mean value of predictions: 0.59807694
Proportion of valid SMILES: 0.7204030226700252
Sample trajectories:
B[PH](=O)(Nc1cccc(Nc2nccc(Br)n2)c1)(c1ccccc1)c1ccccc1
BrNc1ccc2c(Br)cc3ncnc(Nc4ccccc4Br)c3c2c1
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Brc1cc(Br)c2cc(Br)ccc2c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.6365184
Proportion of valid SMILES: 0.6608315098468271
Sample trajectories:
B[PH](=O)(Br)(NCCCCl)NC(=O)C(F)(F)F
BrCCCNCCCCCNc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BrNc1nc(Nc2ccccc2)c2sccc2n1
Brc1cc(Br)cc(Nc2ncnc3cnccc23)c1
Brc1ccc(-c2c(Br)cccc2Nc2ncns2)s1

 19 Training on 38457 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.992610
Reward: 4.022121
Trajectories with max counts:
24	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.66831124
Proportion of valid SMILES: 0.6591619762351469
Sample trajectories:
Bc1ccc(Nc2ncnc3c(Br)cnc(Nc4ccc(Br)cc4)c23)cc1
BrCCNc1ncc2ncnc(Nc3ccc(Br)cc3)c2n1
BrCc1ccc2c(Nc3ccc(Br)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(I)c3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.5185544
Proportion of valid SMILES: 0.7189480354879595
Sample trajectories:
Bc1cc(Nc2ncccc2Br)ccc1Br
Bc1ccc(Nc2ncnc3ncncc23)cc1
BrCCCNc1ccccc1Nc1ccccc1Nc1ccccc1
Brc1cc2c(nc1Br)-c1ccccc1SS2
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1Br
Fine tuning...
Mean value of predictions: 0.6265105
Proportion of valid SMILES: 0.6671875
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNc1ccc(Nc2ncnc3cccc(Br)c23)nc1
BrCc1ccccc1-c1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(Br)c2c(Nc3cc(-c4ccccc4Br)c4ccsc4n3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc(Nc2cc3ncsc3cn2)nc(Nc2ccnc3ccc(Br)cc23)c1

 20 Training on 41672 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.160274
Reward: 4.377375
Trajectories with max counts:
37	CS(=O)(=O)c1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.63076925
Proportion of valid SMILES: 0.6302783859868627
Sample trajectories:
BP(=O)(NO)c1cccc(Br)c1Br
BrC1(CCCCCCCNc2ccccc2)Nc2ccccc21
BrC=CC=CCCC=CC=NNc1ccccc1-c1ccccc1
Brc1cc(Br)c2c(c1)Nc1ncnc2ccc(Br)c2ccccc12
Brc1ccc(-c2ccc3ncnc(Nc4ccc(CNc5ccccc5)cc4)c3c2)cc1
Policy gradient replay...
Mean value of predictions: 0.6465324
Proportion of valid SMILES: 0.7168056446440025
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1ccc(NCc2ccc3ncnc(Nc4cccc(Nc5ccc(Br)cc5)c4)c3c2)cc1
Brc1ccc(Nc2cc(Br)cc3ncnc(Nc4ccc(Br)cc4)c23)cc1
Brc1ccc(Nc2cc(Br)cnc2Oc2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.6565666
Proportion of valid SMILES: 0.6679197994987469
Sample trajectories:
Brc1ccc(Nc2cc3nc(NC(=Cc4ccccc4)c4ccccc4)cnc(Nc4ccccc4)c3ncn2)cc1
Brc1ccc(Nc2cc3ncncc3n3c(Nc4ccccc4Br)ncnc3c2)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4ccc(Br)cc4)c3c2)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4cccc(Br)c4)c3c2)cc1
Brc1ccc(Nc2ccnc(Nc3ncnc4ccccc34)c2)cn1

Trajectories with max counts:
110	CS(=O)(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6172388
Proportion of valid SMILES: 0.5943994474791235
Mean Internal Similarity: 0.4809660813308171
Std Internal Similarity: 0.09726498966753833
Mean External Similarity: 0.42237297734825024
Std External Similarity: 0.07232540480134232
Mean MolWt: 460.46706273458454
Std MolWt: 150.17551264592973
Effect MolWt: -0.312132601866318
Mean MolLogP: 5.993018191242181
Std MolLogP: 3.6960741655444815
Effect MolLogP: 0.4241446410991864
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.569757% (1084 / 1111)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 20, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5730.2070100307465, 'valid_fraction': 0.5943994474791235, 'active_fraction': 0.5910003168902503, 'max_counts': 110, 'mean_internal_similarity': 0.4809660813308171, 'std_internal_similarity': 0.09726498966753833, 'mean_external_similarity': 0.42237297734825024, 'std_external_similarity': 0.07232540480134232, 'mean_MolWt': 460.46706273458454, 'std_MolWt': 150.17551264592973, 'effect_MolWt': -0.312132601866318, 'mean_MolLogP': 5.993018191242181, 'std_MolLogP': 3.6960741655444815, 'effect_MolLogP': 0.4241446410991864, 'generated_scaffolds': 1111, 'novel_scaffolds': 1084, 'novel_fraction': 0.9756975697569757, 'save_path': '../logs/replay_ratio_mixed_s1-1.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.688360
Reward: 1.000000
Mean value of predictions: 0.0011904762
Proportion of valid SMILES: 0.7912087912087912
Sample trajectories:
BrC1=Nc2ccccc2C(Nc2ccccc2)=N1
Brc1ccc(-n2cc3ccncc3c2)cc1
Brc1cccc(-c2ccc(-c3noc(-c4ccccc4)n3)cc2)c1
Brc1cccc(Br)c1Br
C#CC(C)(O)C1C=C(CO)C2CCC13CC(C)C(O)C23
Policy gradient replay...
Mean value of predictions: 0.02200865
Proportion of valid SMILES: 0.6515341264871635
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC)OCOC(=O)C=CC1OC(N2C=CC(=O)NC2=O)C(O)C1O
Brc1ccc(-c2cc(-c3ccccc3)nc3nccc(N4CCOCC4)c23)cc1
Brc1ccc(-c2cn[nH]n2)cc1
Brc1ccc(-c2ncc(Cc3ccc4[nH]cnc4c3)c3cccnc23)cc1
Fine tuning...
Mean value of predictions: 0.023302842
Proportion of valid SMILES: 0.6506892230576441
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C(O)C1O)Oc1ccc(Cl)cc1
BrC1=C(Oc2ccccc2Br)C=CC=CC1Br
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cccc(Nc2ncnc3[nH]ncc23)c1
Brc1cccc(Nc2ncnc3cccc(-c4ccccc4)c23)n1

  2 Training on 364 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.064787
Reward: 1.090000
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.033134922
Proportion of valid SMILES: 0.631578947368421
Sample trajectories:
BP1(=O)OCC2OC(C(O)C2O)N(C=CC(=O)NS(=O)(=O)N2CCCCCC2)CC1OC
Brc1ccc(-c2ccccn2)c2ncnc(Nc3ccccc3)c12
Brc1ccc(-c2nc(-c3ccccc3)n3ncnc3n2)o1
Brc1ccc(CCN2CCSc3ccccc32)cc1
Brc1ccc(Nc2cc(N3CCN(C4CCCCC4)CC3)ncn2)cc1
Policy gradient replay...
Mean value of predictions: 0.059050772
Proportion of valid SMILES: 0.5669586983729662
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)O
BrC(=NNc1ncnc(Br)c1Br)c1ccco1
BrC1=C(Nc2ccc(I)cc2Br)CCCCCCCC1
Brc1cc(Br)c(Br)cn1
Brc1ccc(-c2ccccc2)cc1Nc1ncnc2c1cnn2-c1ccncc1
Fine tuning...
Mean value of predictions: 0.08751334
Proportion of valid SMILES: 0.5867251095804634
Sample trajectories:
BrC=Cc1ccccc1
Brc1ccc(-c2nc(CN3CCCC3)nc3ccccc23)cc1
Brc1ccc(C2=CN3CCCCC23)o1
Brc1ccc(CCNc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(CNc2nc3ccccc3s2)cc1

  3 Training on 840 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.135282
Reward: 1.093731
Trajectories with max counts:
8	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.08472146
Proportion of valid SMILES: 0.5678045725023488
Sample trajectories:
Brc1cc(Br)c(Nc2ncnc3c2-c2nc4ccccc4ncnc23)cn1
Brc1cc(Br)cc(Nc2ncnc3cccnc23)c1
Brc1ccc(-c2ccncc2)c(NN=C2CCCCCC2)c1
Brc1ccc(-c2cnc(Nc3c4ccccc4nc4ccccc34)s2)c2ccccc12
Brc1ccc(-c2cncnc2)cc1
Policy gradient replay...
Mean value of predictions: 0.1127714
Proportion of valid SMILES: 0.4973007303906002
Sample trajectories:
BrCC(Oc1ccc(Nc2c(-c3ccnc(Nc4ccc(Br)cc4)c3)c(Br)c(Br)c(Br)c3c(Nc4ccc(Br)cn4)nnc23)cc1)=C(Br)Br
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3cccnc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ncc(Br)c(Br)n3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.12180686
Proportion of valid SMILES: 0.6030056355666875
Sample trajectories:
BP(=O)(Nc1cccc(Br)c1)N(c1ccc(Br)cc1)c1cc(Cl)cc(Br)c1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCCCC)ONC1CCCC1
BrC=CCCCCCCCCCCC=CC=CC=CC=CCN1CCN(Cc2ccccc2)CC1
BrNc1c2cc(Br)c(Br)cc2c(Br)c(Br)nc2ccsc12

  4 Training on 1613 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 21.507823
Reward: 1.394407
Trajectories with max counts:
17	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.1509081
Proportion of valid SMILES: 0.5687010954616588
Sample trajectories:
BP(=O)(NP(=O)(O)OCC1OC(C(F)(F)F)NC1NS(=O)(=O)OC(C)(F)F)OCC1(CC=C(Br)C(F)(F)F)CCCO1
BP(=O)(OCC1OC(n2nnc3c(N)ncnc32)C(O)C1O)P(=O)(O)O
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2cc(Br)cc(Br)n2)c1
Brc1ccc(-c2cc(Nc3ncnc4c3ncn4-c3ccccc3)c3ccccc3n3ccnc3n2)cc1
Policy gradient replay...
Mean value of predictions: 0.0772007
Proportion of valid SMILES: 0.71
Sample trajectories:
BP(=O)(OCC(=O)Nc1cccc2ccccc12)c1cccc2ccccc12
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)S(=O)(=O)c1ccc(Br)cc1
BrCC(Br)=CCn1ccc2c(-c3ccccc3Br)ncnc21
Brc1cc(Nc2ncnc3ccccc23)ccn1
Brc1cc2c(-c3ccccc3Br)ncnc2[nH]1
Fine tuning...
Mean value of predictions: 0.20480089
Proportion of valid SMILES: 0.5733500156396621
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC)OC(=O)OP(=O)(O)OP(=O)(O)OCCCl
Brc1cc(Br)c(-c2ccc(Nc3ncnc4ccccc34)cc2)cc1Br
Brc1cc(Br)c2c(Nc3cc(Br)c(Br)cn3)ncnc2c1
Brc1cc(Br)nc(-c2nc3ccccc3n2-c2ccccc2)c1

  5 Training on 2708 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 20.860355
Reward: 1.699636
Trajectories with max counts:
39	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.2648712
Proportion of valid SMILES: 0.5344180225281602
Sample trajectories:
BP(=O)(OCC)C(=O)OC
Br
BrC(Br)=C(Br)I
BrC=C1Nc2ncnc(Nc3cc(Br)cc(Br)c3)c2Nc2ccccc21
BrCBr
Policy gradient replay...
Mean value of predictions: 0.103889145
Proportion of valid SMILES: 0.6990625
Sample trajectories:
BrC(Br)Br
Brc1ccc(-c2nc3ccccc3s2)o1
Brc1ccc(Cc2ccc3ccccc3c2)cc1
Brc1ccc(Nc2cccc3c(Nc4ccccc4Br)ncnc23)cc1
Brc1ccc(Nc2cccnc2)cc1
Fine tuning...
Mean value of predictions: 0.23333335
Proportion of valid SMILES: 0.5649045980606819
Sample trajectories:
BP(=O)(NC(c1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O)Oc1nc(Br)c(Br)[nH]1
BP(=O)(OCC)C(Nc1ccc(Br)cc1)P(=O)(Oc1ccc(Br)cc1)P(=O)(Oc1ccccc1)N(=O)=O
BrC1=CCCCC1
Brc1cc(Br)c2c(Nc3ccc(Br)c4ccccc34)ncnc2c1
Brc1cc(Br)c2ncsc2c1Nc1cc2c(Nc3ccccc3)sc(Br)c2[nH]1

  6 Training on 3926 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 21.454012
Reward: 1.871241
Trajectories with max counts:
50	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.29263407
Proportion of valid SMILES: 0.5303125
Sample trajectories:
BP(=O)(C=O)OCCO
BP(=O)(OCC)OC(=O)c1ccc(Br)c(Br)c1N(=O)=O
BP(=O)(Oc1ccc(N(=O)=O)cc1)P(=O)(O)Oc1ccc(Cl)cc1
Brc1cc(-c2ccccc2)c2scnc2n1
Brc1cc(-c2cncnc2)c2c(c1Br)c1cn[nH]c1S2
Policy gradient replay...
Mean value of predictions: 0.22374207
Proportion of valid SMILES: 0.6396875
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Bc1cccc(Nc2ncnc3ccsc23)c1
BrBr
BrC(=Cc1cc(Br)ccc1Br)c1ccc2ccccc2c1
Fine tuning...
Mean value of predictions: 0.2889129
Proportion of valid SMILES: 0.5783547075383172
Sample trajectories:
BP(=O)(Nc1ccc(C(F)(F)F)cc1)P(=O)(O)O
BP(=O)(Oc1cc(Br)c(Br)c(Br)c1)N(O)C=O
BrC(Br)(Br)Br
BrCCBr
BrCc1ccccc1Nc1ncnc2c(Nc3ccc(Br)cc3)ncnc12

  7 Training on 5529 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 24.179576
Reward: 2.391537
Trajectories with max counts:
63	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.32920694
Proportion of valid SMILES: 0.48483901219130976
Sample trajectories:
BP(=O)(Br)Oc1ccc(Nc2nc(Cl)cc(Cl)c2O)c(O)c1O
BP(=O)(Nc1cc(Br)cc(Br)c1)P(=O)(O)Oc1ccc(Br)cc1
BP(=O)(OC(C)=O)C(=O)Oc1ccc(Br)cc1
BP(=O)(OC)OC
BP(=O)(OC)OCC
Policy gradient replay...
Mean value of predictions: 0.2737309
Proportion of valid SMILES: 0.6340625
Sample trajectories:
BP(=O)(OCC)OC(=O)COP(=O)(NC(=O)c1ccc(F)cc1)C(=O)O
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)OCP(=O)(O)O
BP(=O)(OCOc1ccccc1)C(Cl)P(=O)(O)O
BrCCNc1ncnc2c(Nc3ccc(Br)cc3)ncnc12
Brc1cc(Br)c2ccccc2c1Nc1ccccc1
Fine tuning...
Mean value of predictions: 0.29347366
Proportion of valid SMILES: 0.5941213258286429
Sample trajectories:
BP(=O)(OCC)N(O)c1ccc(Br)cc1Br
BrC(Br)Br
BrCC(=NNc1cccc(Br)c1)Nc1ncnc2sccc12
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc2c(Nc3cccc(Br)c3-c3ccccc3)ncnc2cn1

  8 Training on 7201 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 25.781413
Reward: 2.682025
Trajectories with max counts:
53	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.32191464
Proportion of valid SMILES: 0.5422138836772983
Sample trajectories:
BP(=O)(C=C(O)CNP(=O)(OC(C)=O)C(O)CNC(=O)OP(=O)(O)OP(=O)(O)OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)NO
BP(=O)(COc1ccc(Nc2c(F)cccc2F)nc1)NC(=O)OCc1cncnc1
BP(=O)(NCCCl)N1CCN(C(=O)c2ccc(Br)cc2)CC1
BP(=O)(OCC)OC(=O)C=CC(=O)NN=Cc1ccc(Br)cc1
BrC(=CC=Cc1ccccc1)c1ccccc1Nc1cccc2cc(Br)ccc12
Policy gradient replay...
Mean value of predictions: 0.370615
Proportion of valid SMILES: 0.5494367959949937
Sample trajectories:
BP(=O)(Oc1ccc(F)cc1)c1ccc(F)cc1F
B[PH](=O)(=NO)Nc1cc(Br)c(Cl)c(Br)c1
B[PH](=O)(OCC)=C(Nc1ccc(Br)o1)[PH](=O)(=S)Br
Bc1cccc(Nc2ncnc3sc4ccccc4c23)c1
Brc1cc(-c2cnc(Nc3ccc(Br)s3)c(Br)c2)on1
Fine tuning...
Mean value of predictions: 0.3755592
Proportion of valid SMILES: 0.5733500156396621
Sample trajectories:
BP(=O)(NC(=O)CCCCl)OP(=O)(O)c1cc(Br)cc(Br)c1
BP(=O)(OCC)OC(=O)C(=O)OCC(=O)Nc1ccc(Br)cc1
BP1(=O)OCC(CC(=O)Oc2ccc(NC(=S)Oc3cccc4c(c3)nc3ccccc3C4=O)cc2)C(O)C(O)C1O
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1

  9 Training on 9010 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 27.046094
Reward: 3.046330
Trajectories with max counts:
171	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.33592704
Proportion of valid SMILES: 0.5140625
Sample trajectories:
BP(=O)(NP(=O)(O)OP(=O)(O)CBr)OCCCCCCCCCl
BP(=O)(OC)OC(=O)C=CCCC=CC(=O)N(O)Br
BP(=O)(OCC)Oc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
B[PH](=O)(CC=C)(OCC)P(=O)(Oc1ccccc1)Oc1ccccc1F
Bc1ccc(Nc2cc(Br)cc(Br)c2)cc1Nc1cccc(Nc2c(Br)cccc2Br)c1
Policy gradient replay...
Mean value of predictions: 0.43751466
Proportion of valid SMILES: 0.533458411507192
Sample trajectories:
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccccc3Br)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.39286944
Proportion of valid SMILES: 0.5437773608505316
Sample trajectories:
BP(=O)(C(=O)Nc1cccc(Br)c1)N1CCNC1
BP(=O)(OCCCCN)n1cnc2c(Br)c3c(Br)cc(Br)cnc3cc2c1N
Bc1cccc(Nc2ncnc3cccc(Br)c23)c1
Br
BrC=Cc1cnc2ccccc2c1Nc1cccc(Br)c1

 10 Training on 10548 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 26.001338
Reward: 3.330106
Trajectories with max counts:
102	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.47984618
Proportion of valid SMILES: 0.40663121676571784
Sample trajectories:
BP(=O)(Nc1nc(N)nc2c1ncn2C(F)(F)P(=O)(O)O)OCC
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC)OC(=O)C=CC(=O)Nc1cc(Br)sc1Br
BP(=O)(OCC)OC(=O)CCC(O)C(Br)Br
BP(=O)(OCC)OC(=O)CCCCCl
Policy gradient replay...
Mean value of predictions: 0.4523622
Proportion of valid SMILES: 0.47699530516431926
Sample trajectories:
BP1(=O)OCC(OC(=O)C(C)(C)P(=O)(O)O)C(O)C(n2cnc3c(N)ncnc32)O1
Bc1ccc(Nc2ncnc3ccsc23)cc1Br
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Br
BrC=C(Br)Br
Fine tuning...
Mean value of predictions: 0.43337235
Proportion of valid SMILES: 0.5340838023764853
Sample trajectories:
BP(=O)(OCCCCCl)OP(=O)(O)O
Bc1ccnc(Nc2ncnc3cc(Br)ccc23)c1
BrCCCC1Oc2ccc1nc(Nc1cc3c(Br)c(Br)ccc3s1)cc2Br
Brc1cc(-c2ccccc2)n2ncnc2n1
Brc1cc(Br)c(-c2ccc(Nc3ncncn3)cc2)cc1Br

 11 Training on 12063 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.917060
Reward: 3.092629
Trajectories with max counts:
87	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.46206453
Proportion of valid SMILES: 0.48513302034428796
Sample trajectories:
BC(=O)Nc1cc2c(Nc3ccc4nc(Cl)sc4c3Cl)ncnc2s1
BP(=O)(N(CCCNC(=O)c1ccc(Nc2ncnc3c(N)cc(Br)cc23)cc1)P(=O)(O)O)P(=O)(O)O
BP(=O)(NS(=O)(=O)Nc1cc(Br)cc(N(=O)=O)c1)OCCS
BP(=O)(OCC)OC(=O)C=CC=CC=CCC(=O)O
BP(=O)(OCC)OC(=O)CNC(=O)C=CCl
Policy gradient replay...
Mean value of predictions: 0.47787717
Proportion of valid SMILES: 0.5242263207252267
Sample trajectories:
BP(=O)(On1cnc2cnc(Nc3ccc(Br)cc3)nc21)N1CC2CCCCC2C(=O)NC1=O
BP(=O)(c1cc2c(Nc3ccc(Cl)s3)c(-c3ccc(Br)cc3)ncn2c1Br)N1CCOCC1
Bc1cc2c(Nc3cc(Br)cc(Br)c3)ncnc2cc1Br
Bc1cc2cnc3c(Nc4cccc(Br)c4)ncnc3c2cc1Br
BrCc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1Br
Fine tuning...
Mean value of predictions: 0.46543947
Proportion of valid SMILES: 0.5264145045326665
Sample trajectories:
BP(=O)(OCC)c1ccc(Nc2ncnc3c(Br)cncc23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1Br
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCc1ncc2ncnc(Nc3ccc(Br)c(Br)c3)c2n1

 12 Training on 13789 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.881936
Reward: 3.370844
Trajectories with max counts:
78	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5065205
Proportion of valid SMILES: 0.5415625
Sample trajectories:
BP(=O)(OCCOC)n1cnc2c(Nc3ccc(Br)cc3Br)ncnc21
Bc1ccc(N(=O)=O)cc1
Br
BrC(Br)(Br)Br
BrC1CCC(N2CC(CC3CCC3=Cc3ccc(Nc4ncnc5sccc45)nc3)O2)C1
Policy gradient replay...
Mean value of predictions: 0.467893
Proportion of valid SMILES: 0.5615023474178403
Sample trajectories:
BP(=O)(Nc1ccc(Nc2ncnc3c(Cl)c(Br)cnc23)cc1)c1ccc(Br)cc1
Br
BrC(Br)=C(I)c1cccc(Nc2ncnc3ccc(Br)cc23)n1
BrC1=Nc2ccc(Br)cc2Nc2ncnc(Nc3ccc(Br)s3)c21
BrC=CBr
Fine tuning...
Mean value of predictions: 0.4375
Proportion of valid SMILES: 0.58
Sample trajectories:
BP(=O)(OCCC)P(=O)(OCC(F)(F)F)C(F)(F)F
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Nc2cc(Br)c(Br)s2)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3ccc(I)cc23)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3cccc(Br)c23)cc1Br

 13 Training on 15727 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.333531
Reward: 3.375973
Trajectories with max counts:
64	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.49525464
Proportion of valid SMILES: 0.5406758448060075
Sample trajectories:
BP(=O)(NC(=O)Cc1ccc(Br)cc1)P(=O)(O)O
BrC(=Cc1cccc(Br)c1)Cc1cc(Nc2ncnc(N3CCCC3)c2I)ccc1Br
BrC(=NNc1nccs1)c1ccc(Br)cc1
BrC1=Nc2ncnc(Nc3ccc(Br)cc3)c2N=C1
BrCc1cc(Br)cc(Nc2ncc(Br)c(Nc3ccc(Br)cc3)n2)c1
Policy gradient replay...
Mean value of predictions: 0.49863634
Proportion of valid SMILES: 0.5508607198748043
Sample trajectories:
BP(=O)(Nc1cc2c(c(Nc3sc(Br)c(Br)c3Br)n1)OC2=O)OCCBr
BrCCCNCCCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)c(Nc2cc(I)ccc2Br)cc1Br
Brc1cc(Br)c(Nc2ncnc3ccsc23)cc1Br
Fine tuning...
Mean value of predictions: 0.43694955
Proportion of valid SMILES: 0.5820568927789934
Sample trajectories:
BP(=O)(NCCCCN)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCC(=O)Nc1ccc(Br)cc1)P(=O)(Oc1ccccc1)Oc1ccc(Br)c(Br)c1
BP(=O)(ONCCCCN1C(=O)Oc2ccccc21)c1c(Br)cc(F)c(F)c1F
Bc1ccc(Nc2ncnc3ccsc23)cc1
Br

 14 Training on 17651 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.934629
Reward: 3.283758
Trajectories with max counts:
12	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
12	Clc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4413723
Proportion of valid SMILES: 0.6211219053588217
Sample trajectories:
BP(=O)(OCC1OC(N2C=C(Br)C(=O)NC2=O)C(O)C(O)C(O)C1O)Oc1ccc(Br)cc1
BrCc1nc2c(Nc3ccc(Br)c(-c4ccccc4Br)c3)ncnc2s1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.51980036
Proportion of valid SMILES: 0.564142678347935
Sample trajectories:
BP(=O)(OCC)N1C=C(Br)C(=O)Nc2cc(Br)c(Br)cc21
BP(OP(=O)(O)OP(=O)(O)OCCNCCCCCl)C(Cl)(Br)C(=O)OP(B)(=O)OCC
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1Br
BrCc1ccc2c(Nc3ccc(Br)cc3Br)ncnc2c1
Brc1cc(Br)c(Br)c(-c2ccc(Nc3ncnc4cc(Br)ccc34)cc2)c1
Fine tuning...
Mean value of predictions: 0.51626605
Proportion of valid SMILES: 0.5594121325828643
Sample trajectories:
BP(=O)(OCC)N1CCCC(N(O)C(=O)Nc2ccc(Br)cc2)C1
BP(=O)(OCCCF)OP(=O)(O)OP(=O)(O)O
Bc1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1
Brc1cc(Br)c(-c2ccnc3cccnc23)nc1Nc1ncnc2sc(Br)c(Br)c12
Brc1cc(Br)c(Nc2ncnc3cnc(Nc4ccccc4)cc23)nc1Br

 15 Training on 19766 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.929536
Reward: 3.697371
Trajectories with max counts:
157	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5722187
Proportion of valid SMILES: 0.49765405067250545
Sample trajectories:
BP(=O)(NO)c1cc(Nc2cc(Br)c(Br)c(Br)c2Br)ccc1Br
BP(=O)(Nc1ccc(Nc2ncnc3ccc(Br)cc23)cc1)c1ccc(Br)cc1
BP(=O)(OCCSSCC(=O)O)N1CCCC1
BP(=O)(c1cc(O)c(Br)c(Br)c1)N(O)C=O
BrBr
Policy gradient replay...
Mean value of predictions: 0.41000003
Proportion of valid SMILES: 0.6376992810253204
Sample trajectories:
BP(=O)(Br)Oc1cccc(Nc2ncnc3ccccc23)c1
BP(=O)(NC=O)OCC
BP(=O)(OCC)c1cccc(Nc2ncnc3c(Br)c(F)cc(F)c23)c1
Bc1ccccc1Nc1cccc(Nc2ncnc3ccsc23)c1
BrC#CCNc1cccc(Nc2ncnc3cc(Br)c(Nc4ccccc4Br)nc23)c1
Fine tuning...
Mean value of predictions: 0.514131
Proportion of valid SMILES: 0.5773679274773367
Sample trajectories:
BrC(Br)(Br)Br
BrCCN1CCN2CCCCC12
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Br)c(Nc2cnc(Br)cn2)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1

 16 Training on 21842 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.972112
Reward: 3.911325
Trajectories with max counts:
228	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.49667197
Proportion of valid SMILES: 0.394375
Sample trajectories:
BP(=O)(Nc1cccc(Br)c1)C1CCC(=O)c2cc(Br)sc2N1
BP(=O)(OCC)c1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Bc1cc2c(Nc3cc(Br)cc(Br)c3Br)cccc2cc1Br
Bc1ccc(Nc2ncnc3c(Br)c(Br)cc(Br)c23)cc1Br
Bc1ccc(Nc2ncnc3c(Br)sc(Br)c23)cc1Br
Policy gradient replay...
Mean value of predictions: 0.5107726
Proportion of valid SMILES: 0.5745545482963426
Sample trajectories:
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Nc2ncnc3c(Br)c(Br)c(Nc4ccc5ccccc5c4)cc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.51856536
Proportion of valid SMILES: 0.5926852141294154
Sample trajectories:
BP(=O)(NO)c1ccc(Nc2ncnc(N)c2C(N)=O)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1Br
BrC(=Nc1ccc(Nc2ncnc3ccccc23)cc1)c1ccc(Br)cc1Br
BrCc1sc2ncnc(Nc3ccc(Br)cc3)c2c1Br

 17 Training on 23819 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.968132
Reward: 3.813899
Trajectories with max counts:
50	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.50721866
Proportion of valid SMILES: 0.5891181988742964
Sample trajectories:
BP(=O)(OCC)c1c(Cl)cc(-c2ccc(Br)cc2)c(C(=O)OCC=C)c1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccccc4)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3cccc(Nc4cc(Nc5ncnc6ccc(Br)cc56)ccc4Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Policy gradient replay...
Mean value of predictions: 0.49597824
Proportion of valid SMILES: 0.575
Sample trajectories:
BrC(Br)=Cc1cc2ncnc(Nc3cc(Br)cc(Br)c3)c2cc1Br
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(-c2ccc(Nc3ncnc4cc(Br)c(Br)cc34)cc2)cc1Br
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c(Nc2ncnc3sc(Br)cc23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.5393397
Proportion of valid SMILES: 0.5586120662707096
Sample trajectories:
BP1(=O)Nc2cc(Br)cc(Br)c2N1
Br
BrC(=Cc1ccc(Br)cn1)Nc1ncnc2sccc12
BrC=CBr
Brc1c(Nc2ccccc2)c2c(Nc3ccccc3)ncnc12

 18 Training on 26088 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.220839
Reward: 3.842926
Trajectories with max counts:
114	Brc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.48613992
Proportion of valid SMILES: 0.48265082838386997
Sample trajectories:
BP(=O)(NO)c1ccc(Br)cc1
BP(=O)(Nc1cc(Br)c(Br)cc1Br)c1cccc(Br)c1
BP(=O)(OCC(=O)Nc1ccccc1)c1ccc(Br)cc1
BP(=O)(OCC)C(F)(F)F
BrC=CC=CC#CC(Br)Br
Policy gradient replay...
Mean value of predictions: 0.2713157
Proportion of valid SMILES: 0.6721875
Sample trajectories:
BP(=O)(OCC)c1ccccc1-c1ccc(Br)cc1
BrC=CBr
BrC=CC=CBr
Brc1cc(Br)c2cccc(Br)c2c1
Brc1cc(Nc2ccccc2Br)c2ncnc(Nc3c(Nc4ccccc4Br)ccc4cccnc34)c2n1
Fine tuning...
Mean value of predictions: 0.5572762
Proportion of valid SMILES: 0.5692403876211316
Sample trajectories:
BP(=O)(NC(=O)CCCCCCCCCCCCCCCCCCCCl)OCCCF
Bc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Bc1ccc(Nc2ncnc3cnc(-c4ccccc4Br)nc23)cc1
BrCCNCCCSc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Nc2ncnc3c(Br)cccc23)c(Br)c1

 19 Training on 27921 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.608507
Reward: 4.113857
Trajectories with max counts:
125	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.59921104
Proportion of valid SMILES: 0.4753125
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1ccc(Br)c(Br)c1
B[PH](=O)(=Nc1cc(Br)c(Br)c(Br)c1)Nc1ccc(Br)c(Br)c1
Bc1cc(Br)c2c(Nc3ccc(Br)cc3Br)nc(Nc3ccc(Br)cc3)n2c1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCc1nc2c(Nc3ccc(Br)cn3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.5439544
Proportion of valid SMILES: 0.6021875
Sample trajectories:
BP(=O)(CC(=C)Br)NO
BP(=O)(CC)NCc1ccc(F)cc1
BP(=O)(NC(c1ccccc1)P(=O)(O)OP(=O)(O)Nc1cc(Br)c(Br)cc1Cl)C(=O)O
BP(=O)(NCCO)NCCCO
B[PH](=O)OCC1CCCN(C(=O)C(Cl)Cl)C1
Fine tuning...
Mean value of predictions: 0.58679545
Proportion of valid SMILES: 0.5827338129496403
Sample trajectories:
BP(=O)(NN=Cc1ccc(Br)cc1)P(=O)(Oc1ccccc1)Oc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1Br
BrC=CC=CC=CC=CCBr
BrCCCN(c1cccc(Br)c1)c1nc2c(Nc3ccc(Br)cc3)ncnc2cc1Br
Brc1c(Br)c2c(Nc3c[nH]cn3)cccn2c1Br

 20 Training on 30364 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 33.913343
Reward: 4.492300
Trajectories with max counts:
104	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5409657
Proportion of valid SMILES: 0.6385886840432295
Sample trajectories:
BP(=O)(CCCl)N(Nc1ccc(Br)cc1)P(=O)(c1ccccc1)c1ccccc1
BP(=O)(Nc1ccc(Br)c(Br)c1)C(=O)c1ccc(Br)cc1
Br
BrBr
BrC#CCc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.49183884
Proportion of valid SMILES: 0.6053783614759225
Sample trajectories:
BP(=O)(c1ccc(F)cc1)N(CCl)CCCl
BrC(=NNc1ccc(Br)c(Br)c1)c1ccc(Br)cc1
BrCCCCCNc1ccc2ccc3ccccc3c2n1
BrCCN(Br)c1ccc(Br)c(Br)c1Br
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.5808668
Proportion of valid SMILES: 0.59125
Sample trajectories:
BP(=O)(NC(=O)C(Br)Br)OCC
Bc1ccc2c(Nc3ccc(Br)o3)ccnc2c1
BrCSc1nc2c(Nc3cccc(Br)c3)ncnc2s1
BrCc1ccc2c(Nc3ccc(Br)cc3)ncnc2c1
Brc1c(Br)c2c(Br)cc3c(nc4ccccc4N3)c2c1Br

Trajectories with max counts:
299	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.52784365
Proportion of valid SMILES: 0.5117544079029636
Mean Internal Similarity: 0.4608271807986735
Std Internal Similarity: 0.0958529585274817
Mean External Similarity: 0.4138999553262296
Std External Similarity: 0.07079602870663355
Mean MolWt: 416.4620939092454
Std MolWt: 97.65958489148842
Effect MolWt: -0.820314487551149
Mean MolLogP: 5.0231990584809525
Std MolLogP: 1.5359716157445895
Effect MolLogP: 0.21149052837659704
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.338403% (1024 / 1052)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 10, 'n_policy_replay': 15, 'n_fine_tune': 20, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5624.710787296295, 'valid_fraction': 0.5117544079029636, 'active_fraction': 0.5034819792302994, 'max_counts': 299, 'mean_internal_similarity': 0.4608271807986735, 'std_internal_similarity': 0.0958529585274817, 'mean_external_similarity': 0.4138999553262296, 'std_external_similarity': 0.07079602870663355, 'mean_MolWt': 416.4620939092454, 'std_MolWt': 97.65958489148842, 'effect_MolWt': -0.820314487551149, 'mean_MolLogP': 5.0231990584809525, 'std_MolLogP': 1.5359716157445895, 'effect_MolLogP': 0.21149052837659704, 'generated_scaffolds': 1052, 'novel_scaffolds': 1024, 'novel_fraction': 0.973384030418251, 'save_path': '../logs/replay_ratio_mixed_s1-2.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.761316
Reward: 1.000000
Trajectories with max counts:
2	Cc1cccc(O)c1
Mean value of predictions: 0.00081103
Proportion of valid SMILES: 0.7766929133858268
Sample trajectories:
Brc1ccc(Nc2ncnc3nc(Br)nn23)cc1
C#CC(=O)N1CCN(CCOC(=O)c2ccccc2)C(C)C1
C#CC(CCC(=C)CCCCC#N)c1cn(O)c2c1C(C)(C)NC2(C)C
C#CCCCCCC1C(C)CCCN1CCC#N
C#CCCCCCCCC(=O)N1CCC(P(=O)(O)COC)CC1
Policy gradient replay...
Mean value of predictions: 0.012785388
Proportion of valid SMILES: 0.6169014084507042
Sample trajectories:
Brc1cc2c(c3c1CCNC3)OCCO2
Brc1ccc(NN=C2CCCCCC2)cc1
Brc1ccc(Nc2ccnc3ncnc(Nc4cccc(Br)c4)c23)cc1
Brc1ccc(Nc2ncnc3ncncc23)nc1
Brc1ccc(Nc2onc(-c3ccccc3)c2-c2ccc3[nH]ccc3n2)cc1
Fine tuning...
Mean value of predictions: 0.020717131
Proportion of valid SMILES: 0.6286787726988102
Sample trajectories:
B[PH](=O)(NO)(NC(=O)c1ccc(Br)c(Br)c1)Nc1ccc(Br)cc1
Brc1ccc(Nc2ncnc3cccnc23)nc1
Brc1cccc(-n2ncc3ccccc32)n1
Brc1cccc(Nc2ncnc3c(Br)cc(Br)cc23)c1
Brc1cccc2ccc(Nc3nc(Br)c4ccccc4n3)cc12

  2 Training on 323 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.489611
Reward: 1.043665
Trajectories with max counts:
5	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.028792571
Proportion of valid SMILES: 0.606762680025047
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3c(-c4ccncc4)ncnc23)c1
Brc1cc2onc(Br)c2cc1Br
Brc1ccc(-c2ccc3c(c2)OCO3)c2ccccc12
Brc1ccc(-c2nc3ccc(Br)nc3[nH]2)cc1
Brc1ccc(Cc2ncnc(N3CCN(c4ccncc4)CC3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.038625002
Proportion of valid SMILES: 0.5047318611987381
Sample trajectories:
Brc1ccc2c(c1)-c1ncnc(-c3csc(Br)c3)c1CCC2
Brc1ccc2c(c1)C(N1CCCCC1)=N2
Brc1ccc2c(c1-c1ccccc1)-c1[nH]c(-c3ccccc3)nc1-c1ccccc1-c1nn2[nH]1
Brc1ccc2oc(-c3ccc(-c4ccc(OCCN5CCCC5)cc4)s3)nc2c1
Brc1cnc2ncnn2c1
Fine tuning...
Mean value of predictions: 0.07135263
Proportion of valid SMILES: 0.5298904538341158
Sample trajectories:
Brc1cc(-c2ccc3c(n2)NCCC3)c2ccccc2n1
Brc1ccc(-c2ccc(Nc3ncnc4cc(Br)ccc34)s2)cc1
Brc1ccc(-c2ccccc2)c2ccc(Nc3ccncc3)cc12
Brc1ccc(C=Nc2ncnc(-c3ccc(Br)cc3)n2)cc1
Brc1ccc(N=Nc2ncnc3nc[nH]c23)cc1

  3 Training on 668 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 22.106880
Reward: 1.351598
Trajectories with max counts:
2	COc1cc2ncnc(Nc3ccc(F)c(Cl)c3F)c2cc1OC
2	COc1cc2ncnc(Nc3cccc(F)c3)c2cc1OC
2	Clc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
2	Clc1ccc(Nc2ncnc3ccc(Cl)cc23)cc1
2	Clc1ccc2c(Nc3ccncn3)ncnc2c1
2	Fc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
2	Fc1ccc(Nc2ncnc3ccc(F)cc23)cc1
2	Nc1ccc2ncnc(Nc3ccc(F)cc3)c2c1
2	Nc1ccc2ncnc2c1
2	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.07767804
Proportion of valid SMILES: 0.5233322893830253
Sample trajectories:
B[PH](=O)(Cl)(OCCCl)P(O)(F)(F)(F)(F)F
BrBr
BrCCN(CCN1CCCCCC1)C(Sc1ccncn1)c1ccccc1
BrCCNc1nc(NCc2cnn(-c3cccc4ccccc34)c2)c2ccc(Br)cc2n1
Brc1cc2c(s1)N2
Policy gradient replay...
Mean value of predictions: 0.14028952
Proportion of valid SMILES: 0.5195863365716077
Sample trajectories:
Brc1cc2ncnc(Nc3ccc(N4CCCCC4)cc3)c2nc1-c1cnc2ccccc2n1
Brc1ccc(-c2ccc(Nc3ncnc4ccsc34)cc2)cc1
Brc1ccc(Nc2cncnc2)cc1
Brc1ccc(Nc2nc(Nc3ccc(Br)c(Br)c3)nc3ncnc(Nc4cccc(Br)c4)c23)cc1
Brc1ccc(Nc2nc3cncnc3s2)cc1
Fine tuning...
Mean value of predictions: 0.11256656
Proportion of valid SMILES: 0.5872420262664165
Sample trajectories:
BP(=O)(OCC)OC(=O)CP(=O)(O)OP(=O)(O)O
Brc1cc(-c2cc(N3CCCC3)c3cccc(Br)c3n2)ccn1
Brc1cc2c(Nc3ccncc3)ncnc2cc1CN1CCCCCC1
Brc1ccc(-c2cc3cncnc3c3ccccc23)c2ccncc12
Brc1ccc(-c2nc3ccccc3[nH]2)c2ccccc12

  4 Training on 1442 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 22.239232
Reward: 1.539709
Trajectories with max counts:
35	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.13599999
Proportion of valid SMILES: 0.5393996247654784
Sample trajectories:
Brc1cc(Nc2nccs2)ccn1
Brc1cc2ncnc(N3CCOCC3)c2cc1Br
Brc1ccc(Nc2ccnc3ccnc(C4CCCC4)c23)cc1
Brc1ccc(Nc2ccnc3ccncc23)cc1
Brc1ccc(Nc2ccncc2)cc1-c1cnc2ncncc2c1
Policy gradient replay...
Mean value of predictions: 0.12446974
Proportion of valid SMILES: 0.6042513285401688
Sample trajectories:
BrC1=C2C=CC=CN2c2ncnn21
Brc1cc2c(Nc3ccccc3)ncnc2cc1NCCN1CCCC1CNc1cccc2ccccc12
Brc1cc2c(cc1-c1ccccc1)-c1ccccc1Cc1nccn1C=N2
Brc1ccc(-c2ncnc3ccccc23)cc1
Brc1ccc(N(c2ccccc2)c2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.1558212
Proportion of valid SMILES: 0.6020025031289111
Sample trajectories:
BrC=Cc1cnc2c(Nc3ccc(Br)cc3)ncnc2c1
BrCc1cc2cc(Br)ccc2[nH]1
Brc1cc2c(s1)c1c(Br)ncc(Br)c1ncnc(-c1ccccc1)N2
Brc1cc2c(sc3ncnn13)CCC2
Brc1ccc(-c2nc(Nc3ccccn3)cnc2-c2ccncc2)s1

  5 Training on 2467 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 22.159357
Reward: 1.680130
Trajectories with max counts:
73	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.14272863
Proportion of valid SMILES: 0.6255079712410128
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2cc1Nc1ncnc2ccccc12
Brc1ccc(-c2cccc3ccccc23)c2cnccc12
Brc1ccc(N=Nc2cccc(Br)c2)cc1
Brc1ccc(Nc2cc3c(ncn2)ncnc2ccc(Nc4ccccc4)cc23)nc1
Brc1ccc(Nc2ccc(Nc3ncnc4ccc(Br)cc34)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.10927644
Proportion of valid SMILES: 0.6739606126914661
Sample trajectories:
BrC1CCN(c2nc3cccnc3nc2Nc2ncnc3ccccc23)CC1
BrCCSc1cc2cc(Nc3ccccc3)ccc2cnc2cc(Br)ccc12
Brc1ccc(Nc2ccc(Br)cc2)cc1
Brc1ccc(Nc2ccc3ccccc3c2)cc1
Brc1ccc(Nc2cccc(Br)c2)cc1
Fine tuning...
Mean value of predictions: 0.19948348
Proportion of valid SMILES: 0.6053783614759225
Sample trajectories:
B[PH](=O)(Nc1ccccc1)(C(=O)NCC(P(=O)(O)O)P(=O)(O)O)c1ccc(Br)cc1
BrC=CC=Cc1ccc(Br)cc1
Brc1cc(Br)cc(Nc2ccc(Nc3cccc4ccc(I)cc34)cc2Br)c1
Brc1cc2c(Nc3cccnc3)ncnc2cc1Nc1ncnc2ccccc12
Brc1ccc(-c2ccc(Br)c3ccccc23)cc1

  6 Training on 3508 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 24.213381
Reward: 1.973468
Trajectories with max counts:
28	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.2535211
Proportion of valid SMILES: 0.5772357723577236
Sample trajectories:
Brc1cc(Br)c2ccc(NN=Cc3ccccc3)cc2n1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccccc23)ccn1
Brc1cc2c(-c3ccccc3)c[nH]c2c(-c2ccc3ccccc3c2)n1
Brc1cc2c(nc3ccc(Nc4cccc(-c5ccccc5)c4)nc3n1)-c1ccccc1-2
Policy gradient replay...
Mean value of predictions: 0.29322752
Proportion of valid SMILES: 0.5911792305286205
Sample trajectories:
BP(=O)(OCC1OC(n2cc(O)c3c(N)ncnc32)C(O)C1O)OP(=O)(O)Oc1ccccc1
Brc1cc(Br)c2c(Nc3cccs3)ncnc2c1
Brc1ccc(-c2n[nH]c3ccc(Br)cc23)cc1
Brc1ccc(I)cc1
Brc1ccc(Nc2cc(Nc3cccc4ccccc34)ncn2)cc1Br
Fine tuning...
Mean value of predictions: 0.23951142
Proportion of valid SMILES: 0.5889896778229591
Sample trajectories:
Brc1cc(Br)c2cccc(Br)c2c1
Brc1cc(Br)cc(Nc2ccc(Nc3ncnc4ccc(I)cc34)s2)c1
Brc1cc2ncn-2c1-c1ccccc1
Brc1cc2ncnc(Nc3cc[nH]n3)c2cc1Br
Brc1ccc(-c2ccnc3c2-c2cc(Br)ccc2N3)cc1

  7 Training on 5047 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 26.704162
Reward: 2.648799
Trajectories with max counts:
49	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.32705742
Proportion of valid SMILES: 0.5283077885517673
Sample trajectories:
BP(=O)(OC(C)C)c1ccc(Nc2ncnc(Nc3cccs3)n2)cc1
BrC(Br)=NNc1ccc(Br)o1
Brc1cc(Br)cc(Sc2ccc3ncncc3c2)c1
Brc1ccc(Br)c(Nc2ncnc3cc(Br)sc23)c1
Brc1ccc(Nc2nccc(NCc3c(Br)ccc4ncsc34)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.32651475
Proportion of valid SMILES: 0.562363238512035
Sample trajectories:
Brc1cc2ncnc(Nc3cc(Br)c(Br)cc3I)n2c1
Brc1cc2ncnc(Nc3ccc4c(c3)CCCC4)n2c1Br
Brc1cc2ncnc(Nc3ccnc4ccccc34)c2cc1OCCCN1CCCCC1
Brc1ccc(N(Cc2ccc(Br)cn2)Cc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4cccc(Br)c4)c3ncn2)cc1
Fine tuning...
Mean value of predictions: 0.30682647
Proportion of valid SMILES: 0.6010021922956468
Sample trajectories:
Brc1ccc(Br)c(Br)c1
Brc1ccc(CNc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2cc(-c3ccccc3)nc3ccccc23)cc1
Brc1ccc(Nc2cncc(Br)c2)cc1
Brc1ccc(Nc2nc3ccc(Br)cc3s2)c(Br)c1

  8 Training on 6791 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 27.127326
Reward: 3.154857
Trajectories with max counts:
193	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.33799598
Proportion of valid SMILES: 0.4646875
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)c1cc(Br)cc(Br)c1O
Brc1cc2ncncc2cc1Nc1ncnc2sc(Nc3ccccc3)cc12
Brc1ccc(-c2cc3ncnc(Nc4ccccc4)c3cc2-c2ccccc2)o1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(CNc2ccc(-c3ncnc4ccccc34)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.3332588
Proportion of valid SMILES: 0.5599374021909234
Sample trajectories:
Brc1ccc(COc2ccc(Br)cn2)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4cccc(I)c4)sc3c2)cc1
Brc1ccc(Nc2ccnc3cc(Br)c(Br)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.3217656
Proportion of valid SMILES: 0.6161300406376993
Sample trajectories:
Brc1ccc(-c2nc(-c3ccc4ccccc4c3)n(-c3ccccc3Br)n2)cc1
Brc1ccc(Br)c(Nc2ncnc3cc(Br)nn23)c1
Brc1ccc(NCc2c(Br)cc(Br)c(I)c2Br)s1
Brc1ccc(Nc2ccc(Nc3ncnc4ccc(Br)cc4N3)cc2)cc1
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1

  9 Training on 8448 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 25.220006
Reward: 2.759231
Trajectories with max counts:
34	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4389246
Proportion of valid SMILES: 0.5353566958698373
Sample trajectories:
BP(=O)(OCC)c1cc(Br)c(NC(=O)Nc2ccc(Br)c(Br)c2F)c(Br)c1
Brc1cc(Nc2ncnc3cc(Br)sc23)c(Br)s1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc2ncnc(Nc3ccc(Br)s3)n2n1
Brc1ccc(-c2ccsc2)c2scnc12
Policy gradient replay...
Mean value of predictions: 0.35112274
Proportion of valid SMILES: 0.5988117573483427
Sample trajectories:
BrCN1CCCCCC1c1nc(Nc2ncnc3ccccc23)cs1
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)cc(Nc2ccnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3cnc(Nc4cccnc4)cc23)c1
Brc1ccc(-c2cnc3cnc(Nc4ccc5nnnn5c4)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.3589876
Proportion of valid SMILES: 0.6053783614759225
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccccc23)nc(Nc2cc3ccncc3c3ccccc23)c1
Brc1cc2ncnc(Nc3ccccc3)n2n1
Brc1cc2ncnc(Nc3cccnc3)c2cc1Br

 10 Training on 9792 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.193217
Reward: 2.677699
Trajectories with max counts:
49	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.3783814
Proportion of valid SMILES: 0.5653400188028831
Sample trajectories:
BP(=O)(CS)NC(CCCN(C(=O)NO)S(=O)(=O)O)S(=O)(=O)O
BP(=O)(Nc1cnc(Br)c(Br)c1)Nc1cccc(Br)n1
BP(=O)(c1ccc(NS(=O)(=O)Oc2cc(Cl)c(Cl)cc2F)cc1)N(C(=O)OCC)C(F)(F)F
Brc1cc(Br)c(Nc2ccc3ncnc(Nc4cc(Br)cnc4Br)c3c2)c(Br)c1
Brc1cc(Br)c2c(Br)c(Br)c(-c3cscc3Br)n2c1
Policy gradient replay...
Mean value of predictions: 0.35727495
Proportion of valid SMILES: 0.6421875
Sample trajectories:
B[PH](=O)(NO)(Nc1cc(Br)c(Br)cc1F)c1ccc(F)cc1
Brc1cc(Br)c2c(Br)cccc2c1Nc1ccccc1
Brc1cc2c(Nc3ccncc3)Nc3ccccc3Nc3ccc(ncnc2s1)c(Br)c3
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.385561
Proportion of valid SMILES: 0.6410256410256411
Sample trajectories:
BP(=O)(OCC)OC(=O)CSCI
Bc1cc(Br)c2ncnc(-c3cccc(Br)c3)c2n1
BrC(Cn1cncn1)Nc1ccnc2ccccc12
BrCBr
Brc1c2c(nc3cccnc13)CCO2

 11 Training on 11282 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.498650
Reward: 2.793227
Trajectories with max counts:
26	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.47879124
Proportion of valid SMILES: 0.5701754385964912
Sample trajectories:
BP(=O)(NCCCOC)c1cc(Br)c(Br)c(Br)c1
BrBr
BrCCN1c2ncnc(Br)c2Nc2ncnc(Nc3ccc(Br)c(Br)c3)c21
Brc1c[nH]c(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.45151016
Proportion of valid SMILES: 0.5690625
Sample trajectories:
BP(=O)(Br)OCCCCCBr
BP(=O)(Nc1ccc(Nc2nc3c(Br)cc(Br)c(Br)c3s2)cc1)N1CC1
BP(=O)(OCCCn1cnc2c(NCCCCCCO)ncnc21)OP(=O)(O)Oc1c(F)cc(F)c(F)c1F
BrCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrSC(=Nc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1)c1cccc(Br)c1
Fine tuning...
Mean value of predictions: 0.41295803
Proportion of valid SMILES: 0.5888055034396498
Sample trajectories:
BP(=O)(NCCO)C(F)(F)F
BP(=O)(OCC(=O)N1C(=O)N(C(=O)C(N)Cc2ccc(Br)cc2)C1CCl)C(=O)C1CCCCC1
BrSc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Brc1cc(-c2nccnc2Br)ncn1
Brc1cc(Br)c(Br)c(Sc2ccccc2-c2ccccc2)c1

 12 Training on 13038 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.471565
Reward: 3.712239
Trajectories with max counts:
309	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4833971
Proportion of valid SMILES: 0.4088207694713794
Sample trajectories:
BBr
BP(=O)(Br)OP(=O)(OC)OC(=O)CBr
BP(=O)(NC(=O)CCC(C#CCCl)Nc1cc(Cl)c(Br)cc1F)OCC
BP(=O)(NC(C(=O)CCC)C(=O)N(C)C)S(=O)(=O)c1ccc2c(Nc3c(F)cc(F)c(F)c3F)ccnc2c1
BP(=O)(NCCO)Nc1cccc(Br)c1Cl
Policy gradient replay...
Mean value of predictions: 0.43723917
Proportion of valid SMILES: 0.5844277673545967
Sample trajectories:
B=C(Sc1nc2cccnc2s1)c1cc2ccccc2s1
BP(=O)(CCC=C(Br)Br)OCC
BP(=O)(OCC)OCCC=C(Br)Br
BP(=O)(OP(=O)(O)CCCl)C(=O)Nc1ccc(F)cc1F
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.44032788
Proportion of valid SMILES: 0.571875
Sample trajectories:
BP(=O)(Oc1cc2cc(Br)c(Br)cc2s1)P(Br)Br
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1

 13 Training on 14670 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.987592
Reward: 3.282719
Trajectories with max counts:
30	Nc1cc2ncnc(Nc3ccc(Br)cc3F)c2s1
Mean value of predictions: 0.5490798
Proportion of valid SMILES: 0.509375
Sample trajectories:
BP(=O)(N=C(NO)c1cc2cnc(Nc3cc(Br)cnc3O)cc2s1)OCC
Brc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)cnc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(CNc4nc5cncn5c5nnc(Nc6cccc(Br)c6)n45)sc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.36458853
Proportion of valid SMILES: 0.6267583619881213
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCOP(=O)(O)OP(=O)(O)OP(=O)(O)O
Br
BrC(Br)=C(CNc1ccnc2cc(Br)ccc12)c1ccccc1Br
BrCc1ccc(N2CCN(Cc3cc(Nc4ccc(Br)s4)ncn3)CC2)cc1
Brc1cc(Br)cc(Nc2ncnc3ccc4ccccc4c23)c1
Fine tuning...
Mean value of predictions: 0.4222668
Proportion of valid SMILES: 0.623125
Sample trajectories:
BP(=O)(CCl)NP(=O)(OC(C)=O)C(=O)NO
BP(=O)(N=C(C)CC(=O)Oc1ccc(Br)cc1)OCC
Bc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Oc1cccc2ncnc(Nc3ccc(Br)cc3)c12
Bc1ccc(Nc2cc(Br)cc(Br)c2)cc1-c1cc(Br)cc(Br)c1O
Bc1cccc(Nc2ncnc3ccccc23)c1

 14 Training on 16466 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.901995
Reward: 3.463344
Trajectories with max counts:
54	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5643545
Proportion of valid SMILES: 0.6177666562402252
Sample trajectories:
BP(=O)(OCC)OC(=O)CN(CCP(=O)(O)CCBr)OP(F)(F)(F)F
Bc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
BrCCBr
BrCc1ccc2ncnc(Nc3ccc(CN4CCCC4)nc3)c2c1
Brc1cc(Br)c(Nc2nc3c(s2)sc2cc(Br)c(Br)cc23)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.49427354
Proportion of valid SMILES: 0.66051891216005
Sample trajectories:
BP(=O)(OCC)C(=O)N1CCC(CCBr)=Nc2sc3c(c2C1)CCCCC3
BP(=O)(OCC)OCC=C
Brc1cc(Nc2ncnc3ccc(-c4ccc(Br)s4)cc23)cs1
Brc1cc2N(c3ccccc3)CCCN2c2sccc2c1Br
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2cn1
Fine tuning...
Mean value of predictions: 0.48008153
Proportion of valid SMILES: 0.6136292591434823
Sample trajectories:
Brc1cc(Br)c(NCCSc2nc3ncncc3s2)c(Br)c1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2ncnc(Nc3cc(Br)c(Br)s3)c2cc1Br
Brc1cc2ncnc(Nc3ccc(NC4CCCC4)cc3)c2s1

 15 Training on 18832 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.960639
Reward: 3.880085
Trajectories with max counts:
123	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.537092
Proportion of valid SMILES: 0.5268918073796123
Sample trajectories:
B[PH](=O)(Nc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O
Bc1cc(Br)c(Br)cc1Br
Bc1cc(Br)cc(Br)c1NP(=O)(OCCCCCC)Oc1cccc(NC(=O)Nc2cc(Br)c(Br)c(Br)c2Br)c1
Bc1cc2ncnc(Nc3ccc(Br)s3)c2cc1Br
Brc1cc(Br)c(-c2ccc(Nc3ncnc4ccsc34)cc2)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.5532152
Proportion of valid SMILES: 0.6181533646322379
Sample trajectories:
BP(=O)(CC(F)(F)F)OCC
B[PH](=O)(NC(c1ccc(Br)cc1)P(Br)Br)=C(Br)Br
BrCCCCCCNc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1Br
Brc1cc(Br)c(Nc2ccsc2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.51203704
Proportion of valid SMILES: 0.6075
Sample trajectories:
BP(=O)(OCC)OC(=O)CSCC=C(Br)P(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCCS(=O)(=O)ON1CCOCC1)C(=O)NO
BP(=O)(c1cccc2ncnc(Nc3ccc(Cl)c(Cl)c3)c12)N(O)Cc1cccc2ccccc12
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1

 16 Training on 21090 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.159895
Reward: 3.854462
Trajectories with max counts:
19	CC(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5754596
Proportion of valid SMILES: 0.6802125664270084
Sample trajectories:
BP(=O)(OCC1OC(Nc2cc3cc(Br)ccc3nc2N)C(O)C1OP(=O)(O)O)c1ccncc1
BrCCNc1cc2ncnc(Nc3ccccc3Br)c2cc1Br
Brc1cc(Br)c2c(c1)C=Nc1sccc12
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1cc2ncnc(Nc3cccs3)c2s1
Policy gradient replay...
Mean value of predictions: 0.5636271
Proportion of valid SMILES: 0.6138211382113821
Sample trajectories:
BC(=O)Nc1cc(Br)c(Br)cc1Cl
BP(=O)(Nc1ccc(Br)cc1)Oc1cc(Br)ccc1O
BP(=O)(Nc1ccc(Br)cc1F)Oc1cccc(F)c1
BP(=O)(OCC)C(=O)C(C)(Cl)Br
BP(=O)(OCC)OCCC(=O)Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Fine tuning...
Mean value of predictions: 0.5386869
Proportion of valid SMILES: 0.6197183098591549
Sample trajectories:
Br
BrC=CBr
BrC=CC1=Nc2sc3c(c21)CCCCC3
Brc1cc(Br)cc(Nc2ncnc3c(Br)cc(Br)cc23)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1

 17 Training on 23837 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.051633
Reward: 4.026634
Trajectories with max counts:
42	Nc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Mean value of predictions: 0.6045249
Proportion of valid SMILES: 0.5528455284552846
Sample trajectories:
BP(=O)(Cn1cnc(Nc2ccc(Br)c(Br)c2F)n1)C(F)(F)P(=O)(O)O
BrCCNc1nc2c(Br)ncnc2s1
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.60711426
Proportion of valid SMILES: 0.62375
Sample trajectories:
BP(=O)(NC(Cc1cccc(Br)c1)P(=O)(O)O)C(N)=O
BP(=O)(OCCS)C(=O)OCO
BP(=O)(Oc1ccc2ncnc(Br)c2c1O)c1cccc2ccccc12
Bc1cc2ncnc(Nc3ccc(Br)c(Cl)c3)c2c(Br)c1Br
BrCc1cc2ncnc(Nc3cc(Br)cc(Br)c3)c2s1
Fine tuning...
Mean value of predictions: 0.56480557
Proportion of valid SMILES: 0.6274632467938692
Sample trajectories:
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BrNc1cnc2nc(Nc3cccnc3)cnc2c1
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(Br)c1
Brc1cc2c(Nc3cccs3)ncnc2s1
Brc1cc2sc3c(Br)ccc(Br)c3c2s1

 18 Training on 26620 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.437901
Reward: 4.434495
Trajectories with max counts:
50	Nc1cc2ncnc(Nc3ccc(Br)cc3F)c2s1
Mean value of predictions: 0.6775029
Proportion of valid SMILES: 0.555625
Sample trajectories:
Bc1cc2ncnc(Nc3ccc(Cl)cc3)c2s1
Bc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)c(-c2cc3ncnc(Nc4ccc(Br)c(Br)c4)c3s2)c(Br)c1
Brc1cc(Br)c(Br)[nH]1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2n1
Policy gradient replay...
Mean value of predictions: 0.63357824
Proportion of valid SMILES: 0.5959974984365228
Sample trajectories:
BP(=O)(C(=O)Oc1ccc(Br)cc1)N1CCN(CC(=O)Nc2cc(Br)c(Cl)c(Br)c2O)CC1
BP(=O)(OC(=O)CBr)C(O)C(N)CC=O
BP(=O)(c1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(c1ccc(Cl)cc1)c1ccc(Br)cc1
Bc1cc2ncnc(Nc3ccc(Br)cc3F)n2n1
Fine tuning...
Mean value of predictions: 0.5470763
Proportion of valid SMILES: 0.6312167657178605
Sample trajectories:
BrC(=NNc1ccccc1)c1ccc(Br)cc1
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1cc2c(Nc3ccccc3)ncnc2cc1NCCc1ccccc1

 19 Training on 29517 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.357644
Reward: 4.675105
Trajectories with max counts:
43	Nc1cc2ncnc(Nc3ccc(F)c(F)c3F)c2s1
Mean value of predictions: 0.6591928
Proportion of valid SMILES: 0.487964989059081
Sample trajectories:
BP(=O)(F)(F)(F)P(=O)(O)OP(=O)(O)OCCl
BP(=O)(Nc1ccc(Br)cc1)P(=O)(Oc1ccc(Br)cc1F)N1CCSS1
Bc1cc(Br)cc(Br)c1Nc1cc2ncnc(Nc3ccc(Br)cc3Br)c2s1
BrCC(Nc1nc2ncnc(Nc3cccc(Br)c3)c2nc1Nc1c(Nc2cc(Br)c(Br)cc2Br)c2cc(Br)ccc12)N1CCCC1
BrCCNc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Policy gradient replay...
Mean value of predictions: 0.5600976
Proportion of valid SMILES: 0.640625
Sample trajectories:
Bc1ccc(Br)c(Nc2ncnc3scnc23)c1
BrC1CCCCCN1
BrCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Oc2cc(Br)ccc2Br)c(Nc2ccc(Br)c(Br)c2)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.5851888
Proportion of valid SMILES: 0.62875
Sample trajectories:
BP(=O)(Nc1cccc(N)c1)P(=O)(OCC)OP(=O)(O)OP(=O)(O)O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Nc2ncnc3ccccc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccccc3)ncnc2c1

 20 Training on 32187 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.544629
Reward: 4.587681
Trajectories with max counts:
89	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6221203
Proportion of valid SMILES: 0.6434896687438505
Sample trajectories:
BP(=O)(CCCCC)NO
BP(=O)(CCCCCCCCCCCCCCCCCCCCCCCCCC(NC(=O)OC(C)(C)C)OCP(=O)(O)O)C(=O)NS(=O)(=O)CC(=O)O
BP(=O)(CCCN)Nc1cc2ncnc(Nc3cccc(Br)c3)c2s1
BP(=O)(OCC1OC(=N)C(Cl)=C(Br)C1Br)P(Br)Br
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.67076
Proportion of valid SMILES: 0.6211316036261332
Sample trajectories:
BP(=O)(Oc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1)c1ccc(Br)cc1
BrCCNc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCc1ccc(Nc2ncnc3cc(Br)sc23)cc1
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.59543073
Proportion of valid SMILES: 0.6569731081926203
Sample trajectories:
BrC=Cc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(-c2ccncc2)c2ncnc(Nc3ccc(Br)s3)c2c1
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)cc(Nc2ccc(Br)c(Nc3c4ccccc4nc4ccccc34)c2)c1

Trajectories with max counts:
191	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.55820125
Proportion of valid SMILES: 0.5563199699793608
Mean Internal Similarity: 0.48492796568545704
Std Internal Similarity: 0.0990081415673887
Mean External Similarity: 0.4209525410656187
Std External Similarity: 0.07516536171944184
Mean MolWt: 437.36867480530424
Std MolWt: 119.59716508117404
Effect MolWt: -0.5702106139908616
Mean MolLogP: 5.4687796484950555
Std MolLogP: 2.175768735889909
Effect MolLogP: 0.40147748690063073
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 96.854764% (1047 / 1081)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 20, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5589.70091843605, 'valid_fraction': 0.5563199699793608, 'active_fraction': 0.5341202922990445, 'max_counts': 191, 'mean_internal_similarity': 0.48492796568545704, 'std_internal_similarity': 0.0990081415673887, 'mean_external_similarity': 0.4209525410656187, 'std_external_similarity': 0.07516536171944184, 'mean_MolWt': 437.36867480530424, 'std_MolWt': 119.59716508117404, 'effect_MolWt': -0.5702106139908616, 'mean_MolLogP': 5.4687796484950555, 'std_MolLogP': 2.175768735889909, 'effect_MolLogP': 0.40147748690063073, 'generated_scaffolds': 1081, 'novel_scaffolds': 1047, 'novel_fraction': 0.9685476410730804, 'save_path': '../logs/replay_ratio_mixed_s1-3.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.990320
Reward: 1.000000
Mean value of predictions: 0.0011400653
Proportion of valid SMILES: 0.7701473816243336
Sample trajectories:
Brc1cc(OCCn2ncc3ccccc32)on1
Brc1ccc(-n2c(-c3cccnc3)nc3ccccc32)cc1
C#CC1(O)CN(C)C(=O)c2c(cnc3cc(Cl)ccc23)C#Cc2nc(N)n3c2ncn3C1
C#CCCCCCCCOP(=O)(C(=O)Nc1ccc(N(=O)=O)cc1)C(N)CCl
C#CCCN(C)P(=O)(OCC)c1ccc(Cl)cc1
Policy gradient replay...
Mean value of predictions: 0.0026343518
Proportion of valid SMILES: 0.599873577749684
Sample trajectories:
BrCC(Br)=CCn1cnc(Br)c1
Brc1ccc2nc(-c3cc(Br)c(OCc4nn[nH]n4)nn3)[nH]c2c1
Brc1cccc(-c2nc(-c3noc(C4CCNC4)n3)no2)c1
Brc1cncc(Br)c1
C#CC#CCCCCCN1CCCCC1=O
Fine tuning...
Mean value of predictions: 0.014754099
Proportion of valid SMILES: 0.6495458816160351
Sample trajectories:
BP(=O)(OCCOCCOCC)Oc1cc(CCCl)cc(Nc2cc(Br)cc(Br)c2OC)c1
Brc1cc2[nH]c(C=NOCCOc3ccc(C=C4COC4)cc3)nc2s1
Brc1ccc(-c2nn3c(-c4ccncc4)nnc3c3nc[nH]c23)cc1
Brc1ccc(Nc2nccc(-c3ccnc4[nH]cc(Br)c34)n2)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1

  2 Training on 279 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.506103
Reward: 1.025638
Trajectories with max counts:
4	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0159799
Proportion of valid SMILES: 0.6240200689871434
Sample trajectories:
BrCCBr
Brc1ccc(-c2[nH]c3ncccc3c2-c2ccccc2)cc1
Brc1ccc(-c2ncnc3ncsc23)c2ncncc12
Brc1ccc(N=Nc2cccc(N=C3CCC3)c2)cc1
Brc1ccc(Nc2nc(NCCc3ccc(Cn4nn[nH]4)nn3)nc3ccc(Nc4ccccc4)ncnc23)cc1
Policy gradient replay...
Mean value of predictions: 0.024438454
Proportion of valid SMILES: 0.6962777604003754
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)OP(=O)(O)P(=O)(O)O
Brc1ccc(CNc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(N=C2N=C(c3ccccc3)SC2=Nc2ccccc2)cc1
Brc1ccc(NN=Nc2ccccc2Br)cc1
Brc1ccc(Nc2cc3c(-c4ccccc4)csc3cn2)cc1
Fine tuning...
Mean value of predictions: 0.036211148
Proportion of valid SMILES: 0.6344287949921753
Sample trajectories:
Brc1cc(N=Nc2ncnc3ncccc23)ccc1-c1nc[nH]n1
Brc1cc2c(cc1Br)Oc1cc(Br)c(Br)cc1NC=C2
Brc1ccc(-c2c[nH]c3ccc(Br)cc23)c(I)c1
Brc1ccc(Br)cc1
Brc1ccc(CNc2nc3cc(Br)ccc3cc2N2CCNCC2)cc1

  3 Training on 493 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.728622
Reward: 1.190207
Trajectories with max counts:
8	Cc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.042235456
Proportion of valid SMILES: 0.682614133833646
Sample trajectories:
B[PH](=O)(=NC(=O)Nc1ccc(Br)cn1)OCC1(CC=C(Br)Br)CN1C(=O)N1CCN(c2ccc3c(c2)NC3=O)C1=O
B[PH](=O)(=O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OCC1NC(=O)OC(n2cnc3c(N)ncnc32)C(n2cnc3c(N)ncnc32)O1
BrCc1ccc(-c2ncc3nc(-c4ccccc4)[nH]c3n2)cn1
Brc1ccc(-c2ccc(-c3nnc4ccccn34)c(CN3CCCCC3)c2)o1
Brc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.04516129
Proportion of valid SMILES: 0.47902869757174393
Sample trajectories:
Brc1cc(NN=Cc2ccnc3ccncc23)ncn1
Brc1cc2c(ncnc1N1CCOCC1)OCCO2
Brc1cc2ncnc(N3CCC(N4CCCCC4)CC3)c2s1
Brc1ccc(-c2nnc(-c3ccncc3-c3nn[nH]n3)o2)c2ncnc(N3CCCC3)c12
Brc1ccc(Nc2ncnc(Nc3ccncn3)n2)cc1
Fine tuning...
Mean value of predictions: 0.058423493
Proportion of valid SMILES: 0.6067521100343858
Sample trajectories:
BrCC(Cc1ccc2c(c1)OCO2)Nc1ccc2ccccc2c1
Brc1cc(Nc2ccc3[nH]ncc3c2)sc1Br
Brc1ccc(-c2c(Br)ccc3ccccc23)cc1
Brc1ccc(NN=C2c3ccccc3-c3ccccc32)cc1
Brc1ccc(Nc2ccnc3cc(N4CCCC4)ccc23)cc1

  4 Training on 868 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.608576
Reward: 1.331263
Trajectories with max counts:
35	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.09608695
Proportion of valid SMILES: 0.5762605699968681
Sample trajectories:
BP(=O)(OCC1C=CC(N2C=CC(Oc3ccc(Br)cc3)N2)O1)Oc1ccc(Br)cc1
BP(=O)(OCC1OC(N)C(N)C(O)(CO)O1)P(=O)(O)O
BrCc1ccc(CNc2ncc(-c3cccc(-c4ncncc4Br)n3)c3ccccc23)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Cn2ccc(Nc3ccc(N4CCOCC4)cc3)n2)nc1
Policy gradient replay...
Mean value of predictions: 0.09393393
Proportion of valid SMILES: 0.5230914231856739
Sample trajectories:
Brc1ccc(-c2cnc[nH]2)s1
Brc1ccc(C=NNC2=Nc3ccc(Br)cc32)cc1
Brc1ccc(NN(c2cccnc2)C2CCC2)nc1
Brc1ccc(Nc2nc(-c3c[nH]cn3)cs2)cc1
Brc1ccc(Nc2ncc(-c3noc(-c4cc[nH]n4)n3)s2)cc1
Fine tuning...
Mean value of predictions: 0.106071614
Proportion of valid SMILES: 0.6025641025641025
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccncc23)c1
Brc1cc2c(Br)ccc(Nc3ncnc4oc5ccccc5c34)n2c1
Brc1ccc(-c2[nH]cnc2Nc2ccccc2)c(-c2ccccc2)c1
Brc1ccc(-c2nc3ccccn3c2Nc2ccc3ccncc3c2)cc1
Brc1ccc(Br)c2c1Nc1ccccc12

  5 Training on 1608 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.005974
Reward: 2.017917
Trajectories with max counts:
382	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.14004345
Proportion of valid SMILES: 0.43169740543919977
Sample trajectories:
BP(=O)(OCC1CC(Br)C(O)C1NC(N)=O)N1C=C(Br)C(=O)NC1=O
BP(=O)(OCC1OC(n2cnc3c(N)c(F)c(F)cc32)C(O)C1O)N1CCC(N)C(N)C1
Brc1cc(-c2ccccc2)c2ccccc2n1
Brc1ccc(-c2ccc(Nc3ccc4ccccc4c3)cc2)cc1
Brc1ccc(-c2cccnc2)s1
Policy gradient replay...
Mean value of predictions: 0.06331198
Proportion of valid SMILES: 0.683125
Sample trajectories:
BrC1CN2CCC1CC2
Brc1[nH]c2ccccc2c1Nc1ccccc1
Brc1ccc(CNc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ccc3ncncc3c2)nc1
Brc1ccc(Nc2cccnc2)cc1
Fine tuning...
Mean value of predictions: 0.17632468
Proportion of valid SMILES: 0.5557644110275689
Sample trajectories:
BP(=O)(OCCCCC)ON(CC(=O)OC(C)(C)C)P(=O)(OCOCOC(=O)OCc1ccc(Br)cc1)P(=O)(OC(C)C)OP(=O)(O)O
Brc1cc(OCCN2CCCCC2)cc(-c2ccc3scc(Br)c3c2)c1
Brc1ccc(Br)c(-c2cccc(CN3CCC(Oc4ccccn4)C3)c2)c1
Brc1ccc(Cc2c(Nc3ccc(Br)cn3)nc3ccncn23)cc1
Brc1ccc(NCc2cnc(Nc3ccccc3Br)cn2)nc1

  6 Training on 2489 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 20.460624
Reward: 2.533879
Trajectories with max counts:
370	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.21266374
Proportion of valid SMILES: 0.429375
Sample trajectories:
BP(=O)(NO)c1ccc(Cl)cc1
BP(=O)(Nc1ccc(-c2cccc(Br)c2)cc1)P(=O)(Oc1ccccc1)Oc1ccccc1
B[PH](=O)(Nc1cccc(F)c1)=[PH](=O)(Oc1ccc(F)cc1)c1ccc(F)cc1
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Brc1ccc(-c2ncnc3ccccc23)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.18486056
Proportion of valid SMILES: 0.5506110937010341
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1cc(Br)ccc1O
BP(=O)(OCC=C)[PH](O)(CNP(=O)(O)O)P(=O)(O)O
BP1(=O)OCC2OC(C(O)C2O)N(C=CBr)OC=CC1=O
BrC1CCC2(CCCC2)CO1
BrC=CC=NNC1=Nc2ccc(Nc3ccc(Br)cc3)cc2N=C1
Fine tuning...
Mean value of predictions: 0.18934427
Proportion of valid SMILES: 0.5349201378014407
Sample trajectories:
Bc1ccc(Nc2ccnc(Nc3ccccc3)c2)cc1
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1
BrCCNc1ncnc2c1ncn2C1CC2CCC1C2
Brc1ccc(-c2cc(Nc3ccc(Br)cn3)ccc2Br)cc1
Brc1ccc(-c2cccc(Nc3ncnc4ccsc34)c2)s1

  7 Training on 3675 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 23.141581
Reward: 3.048416
Trajectories with max counts:
320	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.24896552
Proportion of valid SMILES: 0.45326664582682086
Sample trajectories:
Brc1cc(Br)c(-c2nccc3ccccc23)cc1Nc1ncnc2ncccc12
Brc1cc(Br)c(Br)cn1
Brc1cc(Br)c(Nc2nncnc2Br)cc1Br
Brc1cc(Br)c2c(N3CCCN(c4sc5c(c4Br)CCCC5)CC3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)nncc2c1
Policy gradient replay...
Mean value of predictions: 0.27277228
Proportion of valid SMILES: 0.5057902973395931
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)c1cc(Br)cc(Br)c1O
BP(=O)(OCC=CBr)C(=O)ONc1ccc(Br)cc1
BP(=O)(OCCOC(=O)C(C)(O)CNC(=O)OP(=O)(O)OP(=O)(O)O)C(F)(F)F
BrC1=Nc2cc(Br)c(Nc3nsc4c3N=CN4)ncnc2-c2ccccc2N1
BrCBr
Fine tuning...
Mean value of predictions: 0.28590852
Proportion of valid SMILES: 0.5073690812166823
Sample trajectories:
BP(=O)(ON(O)C(=O)OCC1C=CC(C(=O)O)O1)N(=O)=O
BrBr
BrCCN1CCCC1c1ccc(Nc2ncnc3ccccc23)cc1
BrCc1cc(Nc2ncnc3ccccc23)nc2sc(Br)cc12
BrNC1=Nc2ccc(Br)cc2Nc2cccnc21

  8 Training on 5075 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 22.666979
Reward: 3.624696
Trajectories with max counts:
399	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.31129786
Proportion of valid SMILES: 0.3346875
Sample trajectories:
BP(=O)(N1CCOCC1)N1CCC(F)(F)C(F)C1
BP(=O)(Nc1cc(Br)c(Br)cc1Br)C(=O)O
BP(=O)(Nc1ccc(Nc2ccc(Br)cc2)cc1)c1cccc(Nc2cccc(Br)c2)c1
BP(=O)(O)Nc1ccc(Br)cc1
BP(=O)(OC)OC(=O)O
Policy gradient replay...
Mean value of predictions: 0.30988413
Proportion of valid SMILES: 0.45858080650203187
Sample trajectories:
BrC1=C2c3ncnc(Nc4cc(Br)ncn4)c3C3(CCCC3)CC2C=C1
BrCc1ncc(Nc2ncnc3cc(Br)ccc23)cc1-c1ccccc1Br
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2csc(Nc3ccccc3Br)c2)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.27203065
Proportion of valid SMILES: 0.48952797749296656
Sample trajectories:
BP(=O)(NO)C(P(=O)(O)O)P(=O)(O)O
BP(=O)(OCC)OC(=O)C(F)(F)F
BP1(=O)OCC(COc2ccc(Br)cc2)O1
Bc1ccc(Nc2ncnc3sccc23)cc1
BrC=C=Nc1cccc(Nc2ncnc3ccccc23)c1

  9 Training on 6349 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 22.940260
Reward: 4.288024
Trajectories with max counts:
757	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.3324201
Proportion of valid SMILES: 0.27375
Sample trajectories:
BP(=O)(N(O)C(Cl)Cl)P(=O)(NO)C(=O)C(F)(F)F
BP(=O)(NC(=O)C(CCC(=O)O)NC(=O)Nc1ccc(F)c(F)c1)OCCCCC(F)(F)F
BP(=O)(NC(=O)Oc1ccc(Cl)c(Nc2nc3ccccc3nc2S(=O)(=O)Nc2cccnc2)c1)OCC(N)=O
BP(=O)(Nc1ccc(Br)cc1)Oc1ccc(Br)cc1
BP(=O)(Nc1ccc(Br)cn1)c1ccc(F)c(F)c1
Policy gradient replay...
Mean value of predictions: 0.32320407
Proportion of valid SMILES: 0.49233176838810644
Sample trajectories:
BP1(=O)CCC(CC(Br)C(F)(F)F)(C(F)(F)C(F)(F)F)CO1
B[PH](=O)(CCNc1ccc(F)c(F)c1F)=NO
BrCCCOc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCCOc1cc(Nc2ncnc3ccsc23)ccc1Br
BrCCOc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Fine tuning...
Mean value of predictions: 0.29960784
Proportion of valid SMILES: 0.47842401500938087
Sample trajectories:
BP(=O)(OCC1OC(OP(=O)(O)O)C(O)C1O)n1cnc2c(N)ncnc21
BrC=CC=CC=CC=CC=CC=CC#CC=C(I)I
Brc1cc(Br)c(Nc2ncnc3[nH]cnc23)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1

 10 Training on 7558 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 22.396228
Reward: 4.389832
Trajectories with max counts:
858	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.23680852
Proportion of valid SMILES: 0.29375
Sample trajectories:
BP(=O)(N=C(NO)n1cnc2c(Nc3ccccc3)ncnc21)OCCOC
BP(=O)(N=C(Nc1ccc(Br)s1)C(=O)Nc1ccc(Br)cc1)OCC
BP(=O)(Nc1cc(Br)ccc1F)c1ccc(F)cc1
BP(=O)(Nc1ccc(Br)cc1)c1cccs1
BP(=O)(O)OCOC(=O)C(Cl)(Br)Br
Policy gradient replay...
Mean value of predictions: 0.35966182
Proportion of valid SMILES: 0.5186345129971813
Sample trajectories:
BBr
BP(=O)(NCCCCCCCCCCCC)C(=O)OCCCCCCCCC
BP(=O)(OCC1OC(=O)C(C)C(O)C(O)C1O)OC(=O)CCCCCCCCC
BP(=O)(OCC1OC(OCCCCCBr)C(O)(O)C(O)C1O)OC(=O)C(O)C(O)CO
BP1(=O)OCC(OC(C(=O)O)C(=O)Br)OC(=N)O1
Fine tuning...
Mean value of predictions: 0.31007954
Proportion of valid SMILES: 0.47125
Sample trajectories:
BP(=O)(OCC)OCC1(CC=CBr)CC=CO1
BP(=O)(OCCCCC)P(=O)(OCOC(=O)C(F)(F)F)Oc1ccc(N)cc1
Bc1ccc(N(=O)=O)cc1NS(=O)(=O)c1ccc(Br)cc1
BrC1=CC(=Nc2cccc(Br)c2)N1c1cc(Br)ccc1Br
BrCc1cc(Nc2ncnc3cc(Br)ccc23)ccc1Br

 11 Training on 8651 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 22.851558
Reward: 4.402766
Trajectories with max counts:
435	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.41350296
Proportion of valid SMILES: 0.319375
Sample trajectories:
BP(=O)(NCCCCl)P(=O)(OCC)OCC(Cl)(Cl)Cl
BP(=O)(Nc1ccc(F)c(F)c1)(c1ccc(F)cc1)(P(=O)(O)O)P(=O)(O)O
BP(=O)(Nc1ccc(Nc2ncnc3sc(Cl)nc23)cc1)NC1CCCCCC1
BP(=O)(OCC)OCC
BP(=O)(OCC)OCC=C(Br)Br
Policy gradient replay...
Mean value of predictions: 0.36084187
Proportion of valid SMILES: 0.49107422486689634
Sample trajectories:
BP(=O)(C=Nc1ccc(Nc2c(Br)c(Br)c(Br)c(Br)c2Br)cc1)OCOC(=O)CO
BP(=O)(OCC)OC(=O)CCCCn1cnc2c(Nc3ccc(O)c(Br)c3)ncnc21
Bc1ccc(Nc2ncnc3ccccc23)cc1
BrCCCCCCCCCCNc1ccc2c(n1)NCCCN2
BrCCN(CCCCN1CCN(Cc2ccccc2)CC1)c1ccc(Br)cc1
Fine tuning...
Mean value of predictions: 0.32468984
Proportion of valid SMILES: 0.5059635907093534
Sample trajectories:
Bc1cc(Nc2ncnc3ccccc23)ccc1Cl
Br
BrC1COc2ccccc2O1
BrC=CCCC=CC=CCBr
BrCCCCCCC=CC=CCCCCCCCCCCCCCBr

 12 Training on 9700 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.512562
Reward: 4.646917
Trajectories with max counts:
466	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.46469894
Proportion of valid SMILES: 0.2647702407002188
Sample trajectories:
BP(=O)(NC(=O)c1cc(Br)c(Br)cc1Br)OC[PH](=O)(O)(Br)c1ccc(Br)cc1
BP(=O)(NCCCCCCCCCC)C(Br)CCCCl
BP(=O)(OC(=O)Cl)P(=O)(O)O
BP(=O)(c1ccc(Nc2ncnc3sc(Cl)c(Cl)c23)cc1)N1CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC(=O)O1
B[PH](=O)(Cl)(OCCCl)P(=O)(O)Oc1ccc(F)c(F)c1
Policy gradient replay...
Mean value of predictions: 0.33032346
Proportion of valid SMILES: 0.4640400250156348
Sample trajectories:
BP(=O)(CCCCCCCCl)NC(=O)C(=Cc1ccc(Br)cc1)P(=O)(OCCCCCO)Oc1ccc(Br)cc1
BP(=O)(NC(NC(=O)OC(C)(C)C)C(=O)Nc1ccc(Br)cc1)C(=O)NCC(=O)OC(C(N)=O)C(C)C
B[PH](=O)(=Nc1ccc(F)c(Nc2c(F)cccc2F)c1)Nc1ccc(F)c(F)c1
BrC(=NNc1ccc(Br)cn1)c1ccc(Nc2ncccn2)cc1
BrCc1ccc2ncnc(Nc3ccc(CN4CCCCC4)cc3)c2c1
Fine tuning...
Mean value of predictions: 0.34773022
Proportion of valid SMILES: 0.4820256330103157
Sample trajectories:
BP(=O)(Nc1ccc(Br)c(Br)c1)P(=O)(OCC)OCC
Bc1ccc(Nc2nc(Nc3ccncc3)nc(C3CC3)n2)cc1
BrC(=NNc1ccc(Br)cc1)c1ccc(Nc2ncnc3sccc23)nc1
Brc1cc(Br)c(Nc2ncnc3ccsc23)s1
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1

 13 Training on 10685 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.961294
Reward: 4.858736
Trajectories with max counts:
781	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.37837237
Proportion of valid SMILES: 0.2804001250390747
Sample trajectories:
BP(=O)(C(N)=O)N1C(=O)c2ccccc2S1(=O)=O
BP(=O)(NC(C(=O)NO)N1CCOCC1)C(=O)Nc1ccc(F)c(F)c1
BP(=O)(NC(c1ccccc1)c1ccccc1)P(=O)(O)CC(C(=O)O)n1cnc2c(Nc3ccc(Cl)cc3)ncnc21
BP(=O)(OCC)OC(=O)CCC=CCC(=O)Nc1ccc(Br)c(Br)c1
BP(=O)(OCC1(N(=O)=O)NC(Cl)=C1CC=Cc1ccc(Br)cc1F)C(=O)OC(F)F
Policy gradient replay...
Mean value of predictions: 0.427819
Proportion of valid SMILES: 0.4631019387116948
Sample trajectories:
BP(=O)(NC(c1cc(F)c(F)cc1F)P(=O)(O)O)P(=O)(O)O
BP(=O)(NO)c1ccc(Nc2ncnc3sc4ccccc4sc23)cn1
BP(=O)(OCC1OC(N2C=C(Br)C(=O)NC2=O)C(O)C(O)C1O)OC(CC(C)C)P(=O)(O)O
BP(=O)(OCC1OC(c2ccc(F)cc2)C(O)C1O)N1CCOCC1
BrC1=CC(c2cc(I)cc(I)c2)=Nc2ncnn21
Fine tuning...
Mean value of predictions: 0.3658505
Proportion of valid SMILES: 0.48530331457160725
Sample trajectories:
BP(=O)(OCC(=O)Nc1ccc(Br)cc1F)C(=O)Nc1ccc(Br)c(Br)c1
BP(=O)(c1ccc(Br)c(Br)c1)N(O)C=O
Bc1ccc(Nc2ncnc3ccccc23)cc1
BrC=CI
BrCCNc1nc(Nc2ncccc2-c2ccc(Br)cc2)c2ncnc(Nc3cccs3)c2n1

 14 Training on 11784 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.320525
Reward: 5.250269
Trajectories with max counts:
368	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.5035409
Proportion of valid SMILES: 0.2559375
Sample trajectories:
BP(=O)(CCNc1ccc(Nc2ncnc3cccc(Br)c23)c(Br)c1)N(=O)=O
BP(=O)(COC(=O)c1ccc2ncnc(Nc3ccc(Br)cc3)c2c1)OCCO
BP(=O)(OCC(=O)Nc1cc(Br)c(N)c(Br)c1)C(=O)Nc1cc(Br)c(O)c(Br)c1
BP(=O)(OCC)OC(=O)CBr
BP(=O)(OCC1OC(O)C(O)C(O)C1O)N1C=CC(=O)N(CC(F)C(F)(F)F)C1=O
Policy gradient replay...
Mean value of predictions: 0.34506798
Proportion of valid SMILES: 0.5300563556668754
Sample trajectories:
BP(=O)(N(O)C(C=C)CC(Cl)(Cl)Cl)P(=O)(OCOC(=O)CCl)N(O)Cl
BrN(C#CC1CCCCCCC1)CCc1ccc2ncccc2c1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Br)c(Nc2nc(Br)nc3scnc23)c(Br)c1
Brc1cc(Br)c(Oc2ccc(Nc3nccs3)nc2)cc1Br
Fine tuning...
Mean value of predictions: 0.38849014
Proportion of valid SMILES: 0.5054738817641539
Sample trajectories:
BP(=O)(OCC1OC(N(=O)=O)CN1Cl)N(=O)=O
BrC(Br)=Nc1[nH]nc2c(Nc3ccc(Br)cc3)ncnc12
BrCc1cc2ncnc(Nc3ccc(Br)s3)c2cc1Nc1cccc(Br)c1
BrCc1nccc2c(Nc3ccc(Br)o3)ncnc12
Brc1cc(-c2ncnc3cncnc23)ccc1-c1ccc[nH]1

 15 Training on 12962 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.420730
Reward: 5.733913
Trajectories with max counts:
544	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.49720815
Proportion of valid SMILES: 0.2464810760087582
Sample trajectories:
BP(=O)(NC(c1ccc(Br)cc1)P(=O)(O)O)N(=O)=O
BP(=O)(Nc1ccc(NC(=N)N)c(I)c1)N(=O)=O
BP(=O)(OC(=O)Nc1cc(Br)c(Br)c(Br)c1Br)N(O)CBr
BP(=O)(OCCS)C(=O)Nc1ccc(Br)cc1
B[PH](=O)(=NO)Nc1ccc(F)c(F)c1Nc1c(F)c(F)c(F)c(F)c1F
Policy gradient replay...
Mean value of predictions: 0.42481267
Proportion of valid SMILES: 0.3753125
Sample trajectories:
BP(=O)(CCC=CC=C(C)C)OCC
BP(=O)(NCCCO)NC(=O)C(N)CCCCC(N)=O
BP(=O)(OCC)OC(=O)CCCCl
B[PH](=O)(=NP(=S)(N1CCCCCC1)C(F)(F)F)Nc1cc(Br)c(Br)cc1Br
Bc1cc(Br)c2ncnc(Nc3ccc(Br)cc3Br)c2c1
Fine tuning...
Mean value of predictions: 0.39740932
Proportion of valid SMILES: 0.482801751094434
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCCCOc1c(Br)cc(Nc2ncnc3cc(Br)ccc23)cc1Br
BrCCNc1ccc(Nc2ncnc3ccccc23)c(Br)c1
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)c(Br)c1Br

 16 Training on 14064 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.375263
Reward: 5.734052
Trajectories with max counts:
459	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5436585
Proportion of valid SMILES: 0.2564904598060682
Sample trajectories:
BP(=O)(NC(CCCN)C(=O)Nc1ccc(Br)cc1)c1cc(Br)c(Br)c(Br)c1
BP(=O)(NCCCCCC)C(N)C(Cl)(Cl)Cl
BP(=O)(OCC(F)(F)F)c1ccc(Nc2ncnc3c(F)c(F)ccc23)c(F)c1
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCC
BP(=O)(OCC)Oc1ccc(Nc2cc(Br)cc(Br)c2)cc1
Policy gradient replay...
Mean value of predictions: 0.35979608
Proportion of valid SMILES: 0.42933083176985615
Sample trajectories:
BP(=O)(COc1cc(Br)cnc1Br)C(=O)Oc1c(Br)cc(Br)cc1Br
BP(=O)(Nc1ccc(-c2csc3ccccc23)cn1)OCCC(F)F
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)c(Br)c1
BP(=O)(OCC=C)OCCCCCCCC
BP(=O)(Oc1cccc(Br)c1)Oc1ccc(Br)cc1Br
Fine tuning...
Mean value of predictions: 0.42797205
Proportion of valid SMILES: 0.49233176838810644
Sample trajectories:
BP(=O)(NC(Cc1ccc(F)c(F)c1)C(=O)O)C(=O)O
BP(=O)(c1ncnc2scc(Br)c12)N(O)COc1ccc(Cl)cc1
B[PH](=O)(=Nc1ccc(Br)cc1)OCC=C(Br)Nc1ccc(Br)cc1
Bc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1
Bc1ccc(Br)c(Br)c1

 17 Training on 15270 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.356952
Reward: 6.147317
Trajectories with max counts:
580	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.50161725
Proportion of valid SMILES: 0.231875
Sample trajectories:
B=S(=O)(c1ccc(Nc2ncnc3cc(Cl)ccc23)cc1)c1cccs1
BP(=O)(NC(=O)c1cc(F)c(F)c(F)c1)OCC(F)F
BP(=O)(OCC)OC(=O)CCCCC(=O)OCC=C(Br)Br
BP(=O)(OCC)c1ccc(Nc2ncnc3sc(Cl)cc23)cc1
BP(=O)(OCOC=O)C(Cl)(Br)Br
Policy gradient replay...
Mean value of predictions: 0.49145964
Proportion of valid SMILES: 0.4031298904538341
Sample trajectories:
BP(=O)(CCn1cnc(Nc2ccc(Br)cc2)c1)P(=O)(O)O
BP(=O)(NCCCCCCCCC(=O)O)c1cc(Br)cc(Br)c1
BP(=O)(OCC)OC(=O)C=CCCCCCCCCCCC=CCBr
BP(=O)(c1cc(Br)cc(Br)c1)N1CCN(CCO)CC1
B[PH]1(=O)(C(N)=O)CCN1CCN(CC)CC
Fine tuning...
Mean value of predictions: 0.42140082
Proportion of valid SMILES: 0.4823271817328746
Sample trajectories:
BP(=O)(C(=O)c1ccc(I)o1)P(=O)(O)O
BP(=O)(OCC)OC(=O)C(Cl)(Cl)P(=O)(O)O
BP(=O)(OCC)OC(=O)CCCC(=O)Nc1ccc(F)c(F)c1
BrC=CC=CC=CBr
BrCN1Cc2ccccc2-c2cncnc21

 18 Training on 16529 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.580086
Reward: 6.445941
Trajectories with max counts:
842	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.4267742
Proportion of valid SMILES: 0.19375
Sample trajectories:
BP(=O)(N1CCCC1)N1CCC(F)(NCC(=O)N2CCOCC2)C1
BP(=O)(NC1CCCCC1)c1cnc2cc(Br)c(Br)cc2c1F
BP(=O)(OC(C=O)Nc1ccc(Br)cc1)c1ccc(Br)cc1
Bc1cc(Br)c(Nc2ncc3ccccc3c2I)cc1Cl
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.44052377
Proportion of valid SMILES: 0.453579243513598
Sample trajectories:
BP(=O)(NCCCl)N1CCCCC1
Bc1ccc(Br)cc1Br
Bc1ccc(Nc2ncnc3ccccc23)cc1
BrCCc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.46128815
Proportion of valid SMILES: 0.4613700344072568
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCC(=C)CCCCCCCCCCCCCCCCCCN
BrC1=CC2=NCCC(C1)N2c1ncnc2sccc12
BrC=CC=CC=CC=C(Br)I
BrC=CC=Cc1cc2c(Nc3ccc(Br)s3)ncnc2s1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1

 19 Training on 17729 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.500387
Reward: 6.583530
Trajectories with max counts:
298	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5629362
Proportion of valid SMILES: 0.2598499061913696
Sample trajectories:
BP(=O)(NCCCCN)n1cnc2c(Br)cnc(NC3CCCCCC3)c21
Bc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Bc1cc(Br)ccc1Nc1cc2ncnc(Nc3cc(Br)c(Br)cn3)c2s1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1Br
Policy gradient replay...
Mean value of predictions: 0.3933667
Proportion of valid SMILES: 0.4996873045653533
Sample trajectories:
B[PH](=O)(NCc1ccc(F)cc1)=C1Nc2ccccc2-c2ccccc21
Bc1ccc(Nc2ncnc3ccsc23)cc1Br
BrC=CC=CC=CC=CC=CCC=CC=C=CC=CC=CC=NNc1ccc(Nc2ncnc3sc(Br)cc23)cc1
BrCCC=CC=CC=CC=CC=CC=CC=CCC=CC=CC=CC=CC=CCC=CCC=CC=CC=CCN1CCCOc2ccccc21
BrCCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.44925576
Proportion of valid SMILES: 0.461875
Sample trajectories:
BP(=O)(OC)OCC
BrBr
BrC=CBr
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1

 20 Training on 19082 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.302224
Reward: 6.465721
Trajectories with max counts:
323	Brc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.48967084
Proportion of valid SMILES: 0.2753125
Sample trajectories:
BP(=O)(Nc1ccc(Br)c(Br)c1)C(C(=O)Nc1ccc(Br)cc1)N1CCNCC1
BP(=O)(Nc1ccc(Br)cc1)P(=O)(Nc1ccc(F)c(Br)c1)Oc1ccc(F)c(F)c1
BP1(=O)CC(Cl)N1CC(F)(F)F
B[PH](=O)(=O)Nc1cccc(Nc2ncnc3sc(Br)cc23)c1
Br
Policy gradient replay...
Mean value of predictions: 0.4901704
Proportion of valid SMILES: 0.4773224898342196
Sample trajectories:
BP(=O)(OCC)OC(=O)CCC(CC=C)Nc1ccc(Nc2ncnc3c(N)ncnc23)cc1
Bc1c(Br)c(F)c(Br)c(Br)c1-c1cc(Br)c(O)c(Br)c1
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)c(Br)c1
BrCCBr
BrCCNc1ccc(Nc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.48008102
Proportion of valid SMILES: 0.4629571741169115
Sample trajectories:
BP(=O)(OC)OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O
Bc1cc(Br)c2c(c1)NC(Br)=CC(=O)N2
Bc1cc(Br)cc(Nc2cccc(Br)c2)c1Br
BrC(=NNc1ccccc1)c1ccc(Nc2ncnc3ccccc23)cc1
BrC=CC=NNc1nc(Nc2ccccc2)ncc1-c1ccc2ncncc2c1

Trajectories with max counts:
539	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.43964732
Proportion of valid SMILES: 0.397136068034017
Mean Internal Similarity: 0.47184957215706225
Std Internal Similarity: 0.10468621538888412
Mean External Similarity: 0.4081898031693409
Std External Similarity: 0.07432218265787317
Mean MolWt: 408.5643271417742
Std MolWt: 96.96206439762423
Effect MolWt: -0.8951494832851861
Mean MolLogP: 5.100196463229722
Std MolLogP: 1.3493924102679045
Effect MolLogP: 0.2837324446265632
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 96.857923% (709 / 732)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 20, 'n_policy_replay': 5, 'n_fine_tune': 20, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5589.869183063507, 'valid_fraction': 0.397136068034017, 'active_fraction': 0.4153676586364352, 'max_counts': 539, 'mean_internal_similarity': 0.47184957215706225, 'std_internal_similarity': 0.10468621538888412, 'mean_external_similarity': 0.4081898031693409, 'std_external_similarity': 0.07432218265787317, 'mean_MolWt': 408.5643271417742, 'std_MolWt': 96.96206439762423, 'effect_MolWt': -0.8951494832851861, 'mean_MolLogP': 5.100196463229722, 'std_MolLogP': 1.3493924102679045, 'effect_MolLogP': 0.2837324446265632, 'generated_scaffolds': 732, 'novel_scaffolds': 709, 'novel_fraction': 0.9685792349726776, 'save_path': '../logs/replay_ratio_mixed_s1-4.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.060398
Reward: 1.000000
Mean value of predictions: 0.0012028869
Proportion of valid SMILES: 0.782063342740671
Sample trajectories:
BrCc1ccc[n+](CCCCCCNCCC2CCCC2)c1
Brc1ccc(I)cc1
Brc1ccc(OCCc2ccccn2)nc1
Brc1cccc(C2=NC3=C(C=Cc4cc(Br)ccc4NC=N2)CCCC3)c1
C#CCCN(C)C1Cc2ccc2OCc2cc(F)ccc21
Policy gradient replay...
Mean value of predictions: 0.0013654619
Proportion of valid SMILES: 0.7810539523212046
Sample trajectories:
Brc1cc(C=Cc2ccccc2)nc(-c2ccccc2)c1
Brc1ccc(Sc2[nH]c3cccnc3c2Br)cc1Br
Brc1cccc(-c2cnnc(Nc3ccccc3)n2)c1
Brc1cccc(-c2nnc3ccnc(NCCN4CCCCC4)c3n2)c1
C#CC(O)CCCCC
Fine tuning...
Mean value of predictions: 0.006451613
Proportion of valid SMILES: 0.6996865203761755
Sample trajectories:
Brc1ccc(-c2cc(Nc3ccc(Br)cn3)c3ncnnc3n2)cc1
Brc1ccc(CN2C=Nc3ccccc3SC2=Nc2ccccc2)cc1
Brc1ccc(Nc2nc3ccccc3n2-c2ccccc2)cc1
Brc1ccc(Nc2ncc[nH]2)c2ccccc12
Brc1ccc2c(c1)[nH]c1c(CCCN3CCN(c4ccccc4)CC3)CC12

  2 Training on 247 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.491705
Reward: 1.051394
Trajectories with max counts:
2	CC(C)Br
2	Fc1ccc(Nc2ncnc3ccccc23)cc1
2	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
2	O=C1Nc2ccccc2C1=O
Mean value of predictions: 0.012524655
Proportion of valid SMILES: 0.6355374490755249
Sample trajectories:
BrC1=CC2C=CCCCC2=Nc2ccccc21
Brc1ccc(-c2cc(-c3cccnc3)ncn2)s1
Brc1ccc2[nH]c(-c3ncnc4[nH]c5ccccc5c34)nc2c1
Brc1ccc2[nH]cc(CCNCc3ccnc4ccccc34)c2c1
Brc1ccccc1Nc1ncnc2cc3ncnn3-n3ccnc3nc12
Policy gradient replay...
Mean value of predictions: 0.011246944
Proportion of valid SMILES: 0.6410658307210031
Sample trajectories:
BrC12CCC1N(Cc1ccccc1)C2
Brc1ccc(C=NN=C2Nc3ccccc32)cc1
Brc1ccc(C=NNc2cc(Nc3ccc4[nH]cnc4c3)ncn2)cc1
Brc1ccc(CCN2C(Nc3ccccc3Br)=NC23CCC(Br)(c2ccccc2Nc2ncnc4ccccc24)CC3)cc1
Brc1ccc(Nc2ccncc2)nc1
Fine tuning...
Mean value of predictions: 0.018584907
Proportion of valid SMILES: 0.6643685365089314
Sample trajectories:
Brc1ccc(Nc2ncc3ncn(CC4CCC4)c3n2)nc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cccc(Nc2nnc(-c3ccoc3)n2-c2ccccc2)c1
Brc1cccc2ccccc12
Brc1cnc(Nc2ccc3ncccc3c2)nc1

  3 Training on 380 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.866967
Reward: 1.087372
Trajectories with max counts:
3	Fc1ccccc1F
Mean value of predictions: 0.023253676
Proportion of valid SMILES: 0.6823455628723738
Sample trajectories:
BP(=O)(N(c1ccc(Br)cc1)c1cc(Cl)cc(Cl)c1)P(=O)(c1ccccc1)c1cccc(Br)c1
Brc1ccc(C=NN2CCN(c3ccccn3)CC2)cc1
Brc1ccc(Nc2cc(Br)c(-c3nc4ccccc4o3)cn2)cc1
Brc1ccc(Nc2ccc3nc(-c4cscn4)oc3c2)cc1
Brc1ccc(Nc2nc(Nc3ccccc3)nc(Nc3ccc(CN4CCCCC4)cc3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.025230205
Proportion of valid SMILES: 0.67981220657277
Sample trajectories:
Brc1ccc(-c2nc(Nc3ccc(I)cc3)c3cc(Br)ccc3n2)cc1
Brc1ccc(Nc2c(Br)cnn2-c2ccccn2)cc1
Brc1ccc(Nc2nc(Nc3ccc(Nc4ccccc4)cc3)nc(Nc3cccs3)n2)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cccc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.029162562
Proportion of valid SMILES: 0.636563185951709
Sample trajectories:
BP(=O)(Nc1ccccc1)OC(=CSC(=S)N1CCOCC1)S(=O)(=O)c1cccc(Br)c1
BrC(=CCN1CCCCCC1)c1cccc2ccccc12
Brc1c(Br)c2sc(N3CCN(c4ccc5nccn5c4)CC3)nc12
Brc1ccc(-c2nc(-c3ccc(Br)o3)no2)cc1
Brc1ccc(NN=C2Nc3ccccc3S2)cc1

  4 Training on 631 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.485272
Reward: 1.226776
Trajectories with max counts:
11	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.040620588
Proportion of valid SMILES: 0.6653112292774476
Sample trajectories:
BP(=O)(N1CCN(C(=O)c2c(F)cccc2F)CC1)P(=O)(O)O
Brc1cc(Br)c2cccnc2c1
Brc1ccc(-c2sc3nccnc3c2-c2cccnc2)o1
Brc1ccc(Nc2ccnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2n[nH]c3ncnc(-c4ccccc4)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.037664782
Proportion of valid SMILES: 0.6641651031894934
Sample trajectories:
B[PH](=O)(=CN1C=C(F)C(=O)Nc2c(F)c(F)c(F)c(F)c2C1=O)OCC
Brc1cc(-c2cccc3ccccc3-c3ncccc23)on1
Brc1ccc(-c2ccccc2)c2c1-c1ccccc1S2
Brc1ccc(C(I)=NN=C2CCCCC2)cc1
Brc1ccc(Nc2cc(-c3cccnc3)ncn2)cc1
Fine tuning...
Mean value of predictions: 0.048621554
Proportion of valid SMILES: 0.6246086412022542
Sample trajectories:
Brc1cc(Nc2ccncc2)c2ccccc2n1
Brc1ccc(Br)c(Nc2ccncc2)c1
Brc1ccc(C=NNc2ccc3ccccc3n2)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccncc23)cc1

  5 Training on 1001 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.943720
Reward: 1.374911
Trajectories with max counts:
23	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.08266528
Proportion of valid SMILES: 0.6018170426065163
Sample trajectories:
BP(=O)(OCc1ccc(Br)cc1)Oc1ccc(F)c(F)c1
Brc1ccc(-n2cnc3c(N4CC4Br)ncnc32)cc1
Brc1ccc(C2Nc3cc(Br)ccc3Nc3ccccc32)cc1
Brc1ccc(CNc2ncnc3[nH]cnc23)cc1
Brc1ccc(NN=Cc2ccccc2)cc1
Policy gradient replay...
Mean value of predictions: 0.075650364
Proportion of valid SMILES: 0.6017532874139011
Sample trajectories:
Brc1ccc(I)cc1
Brc1ccc(Nc2cc3cc(Br)ccc3cn2)nc1
Brc1ccc(Nc2ccc(Nc3ccnc4cnc(Br)cc34)cc2)cc1
Brc1ccc(Nc2ccnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2cncnc2)cc1
Fine tuning...
Mean value of predictions: 0.089801975
Proportion of valid SMILES: 0.631447327289778
Sample trajectories:
Brc1ccc(-c2nc3ccccc3s2)c(C=Cc2ccco2)c1
Brc1ccc(Br)c(Nc2ccc3nncn3c2)c1
Brc1ccc(Br)c2c1Nc1ncccc1-2
Brc1ccc(N=Nc2ccc3c(c2)-c2cc(Br)sc2-3)cc1
Brc1ccc(NN=Nc2ccccc2)cc1

  6 Training on 1673 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.801517
Reward: 2.230560
Trajectories with max counts:
489	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.1219544
Proportion of valid SMILES: 0.4796875
Sample trajectories:
Brc1ccc(-c2nccnc2Oc2ccc(Br)s2)cc1
Brc1ccc(Nc2ncnc3c(Nc4ccccc4Br)ncnc23)cc1
Brc1ccc(Nc2ncnc3c4cc5ncccnc5Nc(ncnc23)n4)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc4ccccc4nc23)cc1Nc1ccc2ncnc(Nc3cccc4ccccc34)c2c1
Policy gradient replay...
Mean value of predictions: 0.117914446
Proportion of valid SMILES: 0.4675
Sample trajectories:
Brc1cc2c(Nc3ccccc3Br)cccc2s1
Brc1ccc(-n2cnc3c(NCc4ccccc4)ncnc32)cc1
Brc1ccc(Nc2ccncc2N2CCN(Cc3ccccc3)CC2)cc1
Brc1ccc(Nc2ncnc3ccc(I)cc23)cc1
Brc1ccc(Nc2ncnc3cccc(Br)c3s2)cc1
Fine tuning...
Mean value of predictions: 0.14497109
Proportion of valid SMILES: 0.5411323115420706
Sample trajectories:
BrC1=CC2=CNC2=Nc2cc3c(cc2O1)OCO3
Brc1ccc(-c2cc(CNc3ccnc4ccc(-c5ccncc5)cc34)ncn2)nc1
Brc1ccc(C2CC(c3ccccn3)=NN2CC2CC2)cc1
Brc1ccc(N2CCCC2)nc1
Brc1ccc(NN=Cc2ccc3ccccc3c2)cc1

  7 Training on 2500 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 22.923143
Reward: 2.707955
Trajectories with max counts:
337	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.2090037
Proportion of valid SMILES: 0.4238348451673444
Sample trajectories:
BP(=O)(OCCC=C)Oc1ccc(Br)cc1
Brc1ccc(Nc2ncc3c4ncc(Br)cc4c3sc2Br)cc1
Brc1ccc(Nc2ncnc(Nc3ccc(Br)cc3)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.21729894
Proportion of valid SMILES: 0.4120037511722413
Sample trajectories:
Brc1cc2cn[nH]c2cc1OCCN1CCOCC1
Brc1ccc(-c2cc3sc4ncnc(Nc5cccc(Br)c5)c4c3[nH]2)cc1
Brc1ccc(Nc2ncnc(Nc3ccc(Br)cc3)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.18012197
Proportion of valid SMILES: 0.5125
Sample trajectories:
BP(=O)(OCC1OC(c2ccc(Br)cc2)C(O)C1O)c1ccc(Br)cc1
BP(=O)(OCCC(N)=O)N(=O)=O
B[PH](=O)(Nc1ccc(Br)cc1)(c1ccccc1)c1ccc(Br)cc1
Brc1ccc(Nc2cccnc2)cc1
Brc1ccc(Nc2ccncc2)nc1

  8 Training on 3543 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 23.108101
Reward: 3.851432
Trajectories with max counts:
869	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.23403361
Proportion of valid SMILES: 0.2975
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cccc(Br)c23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cn1
Brc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.24244985
Proportion of valid SMILES: 0.2959375
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Nc4ccccc4)ccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3sc(Cc4ccccc4)cc23)cc1
Brc1ccc2ncnc(Nc3ccncc3)Nc3ccccc3Sc2c1
Brc1cccc(Nc2ncnc3ccccc23)c1
Fine tuning...
Mean value of predictions: 0.22961877
Proportion of valid SMILES: 0.42638324476398876
Sample trajectories:
BP(=O)(OCC1CCCCC1)n1cnc2c(N)ncnc21
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1OP(=O)(O)O)C(N)=O
BP(=O)(OP(=O)(O)OP(=O)(O)OCC)N(CC(=O)N(O)Cc1cc(I)c(O)c(Br)c1Br)c1ccc(Br)cc1
Brc1ccc(Nc2ccncn2)nc1
Brc1ccc(Nc2ncccc2-c2ncnc3sccc23)cc1

  9 Training on 4409 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 24.786628
Reward: 4.123285
Trajectories with max counts:
98	Fc1ccc(Nc2ncnc3cc(F)cc(F)c23)cc1
Mean value of predictions: 0.33691275
Proportion of valid SMILES: 0.46606193306224586
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3oc(Br)cc3Br)ncnc2c1
Brc1ccc(Nc2cc3c(CN4CCC(c5nc6cc(-c7ccc(Br)cc7)c(-c7cccs7)nc6s5)CC4)cc(Br)cc3s2)cc1
Brc1ccc(Nc2ncnc3c(Br)cc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Nc4cc(Br)c(Br)c(Br)c4)ncnc23)cc1
Policy gradient replay...
Mean value of predictions: 0.3247268
Proportion of valid SMILES: 0.45807259073842305
Sample trajectories:
BP(=O)(OC)OCC
Brc1cc(Br)c(Nc2ncnc3cc(Br)cc(Br)c23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccco3)ncnc2c1
Brc1cc(Br)nc(Nc2nc(N3CCOCC3)nc3cc(Br)cc(Br)c23)c1
Brc1cc(Nc2cc(Br)c(Br)cc2c2ccc(Br)c(Br)c2)c2c(Nc3ccc(I)cc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.26386914
Proportion of valid SMILES: 0.439375
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)C(=O)Oc1ccc(Br)cc1
BrC(Br)(Br)Br
BrCC1CCCC(N2CCc3c(NCc4ccccc4)ncnc32)C1
Brc1ccc(-c2ccc3ccccc3c2)nc1
Brc1ccc(-c2ccccc2)c(Nc2ncnc3ccccc23)c1

 10 Training on 5891 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 22.450285
Reward: 4.788151
Trajectories with max counts:
707	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.28975832
Proportion of valid SMILES: 0.2715625
Sample trajectories:
BrC1=CC2(CCCN2CCc2ccccc2)CCN1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2nncnc2Nc2ccc3ccccc3c2)cc1
Policy gradient replay...
Mean value of predictions: 0.28229886
Proportion of valid SMILES: 0.2719599874960925
Sample trajectories:
Brc1cc2c(N3CCN(Cc4ccc(Nc5ccccc5)cc4)CC3)c(Br)ccc2c(Br)c1Br
Brc1ccc(Nc2ncnc(Nc3ccccc3)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.30805153
Proportion of valid SMILES: 0.388125
Sample trajectories:
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2cc(Br)nc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3c(Br)cc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3c2COc2ccccc2-3)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1

 11 Training on 6786 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 22.939817
Reward: 5.407569
Trajectories with max counts:
809	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.173235
Proportion of valid SMILES: 0.2965625
Sample trajectories:
Bc1ccc(Br)cc1Br
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cccc(Nc2ncccc2-c2ncnc3ccccc23)c1
Brc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.1891892
Proportion of valid SMILES: 0.300625
Sample trajectories:
Bc1cccc(Nc2ncnc3ccccc23)c1
Brc1cc(Nc2ncnc3ncncc23)ccc1Nc1cccc(Nc2ncccn2)c1
Brc1ccc(Nc2ncnc3cc(-c4ccccc4)nc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.31367293
Proportion of valid SMILES: 0.3496875
Sample trajectories:
BP(=O)(OC(=O)Cl)c1ccc(Nc2ncnc3c(Br)ccc(Cl)c23)cc1
Brc1ccc(Br)c(Nc2cccnc2)c1
Brc1ccc(C2=Nc3cc(Br)ccc3Nc3ccc(Br)cc32)cc1
Brc1ccc(Nc2ncnc3c2c2ccccc2S3)cc1
Brc1ccc(Nc2ncnc3cc(-c4nn[nH]n4)sc23)cc1

 12 Training on 7158 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 20.269648
Reward: 4.320008
Trajectories with max counts:
585	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.3635659
Proportion of valid SMILES: 0.3225
Sample trajectories:
Brc1cc(Nc2ncnc3ccsc23)cs1
Brc1ccc(-c2csc3c4ncncc4cc(Br)nn23)cc1
Brc1ccc(Nc2ccnc3ccsc23)cc1
Brc1ccc(Nc2ccncn2)cc1
Brc1ccc(Nc2nc3ccccc3s2)nc1
Policy gradient replay...
Mean value of predictions: 0.37629485
Proportion of valid SMILES: 0.31375
Sample trajectories:
BP(=O)(NO)c1ccc(Nc2nc(Cl)nc(Nc3cc(Br)cs3)c2Nc2sc(Br)cc2Br)cc1
Brc1[nH]nc2ncnc(Nc3ccccc3)c12
Brc1cc(Nc2ncnc3ccsc23)cs1
Brc1ccc(-c2ncnc3scc(Br)c23)cc1
Brc1ccc(Nc2cc(Nc3ncnc4ccsc34)ccc2Br)cc1
Fine tuning...
Mean value of predictions: 0.3304805
Proportion of valid SMILES: 0.41625
Sample trajectories:
Brc1cc(Br)c(Nc2ncnc3ccccc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Nc2ncnc3ccsc23)cc2c1sc1ccccc12
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1

 13 Training on 7833 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.270005
Reward: 5.031523
Trajectories with max counts:
1050	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.43843135
Proportion of valid SMILES: 0.2390625
Sample trajectories:
BrC(=NN1Sc2cc(Br)ccc2OC1Cn1ccnc1)c1ccc(Br)cc1
Brc1cc2ncnc(Nc3ccsc3)c2cc1Br
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ccc3nncn3n2)c1
Brc1ccc(N=Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.4451613
Proportion of valid SMILES: 0.2325
Sample trajectories:
BrCc1nc2c(N=Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Nc2ncnc3ccsc23)c2ccccc2n1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc2ccccc2nc1Cc1ccncc1
Brc1ccc(Nc2nc3ccccc3s2)cc1
Fine tuning...
Mean value of predictions: 0.38754326
Proportion of valid SMILES: 0.36125
Sample trajectories:
BP(=O)(Nc1cccc(Nc2ncnc3cc(F)cc(F)c23)c1F)C(F)(F)F
BP(=O)(OCC)N(C(F)(F)F)P(=S)(Nc1ccc(Br)cc1)C(F)(F)F
BP(=O)(OCC1(Nc2ccc(F)cc2)CC(=O)Oc2ccccc21)C(F)(F)F
BP(=O)(OCC1OC(CO)C(O)C(O)C1O)Oc1cccc2c(Br)ccc(Br)c12
BP(=O)(OCC1OC(N2C=C(Br)C(=O)NC2=O)C(O)C(O)C1O)Oc1ccc(Br)cc1

 14 Training on 8606 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 18.900795
Reward: 5.320633
Trajectories with max counts:
1379	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5068853
Proportion of valid SMILES: 0.190625
Sample trajectories:
BP(=O)(Nc1cc(F)c(F)c(F)c1)OCC
BP(=O)(Nc1ccc(Cl)cc1)Nc1ccc(Br)cc1
BP(=O)(OCCO)C(=O)Nc1cc(Nc2ncnc3sccc23)cs1
Brc1cc(Nc2ncnc3ccsc23)cs1
Brc1ccc(Nc2ccc(Nc3ccnc4cccnc34)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.51137924
Proportion of valid SMILES: 0.18125
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1cccc(Nc2ncnc3ccsc23)c1
BrC(Br)c1ccc[nH]1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1ccc(Nc2cc(Br)cnc2Br)cc1
Fine tuning...
Mean value of predictions: 0.43183675
Proportion of valid SMILES: 0.30625
Sample trajectories:
BP(=O)(C(=O)OCC)N(CC(=O)Nc1cccc(Br)c1)P(=O)(O)O
BP(=O)(c1ccc(F)c(F)c1)N(O)C(F)(F)F
B[PH](=O)(=NC)OC(C)=O
Bc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1

 15 Training on 9314 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.771240
Reward: 5.880230
Trajectories with max counts:
1430	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5055644
Proportion of valid SMILES: 0.1965625
Sample trajectories:
BP(=O)(OC(C)CO)P(=O)(O)O
BrC=C(Br)Br
BrCc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Brc1cc(Nc2ncnc3ccsc23)cc2ccccc12
Brc1cc(Nc2ncnc3ccsc23)ccc1Nc1ncnc2sccc12
Policy gradient replay...
Mean value of predictions: 0.49965867
Proportion of valid SMILES: 0.183125
Sample trajectories:
BP(=O)(Nc1ccc(F)cc1)C(=O)Oc1ccc(Nc2ncnc3ccccc23)cc1
BP(=O)(c1ccc(Br)c(Br)c1)N1CCN(C(=O)c2ccc(N)c(I)c2)CC1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1ccc(NNc2ncnc3ccsc23)cc1
Brc1ccc(Nc2ccnc(Nc3ccsc3)c2)cc1
Fine tuning...
Mean value of predictions: 0.3971831
Proportion of valid SMILES: 0.310625
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCCN1Oc2cc(Br)cc(Br)c2C=Cc2c(Nc3ccc(Br)s3)ncnc21
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncc(Br)c(Br)c2c1
Brc1ccc(-c2nc3ccc(Br)cn3c2-c2cccs2)cc1

 16 Training on 10034 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 18.244940
Reward: 6.613193
Trajectories with max counts:
1639	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5289157
Proportion of valid SMILES: 0.155625
Sample trajectories:
Brc1cc(Br)c(Br)c(Sc2ccccc2Nc2ccccc2Br)n1
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c(Nc2ncnc3sc4c(Nc5ccccc5)cc4c23)cc1Br
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(Br)nc(Nc2ncnc3cc(Br)c(Br)nc23)c1
Policy gradient replay...
Mean value of predictions: 0.51382405
Proportion of valid SMILES: 0.1740625
Sample trajectories:
Brc1cc(Br)c(N2CCN(CCNc3ncncn3)CC2)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(N(c2cnn(C3CC3)c2)c2ncnc3ccsc23)cnc1-c1cccs1
Brc1cc(Nc2ncnc3c(Nc4ccccc4Br)ncn23)ncn1
Brc1cc(Nc2ncnc3ccsc23)ccc1Nc1ncccn1
Fine tuning...
Mean value of predictions: 0.44948983
Proportion of valid SMILES: 0.245
Sample trajectories:
Bc1ccc(Nc2nc(Nc3ccccc3)sc2C#N)cc1
BrCCSc1ccc(Nc2ncnc3ccccc23)cc1
BrCc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(-c2ccccc2)n(-c2ccccc2)c1-c1ccccc1
Brc1cc(I)cc(Nc2ncnc3sccc23)c1

 17 Training on 10690 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.151334
Reward: 6.889933
Trajectories with max counts:
976	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.44771242
Proportion of valid SMILES: 0.19125
Sample trajectories:
BP(=O)(Nc1ccc(Nc2ncnc3ccsc23)cc1)Nc1ccc(Br)cc1F
BP(=O)(O)OP(=O)(O)OP(=O)(O)O
Bc1cc(Nc2ncnc3ccsc23)ccc1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrC=Nc1ccccc1-c1ccccc1
Policy gradient replay...
Mean value of predictions: 0.4858569
Proportion of valid SMILES: 0.1878125
Sample trajectories:
BP(=O)(CCCc1ccccc1)NO
BP(=O)(OCC1OC(=N)NC(c2ccsc2)=N[PH](c2cccnc2)(C(F)(F)F)C(O)C1O)C(O)CP(=O)(O)O
BP(=O)(OCC1OC(=NO)C(O)C1O)c1ccc(Br)cc1
B[PH](=O)(Nc1ccc(Br)cc1)=P(=NO)c1ccccc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Fine tuning...
Mean value of predictions: 0.42436883
Proportion of valid SMILES: 0.2846875
Sample trajectories:
BP(=O)(Oc1ccc(Br)c(Br)c1)OC(C)COC(=O)CCCCCCCCC
BP(F)(F)(F)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)CP(=O)(O)OP(=O)(O)C(F)(F)P(=O)(O)O
BP1(=O)OCC2OC(=N)N(O1)C2C(=O)Oc1cc(Br)cc(Br)c1
B[PH](=O)(CCNc1ccc(Br)cn1)=[PH](c1ccc(Br)cc1)P(=O)(O)Oc1cccc(Br)c1
Bc1ccc(Nc2ncnc3cccc(Br)c23)cc1

 18 Training on 11392 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.798561
Reward: 7.461629
Trajectories with max counts:
1447	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.502193
Proportion of valid SMILES: 0.1425
Sample trajectories:
B[PH](=O)(Nc1ccc(Nc2ccccc2Br)cc1)(P(=O)(O)O)[PH](O)(O)OP(=O)(O)O
Bc1cc(Nc2ncnc3ccsc23)ccc1F
BrBr
Brc1cc(Br)c(Nc2ccc(Br)c(Br)c2)c(I)c1
Brc1cc(Br)c2cncnc2c1Nc1ncnc2ccsc12
Policy gradient replay...
Mean value of predictions: 0.5134474
Proportion of valid SMILES: 0.1278125
Sample trajectories:
BP(=O)(Nc1ccc(F)c(F)c1)c1ccc(F)c(Br)c1
BP(=O)(Nc1ccc(NP(=O)(O)OCC(F)(F)F)cc1)c1ccc(F)cc1
BP(=O)(OCC)ON(C(=O)OP(=O)(O)OP(=O)(O)OP(=O)(O)C(F)(F)F)c1cnc(NP(=O)(OC)OC(F)(F)F)cc1F
Bc1c(I)cc(I)c(Nc2ncnc3ccsc23)c1I
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br
Fine tuning...
Mean value of predictions: 0.45927978
Proportion of valid SMILES: 0.225625
Sample trajectories:
BP(=O)(N(O)C(Cc1ccccc1)NCP(=O)(O)OP(=O)(O)O)P(=O)(O)O
BP(=O)(NO)N(O)C(=O)OC
BP(=O)(NO)c1ccc(Br)cc1
Bc1ccc(Nc2nc3cc(Br)ccc3s2)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1

 19 Training on 11956 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.328473
Reward: 7.696481
Trajectories with max counts:
1561	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49582505
Proportion of valid SMILES: 0.1571875
Sample trajectories:
BP(=O)(NO)c1ccc(Nc2cc(Br)cc(Br)c2)cc1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(Nc2cncnc2)cs1
Brc1ccc(-c2ccc(Nc3ncnc4ccsc34)cc2)s1
Brc1ccc(-c2ccc(Nc3ncnc4sccc34)cc2)s1
Policy gradient replay...
Mean value of predictions: 0.4568138
Proportion of valid SMILES: 0.1628125
Sample trajectories:
BP(=O)(NCc1ccc(Br)cc1)c1c(Br)c(Br)c(Br)c(Br)c1Br
BP(=O)(Nc1cc(Br)cc(Nc2ncnc3ccccc23)c1)OCCO
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)Oc1cccc2c(I)cc(Br)cc12
Bc1cc(Br)c(Br)cc1Br
Brc1c(Br)c(Br)c(Br)c(Br)c1Br
Fine tuning...
Mean value of predictions: 0.5041769
Proportion of valid SMILES: 0.254375
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Brc1cc(Nc2ncnc3ccccc23)ccc1C=NNc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(-c2cc(Nc3ncnc4ccsc34)co2)cc1

 20 Training on 12608 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.147293
Reward: 7.970521
Trajectories with max counts:
791	Fc1ccc(Nc2ncnc3ccsc23)cc1F
Mean value of predictions: 0.56970954
Proportion of valid SMILES: 0.150625
Sample trajectories:
BP1(=O)OCC2OC(OC(=O)CCCCCCC(C(C)(Br)C(O)C(C)O)C(C)(C)C(=CCBr)CC1O)C(O)C2Br
BrCC1CCCN1Sc1ccc(Nc2cc3ccsc3cn2)o1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(Nc2ncnc3ccsc23)ccn1
Brc1ccc(Br)c(Nc2ccc(Nc3nccs3)cc2)c1
Policy gradient replay...
Mean value of predictions: 0.6097473
Proportion of valid SMILES: 0.173125
Sample trajectories:
BP(=O)(O)C(F)(F)F
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)O
BP1(=O)OCC2OC(C(O)C2O)N(C=C(Br)Br)C(=O)NC1=O
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(c2ccccc2Br)sc1-c1ccc(Nc2ncnc3ccsc23)nc1
Fine tuning...
Mean value of predictions: 0.54013604
Proportion of valid SMILES: 0.2296875
Sample trajectories:
BP(=O)(c1ccc(Br)cc1)N(CC(=O)OC)C(F)(F)F
Br
BrCBr
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Nc2nc(Nc3ccsc3)nc3ccccc23)cc1Br

Trajectories with max counts:
4759	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.46281743
Proportion of valid SMILES: 0.1796875
Mean Internal Similarity: 0.47822129479082975
Std Internal Similarity: 0.11629405820111426
Mean External Similarity: 0.39626110380215046
Std External Similarity: 0.06825734681377792
Mean MolWt: 359.07231389102077
Std MolWt: 83.12793875671748
Effect MolWt: -1.3219425962291051
Mean MolLogP: 4.671416078280892
Std MolLogP: 1.1441292095383229
Effect MolLogP: -0.03656472299061179
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 95.982143% (430 / 448)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 20, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5579.556708097458, 'valid_fraction': 0.1796875, 'active_fraction': 0.45321739130434785, 'max_counts': 4759, 'mean_internal_similarity': 0.47822129479082975, 'std_internal_similarity': 0.11629405820111426, 'mean_external_similarity': 0.39626110380215046, 'std_external_similarity': 0.06825734681377792, 'mean_MolWt': 359.07231389102077, 'std_MolWt': 83.12793875671748, 'effect_MolWt': -1.3219425962291051, 'mean_MolLogP': 4.671416078280892, 'std_MolLogP': 1.1441292095383229, 'effect_MolLogP': -0.03656472299061179, 'generated_scaffolds': 448, 'novel_scaffolds': 430, 'novel_fraction': 0.9598214285714286, 'save_path': '../logs/replay_ratio_mixed_s1-5.smi'}
