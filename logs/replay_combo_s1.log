starting log


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.548579
Reward: 1.000000
Mean value of predictions: 0.0012553942
Proportion of valid SMILES: 0.7983088005010961
Sample trajectories:
Brc1ccccc1-c1nc2cccnc2[nH]1
Brc1ccccc1C=Nc1nc(-c2ccccc2)cs1
Brc1cnc(-c2cnc3ncnc(Nc4ccncc4)n23)nc1
C#CC(=CCC=CCCC)CCCC(=O)O
C#CCCN(Cc1ccccc1OCCO)c1cccc(COC(Cn2cncn2)c2ccc3ccccn23)c1
Policy gradient replay...
Mean value of predictions: 0.010656754
Proportion of valid SMILES: 0.7565625
Sample trajectories:
BP(=O)(NCCCCOc1ccccc1Cl)S(=O)(=O)Nc1nccs1
BrCC1Cc2cccn2C1
Brc1ccc(Nc2ccccc2)cc1
Brc1ccc2c(Nc3ccccc3)ncnc2c1
Brc1ccc2ccc(CN(Cc3ccncc3)Cc3ccccn3)cc2c1
Fine tuning...
Mean value of predictions: 0.011087779
Proportion of valid SMILES: 0.7445278298936836
Sample trajectories:
BP(=O)(OCCC#N)c1ccccc1
Brc1cc(Br)n(Cc2ccccc2Br)c1
Brc1ccc(-c2nc3ccccc3nc2N=Cc2ccccc2Br)cc1
Brc1ccc(Nc2nc3ccccc3s2)s1
Brc1ccc(Nc2ncnc3ccccc23)cc1

  2 Training on 308 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 15.575635
Reward: 1.000000
Trajectories with max counts:
28	Cc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.010370995
Proportion of valid SMILES: 0.74125
Sample trajectories:
BP(=O)(OCC1(O)Oc2ccccc21)Oc1ccc(F)cc1
BrC1CC=C(Cc2ccccc2)N1CCc1ccccc1
Brc1cc2ccccc2nc1-c1ccccc1
Brc1ccc(C=NNc2ncnc3ccccc23)cc1
Brc1ccc2c(Nc3ccccc3OC(c3ccccc3)c3ccccc3)cccc2c1
Policy gradient replay...
Mean value of predictions: 0.09180709
Proportion of valid SMILES: 0.5379806189434199
Sample trajectories:
BP(=O)(CCCl)OCCCl
Bc1ccc(C=Nc2ncnc3[nH]c(-c4ccco4)nc23)cc1
BrC1=C(Br)OC2c3ccccc3N2C(c2cccnc2)=C1
Brc1cc2c(Nc3ccc(I)cc3)ncnc2cn1
Brc1ccc(-c2nc[nH]c2-c2ccc3ncnc(Nc4cccc(NCCN5CCOCC5)c4)c3n2)cc1
Fine tuning...
Mean value of predictions: 0.09842165
Proportion of valid SMILES: 0.5550688360450563
Sample trajectories:
BP(=O)(CCC)N(NC(=O)c1ccccc1)Nc1ccc(Br)cc1
Brc1cc(-c2cc3c(cc2-c2ccccc2)OCO3)c2ncnn2n1
Brc1cc2c(cc1Br)N=C(c1ccccc1)C=C2
Brc1ccc(-c2ccc3ncnc(Nc4cccc(Nc5cccc(Br)c5)c4)c3c2)cc1
Brc1ccc(-c2ccc3ncnc(Nc4ccccc4Br)c3n2)nc1

  3 Training on 812 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.357710
Reward: 1.132669
Trajectories with max counts:
6	Brc1cccc(Nc2ncnc3ccccc23)c1
6	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
6	Oc1cccc2ccccc12
Mean value of predictions: 0.1
Proportion of valid SMILES: 0.5649045980606819
Sample trajectories:
BP(=O)(NP(=O)(OCC)OCC)OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O
BP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCCCC)OC(=O)CCCl
Brc1c(Nc2ncnc3c2Nc2ccccc23)sc2ccccc12
Brc1cc(I)ccc1Nc1ncnc2cccnc12
Policy gradient replay...
Mean value of predictions: 0.10815851
Proportion of valid SMILES: 0.6705220381369178
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCCCC)Oc1ccc(Nc2ccc3ncccc3c2)cc1
B[PH](=O)OCC1OC(C(O)C#N)C(F)C1Br
Bc1cccc(Nc2ncnc3c(Nc4ccccc4)cc23)c1
Bc1cccc2ccc(Br)cc12
Fine tuning...
Mean value of predictions: 0.11756168
Proportion of valid SMILES: 0.6459375
Sample trajectories:
BP(=O)(NC(=O)OCc1ccccc1)NC(=O)OCc1ccccc1
Bc1ccc(Nc2nccc3ccc(NC(=O)CBr)cc23)cc1Cl
BrCCOc1cccc2ncncc12
Brc1cc2cc(Nc3ccc4ccccc4c3)ccc2nc1Br
Brc1cc2nc(Nc3ccncc3)[nH]c2cc1Br

  4 Training on 1717 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 16.926292
Reward: 1.242465
Trajectories with max counts:
30	Brc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.11317393
Proportion of valid SMILES: 0.6669793621013134
Sample trajectories:
Bc1cc(Br)ccc1Nc1ncnc2ccccc12
Brc1cc(Br)c2c(Nc3cnc(CNC4CCN(c5ccccc5)CC4)nc3-c3ccccc3Br)ncnc2c1
Brc1ccc(-c2ccc3cncc(Nc4ccccc4)c3c2)s1
Brc1ccc(-c2nc(Nc3ccccc3)c3ccccc3n2)cc1
Brc1ccc(-c2nccs2)s1
Policy gradient replay...
Mean value of predictions: 0.17699745
Proportion of valid SMILES: 0.6152160300563556
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
B[PH](=O)(Br)(OCCCCCCCCBr)c1cccc(Br)c1
BrCC1OCC(c2nnc3cncnc3n2)O1
BrSc1ccc(Nc2nc3ccc(Br)cc3s2)cc1
Brc1cc(Br)cc(Nc2cccc(CSc3ccc4ncnc(-c5ccccc5)c4n3)c2)c1
Fine tuning...
Mean value of predictions: 0.18944697
Proportion of valid SMILES: 0.6172878170999061
Sample trajectories:
BP(=O)(NCCCCCCCCCCCCCC(=O)O)n1cnc2c(N)ncnc21
BP(=O)(NCCCCCNC(=O)C(Nc1ccc(Br)cc1)P(=O)(O)O)N(=O)=O
BrCCNc1ncc(Br)c2c1-c1ccccc1N2
BrCCc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cc(Nc2ccccc2)nc(-c2ccccc2)c1-c1cccnc1

  5 Training on 2933 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 17.454379
Reward: 1.363316
Trajectories with max counts:
48	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.22098434
Proportion of valid SMILES: 0.5792606516290727
Sample trajectories:
BrCCOc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cc(Nc2ncnc3ccccc23)cc(Br)c1-c1cccnc1
Brc1ccc(-c2cccc(Nc3ncnc4ccccc34)c2)cc1
Brc1ccc(-c2nc(Nc3ccc4c(c3)OCCO4)ncc2-c2ccccc2)cc1
Brc1ccc(C=NNc2ccc(Br)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.26950276
Proportion of valid SMILES: 0.5665101721439749
Sample trajectories:
BP(=O)(NCC(F)F)P(=O)(O)O
BP1(F)(F)NC(SCC(=O)Nc2ccc(F)cc2)=NCC1F
Bc1cccc(Nc2ncnc3cc(Br)c(Cl)cc23)c1
Bc1ncnc(Nc2ccc(Br)cc2)n1
BrC=CBr
Fine tuning...
Mean value of predictions: 0.28172645
Proportion of valid SMILES: 0.5583724569640063
Sample trajectories:
BP(=O)(OCC)OC(=O)OCCCCCCCCC
BrC=CBr
BrCc1ccc2c(Nc3ccc(Br)cc3)ncnc2c1
BrCc1nc2c(Nc3ccc(Br)o3)ncnc2s1
Brc1cc(Br)c(Nc2ccccc2)c(Nc2ccc3ncnc(Nc4ccc(I)cc4)c3c2)c1

  6 Training on 4472 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 18.142192
Reward: 1.549752
Trajectories with max counts:
35	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.2506893
Proportion of valid SMILES: 0.589375
Sample trajectories:
BP(=O)(C(=O)C=Cc1ccc(F)c(N)c1)N1CCN(c2ccc(F)cc2)CC1
BP(=O)(OCC)OCC1Oc2ccc(Br)cc2N(Cc2ccc(Br)c(Br)c2)C1O
BrCN1CCN(CCc2csc(Nc3ncnc4ccccc34)c2)CC1
BrCc1ccccc1Nc1ncc2ccc(Br)cc2n1
Brc1cc(-c2ccc(CN3CCCCC3)cc2)c2ncnc(Nc3c(Br)cccc3Br)c2c1
Policy gradient replay...
Mean value of predictions: 0.26289377
Proportion of valid SMILES: 0.5853125
Sample trajectories:
BP(=O)(OCC)OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Br
Brc1cc(-c2ccncc2)c2ccccc2n1
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.26510173
Proportion of valid SMILES: 0.5990625
Sample trajectories:
BP(=O)(I)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(O)CCCCCCCCCCCC(=O)NCCNP(=O)(O)OP(=O)(O)OP(=O)(O)O
B[PH](=O)(Nc1ccc(Br)cc1)=[PH](c1ccc(Br)cc1)c1cccc(Br)c1
Brc1c(OCCCCCCC[n+]2cccc(I)c2)ccc2ccccc12

  7 Training on 5990 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 21.196538
Reward: 2.110505
Trajectories with max counts:
66	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.39005458
Proportion of valid SMILES: 0.5159574468085106
Sample trajectories:
BP(=O)(NC(c1ccc(Br)cc1)c1cc(Br)cc(Br)c1O)C(=O)O
BP(=O)(NO)C(F)(F)F
BP(=O)(O)CN(CCc1ccccc1)[PH](=O)Oc1ccc(Br)nc1
BP(=O)(O)c1cccc(Nc2ncnc3c(Br)cccc23)c1
BP(=O)(OCC(=O)Nc1ccc(Br)cn1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.32857877
Proportion of valid SMILES: 0.609443402126329
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1ccc(Br)c(Br)c1
BP(=O)(OCC)OC1CCC(c2ccc3ncnc(Nc4ccc(Br)c(Br)c4)c3c2)N1C
BrCc1ccc(Nc2ncnc3ncnc(Nc4ccccc4Br)c3[nH]2)c(Br)c1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.33715454
Proportion of valid SMILES: 0.6108158799624883
Sample trajectories:
BP(=O)(OCC)OP(=O)(O)OP(=O)(O)OP(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC1C=CC(N2C=CC(=O)NC2=O)O1)OP(=O)(O)Oc1cc(NP(=O)(O)O)c(Nc2ccc(I)cc2)nn1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1cccc(-c2nc3ccccn3c2-c2ccccc2Cl)c1
Br

  8 Training on 7859 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 19.766548
Reward: 1.981567
Trajectories with max counts:
31	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.23892301
Proportion of valid SMILES: 0.6615625
Sample trajectories:
BP(=O)(Oc1ccccc1)Oc1ccccc1Cl
Bc1ccc(Nc2ccsc2)cc1
BrCCCCCCCCCCCCCCCCCCCCCCCBr
BrIc1ccccc1-c1ccccc1-c1ncccc1-c1ccccc1
Brc1cc2c(Nc3ccccc3Br)ncnc2cc1I
Policy gradient replay...
Mean value of predictions: 0.38560355
Proportion of valid SMILES: 0.5649045980606819
Sample trajectories:
BP(=O)(OCC)N1C=Nc2c(Br)cnc(Br)c2Nc2c1ccc(F)c2F
BP(=O)(c1cc(Nc2ccccc2Br)c2ccccc2n1)N(O)C=O
BrC1=CSc2ccccc2Nc2ccccc2N1
BrC=CCCBr
BrCC=C(Br)CBr
Fine tuning...
Mean value of predictions: 0.40178075
Proportion of valid SMILES: 0.5617380431384807
Sample trajectories:
BP(=O)(NC(CS(C)=O)c1ccc2c(c1)c1ccccc1O2)C(=O)O
BP(=O)(OCC)N(O)C(=O)Oc1ccc(Br)cc1
BP(=O)(OCC1OC(C[N+](C)(C)C)Oc2cc(Br)cc(Br)c21)C(=O)OCc1ccccc1
BrC#CC#Cc1ccc(Nc2ncnc3cc(Br)sc23)cc1
BrCCNc1nc(Br)c(Nc2c(Br)ccc3c(Br)c(Br)sc23)s1

  9 Training on 9500 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 19.696229
Reward: 1.896230
Trajectories with max counts:
30	Oc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.42630038
Proportion of valid SMILES: 0.5360275689223057
Sample trajectories:
BP1(=O)OCC(OC2(O)C(O)C(O)C(O)C(O)C2O)C(O)C(O)C(Br)C1=O
Bc1ccc(Nc2ncnc3cc(-c4cncnc4)sc23)nc1
BrC(Br)=NCc1nc2c(Nc3cc(Br)ccc3Br)ncnc2s1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
BrNc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.3524655
Proportion of valid SMILES: 0.6343446981545199
Sample trajectories:
BP(=O)(OCC)N1C(=O)c2cccc(Nc3ccsc3)c2C1=O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCc1ccc2ncnc(-c3ccccc3)c2c1
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(N2CCCCC2)c1
Brc1cc(Nc2ncnc3ncnc(Nc4ccccc4Br)c23)c(Br)s1
Fine tuning...
Mean value of predictions: 0.3398186
Proportion of valid SMILES: 0.6201938105658018
Sample trajectories:
BP(=O)(OCC)OCCC=CC1CCC(C2CCCCC2)O1
BrC(Br)(Br)Br
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(I)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2s1

 10 Training on 11011 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 19.557462
Reward: 1.975763
Trajectories with max counts:
31	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.40755874
Proportion of valid SMILES: 0.6120662707095967
Sample trajectories:
BP(=O)(OCC)Oc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BP(=O)(OCc1ccccc1)OCC1OC(n2cnc3c2NC=NC3=O)C(O)C1O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1cnc2nc(Nc3ccc(Br)cc3)cnc2c1Br
BrC(Br)=NNc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)s1
Policy gradient replay...
Mean value of predictions: 0.41618994
Proportion of valid SMILES: 0.5790625
Sample trajectories:
BP(=O)(OCC)OCC=C
BP(=O)(OCC=C)c1cccc(Br)c1
Bc1ccc(Nc2ncnc3sccc23)cc1
Bc1ccccc1I
BrC(Br)(Br)Br
Fine tuning...
Mean value of predictions: 0.4332786
Proportion of valid SMILES: 0.5709375
Sample trajectories:
B[PH]1(=O)(N(C)C)OC(=O)N1CCC(O)(Oc1c(F)cccc1F)P(=O)(O)O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Bc1ccc(Nc2ncnc3sccc23)cc1
BrC=CBr

 11 Training on 12734 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.594178
Reward: 2.520495
Trajectories with max counts:
84	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.44149378
Proportion of valid SMILES: 0.5278473091364205
Sample trajectories:
BP(=O)(NP(=O)(O)Oc1cccc(Nc2nc3c(Br)cc(Br)cc3nc2N=C(N)N)c1F)OCCCC
Bc1ccc(Nc2ncnc3ccsc23)cc1
Br
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(C=NNc2ccc(Br)c(Br)c2)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.45132336
Proportion of valid SMILES: 0.5439749608763693
Sample trajectories:
BP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC)C(=O)Oc1ccc(Br)cc1Br
BP(=O)(OCC)Oc1ccc(-c2c(Br)cc(Br)c(Br)c2Br)c(Br)c1Nc1ccc(Cl)c(Br)c1
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCCCCBr
Fine tuning...
Mean value of predictions: 0.44613665
Proportion of valid SMILES: 0.5588235294117647
Sample trajectories:
BP(=O)(NCc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(O)C(Nc1ccc(Br)cc1)P(=O)(O)OP(=O)(O)O
BP(=O)(O)c1ccc(Br)cc1
BP(=O)(OCC)OC(=O)C(CCCCNC(=N)N)NC(=O)CCCCCCCCCCCCCCCCCCCCCCCCCCCN
BP(=O)(OCC)OC(=O)CC(C)CCC=CCCC=C(Br)Br

 12 Training on 14480 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.488674
Reward: 2.571149
Trajectories with max counts:
128	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.44743836
Proportion of valid SMILES: 0.49437148217636023
Sample trajectories:
BP(=O)(CC(N)C(=O)NCC(=O)OP(=O)(O)OP(=O)(O)O)NO
BP(=O)(Nc1ccc(Br)cc1)OCOc1ccc(Br)cc1Br
BP(=O)(OCC)C(=O)O
BP(=O)(Oc1ccccc1Br)c1ccc(Br)cc1
Bc1ccc(Br)s1
Policy gradient replay...
Mean value of predictions: 0.477374
Proportion of valid SMILES: 0.5349639385387269
Sample trajectories:
BC=C1CCC(=O)O1
BP(=O)(CCCCCCCCCCCCCCCCCCCCCl)NO
BP(=O)(O)CCCCCCCCC#CCCCCCCCCCCCCCCCCCl
BP(=O)(OC)OCC
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCCCCC
Fine tuning...
Mean value of predictions: 0.4538507
Proportion of valid SMILES: 0.5328947368421053
Sample trajectories:
BP(=O)(OCC=C(Cl)Cl)N(Cl)Cl
BP(=O)(c1ccc(Br)cc1)c1ccc(Br)cc1
Bc1ccc(Nc2ncnc3scc(-c4cc(Br)cc(Br)c4)c23)cc1Br
BrCCCCCN1CCCCCCC1
BrCN1CCCCC1CCCN1CCCC1

 13 Training on 16268 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.753423
Reward: 2.647251
Trajectories with max counts:
78	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49421898
Proportion of valid SMILES: 0.5084427767354597
Sample trajectories:
BP(=O)(N=C=Cc1ccc2ccccc2c1)OCC
BP(=O)(Nc1ncnc2cc(Br)cc(F)c12)Oc1cc(Br)ncc1Br
BP(=O)(OCC)OCCCCCCC
BP(=O)(OCC)S(=O)(=O)Nc1ccc(Br)cc1
BP(=O)(Oc1ccccc1Nc1ccc(Br)cc1)N1CCCC1
Policy gradient replay...
Mean value of predictions: 0.472213
Proportion of valid SMILES: 0.5639662183296841
Sample trajectories:
BP(=O)(OCC)N1CCN(CC(=O)Nc2cc(Br)c(Br)cc2F)CC1
BP1(=O)OCC(OC(=O)C=Cc2ccc(Br)cc2)c2ccc(Br)cc21
B[PH](=O)(OCCCl)(c1ccc(Br)cc1)c1ccc(Br)cc1
Bc1ccc(Br)s1
Bc1ccc(Nc2ncnc3c(Br)c(Br)cc(Br)c23)cc1
Fine tuning...
Mean value of predictions: 0.4447123
Proportion of valid SMILES: 0.5712050078247262
Sample trajectories:
BP(=O)(CCSCCCNC(=O)C(CCC(=O)O)NC(=O)C(Cl)(Br)P(=O)(O)O)OCOC(=O)CN
BP(=O)(N(O)CCCl)P(=O)(O)O
BP1(=O)OCC(CCCCCP(=O)(O)O)C(O)C1O
BrC1CNc2ccccc2O1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2cn1

 14 Training on 18177 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.144277
Reward: 2.622367
Trajectories with max counts:
36	Brc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Mean value of predictions: 0.50486934
Proportion of valid SMILES: 0.5265791119449656
Sample trajectories:
BP(=O)(CC(=O)Nc1ccc(Br)c(Br)c1)NO
BP(=O)(O)CC(=NO)Nc1cc(Br)c(Br)c(Br)c1
BP(=O)(O)c1cc(Br)cc(Br)c1Br
BP(=O)(OCC)OC(=O)C(I)CCl
Bc1cc(Br)c(Br)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.482372
Proportion of valid SMILES: 0.5796875
Sample trajectories:
BP(=O)(NCCSP(=O)(O)O)P(=O)(O)O
BP(=O)(Nc1cc(Br)c(Br)cc1Br)c1ccc(Br)cc1
BP(=O)(OCC)Oc1ccc(Br)c(Br)c1
Bc1ccc(Nc2ncnc3sccc23)cc1Br
Bc1cccc(Br)c1Nc1ncnc2sccc12
Fine tuning...
Mean value of predictions: 0.48109588
Proportion of valid SMILES: 0.5708476696903347
Sample trajectories:
BP(=O)(OC)OCS
BP(=O)(OCC)C(=O)OCC
BP(=O)(OCC)OCCCCCCC=C
BP(=O)(Oc1ccc(Br)cc1)Oc1ccc(Br)cc1
B[PH](=O)Oc1cc2ncnc(c1Br)Nc1cc(Br)c(Br)c(Br)c12

 15 Training on 20209 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.625083
Reward: 2.851462
Trajectories with max counts:
107	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.46049544
Proportion of valid SMILES: 0.479375
Sample trajectories:
BP(=O)(C=O)NO
BP(=O)(OCC)OCCCC
BP(=O)(OCC)OCOc1ccccc1
BP(=O)(OCC)Oc1ccc(Br)cc1Nc1c(Br)cc(Br)c(Br)c1I
BP(=O)(Oc1ccc2ccccc2c1)Oc1ccc2cc(Br)c(Br)cc2c1
Policy gradient replay...
Mean value of predictions: 0.37715337
Proportion of valid SMILES: 0.6784375
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Nc2ncnc3[nH]cc(Br)c23)ccc1I
Brc1cc2ncnc(Nc3ccc(Nc4ccccc4)c(Br)c3)c2cc1Br
Brc1cc2ncnc(Nc3cccc4ccccc34)c2cn1
Brc1ccc(-c2cc3c(Nc4cnc5ccccc5c4)ncnc3cc2Br)cc1
Fine tuning...
Mean value of predictions: 0.3770353
Proportion of valid SMILES: 0.6640625
Sample trajectories:
BP(=O)(NO)c1ccccc1Br
BP(=O)(Nc1nc2c(Br)c(Br)c(Br)cc2s1)OCC
BrCCc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
BrNc1nc(Br)ccc1-c1ncnc2ccsc12
Brc1cc(Nc2ncnc3ccsc23)ccc1I

 16 Training on 21898 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.605175
Reward: 2.950300
Trajectories with max counts:
43	Oc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5169652
Proportion of valid SMILES: 0.5944253053554651
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Nc2ncnc3sc(C)nc23)cc1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Nc2ncnc3scnc23)sc1-c1ccccc1
Brc1cc2c(Nc3ccccc3I)ncnc2s1
Brc1cc2ncnc(Nc3ccc(C4CCCC4)cc3)c2cc1-c1cccc(Nc2ncnc3ccsc23)c1
Policy gradient replay...
Mean value of predictions: 0.42509425
Proportion of valid SMILES: 0.5803125
Sample trajectories:
BP(=O)(CCCl)NO
BP(=O)(NO)n1cc(Br)c2ncnc(N)c21
BP(=O)(OCC)ON(O)C[N+](C)(C)C
BP(=O)(Oc1ccccc1)Oc1ccc2c(N)ncnc2c1
BP(=O)(Oc1ccccc1)Oc1ccccc1
Fine tuning...
Mean value of predictions: 0.4387788
Proportion of valid SMILES: 0.5834375
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1cccc(Br)c1
BP(=O)(OCCC)Oc1ccccc1
Bc1ccc(Nc2nc3cccc(I)c3s2)cc1
BrSc1ccc(Nc2ncnc3scc(-c4ccccc4)c23)cc1
Brc1cc(-c2ccccc2Br)c2c(cnc3ccccc32)c1

 17 Training on 23829 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.536105
Reward: 3.186516
Trajectories with max counts:
90	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5434988
Proportion of valid SMILES: 0.52875
Sample trajectories:
BP(=O)(CCCCS(=O)(=O)O)NO
BP(=O)(NC(=O)C(Br)=CBr)OC(C)C
BP(=O)(O)c1ccc(Br)c(Br)c1Nc1ccc(Br)cc1
Bc1ccc(CNC(=S)Nc2ccc(Br)cc2)cc1Br
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cn1
Policy gradient replay...
Mean value of predictions: 0.5472928
Proportion of valid SMILES: 0.5670426065162907
Sample trajectories:
BP(=O)(Oc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1)Oc1cc(Br)c(Br)cc1Br
Bc1cc(I)c2c(Nc3cc(Br)cnc3Br)ncnc2n1
BrCc1cc2ncnc(Nc3ccc(Br)s3)c2cc1Nc1cccc(Br)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)c(Br)c(Br)c23)n1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)sc23)c1
Fine tuning...
Mean value of predictions: 0.5409091
Proportion of valid SMILES: 0.5662272441933459
Sample trajectories:
Bc1cc(Br)cc(Br)c1Br
BrC(=Nc1sccc1Br)c1ccc(Br)cc1
BrCC(Br)CBr
Brc1cc(Br)c(Br)c(Nc2ncnc3c(I)c(Br)cc(Nc4ccccc4)c23)c1Br
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)c(Br)nc23)c1

 18 Training on 26173 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.586809
Reward: 3.011490
Trajectories with max counts:
19	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5353933
Proportion of valid SMILES: 0.5579937304075235
Sample trajectories:
BP(=O)(CCCC(=O)Nc1cc(Br)c(Br)c(Br)c1Br)OCC
BP(=O)(CCCC)OCC
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCCCC[N+](C)(C)C)C(O)CCCCCCCCCCC
Bc1ccc(Nc2ncnc3c(Br)cncc23)cc1
Policy gradient replay...
Mean value of predictions: 0.5610376
Proportion of valid SMILES: 0.5912363067292645
Sample trajectories:
BP(=O)(CCCCCCCCCCCC(=O)O)N(O)CCCl
BP(=O)(OC)OCCCCCCCCCCCCCCCC1CCCCCC1
BP(=O)(OCC)OCCCCCCCCCCCCCCCCCCCCCCC
Bc1ccc(Br)cc1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1
Fine tuning...
Mean value of predictions: 0.56399995
Proportion of valid SMILES: 0.5784865540963102
Sample trajectories:
BP(=O)(OCC)OC(=O)c1cc2c(Br)c(Br)c(Br)c(Br)c2s1
Bc1cc(Br)ccc1Nc1ncnc2c(Br)cnc(Nc3ccc(Br)c(Br)c3Br)c12
Bc1ccc(Nc2ncnc3scnc23)cc1
BrC1CCN(Cc2ncnc3scnc23)CC1
BrCCNc1cc(Br)c(Nc2ncnc3cc(Br)cc(Br)c23)c(Br)c1Br

 19 Training on 28640 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.659580
Reward: 3.345370
Trajectories with max counts:
38	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.59402657
Proportion of valid SMILES: 0.565
Sample trajectories:
BP(=O)(CCCN)N(N)NCC(=O)Nc1ccc(Br)cc1
Bc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BrC=CC=CC=Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCC(Nc1cc(Br)c(Br)c(Br)c1)c1cc(Br)ccc1Br
BrCCCCCCCCCCCCCC=CC=CC=NNc1ncccn1
Policy gradient replay...
Mean value of predictions: 0.5279344
Proportion of valid SMILES: 0.609878086902157
Sample trajectories:
BP(=O)(C=CC(=O)OCC1CCCN1CC(=O)NCCP(=O)(O)O)OCC
BP(=O)(OCCCCCCCBr)ON(=O)=O
Bc1ccc(Br)cc1CNc1ccnc(Nc2ccc3ccncc3n2)c1
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(Br)c(Nc2cc(Nc3ncnc4cc(Br)ccc34)nc3sccc23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.5355135
Proportion of valid SMILES: 0.6121363778542384
Sample trajectories:
BP(=O)(OCCS)C(=O)NCc1ccc(Br)cc1
BrCC(Br)C(Br)Br
BrCCNc1ncnc2sc(Nc3ccccc3)nc12
BrCCNc1ncnc2sc3ccccc3c12
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1

 20 Training on 31161 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.102502
Reward: 3.456505
Trajectories with max counts:
32	Brc1ccc(Nc2ncnc3ccsc23)cc1
32	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.53033227
Proportion of valid SMILES: 0.583125
Sample trajectories:
BP(=O)(CCC(O)P(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O)NO
BP(=O)(O)C=C(Br)CBr
BP(=O)(OCC)c1cc(Nc2ncnc3c(F)c(Br)nc(F)c23)nc2c(N)ncnc12
BrCc1ccc(I)c(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(-c2ccncc2)c(Br)cn1
Policy gradient replay...
Mean value of predictions: 0.555726
Proportion of valid SMILES: 0.6114410753360425
Sample trajectories:
BP(=O)(O)CCCCC=C(Br)Br
BrCc1cccc(Nc2ncnc3ccsc23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(I)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c4sccc34)ncnc2c1
Brc1cc(Br)c2c(c1)sc1ncnc(Nc3ccccc3Br)c12
Fine tuning...
Mean value of predictions: 0.522007
Proportion of valid SMILES: 0.6292591434823382
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(I)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(Nc2ncnc3ccc(I)cc23)ccc1Nc1cccc(I)c1
Brc1cc(Nc2ncnc3ccc(Nc4ccccc4Br)cc23)ccn1

Trajectories with max counts:
200	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.50226307
Proportion of valid SMILES: 0.5305484334938403
Mean Internal Similarity: 0.4680128365214941
Std Internal Similarity: 0.09387211825148996
Mean External Similarity: 0.41253261056489443
Std External Similarity: 0.07353812305094874
Mean MolWt: 415.990804914005
Std MolWt: 101.53661957243735
Effect MolWt: -0.8097367481922252
Mean MolLogP: 5.315819329238331
Std MolLogP: 1.8789634322887223
Effect MolLogP: 0.3618073757973095
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.811448% (1162 / 1188)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 0, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5511.080323219299, 'valid_fraction': 0.5305484334938403, 'active_fraction': 0.4797265440829797, 'max_counts': 200, 'mean_internal_similarity': 0.4680128365214941, 'std_internal_similarity': 0.09387211825148996, 'mean_external_similarity': 0.41253261056489443, 'std_external_similarity': 0.07353812305094874, 'mean_MolWt': 415.990804914005, 'std_MolWt': 101.53661957243735, 'effect_MolWt': -0.8097367481922252, 'mean_MolLogP': 5.315819329238331, 'std_MolLogP': 1.8789634322887223, 'effect_MolLogP': 0.3618073757973095, 'generated_scaffolds': 1188, 'novel_scaffolds': 1162, 'novel_fraction': 0.9781144781144782, 'save_path': '../logs/replay_combo_s1-1.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.548579
Reward: 1.000000
Mean value of predictions: 0.0012553942
Proportion of valid SMILES: 0.7983088005010961
Sample trajectories:
Brc1ccccc1-c1nc2cccnc2[nH]1
Brc1ccccc1C=Nc1nc(-c2ccccc2)cs1
Brc1cnc(-c2cnc3ncnc(Nc4ccncc4)n23)nc1
C#CC(=CCC=CCCC)CCCC(=O)O
C#CCCN(Cc1ccccc1OCCO)c1cccc(COC(Cn2cncn2)c2ccc3ccccn23)c1
Policy gradient replay...
Mean value of predictions: 0.010656754
Proportion of valid SMILES: 0.7565625
Sample trajectories:
BP(=O)(NCCCCOc1ccccc1Cl)S(=O)(=O)Nc1nccs1
BrCC1Cc2cccn2C1
Brc1ccc(Nc2ccccc2)cc1
Brc1ccc2c(Nc3ccccc3)ncnc2c1
Brc1ccc2ccc(CN(Cc3ccncc3)Cc3ccccn3)cc2c1
Fine tuning...
Mean value of predictions: 0.03267974
Proportion of valid SMILES: 0.6225352112676056
Sample trajectories:
Brc1cc2c(-c3ccccc3)ncnc2cn1
Brc1ccc(Nc2ccnc(Nc3ccc(Br)cc3)n2)cc1
Brc1ccc(Nc2ccnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3sc(C=CCN4CCOC4)cc23)cc1

  2 Training on 364 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.779687
Reward: 1.016071
Trajectories with max counts:
3	COc1ccc(Nc2ncnc3ccccc23)cc1
3	Cc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.03174442
Proportion of valid SMILES: 0.6172143974960876
Sample trajectories:
BP(=O)(OCc1ccccc1)OCC1OC(N2C=CC(=N)OC2=O)C(O)C1O
Brc1cc(-c2ccccc2)c2ncnc(Nc3ccccc3)c2c1
Brc1cc2c(-c3ccccc3)c(-c3ccccc3)c2cn1
Brc1ccc(Br)cc1
Brc1ccc(Nc2ccc(-c3ccc(-c4ccccc4)c4ccccc34)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.089852214
Proportion of valid SMILES: 0.6345733041575492
Sample trajectories:
BrC1=NC2(CCCCC2)C1Cc1cccc(Br)c1
BrCCN1CCC(Cc2ccc3ccccc3c2)CC1
Brc1cc(-c2cccc3ccccc23)c2ccccc2n1
Brc1ccc(-c2nc3ccccc3o2)c2ccccc12
Brc1ccc(Nc2ccnn2Cc2ccc(Nc3ncnc4ccccc34)cc2)cc1
Fine tuning...
Mean value of predictions: 0.09249875
Proportion of valid SMILES: 0.6298498122653317
Sample trajectories:
BP(=O)(OCC)OC(=O)COc1ccccc1
Brc1c(-c2ccc(Oc3ccccc3)cc2)c2c3cccnc3c3cccc(cc-2c2ccccc12)-c1ccccc1-3
Brc1cCc2ccccc2CCc(Nc2ncnc3cccnc23)ccc1
Brc1ccc(-c2ccc3ccncc3c2)c2ccncc12
Brc1ccc(-c2ccc3oc(-c4ccc(Br)cc4)nc3c2)cc1

  3 Training on 978 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.592878
Reward: 1.258641
Trajectories with max counts:
23	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.10744403
Proportion of valid SMILES: 0.6006879299562227
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)CCl
Brc1ccc(-c2ccccc2)c2ccccc12
Brc1ccc(-c2cccnc2)cc1
Brc1ccc(Br)c(Nc2cccc(Nc3ccccc3Br)n3ncnc3n2)c1
Brc1ccc(C2CCNC2)cc1
Policy gradient replay...
Mean value of predictions: 0.030624995
Proportion of valid SMILES: 0.7
Sample trajectories:
BP(=O)(O)CN(C(=O)c1ccc(Br)cc1)c1ccccc1
Brc1ccc(Nc2ncnc3ncnc(Nc4ccccc4Br)c23)cc1
Brc1ccc2ccccc2c1-c1ccc2ncnc(Nc3ccccc3)c2c1
Brc1ccc2ccccc2c1-c1cccc2ccccc12
Brc1ccc2ccccc2c1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.18188368
Proportion of valid SMILES: 0.607567229518449
Sample trajectories:
BP(=O)(NCCCCCCCN)c1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Bc1ccccc1I
BrC(=NN1CCCCCC1)c1ccccc1
BrC(=Nc1ncnc2ccccc12)c1ccccc1
BrC1=CN(Cc2ccc(Br)cc2Br)C=Nc2cc(Br)ccc21

  4 Training on 1854 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 20.140313
Reward: 1.470333
Trajectories with max counts:
37	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.19436765
Proportion of valid SMILES: 0.6106941838649156
Sample trajectories:
Bc1ccccc1Nc1ncnc(Nc2ncnc3cc(Br)ccc23)c1C#N
BrC1=CC2=Nc3ccccc3c2c1
Brc1cc(Br)c(Nc2ccc(N3CCOCC3)c(Br)c2)c(Br)c1
Brc1cc(Nc2ncnc3ccccc23)ccc1Oc1ccccc1
Brc1ccc(-c2cccnc2)c2sccc12
Policy gradient replay...
Mean value of predictions: 0.25630602
Proportion of valid SMILES: 0.5236306729264476
Sample trajectories:
BP(=O)(N1CCC(F)C1)N(=O)=O
BP(=O)(OCC1OC(N2C=CC(=O)NN=[SH]2)C(O)C(O)C1O)c1ccc(Br)cc1
BrCN1c2ccc(Br)cc2Sc2ccc(Br)cc2C1Cc1ccc2ccccc2c1
BrCc1nc2ncnc(Nc3ccc(Br)cc3)c2o1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.24366198
Proportion of valid SMILES: 0.62125
Sample trajectories:
BP(=O)(CC(=O)O)N(O)Cc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BP(=O)(NCCOC(F)(F)CF)Nc1cnc(Nc2ccccc2F)cn1
BP(=O)(Nc1ccc(Br)cc1)N1CCOCC1
BrC1CCc2ccc3NCCN3c3cccc1c23
BrCCNc1ncc2ncnc(-c3ccccc3)c2n1

  5 Training on 3464 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 20.450738
Reward: 1.578016
Trajectories with max counts:
48	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.20362976
Proportion of valid SMILES: 0.6904761904761905
Sample trajectories:
BP(=O)(NCCCCCCC)C(=O)Nc1ccc(Cl)c([N-]C(=O)NO)c1
BP(=O)(OCCC)C(F)(F)F
BP(=O)(c1nc2c(F)cc(F)cc2s1)N1CCOCC1
Bc1cccc(Nc2ncnc3ccccc23)c1
BrCC1(Br)CCCN(c2ccc(Nc3ncnc4ccccc34)cc2)CC1
Policy gradient replay...
Mean value of predictions: 0.31394958
Proportion of valid SMILES: 0.5595611285266457
Sample trajectories:
BP(=O)(NCCCO)Nc1nc2c(Br)c(Br)[nH]c2nc1N(=O)=O
BP(=O)(Nc1nc(Nc2ccc(Br)cc2)c2ncn(C(CBr)C(=O)Br)c2n1)OCC
B[PH](=O)(CC(=O)Nc1ccc(O)c2c1c1cc(Br)c(Br)cc1N2)=Nc1cccs1
BrC1=Nc2cc(Br)ccc2Nc2ccc(Br)cc21
BrC=C=CC=CC=CCC=CCC=CCCCCCCCCCCCCBr
Fine tuning...
Mean value of predictions: 0.28403118
Proportion of valid SMILES: 0.641875
Sample trajectories:
BP(=O)(CC(=O)O)c1cccc(Br)c1
Brc1cc(Nc2ccncc2)c2ccccc2n1
Brc1cc2c(-c3ccncc3Oc3ccccc3Br)ncnc2[nH]1
Brc1cc2cc3ccccc3nc2nc1Sc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(-c2ccc(Nc3ncnc4ccsc34)cc2)cc1

  6 Training on 5227 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 25.613978
Reward: 2.115659
Trajectories with max counts:
27	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.31855565
Proportion of valid SMILES: 0.623319787433573
Sample trajectories:
BP(=O)(Oc1cc(NN=Cc2ccnc(Nc3cccnc3)c2)ccc1O)OP(=O)(O)CCNC(C)=O
Brc1ccc(-n2cnnc2Nc2cccc(-c3ccccc3Br)c2)cc1
Brc1ccc(Br)c(Nc2cc(Nc3ccnc4ccc(Br)cc34)ncn2)c1
Brc1ccc(Br)s1
Brc1ccc(N2CCOCC2)c2cccnc12
Policy gradient replay...
Mean value of predictions: 0.41156012
Proportion of valid SMILES: 0.6109375
Sample trajectories:
BP(=O)(NCCCl)SC(Cl)Br
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1
BrCCn1cnc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2cnc(NC3=NCC3)nc2s1
Fine tuning...
Mean value of predictions: 0.37206933
Proportion of valid SMILES: 0.6135084427767354
Sample trajectories:
BP(=O)(NCCCCCCCCCOc1cccc2c(OC(F)(F)F)cc(O)cc12)C(=O)O
BP(=O)(OCC)OC(=O)CCCCCCCCCCl
BP(=O)(OCCS(=O)(=O)OCC(Br)C(Br)Br)OC(=O)CBr
B[PH](=O)(=Nc1cc(Br)c(Br)c(Br)c1F)OCOc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)c(Br)c1

  7 Training on 7446 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 23.564168
Reward: 2.058709
Trajectories with max counts:
42	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.36520842
Proportion of valid SMILES: 0.6071875
Sample trajectories:
B[PH](=O)(=Nc1ccc(Br)cc1)Nc1ccc(Br)c(Br)c1
Brc1cc(Nc2cc(-c3csc(Br)n3)ncn2)cc(Nc2ccccc2)n1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Brc1cc2ncnc(Nc3ccc(Br)s3)c2cn1
Brc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.35257226
Proportion of valid SMILES: 0.6384110103221771
Sample trajectories:
BP(=O)(CCCCCC=C(Br)Br)OCC
BrCCCNc1cccc(Nc2ncnc3ccccc23)c1
BrCCNc1cccc(Nc2ncnc3cc2c2ccccc2c2c4ccccc4nc4ccccc4c32)n1
Brc1cc2cc3sc(-c4ccccc4Br)nc3cc2s1
Brc1ccc(-c2ccccc2)c2c(CCN3CCCCC3)ncnc12
Fine tuning...
Mean value of predictions: 0.38897717
Proportion of valid SMILES: 0.6297686053783614
Sample trajectories:
B[PH](=O)(=CC)NC(C(=O)c1ccc(N)cc1)N1CCOCC1
BrSc1sc2ccccc2c1-c1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(Br)c2c(c1)Nc1ccc(Br)cc12
Brc1cc2c(Nc3ccccc3-c3ccccc3Br)ncnc2s1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1

  8 Training on 9556 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 25.084673
Reward: 2.284486
Trajectories with max counts:
33	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.40392157
Proportion of valid SMILES: 0.6058143169740544
Sample trajectories:
BP(=O)(CCF)CCCCC
BrCCN1CCN(c2cccc3c(Nc4ccccc4Br)ncnc23)CC1
BrCN1CCN(CCNc2ccccc2)CC1
Brc1cc(Nc2ncnc3ccccc23)cc(Br)c1Oc1ccccc1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.4452848
Proportion of valid SMILES: 0.6536295369211514
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccncc23)c1
Brc1cc(Nc2ccncc2)cc2ccccc12
Brc1cc(Nc2ncnc3scc(-c4ccccc4)c23)cc(Br)c1OCCCN1CCCCC1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Brc1cc2oc(Br)c(Br)c2cc1Br
Fine tuning...
Mean value of predictions: 0.4483342
Proportion of valid SMILES: 0.6290272130121989
Sample trajectories:
BP(=O)(CCCCl)Nc1ccc(Nc2ccnc(Nc3c(Cl)cccc3Cl)c2)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(-c2ccncc2)cnc1Nc1cccc2ncccc12
Brc1cc(Br)c(Br)cn1
Brc1cc(Br)c2sc3c(Nc4ccccc4Br)ncnc3c2c1

  9 Training on 11797 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 27.051723
Reward: 2.510308
Trajectories with max counts:
21	CS(=O)(=O)Nc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.40413517
Proportion of valid SMILES: 0.6200750469043153
Sample trajectories:
BP(=O)(OCCS)C(=O)Nc1cccc(Br)c1
BrC1=CN2C(C=Nc3ccccc3)=Nc3ccc(Br)cc3N=CN=C2S1
BrCCNc1nc(Nc2ccc3ccccc3n2)c2sccc2n1
BrCCOc1cccc(Nc2ncnc3scnc23)c1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Policy gradient replay...
Mean value of predictions: 0.5230769
Proportion of valid SMILES: 0.6176930290715849
Sample trajectories:
BP(=O)(NCC(=O)Nc1ccc(Br)cc1)C(=O)Oc1ccc(F)cc1F
Brc1cc(Br)c2cc(-c3ccccc3Br)sc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(-c4ccc(Br)cc4Br)sc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccc(Br)c(Br)c23)c2ccccc2c1
Fine tuning...
Mean value of predictions: 0.49652433
Proportion of valid SMILES: 0.6295717411691153
Sample trajectories:
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1
BrC(=NNc1ccccc1)c1ccc2ncnc(Nc3ccc(Br)cc3Br)c2c1
BrCCCCCNCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrCCNCCCNc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2cc1Br
BrNc1ccc(Br)cc1

 10 Training on 13872 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.758358
Reward: 2.506320
Trajectories with max counts:
37	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.50609356
Proportion of valid SMILES: 0.574375
Sample trajectories:
BP(=O)(Nc1ccccc1)N(c1ccccc1)c1ccccc1
BP(=O)(OCC=C)C(=O)Nc1cccc(F)c1F
Bc1ccccc1P(=O)(Nc1ccccc1)c1ccc(Br)cc1
BrC=CC=CBr
BrCc1ccccc1Nc1ncnc2c3ccc(Br)c2c(ncn3)s1
Policy gradient replay...
Mean value of predictions: 0.5307022
Proportion of valid SMILES: 0.6114070824193043
Sample trajectories:
BP(=O)(OCC)c1cc(Nc2cc(Cl)cc(Cl)c2)cc(Br)c1O
BrCCCCCCCCNc1ccnc(Nc2ccccc2)c1
Brc1cc(Br)c(Nc2ncnc(Nc3cccc4cc(Br)ccc34)n2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(I)cc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.4693698
Proportion of valid SMILES: 0.6398874648327603
Sample trajectories:
BP(=O)(NCCCCCC(=O)O)C(=O)OCF
BP(=O)(OCC(=O)NO)C(=O)NCCCN
BP(=O)(OCC1OC(N2C=C(Br)C(=O)NC2=O)C(O)C1O)Oc1cccc(Br)c1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1cccc(Nc2ncnc3cc(Br)ccc23)c1

 11 Training on 16077 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.402956
Reward: 2.646939
Trajectories with max counts:
89	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.49345893
Proportion of valid SMILES: 0.5977478886456052
Sample trajectories:
BP(=O)(COc1cc(Br)c(Br)cc1Br)Nc1ccc(Br)cc1
BP(=O)(NCCCCl)N(=O)=O
Bc1ccc(NC(=O)c2nnc(Nc3ccc(Br)cc3F)s2)cc1
BrC1=CN=C2Nc3ccc(Br)cc3Nc3ccc(Br)cc3CN12
BrCCCCBr
Policy gradient replay...
Mean value of predictions: 0.5372116
Proportion of valid SMILES: 0.623709727869878
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Nc2ncnc3ccccc23)cc1
BP(=O)(Oc1ccc(Br)cc1)Oc1ccc(Br)cc1
B[PH](C(=O)O)=[PH](=O)(O)CCCN
Brc1cc(Br)c2c(Nc3cccc(C=Cc4ccc(Nc5ncnc6ccccc56)cc4)c3)ncnc2c1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Fine tuning...
Mean value of predictions: 0.54636645
Proportion of valid SMILES: 0.6111979981232405
Sample trajectories:
BP(=O)(CCCCC(NC(=O)Nc1ccc(F)c(F)c1)c1ccc(Nc2ncnc3c(N)cccc23)cc1)C(F)(F)F
BP(=O)(NCCCCCCCCCCCCCl)NS(=O)(=O)c1ccc2ncnc(N3CCCCC3)c2c1
BrCC(=NNc1ccc(Br)c(Br)c1)c1cccnc1
BrCCCCCC=C(Br)Br
BrCCN1sc2ncnc(Nc3cccc(Br)c3)c21

 12 Training on 18492 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.584022
Reward: 2.964296
Trajectories with max counts:
72	CS(=O)(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49935693
Proportion of valid SMILES: 0.583125
Sample trajectories:
BrCCCBr
BrCN1C=Cc2c(Nc3ccccc3)ncnc21
BrCN1CC=C(Nc2ccccc2Nc2cccc(Nc3ncnc4ccccc34)c2)CC1
Brc1cc(Br)nc(Nc2ncnc3cc(Nc4ccccc4Br)sc23)c1
Brc1cc2ncnc(Nc3ccc(-c4ccccc4Br)cc3)c2s1
Policy gradient replay...
Mean value of predictions: 0.51758796
Proportion of valid SMILES: 0.621875
Sample trajectories:
BrC=CC=CBr
BrSc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(Br)c2nc(Nc3ccccc3Br)sc2c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.5504715
Proportion of valid SMILES: 0.6298843388558925
Sample trajectories:
B[PH](=O)(Nc1cccc(Nc2ncnc3cc(Br)ccc23)c1)(P(=O)(O)O)[PH](O)(F)P(=O)(O)O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2cn1
Brc1cc2c(Nc3ccccc3)ncnc2s1

 13 Training on 20837 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.289870
Reward: 3.236532
Trajectories with max counts:
134	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5681449
Proportion of valid SMILES: 0.5003125
Sample trajectories:
BP(=O)(Nc1ccccc1)P(=O)(O)Oc1ccc(Br)cc1
BP(=O)(OCC)c1ccc(Nc2ncnc3sc(Br)cc23)cc1
BP(=O)(OCCBr)n1cnc2c(Br)c3c(Br)cc(Br)ccc3c21
Bc1ccc(Br)cc1Nc1cc2ncnc(Nc3ccc(Br)s3)c2cc1Br
Bc1ccc(Br)cc1Nc1ccc(Br)cc1Nc1cc(Br)cc(Br)c1
Policy gradient replay...
Mean value of predictions: 0.56681436
Proportion of valid SMILES: 0.6356807511737089
Sample trajectories:
BP(=O)(SCCS)C(=O)O
BrCC1(c2ccccc2Nc2ncnc3c(Nc4ccncc4)cc(Br)nc23)CCC1
BrCCCCCCCNc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrCCCCOCCc1cc2c(Nc3ccc(Br)c4ccsc34)ncnc2s1
Brc1cc(Br)nc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.5860442
Proportion of valid SMILES: 0.6226945920600188
Sample trajectories:
BP(=O)(Nc1cccc(Nc2ncnc3c(Br)ccc(Br)c23)c1)c1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cn1
Bc1ccc(Nc2ncnc3cccc(Br)c23)cc1
Brc1cc(Br)c(Nc2ncnc3c(Br)cccc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(N4CCCCCC4)c3)ncnc2c1

 14 Training on 23447 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.724532
Reward: 3.271108
Trajectories with max counts:
38	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
38	CS(=O)(=O)Nc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.60480094
Proportion of valid SMILES: 0.573170731707317
Sample trajectories:
BP(=O)(O)C(=O)Nc1cc(Br)c(Br)c2c1C(=O)P2(=O)O
Bc1cc2cncnc2c(Nc2cccc(Br)c2)c1-c1ccc(Br)cc1Br
Bc1ccc(Nc2ncnc3scnc23)cc1Br
BrCCCCBr
Brc1c(-c2ccc(N=CN3CCOCC3)c3ncsc23)cnc2ccccc12
Policy gradient replay...
Mean value of predictions: 0.5994673
Proportion of valid SMILES: 0.5871129183609634
Sample trajectories:
BP(=O)(COC(=O)CN(O)C=O)OCCCCO
BrCc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.5577114
Proportion of valid SMILES: 0.6283213504220069
Sample trajectories:
BP(=O)(N=C(C)Nc1ccc(Br)cc1)OCC
BP(=O)(O)Cc1csc(NN=Cc2ccccc2Br)c1
BrC=C=CCOc1ccc(Nc2ncnc3cccnc23)cc1
BrC=CBr
BrC=CC=CBr

 15 Training on 26177 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.729852
Reward: 3.543273
Trajectories with max counts:
12	CS(=O)(=O)c1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5220859
Proportion of valid SMILES: 0.784192439862543
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCNc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc(Nc2ccc(Br)c(Br)c2)c2cc(Br)sc2n1
Brc1cc2c(Nc3ccccc3)ccnc2nc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Brc1cc2c(Nc3ccccc3Br)cccc2s1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Policy gradient replay...
Mean value of predictions: 0.5949612
Proportion of valid SMILES: 0.6523388116308471
Sample trajectories:
BrSC1(c2cc(Br)cc(Nc3ncnc4scc(Br)c34)c2I)Nc2ccccc2S1
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4ccc5ccsc5c34)cc2)cc1Br
Brc1cc(Br)c2c(Nc3ccc(I)cc3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2n1
Fine tuning...
Mean value of predictions: 0.59330744
Proportion of valid SMILES: 0.6457876605073598
Sample trajectories:
BrC(=Nc1ccc(Br)s1)c1ccnc(Nc2ncnc3ccc(Br)cc23)n1
BrC=CC=CCBr
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ccccc2Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccc(Br)nc23)ccn1

 16 Training on 29156 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.831076
Reward: 3.886192
Trajectories with max counts:
47	CS(=O)(=O)Nc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.6084656
Proportion of valid SMILES: 0.5913642052565707
Sample trajectories:
BrCN1CCCN(Cc2c(Br)cccc2-c2ccc(Br)cc2)CC1
Brc1cc(Br)c(CNc2ccc(Br)o2)c(Br)c1
Brc1cc(Br)c2cc(Br)nc(Nc3ccc(I)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.60513633
Proportion of valid SMILES: 0.5964363863707408
Sample trajectories:
BP(=O)(CC)Nc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1O
Bc1cccc(Nc2ncnc3cc(Br)ccc23)c1
BrCCCN=C1Nc2c(Br)cc(Nc3ncnc4ccccc34)cc2O1
BrCCSc1ccccc1-c1ccccc1Nc1ccc(CNc2ncnc3cccc(Br)c23)cc1
Fine tuning...
Mean value of predictions: 0.6041016
Proportion of valid SMILES: 0.64
Sample trajectories:
BP(=O)(O)CNC(=O)C(CC(=O)NCP(=O)(O)O)C(=O)O
Bc1cccc(Nc2ncnc3ccc(Br)cc23)c1
BrCc1ccc2ncnc(Nc3cccc(Nc4ccccc4Br)c3)c2c1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2sccc12
Brc1cc(Br)cc(Nc2ncnc3ccc(Nc4ccccc4Br)nc23)c1

 17 Training on 32044 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.975317
Reward: 4.115313
Trajectories with max counts:
163	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6455927
Proportion of valid SMILES: 0.514866979655712
Sample trajectories:
BP(=O)(CCNC(=O)Nc1ccc(Br)cn1)OCC
BP(=O)(NCCCCCCN)c1ccc(Br)c(Nc2ncnc3sc(Br)cc23)c1
B[PH](=O)(Nc1ccc(Nc2ncnc3sc(Cl)cc23)cc1)(C(=O)O)c1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
BrC=CBr
BrC=CC=CCC=CC=CCC=CC=CC=CCCCC=CCC=CCC=CC=CCC=C(Br)Br
Policy gradient replay...
Mean value of predictions: 0.586649
Proportion of valid SMILES: 0.7174119885823026
Sample trajectories:
BrCCCCCCCCCCCCCCCCCNCCSc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Nc4ccccc4Br)sc23)c1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(CNc2ncnc3c(Nc4ccccc4Br)ncnc23)cc1
Fine tuning...
Mean value of predictions: 0.6140009
Proportion of valid SMILES: 0.6765997490589711
Sample trajectories:
BrC(=Nc1ccc(Br)cc1Nc1ncnc2cc(Br)sc12)C1CC1
BrCCCCCCCCNc1cc(Nc2ncnc3c(Br)cccc23)ccc1Br
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Nc2ccc(Br)c(Br)c2)c2c(Br)sc(Br)c2n1
Brc1cc2c(s1)-c1ccccc1N2

 18 Training on 35150 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.338539
Reward: 3.866268
Trajectories with max counts:
35	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.60259616
Proportion of valid SMILES: 0.6742732103782432
Sample trajectories:
Bc1cccc(Nc2ncnc3cc(Br)c(Br)cc23)c1
BrC=CC=Nc1cccc(Nc2ncnc3ccccc23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(Nc2ncnc3c(Br)csc23)ccc1I
Policy gradient replay...
Mean value of predictions: 0.59807694
Proportion of valid SMILES: 0.7204030226700252
Sample trajectories:
B[PH](=O)(Nc1cccc(Nc2nccc(Br)n2)c1)(c1ccccc1)c1ccccc1
BrNc1ccc2c(Br)cc3ncnc(Nc4ccccc4Br)c3c2c1
Brc1cc(Br)c2c(Nc3ccc(-c4ccccc4Br)cc3)ncnc2c1
Brc1cc(Br)c2cc(Br)ccc2c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.6365184
Proportion of valid SMILES: 0.6608315098468271
Sample trajectories:
B[PH](=O)(Br)(NCCCCl)NC(=O)C(F)(F)F
BrCCCNCCCCCNc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BrNc1nc(Nc2ccccc2)c2sccc2n1
Brc1cc(Br)cc(Nc2ncnc3cnccc23)c1
Brc1ccc(-c2c(Br)cccc2Nc2ncns2)s1

 19 Training on 38457 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.992610
Reward: 4.022121
Trajectories with max counts:
24	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.66831124
Proportion of valid SMILES: 0.6591619762351469
Sample trajectories:
Bc1ccc(Nc2ncnc3c(Br)cnc(Nc4ccc(Br)cc4)c23)cc1
BrCCNc1ncc2ncnc(Nc3ccc(Br)cc3)c2n1
BrCc1ccc2c(Nc3ccc(Br)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(I)c3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.5185544
Proportion of valid SMILES: 0.7189480354879595
Sample trajectories:
Bc1cc(Nc2ncccc2Br)ccc1Br
Bc1ccc(Nc2ncnc3ncncc23)cc1
BrCCCNc1ccccc1Nc1ccccc1Nc1ccccc1
Brc1cc2c(nc1Br)-c1ccccc1SS2
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1Br
Fine tuning...
Mean value of predictions: 0.6265105
Proportion of valid SMILES: 0.6671875
Sample trajectories:
BrCCCCCCCCCCCCCCCCCCCCCCCCCCCCCNc1ccc(Nc2ncnc3cccc(Br)c23)nc1
BrCc1ccccc1-c1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(Br)c2c(Nc3cc(-c4ccccc4Br)c4ccsc4n3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc(Nc2cc3ncsc3cn2)nc(Nc2ccnc3ccc(Br)cc23)c1

 20 Training on 41672 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.160274
Reward: 4.377375
Trajectories with max counts:
37	CS(=O)(=O)c1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.63076925
Proportion of valid SMILES: 0.6302783859868627
Sample trajectories:
BP(=O)(NO)c1cccc(Br)c1Br
BrC1(CCCCCCCNc2ccccc2)Nc2ccccc21
BrC=CC=CCCC=CC=NNc1ccccc1-c1ccccc1
Brc1cc(Br)c2c(c1)Nc1ncnc2ccc(Br)c2ccccc12
Brc1ccc(-c2ccc3ncnc(Nc4ccc(CNc5ccccc5)cc4)c3c2)cc1
Policy gradient replay...
Mean value of predictions: 0.6465324
Proportion of valid SMILES: 0.7168056446440025
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1ccc(NCc2ccc3ncnc(Nc4cccc(Nc5ccc(Br)cc5)c4)c3c2)cc1
Brc1ccc(Nc2cc(Br)cc3ncnc(Nc4ccc(Br)cc4)c23)cc1
Brc1ccc(Nc2cc(Br)cnc2Oc2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.6565666
Proportion of valid SMILES: 0.6679197994987469
Sample trajectories:
Brc1ccc(Nc2cc3nc(NC(=Cc4ccccc4)c4ccccc4)cnc(Nc4ccccc4)c3ncn2)cc1
Brc1ccc(Nc2cc3ncncc3n3c(Nc4ccccc4Br)ncnc3c2)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4ccc(Br)cc4)c3c2)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4cccc(Br)c4)c3c2)cc1
Brc1ccc(Nc2ccnc(Nc3ncnc4ccccc34)c2)cn1

Trajectories with max counts:
110	CS(=O)(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6172388
Proportion of valid SMILES: 0.5943994474791235
Mean Internal Similarity: 0.4809660813308171
Std Internal Similarity: 0.09726498966753833
Mean External Similarity: 0.42237297734825024
Std External Similarity: 0.07232540480134232
Mean MolWt: 460.46706273458454
Std MolWt: 150.17551264592973
Effect MolWt: -0.312132601866318
Mean MolLogP: 5.993018191242181
Std MolLogP: 3.6960741655444815
Effect MolLogP: 0.4241446410991864
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.569757% (1084 / 1111)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 20, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5730.486327648163, 'valid_fraction': 0.5943994474791235, 'active_fraction': 0.5910003168902503, 'max_counts': 110, 'mean_internal_similarity': 0.4809660813308171, 'std_internal_similarity': 0.09726498966753833, 'mean_external_similarity': 0.42237297734825024, 'std_external_similarity': 0.07232540480134232, 'mean_MolWt': 460.46706273458454, 'std_MolWt': 150.17551264592973, 'effect_MolWt': -0.312132601866318, 'mean_MolLogP': 5.993018191242181, 'std_MolLogP': 3.6960741655444815, 'effect_MolLogP': 0.4241446410991864, 'generated_scaffolds': 1111, 'novel_scaffolds': 1084, 'novel_fraction': 0.9756975697569757, 'save_path': '../logs/replay_combo_s1-2.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.548579
Reward: 1.000000
Mean value of predictions: 0.0012553942
Proportion of valid SMILES: 0.7983088005010961
Sample trajectories:
Brc1ccccc1-c1nc2cccnc2[nH]1
Brc1ccccc1C=Nc1nc(-c2ccccc2)cs1
Brc1cnc(-c2cnc3ncnc(Nc4ccncc4)n23)nc1
C#CC(=CCC=CCCC)CCCC(=O)O
C#CCCN(Cc1ccccc1OCCO)c1cccc(COC(Cn2cncn2)c2ccc3ccccn23)c1
Policy gradient replay...
Mean value of predictions: 0.010656754
Proportion of valid SMILES: 0.7565625
Sample trajectories:
BP(=O)(NCCCCOc1ccccc1Cl)S(=O)(=O)Nc1nccs1
BrCC1Cc2cccn2C1
Brc1ccc(Nc2ccccc2)cc1
Brc1ccc2c(Nc3ccccc3)ncnc2c1
Brc1ccc2ccc(CN(Cc3ccncc3)Cc3ccccn3)cc2c1
Fine tuning...
Mean value of predictions: 0.050578184
Proportion of valid SMILES: 0.6233155750548417
Sample trajectories:
BrC=CC=C1C(=Cc2ccccn2)c2ccccc2Nc2ccccc2N1c1ccccc1
Brc1ccc(-c2ncnc(Nc3ccccc3-c3ccccc3)n2)cc1
Brc1ccc(Nc2cc(-c3ccco3)ncn2)nc1
Brc1ccc(Nc2nc(-c3ccc(CN4CCOCC4)cc3)cs2)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1

  2 Training on 419 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.896770
Reward: 1.133929
Trajectories with max counts:
11	Cc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.044785846
Proportion of valid SMILES: 0.6716697936210131
Sample trajectories:
BP(=O)(OCC)OC(=O)CC(c1ccc(Br)cc1)N(CC(=O)Nc1cccc2nc(Nc3ccccc3)ccc12)c1ccccc1
BrCC1=Nc2ccccc2Oc2ccccc2N=C(Nc2ccccc2Br)c2ccccc21
Brc1cc2c(nc3ccccc3N2)c1-c1ccccc1
Brc1cc2ncnc(Nc3ccccc3OCc3ccccc3)c2cc1-c1ccccc1
Brc1ccc(-c2nc3ccccc3n2Cc2ccccc2)c(Nc2cccnc2)c1
Policy gradient replay...
Mean value of predictions: 0.0902834
Proportion of valid SMILES: 0.6182728410513142
Sample trajectories:
BP(=O)(OCC)n1c(S(=O)(=O)c2ccccc2)cc(Br)c1Br
BP(=O)(Oc1ccc(NS(=O)(=O)c2ccc(Br)cc2)cc1)OC(C)C
BrCI
BrCc1ccccc1Nc1ccc2ncnc(Nc3ccccc3Br)c2n1
Brc1cc2ncnc(Nc3ccccc3)c2cc1NC1=CC=CC=CC1
Fine tuning...
Mean value of predictions: 0.10901922
Proportion of valid SMILES: 0.6344590368980613
Sample trajectories:
BC=NP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP1(=O)OC(OC2C(O)OC(n3cnc4c(N)ncnc43)C(O)C2O)C(O)C1O
Bc1ccccc1N(=O)=O
Brc1cc(I)cc(I)c1Nc1ncnc2ccccc12
Brc1cc2ncnc(Nc3ccccn3)c2cc1OCc1ccccc1

  3 Training on 1141 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 20.022763
Reward: 1.594782
Trajectories with max counts:
58	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.08778151
Proportion of valid SMILES: 0.6521875
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)C(=O)CCl
BP(=O)(c1cccc2ccccc12)N(O)C=O
Bc1cccc(Nc2ncnc3ncc(Br)cc23)c1
BrCC1=CN(c2ccccc2)S1
Brc1ccc(N2CCN(CCNc3ncnc4cc(Br)ccc34)CC2)cc1
Policy gradient replay...
Mean value of predictions: 0.07185952
Proportion of valid SMILES: 0.6942794623319788
Sample trajectories:
BrCCCCCCCCCC=CCC=CC=CC=CNc1ccc2ncnc(-c3ccccc3)c2c1
BrCCN=C(Nc1ccc(Nc2c(Br)cc(Br)cc2Nc2ncnc3ccccc23)cc1)Nc1ccccc1Br
BrCCNc1ccc2cccnc2c1
Brc1ccc(Br)c(Nc2ccc3ncnc(Nc4ccccc4)c3c2)c1
Brc1ccc(CNc2ccccc2Nc2ccccc2)cc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.17814207
Proportion of valid SMILES: 0.6298498122653317
Sample trajectories:
Bc1ccccc1-c1cc(NC2CCN(C(=O)c3ccc(I)cc3)C2)ccc1Br
BrCCCN=C(Nc1cncnc1)Oc1ccccc1
Brc1cc(C=NNc2ccccc2)c[nH]1
Brc1ccc(-c2cc(Nc3cncnc3)ncn2)c2ncnc(Nc3ccccc3)c12
Brc1ccc(-c2ccccc2)c2ccccc12

  4 Training on 2139 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 19.741156
Reward: 1.496341
Trajectories with max counts:
43	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.18747434
Proportion of valid SMILES: 0.6097026604068858
Sample trajectories:
BP(=O)(c1ccc(F)cc1)c1ccc(F)c(F)c1
Bc1ccccc1Nc1ncnc2ncnc(Nc3ccccc3)c12
BrC1=CC2=Nc3ccccc3c2c1
Brc1cc(Br)c2c(c1)Sc1ccccc1N2
Brc1cc2ncnc(Nc3cccc4ccccc34)c12
Policy gradient replay...
Mean value of predictions: 0.2848268
Proportion of valid SMILES: 0.5237797246558198
Sample trajectories:
Br
Brc1cc(Nc2ccnc3c(-c4ccc5cnn(-c6ccccc6)c5c4)c(Br)ccc23)ccc1I
Brc1ccc(-c2cc3c(Nc4ccc(Br)c(Br)c4)ncnc3cc2Br)cc1
Brc1ccc(Br)c(-c2c3cccnc3cc(Br)c3ccccn23)c1
Brc1ccc(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.26771343
Proportion of valid SMILES: 0.6372732958098811
Sample trajectories:
BP(=O)(CC(F)(F)F)c1cnc(Nc2cc(Br)cnc2Br)nc1
BP(=O)(NCCO)c1cccc(Br)c1
BrCCNc1ncnc2cc(Br)ccc12
Brc1cc(-c2ccnc(Nc3cccnc3)c2)cs1
Brc1cc(Br)c(Nc2ncnc3ccccc23)c(Br)c1

  5 Training on 3881 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 21.689917
Reward: 1.743072
Trajectories with max counts:
50	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.25927362
Proportion of valid SMILES: 0.6455142231947484
Sample trajectories:
BP(=O)(C(F)(F)Cl)C(F)(F)P(F)(F)(F)F
BP(=O)(NO)c1cccc(Br)c1
BrC(=CNc1ccc(Br)cn1)c1ccccc1
BrC=Nc1nc(-c2ccccc2)c2ncn(-c3ccccc3)c2n1
Brc1cc(Nc2ncnc3ccccc23)cc2ccccc12
Policy gradient replay...
Mean value of predictions: 0.30467194
Proportion of valid SMILES: 0.5954985933104096
Sample trajectories:
BrCCN1CCCN(Cc2nc(CN3CCCCC3)ncc2Br)CC1
Brc1cc(Br)c(Nc2ncnc3cc(Br)ccc23)c(-c2ccccc2)c1
Brc1cc(Nc2ncnc3nc(Nc4ccccc4)cnc23)ncn1
Brc1cc2ncnc(Nc3ccccc3)c2cc1OCc1ccccc1
Brc1cc2ncnc(Nc3ccccc3)c2nc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Fine tuning...
Mean value of predictions: 0.28898853
Proportion of valid SMILES: 0.6273835573616755
Sample trajectories:
BP(=O)(CC(=O)Nc1ccccc1Nc1ccc(I)c(O)c1)NO
BP(=O)(OC(=O)Cc1ccc(Br)c(Br)c1)OC(C)C
BP(=O)(OCC1NN(C(F)(F)[PH](F)(F)F)C(=O)O1)Oc1ccc(F)c(F)c1
BP(=O)(OP(O)(F)(F)OP(=O)(O)O)C(=O)c1ccccc1
BrCC1CC=C(Nc2nccs2)C1

  6 Training on 5809 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 24.112237
Reward: 2.091932
Trajectories with max counts:
22	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.3196271
Proportion of valid SMILES: 0.6038148843026891
Sample trajectories:
BrCC1=Nc2sc(Br)c(Nc3ccc(Br)cc3)c2N=C1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Brc1cc2nc(CNc3ncnc4ncncc34)c(Nc3ncnc4ncnc(Nc5ccccc5)c34)nc2cc1Br
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc2ncnc(Nc3ccccc3Sc3cccnc3Br)c2cn1
Policy gradient replay...
Mean value of predictions: 0.33107692
Proportion of valid SMILES: 0.609375
Sample trajectories:
BrC1=CN(Cc2ncccn2)c2ncnc(Nc3ccccc3)c2N1
Brc1cc(Br)c2c(c1)N(CC1CC1)CCN1CCCN1CCN2
Brc1cc2ncnc(Nc3cccc4ccccc34)c2s1
Brc1ccc(-c2ccccc2Br)o1
Brc1ccc(-c2ncnc3sccc23)cc1
Fine tuning...
Mean value of predictions: 0.36171913
Proportion of valid SMILES: 0.6255079712410128
Sample trajectories:
BP(=O)(OCC)C(=O)O
Bc1ccc(Nc2ccc(Br)c(Br)c2)cc1Br
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Brc1cc2ncnc(Nc3ccccc3)c2cc1-c1ccccc1

  7 Training on 7868 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 24.217980
Reward: 2.363877
Trajectories with max counts:
27	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.40223527
Proportion of valid SMILES: 0.5875547217010632
Sample trajectories:
BP(=O)(OCC)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(B)(=O)Oc1ccc(Br)cc1Nc1ccc(Br)cc1
BrOCCCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Br)c(Nc2ccc3nnc(-c4cccs4)n3c2)c(Br)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1ccc(-c2ccc3ncnc(-c4ccnc(Br)c4)c3c2)cn1
Policy gradient replay...
Mean value of predictions: 0.35173124
Proportion of valid SMILES: 0.6046875
Sample trajectories:
BP(=O)(OCC)OC(=O)CN(C)C(=O)c1ccc2ccc(Br)c(Br)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1cc2c(Nc3ccccc3)ncnc2cc1-c1ccccc1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc2ncnc(Nc3cccc(Br)c3Br)c2s1
Fine tuning...
Mean value of predictions: 0.40566933
Proportion of valid SMILES: 0.5960575719649562
Sample trajectories:
BP(F)(F)(OP(=O)(O)OP(=O)(O)OP(=O)(O)O)P(=O)(O)N(O)C(F)(F)F
Bc1ccc(-c2csc(Nc3nccs3)c2C#N)cc1F
BrC=CCCCBr
Brc1cc(Br)cc(Nc2cncc(Br)n2)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

  8 Training on 10037 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 22.212929
Reward: 2.069934
Trajectories with max counts:
18	Clc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.3931528
Proportion of valid SMILES: 0.6119449656035022
Sample trajectories:
Bc1ccccc1Nc1ncnc2ccc(Br)c(Nc3ncnc4ccccc34)c12
BrC1CC(Br)(Br)C2CCC1CN2
Brc1cc2ncnc(-c3ccccc3)c2c2oc(Cc3ccccc3)cc12
Brc1cc2ncnc(N3CC4CCN(C4)C3)c2s1
Brc1ccc(-c2ccc(Br)c(-c3ccc4ncncc4c3)c2)[nH]1
Policy gradient replay...
Mean value of predictions: 0.37902248
Proportion of valid SMILES: 0.5690625
Sample trajectories:
BP(=O)(Nc1ccc(Br)s1)P(=S)(c1ccccc1)c1ccc(Br)cc1
Bc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Bc1ccccc1Nc1ncnc2sccc12
BrCc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccccc23)c1Br
Fine tuning...
Mean value of predictions: 0.35422936
Proportion of valid SMILES: 0.6023757424195061
Sample trajectories:
BP(=O)(Cc1ccc(Br)cc1)NO
BrCCOc1ccccc1Nc1ccc2ncnc(Nc3cccs3)c2c1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc2ncnc(Nc3ccccc3Br)c2cc1Nc1ccccc1
Brc1cc2ncnc(Nc3ccsc3)c2nc1Nc1cccs1

  9 Training on 11839 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 21.381597
Reward: 2.043989
Trajectories with max counts:
94	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.32897407
Proportion of valid SMILES: 0.554375
Sample trajectories:
BP(=O)(O)c1ccc(Br)cc1Nc1cccc(Br)c1
BP(=O)(OC(F)(F)F)c1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
BP(=O)(OCC)OC(=O)C=CCCCCCCCN
Bc1ccccc1-c1ccccc1Nc1ncnc2ccccc12
BrC=CBr
Policy gradient replay...
Mean value of predictions: 0.16372049
Proportion of valid SMILES: 0.6753125
Sample trajectories:
BP(=O)(O)C(=O)CCCCCC(=O)Oc1ccccc1
Bc1cccc(Nc2ncnc3ccsc23)c1
Bc1ccccc1-c1ccccc1Nc1ncccc1Cl
Bc1ccccc1Nc1ncnc2cc(Br)ccc12
Brc1cc(Nc2ncccc2-c2ccccc2Br)ccc1Nc1cccnc1Nc1ccccc1Br
Fine tuning...
Mean value of predictions: 0.42716795
Proportion of valid SMILES: 0.6234375
Sample trajectories:
BrCCCBr
Brc1cc(OCc2cccnc2Nc2ccccc2)cc2ccccc12
Brc1ccc(-c2cc(Nc3ccccn3)ncn2)cc1
Brc1ccc(-c2ccccc2CCc2ccccc2-c2ccccc2-c2ncnc3ccccc23)cc1
Brc1ccc(-c2ncnc3ccccc23)c(Br)c1

 10 Training on 12911 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.353809
Reward: 2.270429
Trajectories with max counts:
44	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.40610686
Proportion of valid SMILES: 0.6142544545170365
Sample trajectories:
BP(=O)(CO)c1ccccc1P(=O)(c1ccccc1)c1ccccc1
BrBr
Brc1cc(Nc2cnc(I)c(Nc3ccccc3I)n2)c2ccccc2n1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Brc1cc2cccc3c(Nc4ccccc4Br)ncnc(c1)c23
Policy gradient replay...
Mean value of predictions: 0.42270744
Proportion of valid SMILES: 0.6446668751954958
Sample trajectories:
Br
BrC1=C2Nc3ccc(Br)cc3N=C12
BrCCc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
BrCN1CCN(c2cccc(Br)c2)CC1
BrSc1ncccc1-c1cccc2ccc(-c3ccccc3)cc12
Fine tuning...
Mean value of predictions: 0.45796677
Proportion of valid SMILES: 0.639375
Sample trajectories:
Bc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(-c2ccccc2Br)c2scnc2n1
Brc1cc(Br)cc(Nc2ccc(Br)c(Nc3ccccc3)c2)c1
Brc1cc(Nc2ncnc3ccccc23)cc2ccccc12
Brc1cc2c(Nc3ccccc3Br)ncnc2s1

 11 Training on 14613 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.252554
Reward: 2.364385
Trajectories with max counts:
25	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.44709933
Proportion of valid SMILES: 0.635625
Sample trajectories:
Bc1ccccc1Nc1ncnc2ccc(Br)cc12
BrCCNc1cc2ncnc(Nc3ccccc3Br)nc2c1-c1cccc2ccccc12
Brc1cc(Br)c2c(c1Br)Nc1ccccc1-2
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Policy gradient replay...
Mean value of predictions: 0.51051545
Proportion of valid SMILES: 0.6068188927119175
Sample trajectories:
Brc1cc(Br)c(Nc2cc(Nc3ncnc4cc(Br)c(Br)cc34)c(Br)cc2Br)c(Br)c1
Brc1cc(Br)c2c(Nc3cc(Br)ncn3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3cccc(Br)c3Br)ncnc2c1
Fine tuning...
Mean value of predictions: 0.42717707
Proportion of valid SMILES: 0.6535334584115072
Sample trajectories:
BP(=O)(O)c1ccc(N2CCOCC2)c(F)c1
BP(=O)(OCC)OC(=O)C(Br)Br
Bc1ccccc1Nc1ncnc(Nc2ccccc2)c1-c1ccc(Br)cc1
BrCCNc1ncnc2ncnc(Nc3ccccc3)c12
BrSc1scc(Br)c1Br

 12 Training on 16489 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.896753
Reward: 2.560259
Trajectories with max counts:
21	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.49182394
Proportion of valid SMILES: 0.6463414634146342
Sample trajectories:
BP(=O)(OCC)OC(=O)C(CC(Cl)(Cl)P(=O)(O)OP(=O)(O)O)P(=O)(O)OCC1OC(Oc2c(Br)cc(Br)c(Br)c2C(=O)O)C(O)C1O
BP(=O)(OCC)Oc1ccc2ncnc(Nc3ccc(Br)cc3Br)c2c1
BrBr
BrCCCN=Cc1ccc(Nc2ncnc3ccccc23)cc1
BrCCNc1ncnc2c(Br)nc(Nc3ccccc3Br)nc12
Policy gradient replay...
Mean value of predictions: 0.50635386
Proportion of valid SMILES: 0.6397748592870544
Sample trajectories:
Bc1c(Br)ccc2c(Nc3ccc(Br)cc3)ncnc12
Bc1ccc(Nc2ncnc3c(Br)cc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)cc1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.48175392
Proportion of valid SMILES: 0.6628125
Sample trajectories:
BrCc1ncnc2c(Nc3ccc(Br)cc3)ncnc12
Brc1cc(-c2ccccc2)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc2c(cc1-c1ccccc1)n1c3ccccc3N(C2)c2ccccc2-1

 13 Training on 18664 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.484190
Reward: 2.795923
Trajectories with max counts:
20	Brc1ccc(Nc2ncnc3ccsc23)cc1
20	COc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4707663
Proportion of valid SMILES: 0.6610381488430269
Sample trajectories:
BP(=O)(OCC)C1CCC2C(O)C(=O)C(=C)C(=O)C(=O)C=C(C(=O)Nc3ccc(Br)cc3)NC21C
Brc1cc(Br)c(Nc2ncnc3ccc(NCc4ccccc4)cc23)c(Br)c1
Brc1cc(Br)c2c(nc3c(Br)cccc3N2)c(Br)c1
Brc1cc(Br)c2ncnc(Nc3ccc4ccccc4c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(-c4ccccc4Br)c(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.29342657
Proportion of valid SMILES: 0.6703125
Sample trajectories:
BP(=O)(OCC)ON=C(C)C(=O)OS(=O)(=O)CCl
BP(=O)(Oc1ccccc1Br)c1cccc(Br)c1
BrCCCCCCCCNCCN1CCCC1COc1ccccc1-c1ccc2ccccc2c1
BrCCNc1ccc2c(c1-c1ccccc1Nc1ccccc1)-c1ccccc1-2
BrCc1ccccc1-c1ccccc1-c1ccccc1Nc1ccc2ncnc(Nc3ccccc3)c2c1
Fine tuning...
Mean value of predictions: 0.51028216
Proportion of valid SMILES: 0.6536417630509535
Sample trajectories:
BP(=O)(CC)OCC=CI
B[PH](=O)(O)(CCN)CCCN
BrBr
BrC(Br)(Br)Br
BrC=CC=CNc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2nc1Nc1ccc(Br)cc1

 14 Training on 20542 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.093172
Reward: 2.801094
Trajectories with max counts:
25	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5545015
Proportion of valid SMILES: 0.645625
Sample trajectories:
BP(=O)(O)CNC(=O)c1ccc(Br)c(Br)c1
BrCc1ccc2ncnc(Nc3cccc(Br)c3)c2c1
Brc1c[nH]c2ccccc12
Brc1cc(Nc2ncnc3cc(Br)sc23)c(Br)s1
Brc1cc2c(-c3ccc(I)cc3)ncnc2cc1-c1ccc2ncncc2c1
Policy gradient replay...
Mean value of predictions: 0.5673593
Proportion of valid SMILES: 0.6169480925578487
Sample trajectories:
Bc1ccccc1Nc1ncnc2sc(-c3ccccc3)cc12
Brc1cc(-c2ccccc2)c2ncnc(Nc3ccncc3)c2c1-c1ccsc1
Brc1cc(-c2ccccn2)n2ncnc2n1
Brc1cc(-c2cncnc2)c2cccnc2n1
Brc1cc(Br)c(Br)cc1Br
Fine tuning...
Mean value of predictions: 0.5390805
Proportion of valid SMILES: 0.6529080675422139
Sample trajectories:
Bc1cc(Br)cc(Br)c1Br
BrC1CC1(Br)Br
BrCCCNc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1Nc1ncnc2sccc12
BrSc1cccc(Br)c1
Brc1cc(Br)c2c(Nc3ccccc3)ncnc2c1

 15 Training on 23022 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.075823
Reward: 2.975291
Trajectories with max counts:
60	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5484536
Proportion of valid SMILES: 0.6066291432145091
Sample trajectories:
BP(=O)(NO)c1ccc(Br)cc1
BrCCNc1ncc2ncnc(Nc3ccc4ccccc4c3)c2n1
BrCCc1ccc(Br)cn1
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Policy gradient replay...
Mean value of predictions: 0.5702347
Proportion of valid SMILES: 0.665833072835261
Sample trajectories:
BrC1=Nc2sc3ccccc3c2n2ccnc21
Brc1cc(Br)c2c(-c3ccccc3-n3cncn3)c3cccc(Br)c3-c3ccccc3-c3ncnc(c-2nc1Br)-c1cccc(Br)c1c3Br
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Fine tuning...
Mean value of predictions: 0.5334571
Proportion of valid SMILES: 0.6734375
Sample trajectories:
BP(=O)(O)OC(F)(F)C(F)(F)F
BP(=O)(OC(C)C)C(F)(F)F
BP(=O)(OCC1OC(N)C(O)C(O)C1O)Oc1cc(Br)cc(Br)c1
BrSc1ccc2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br

 16 Training on 25542 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.774425
Reward: 3.273799
Trajectories with max counts:
27	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.62116563
Proportion of valid SMILES: 0.61125
Sample trajectories:
BP(=O)(OCC1OC(F)CNC1(F)F)c1cccc(O)c1
BrCCN1CCN(c2ncnc3sccc23)CC1
BrCCOc1cc2c(Nc3ccc(Br)cc3)ncnc2cc1-c1ccccc1Br
Brc1cc(Br)c(Nc2ncnc3sccc23)cc1Br
Brc1cc(Br)c2ncnc(-c3cncnc3)c2n1
Policy gradient replay...
Mean value of predictions: 0.5259539
Proportion of valid SMILES: 0.6636448890278211
Sample trajectories:
BBr
BP(=O)(CCCO)Nc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BP(=O)(NCc1ccc(Br)cc1)P1(=O)Oc2cc(Br)ccc21
BrC1=Nc2ncnc(Nc3ccccc3Br)c2NC1
BrCCCCCCCCCCCCCCNc1ccc2ncnc(Nc3cc(Br)cc(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.5570225
Proportion of valid SMILES: 0.6675
Sample trajectories:
Brc1cc(Br)c(Nc2ncnc3ccccc23)o1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Brc1cc2c(Nc3ccccc3)ncnc2cc1Nc1ccccc1
Brc1cc2c(s1)c(Br)csc1ncnc(cc3sccc31)N2

 17 Training on 28133 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.204639
Reward: 3.540211
Trajectories with max counts:
69	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.5173939
Proportion of valid SMILES: 0.5228125
Sample trajectories:
BP(=O)(O)c1ccccc1Nc1nc2ccccc2s1
BrCCNc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Brc1cc2c(Nc3ccccc3Br)ncnc2s1
Brc1cc2ncnc(Nc3ccc4ccccc4c3)c2cc1Br
Brc1cc2ncnc(Nc3ccccc3)c2cc1Br
Policy gradient replay...
Mean value of predictions: 0.6016618
Proportion of valid SMILES: 0.6395748671459831
Sample trajectories:
BP(=O)(OCC)C(F)(F)F
Bc1cc(Br)ccc1S(=O)(=O)Nc1ccc(Nc2ncnc3sccc23)cc1
Brc1cc(Br)c(Br)c(Nc2cc(Nc3ccc(Br)c(Br)c3)ccn2)n1
Brc1cc(Br)c(Nc2ncnc3c(Br)c(Br)c(Br)c(Br)c23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccc(Br)c(Br)c4)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.57800084
Proportion of valid SMILES: 0.6853299968720675
Sample trajectories:
BP(=O)(NCCCCCCCCCCN)OCCO
BP(=O)(NS(=O)(=O)c1ccc(Br)cc1Nc1ccc(Br)cc1)OCCS(=O)(=O)NO
Bc1cccc(Nc2ncnc3scc(-c4ccccc4)c23)c1
BrC1=NN=C(Nc2ccccn2)CN(c2ncnc3scnc23)CC1
BrCCCCCCCCCCNc1ccc(Nc2ncnc3cc(Br)ccc23)cc1

 18 Training on 30641 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.115167
Reward: 3.402007
Trajectories with max counts:
26	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
26	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.57119226
Proportion of valid SMILES: 0.6421875
Sample trajectories:
BP(=O)(NCCO)c1ncnc2sc(C(=O)O)c(Br)c12
BrCCNc1ccc(Br)cc1Nc1cc(Nc2ccc3ccccc3c2)ncn1
BrCCNc1ccccc1Nc1ncnc2ccc(Br)cc12
Brc1c(-c2nc3ccccc3s2)cc2ncncc2c1Br
Brc1cc(Br)c(N2CCC(Oc3ccc(Br)c(Br)c3-c3ccccc3)C2)o1
Policy gradient replay...
Mean value of predictions: 0.3455869
Proportion of valid SMILES: 0.686875
Sample trajectories:
BC=NNC(=O)C(=O)c1ccccc1-c1ccccc1Br
BP(=O)(CCCC=CBr)NO
BP(=O)(OCC)OC(=O)CN(CC(=O)Nc1ccccc1)c1ccccc1
BP(=O)(OCC)OC(=O)Cc1ccccc1Br
Bc1ccccc1-c1ccccc1Nc1ncnc2ccncc12
Fine tuning...
Mean value of predictions: 0.57987297
Proportion of valid SMILES: 0.6398874648327603
Sample trajectories:
Br
BrSc1sc2ncnc(Nc3ccc(Br)s3)c2c1-c1ccccc1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2[nH]1
Brc1cc2c(Nc3ccccc3)ncnc2cc1-c1ccccc1I

 19 Training on 32937 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.786984
Reward: 3.348583
Trajectories with max counts:
34	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.53401643
Proportion of valid SMILES: 0.61
Sample trajectories:
BP(=O)(NO)c1ccccc1Br
Bc1ccccc1-c1ccccc1Nc1ncnc2ccccc12
Bc1ccccc1Nc1ncnc2sccc12
BrC(=NNc1ccccc1)Nc1cccc(Br)c1
BrC=CCCc1cccc(Nc2cccnc2Nc2ccccc2-c2ccccc2I)c1
Policy gradient replay...
Mean value of predictions: 0.6026083
Proportion of valid SMILES: 0.6709375
Sample trajectories:
BP(=O)(NCCCCCNC(=O)c1cc(Cl)c2c(Nc3cc(Br)c3-c3ccccc3)ncnc2c1)c1cc(F)c(F)c(F)c1
Br
BrC=CC=CBr
BrCCCCBr
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1I
Fine tuning...
Mean value of predictions: 0.58414304
Proportion of valid SMILES: 0.681875
Sample trajectories:
BP(=O)(OCC)OCCCCCl
BrC=Nc1nc(Br)c(Br)s1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3cccc(I)c3)ncnc2c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1

 20 Training on 35639 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.179404
Reward: 3.632651
Trajectories with max counts:
21	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.6645263
Proportion of valid SMILES: 0.59430716296528
Sample trajectories:
BP(=O)(c1cc2c(Br)c(CBr)[nH]c2c(Br)c1Br)N(CC(=O)OCC)Cc1cccc2ccccc12
BrCc1ccccc1-c1ccc2ncnc(Nc3ccccc3)c2c1
BrSc1sc2ncnc(Nc3ccc(Br)cc3)c2c1-c1ccc2ncncc2c1
Brc1c(I)cc2ncnc(Nc3ccccc3)c2c1-c1cccc2ccccc12
Brc1cc(Br)c2c(Nc3ccccc3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.5667144
Proportion of valid SMILES: 0.6553125
Sample trajectories:
Br
BrCCNc1nc(Nc2ccc3ccccc3c2)sc1Br
Brc1c(N2CCCC2)c(Br)c2c(sc3ccccc32)c1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2sc3ncnc(Nc4ccccc4Br)c3c2c1
Fine tuning...
Mean value of predictions: 0.6262889
Proportion of valid SMILES: 0.6728125
Sample trajectories:
Bc1ccc(Br)cc1Br
BrCc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
BrCc1sc2ncnc(Nc3ccc(Br)cc3)c2c1-c1ccccc1
Brc1c(Nc2ncnc3cc(I)sc23)ccc2ccccc12
Brc1cc(Br)c(Nc2ncnc3sc(Br)nc23)c(Br)c1

Trajectories with max counts:
73	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.59705466
Proportion of valid SMILES: 0.5689008378141803
Mean Internal Similarity: 0.4630229394630041
Std Internal Similarity: 0.08805466090436959
Mean External Similarity: 0.42469485451600353
Std External Similarity: 0.07022068146369777
Mean MolWt: 417.03896529786
Std MolWt: 99.36040390278124
Effect MolWt: -0.8064478194576524
Mean MolLogP: 5.2275430248698695
Std MolLogP: 1.5704670013579867
Effect MolLogP: 0.34223769455086606
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.829457% (1262 / 1290)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 5, 'n_policy_replay': 20, 'n_fine_tune': 100, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 6187.286126613617, 'valid_fraction': 0.5689008378141803, 'active_fraction': 0.5700626442466205, 'max_counts': 73, 'mean_internal_similarity': 0.4630229394630041, 'std_internal_similarity': 0.08805466090436959, 'mean_external_similarity': 0.42469485451600353, 'std_external_similarity': 0.07022068146369777, 'mean_MolWt': 417.03896529786, 'std_MolWt': 99.36040390278124, 'effect_MolWt': -0.8064478194576524, 'mean_MolLogP': 5.2275430248698695, 'std_MolLogP': 1.5704670013579867, 'effect_MolLogP': 0.34223769455086606, 'generated_scaffolds': 1290, 'novel_scaffolds': 1262, 'novel_fraction': 0.9782945736434109, 'save_path': '../logs/replay_combo_s1-3.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.761316
Reward: 1.000000
Trajectories with max counts:
2	Cc1cccc(O)c1
Mean value of predictions: 0.00081103
Proportion of valid SMILES: 0.7766929133858268
Sample trajectories:
Brc1ccc(Nc2ncnc3nc(Br)nn23)cc1
C#CC(=O)N1CCN(CCOC(=O)c2ccccc2)C(C)C1
C#CC(CCC(=C)CCCCC#N)c1cn(O)c2c1C(C)(C)NC2(C)C
C#CCCCCCC1C(C)CCCN1CCC#N
C#CCCCCCCCC(=O)N1CCC(P(=O)(O)COC)CC1
Policy gradient replay...
Mean value of predictions: 0.012785388
Proportion of valid SMILES: 0.6169014084507042
Sample trajectories:
Brc1cc2c(c3c1CCNC3)OCCO2
Brc1ccc(NN=C2CCCCCC2)cc1
Brc1ccc(Nc2ccnc3ncnc(Nc4cccc(Br)c4)c23)cc1
Brc1ccc(Nc2ncnc3ncncc23)nc1
Brc1ccc(Nc2onc(-c3ccccc3)c2-c2ccc3[nH]ccc3n2)cc1
Fine tuning...
Mean value of predictions: 0.019393336
Proportion of valid SMILES: 0.6294209702660407
Sample trajectories:
BP(=O)(NN=Cc1ccncn1)c1cccc(Cl)c1
BrC1CCCSc2cc3c(cc2C1)Cnc3-c1ccccc1
Brc1ccc(-c2cn[nH]c2)c2ccccc12
Brc1ccc(-c2nn(Cc3ccccc3)c3ccccc23)cc1
Brc1ccc(N2CCCCC2c2cc3c(cn2)CCCC3)c(-c2nc3ccccc3s2)c1

  2 Training on 325 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.808011
Reward: 1.111544
Trajectories with max counts:
6	Nc1ncnc2c1ncn2C1CCCCC1
Mean value of predictions: 0.020009954
Proportion of valid SMILES: 0.6282051282051282
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)P(=O)(O)O
Brc1cc2c(cn1)C(=Nc1ccccn1)N2c1cccnc1
Brc1ccc(C=NNc2[nH]nc3ccccc23)cc1
Brc1ccc(Nc2nc3cc(Br)ccc3[nH]2)cc1
Brc1ccc(Nc2ncnc3c2cnn3-c2ccncc2)cc1
Policy gradient replay...
Mean value of predictions: 0.036516577
Proportion of valid SMILES: 0.6337409846346818
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)O
BrCN1CCN(Cc2ccncc2)CC1
Brc1cc(I)ccc1-c1ncnc2[nH]ccc12
Brc1cc(Nc2c3ccccc3nc3ccccc23)nc2ccccc12
Brc1ccc(CNc2cccnc2)cc1
Fine tuning...
Mean value of predictions: 0.042073477
Proportion of valid SMILES: 0.6224937343358395
Sample trajectories:
BP(=O)(NCCCN)N(CC)CC
Brc1ccc(N2CCCCCC2)cc1-c1cccc(Nc2ccc3ccncc3c2)c1
Brc1ccc(N=Nc2ccc3ccncc3c2)cc1OCCCc1ccccc1
Brc1ccc(Nc2cc3c2CCCc2ncnnc2-3)cc1
Brc1ccc(Nc2ccnc3cc(Br)cnc23)cc1

  3 Training on 625 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.334131
Reward: 1.059753
Trajectories with max counts:
2	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
2	Brc1cccc(Nc2ncnc3ccccc23)c1
2	COc1ccc(Nc2ncnc3ccc(Cl)cc23)cc1
2	Cc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
2	Cc1ccc(Nc2ncnc3ccccc23)cc1
2	Clc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
2	Clc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
2	Clc1ccc(Nc2ncnc3ccccc23)cc1
2	Fc1ccc(Nc2ncnc3ccccc23)cc1
2	Fc1ccccc1F
2	N#Cc1cccc(Nc2ncnc3ccccc23)c1
2	O=C(Nc1ccccc1)Nc1ccc(Nc2ncccn2)cc1
Mean value of predictions: 0.03496994
Proportion of valid SMILES: 0.6255092447508618
Sample trajectories:
Br
BrC1=CC(COc2ncccn2)O1
Brc1cc(Nc2cc(CNc3ccncc3)ncn2)ccn1
Brc1ccc(-c2cncnc2)cc1
Brc1ccc(Nc2cc(-c3ccccc3)nc3ccccc23)cc1
Policy gradient replay...
Mean value of predictions: 0.006085526
Proportion of valid SMILES: 0.76
Sample trajectories:
BP(=O)(OCC)OC(=O)Nc1ccc2c(Nc3ccccc3Cl)cccc2c1
B[PH](=O)(OC(c1ccccc1)c1ccccc1)=P(N)(=O)(O)PO
Brc1c2ccccc2cc2ccccc12
Brc1ccc(Br)c(Nc2cccc3ccccc23)c1
Brc1ccc(NCc2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.004571429
Proportion of valid SMILES: 0.765625
Sample trajectories:
BP(=O)(OCC)Oc1ccccc1
Brc1cc2cccs2c1Nc1ccccc1
Brc1ccc(-c2ccccc2)cc1
Brc1ccc(N=Nc2cccc(Br)c2)cc1
Brc1ccc(Nc2ccccc2)cc1

  4 Training on 765 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 14.822560
Reward: 1.050288
Trajectories with max counts:
40	O=C(Nc1ccccc1)c1ccccc1
Mean value of predictions: 0.0055888225
Proportion of valid SMILES: 0.7828125
Sample trajectories:
BP(=O)(OC(=O)c1ccccc1)c1ccccc1
Bc1ccccc1-c1ccccc1Br
BrCc1ccccc1Nc1ccccc1Nc1ccccc1Nc1ccccc1-c1ccccc1
Brc1c(Nc2ccccc2I)ccc2ccccc12
Brc1ccc(Nc2ccccc2Oc2ccccc2)cc1
Policy gradient replay...
Mean value of predictions: 0.1358713
Proportion of valid SMILES: 0.5829946858393248
Sample trajectories:
Brc1ccc(-c2c(Br)ccc3nncn23)cc1
Brc1ccc(-c2nc(Nc3ccc4ccccc4n3)nc3ccccc23)cc1
Brc1ccc(-n2cnc(-c3cccnc3)n2)c2ccccc12
Brc1ccc(Nc2ccc3ncncc3n2)cc1
Brc1ccc(Nc2cnc3ncc(Br)cc3n2)cc1
Fine tuning...
Mean value of predictions: 0.128056
Proportion of valid SMILES: 0.5812206572769953
Sample trajectories:
BC(=O)Nc1c(I)cc(I)cc1Nc1ncnc2ncnc(Nc3cccc(I)c3F)c(Nc3ccc(I)cc3F)nc12
Brc1cc(Nc2ccncc2)cc2ncnn12
Brc1cc2c(Br)c(Br)c(Br)c(Br)c2s1
Brc1cc2c(Nc3ccccc3)ncnc2cc1CN1CCOCC1
Brc1ccc(-c2cc3cc(Br)oc3cn2)nc1

  5 Training on 1449 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 17.276234
Reward: 1.345734
Trajectories with max counts:
41	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.15474615
Proportion of valid SMILES: 0.567313713212273
Sample trajectories:
B[PH](=O)(OC(C)P(=O)(O)OP(=O)(O)O)=C(Br)Br
BrC1=C(Br)CNC(c2ccccc2)=C1
Brc1cc(Nc2ncnc3ccccc23)ccn1
Brc1ccc(-c2cc(Nc3ccccc3)on2)c2ncnc(Nc3ccc(N4CCCCC4)cc3)c12
Brc1ccc(C=C(c2ccccc2)c2cccnc2)cc1
Policy gradient replay...
Mean value of predictions: 0.17603487
Proportion of valid SMILES: 0.5746478873239437
Sample trajectories:
Brc1cc2c(s1)-c1ccccc1N2
Brc1ccc(-c2ccc(Br)s2)o1
Brc1ccc(-c2nccs2)c(NCCCN2CCCCC2)c1
Brc1ccc(Br)cc1
Brc1ccc(C=NNc2nnc(-c3ccoc3)o2)cc1
Fine tuning...
Mean value of predictions: 0.18378672
Proportion of valid SMILES: 0.5750938673341677
Sample trajectories:
BC(=O)c1cc(Br)cc(NP(=O)(O)Oc2ccc(N)c(Br)c2)c1
BP(=O)(OCC1OC(N2C=C(C)C(=O)NC2=O)C(O)C1O)OC(=O)CCCNC(=O)C(C)(Cl)P(=O)(O)O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ncnc(N4CCOCC4)c23)cc1
Brc1cc(Br)c(Br)c(Br)c1

  6 Training on 2638 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 19.049791
Reward: 1.628267
Trajectories with max counts:
26	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.23146474
Proportion of valid SMILES: 0.5200626959247648
Sample trajectories:
BrC(=NNc1cccc(Nc2ncnc3cccnc23)c1)c1cccc(Br)c1
BrC=C(Br)Br
BrC=CC=CC=CC1CCCOC1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3cccc(Br)c23)sc1-c1ccccc1
Policy gradient replay...
Mean value of predictions: 0.1941697
Proportion of valid SMILES: 0.6005001562988433
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)c1ccc(F)cc1
BP(=O)(c1ccc(OC(F)F)c(Br)c1)N(O)C(N)=O
Bc1ccc(Nc2ncnc3ccccc23)cc1
BrC(=NNc1cccc(-c2cccc(Br)c2)c1)N1CCCCC1
Brc1cc(Nc2ncnc3ccccc23)ccc1Oc1ccccc1
Fine tuning...
Mean value of predictions: 0.19569662
Proportion of valid SMILES: 0.5816645807259074
Sample trajectories:
BP(=O)(N(O)CCCCCl)[PH](=O)N(Cl)CCl
Bc1ccc(Nc2ncnc3ccc(Cl)cc23)cc1
BrCCCCCCCCc1ccc2ncnc(-c3ccccc3)c2c1
Brc1ccc(-c2ccc3ccc(-c4cncnc4)c23)cc1
Brc1ccc(-c2ccccc2)c2ccccc12

  7 Training on 3894 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 20.655726
Reward: 2.122431
Trajectories with max counts:
117	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.31182644
Proportion of valid SMILES: 0.4475414970247416
Sample trajectories:
BP(=O)(NO)c1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BP(=O)(Nc1ccc(Nc2ccc(Nc3ccc(Cl)c(Br)c3)cc2)nc1)Oc1ccc(Br)cc1Br
BP(=O)(O)c1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BP(=O)(OCCC)OC(=O)Nc1ccc(I)cc1Br
Bc1ccc(Nc2ncnc3c(Nc4ccc(F)cc4)nc(-c4ccc(Cl)cc4)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.2709642
Proportion of valid SMILES: 0.5678973717146433
Sample trajectories:
BP(=O)(OCCCCC=C)c1ccc(Br)cc1
BrCc1ccc2sc(Nc3ncnc4ccc(Br)cc34)cc2c1
Brc1cc2cnn(Cc3ccccc3)c2cc1Br
Brc1ccc(-c2c(Br)cccc2Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(-c2cc(Nc3ccc(N4CCCCC4)c(Br)c3)ncn2)cc1
Fine tuning...
Mean value of predictions: 0.25574136
Proportion of valid SMILES: 0.5615023474178403
Sample trajectories:
BP(=O)(OCCCC)C(NC(=O)C(Cl)(Cl)Cl)P(=O)(OCOC(=O)C(F)(F)F)C(F)(F)F
BrBr
BrC(=NNc1ccccc1)c1ccc(Br)cc1
BrCC1=Nc2ncnc(Nc3ccc(Br)cc3)c2N1
Brc1cc2ncnc(Nc3ccccc3)c2cc1Nc1cccc(Nc2ccccc2)c1

  8 Training on 5316 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 22.474771
Reward: 2.776866
Trajectories with max counts:
363	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.38626543
Proportion of valid SMILES: 0.4051266020631447
Sample trajectories:
BrCCOc1cc(Nc2ncnc(Oc3cccc(Br)c3)n2)cs1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Nc2ccccc2)cc(-c2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccccc23)c2nccnc2n1
Brc1cc(Nc2ncnc3ccsc23)cnc1-c1cncnc1
Policy gradient replay...
Mean value of predictions: 0.30051163
Proportion of valid SMILES: 0.550547730829421
Sample trajectories:
B[PH](=O)(NO)(n1cnc(N)c1)C(F)(F)F
BrCC(Br)Br
Brc1cc(-c2nc3cccnc3s2)c2ccccc2n1
Brc1cc(Br)c2c(Br)cc(Nc3ncnc4ccc(Br)cc34)nc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccccc4)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.31438127
Proportion of valid SMILES: 0.5613266583229036
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCC)c1ccc(Nc2nc(N)nc(Br)n2)cc1
Bc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Br
BrC=CCCC=CC=CCNCCCCCN=C1CCCCCCNCCCC1
BrCc1cc2ncnc(Nc3ccc(Br)cc3)-c(c(Nc3ccc(Br)cc3)n1)c1c(Br)cccc21

  9 Training on 6867 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 26.411581
Reward: 3.726011
Trajectories with max counts:
247	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.3958042
Proportion of valid SMILES: 0.35761175367302284
Sample trajectories:
BP(=O)(NC(CO)S(=O)(=O)Oc1cccc(N)c1)P(=O)(OC(C)(C)C)OC(C)(C)C
BP(=O)(OCC(=O)Nc1ccc(Br)cc1)C(F)(F)F
BP(=O)(OCC(F)(F)F)N(C)C=O
BP(=O)(OCC)OCC(=O)C(F)(F)F
BP(=O)(OCC)ON=C(C#N)NS(=O)(=O)N1CCCC(CC(=O)Nc2nc3cc(Br)cc(Br)c3s2)C1
Policy gradient replay...
Mean value of predictions: 0.34471264
Proportion of valid SMILES: 0.5451127819548872
Sample trajectories:
BP(=O)(NC(=O)C=C(F)F)c1ccc(F)cc1
Bc1ccc(Nc2ccc(Nc3ncnc4cc(Br)ccc34)cc2)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1-c1ccccc1
BrCCCC=CC=CC=CCC=CC=Nc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.34936562
Proportion of valid SMILES: 0.5425531914893617
Sample trajectories:
BrC(=Nc1ccc(Nc2ncnc3ccccc23)cc1)c1cccc(Br)c1
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Nc2ncnc3ccccc23)ccc1-c1ccccc1
Brc1ccc(-c2cc(Nc3ncnc4cc(Br)ccc34)ccc2Br)cc1

 10 Training on 8288 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 24.964223
Reward: 3.352347
Trajectories with max counts:
138	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4074442
Proportion of valid SMILES: 0.5042227087894902
Sample trajectories:
BP(=O)(C#C)c1ccc(Nc2cccc(Br)c2)cc1
BP(=O)(OCC)C(=O)Nc1cccc(Br)c1
BrCCc1cc(-c2ccsc2)c2ccccc2n1
BrSc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Br)s1
Policy gradient replay...
Mean value of predictions: 0.4472469
Proportion of valid SMILES: 0.527977492966552
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCC(Br)Br
BrCCNc1cc(Nc2ncnc3ccc(Br)cc23)c2ccccc2c1
BrCCNc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Fine tuning...
Mean value of predictions: 0.44051826
Proportion of valid SMILES: 0.5307908721475461
Sample trajectories:
Bc1ccc(Br)cc1-c1cccs1
BrC(=NNc1ccc(Br)cc1)C1CCCCCCC1
Brc1cc(Br)c(Oc2ccc3ccsc3c2)c(Nc2ncnc3cccc(Br)c23)c1
Brc1cc(Nc2ncnc3cc(Br)ccc23)c2ncncc2c1
Brc1cc2c(ncnc1-c1cccnc1)-c1ccccc1N2

 11 Training on 9882 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.536644
Reward: 3.960001
Trajectories with max counts:
504	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.42477396
Proportion of valid SMILES: 0.345625
Sample trajectories:
BP(=O)(NP(=O)(O)OP(=O)(O)O)N(O)CP(=O)(O)CF
BP(=O)(NS(=O)(=O)c1ccccc1)OCC1OC(c2ccccc2)C(N2C=C(F)C(=O)NS2(=O)=O)C(O)C1O
BP(=O)(Nc1cccc(Br)c1)P(=O)(OCC)Oc1ccc(Br)cc1
BP(=O)(OCCCCC)Oc1cccc(Nc2ncnc3ccccc23)c1
BP(=O)(Oc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1)C(F)(F)F
Policy gradient replay...
Mean value of predictions: 0.3416309
Proportion of valid SMILES: 0.5825
Sample trajectories:
Bc1ccc(I)cc1
Brc1ccc(Br)c(-c2ccc(Nc3ncnc4ccccc34)cc2)c1
Brc1ccc(Br)c(-c2cccc(Nc3ncnc4c(Br)ccc(Br)c4s3)c2)c1
Brc1ccc(C=NNc2ccc(Br)cc2)cc1
Brc1ccc(NN=Cc2ccccc2Br)cc1
Fine tuning...
Mean value of predictions: 0.35382935
Proportion of valid SMILES: 0.5716072545340838
Sample trajectories:
BP(=O)(Nc1ccccc1Br)NP(=O)(O)O
BP(=O)(OCC)OCC=CC=C
BP(=O)(Oc1ccccc1Nc1ccc(N)cc1)P(=O)(Oc1ccccc1)Oc1ccc(F)cc1
BrC=C(Br)Br
BrCCNc1ccccc1

 12 Training on 11218 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.053985
Reward: 4.181035
Trajectories with max counts:
233	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.52758336
Proportion of valid SMILES: 0.3840625
Sample trajectories:
BP(=O)(NOC(C(=O)O)N(=O)=O)Nc1ccc(N)cc1
BP1(=O)OCC(CCCCCS(=O)(=O)N(C)C)Cc2cc(Br)ccc2S(=O)(=O)NC1=O
BrC(=NNc1ccccc1)Nc1cccc(Nc2ncnc3ccsc23)c1
Brc1cc(Nc2ncnc3ccsc23)ccn1
Brc1cc2ccccc2cc1Br
Policy gradient replay...
Mean value of predictions: 0.49075428
Proportion of valid SMILES: 0.5142320925868001
Sample trajectories:
BP(=O)(OCC)c1ccc(Nc2ccc(Br)cc2)cc1Br
BrCCN1CCCCCCCCCCCCCCCCCCCCCCCCCCCC1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Nc4ccccc4Br)cc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.49071914
Proportion of valid SMILES: 0.5085964363863708
Sample trajectories:
BP(=O)(CCC(N)=O)NS(=O)(=O)c1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BP(=O)(OCC)n1ccc(Nc2ncnc3cc(Br)cc(Br)c23)n1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
BrC(=NNc1ccc(Br)cc1)c1ccccc1
BrC(=NNc1ccccc1)c1ccc(Br)cn1

 13 Training on 12979 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.162115
Reward: 4.330141
Trajectories with max counts:
253	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5046177
Proportion of valid SMILES: 0.4128125
Sample trajectories:
BP(=O)(OCC)c1ccc(Nc2ncnc3sc(Br)c(Br)c23)cc1
Brc1cc(Nc2ncnc3ccsc23)oc1CCCc1ccccc1
Brc1ccc(-c2ccc(Nc3nc4ccccc4s3)cc2)s1
Brc1ccc(-c2nc3ccccc3nc2Nc2ccccc2Br)c(Br)c1
Brc1ccc(Br)c(-c2ccc(Br)cc2-c2cccc(Nc3ncnc4ccsc34)c2)c1
Policy gradient replay...
Mean value of predictions: 0.49182227
Proportion of valid SMILES: 0.485464207564864
Sample trajectories:
BP(=O)(Cc1ccccc1)Nc1ccc(Br)c(Nc2ccc(Br)cc2)c1
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Br
BrCc1ccc(Nc2ccc(Nc3ncnc4ccccc34)cc2)cc1
Fine tuning...
Mean value of predictions: 0.49579287
Proportion of valid SMILES: 0.4828125
Sample trajectories:
BP(=O)(N=C(Nc1ccc(Br)cc1)P(=O)(O)O)OCC
BP(=O)(NNc1cc(Br)ccc1Br)OCC=C
BP(=O)(OCC)Oc1ccc(Br)cc1Nc1ccc(Br)o1
BP(=O)(OCC)c1cc(Br)c(Br)cc1Br
Bc1ccc(Br)cc1-c1ccc(Br)cc1

 14 Training on 14721 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.624797
Reward: 4.850725
Trajectories with max counts:
460	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5598911
Proportion of valid SMILES: 0.344375
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(Nc1ccc(Nc2cccc(Br)c2)nc1)c1ccc(Br)cc1
BP(=O)(OCC)Oc1cc(Br)cc(Nc2c(Br)cnc3cc(Br)ccc23)c1
BP(=O)(OCc1ccc(Br)cc1)Oc1ccc(Nc2ccc(Br)cc2)cc1
BP(=O)(Oc1ccc(Br)cc1)c1cccc(Br)c1
Policy gradient replay...
Mean value of predictions: 0.53202146
Proportion of valid SMILES: 0.5245542696277761
Sample trajectories:
BP(=O)(CCNC(=O)c1ccc(N)c(Nc2ncnc3cc(Br)cc(Br)c23)n1)OCC
BP(=O)(OCC)N(N=O)c1cc(Br)c(Nc2nc(Cl)cc(Br)c2O)nc1N
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Nc2ccc(Nc3nccnc3-c3cccs3)cc2)cc1Br
Fine tuning...
Mean value of predictions: 0.5516108
Proportion of valid SMILES: 0.49561678146524735
Sample trajectories:
BrBr
BrCc1ccc(Nc2ncnc3ncnc(-c4ccccc4)c23)cc1
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4ccsc34)cc2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2cnc3ccc(Br)nc3n2)c1

 15 Training on 16642 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.274699
Reward: 5.412598
Trajectories with max counts:
837	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5588719
Proportion of valid SMILES: 0.2659375
Sample trajectories:
BP(=O)(N(CC#C)c1ccc(Br)cc1)P(=O)(O)O
BP(=O)(N(O)N(=O)=O)N(=O)=O
BP(=O)(NC(=S)Nc1ccc(Br)cc1)c1cccc(Br)c1
BP(=O)(OCC)OC(=O)C(=O)c1ccc(Br)cc1
BP(=O)(ONC(=O)c1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1)P(=O)(O)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.5451991
Proportion of valid SMILES: 0.5342508601814201
Sample trajectories:
BP(=O)(COc1ccc(Br)cc1)Nc1ccc(Br)cc1
BP(=O)(N(=O)=O)N(=O)=O
Bc1ccc2c(Nc3ccc(Br)cc3)oc(Br)c2c1
BrCC(Br)(Br)Br
BrCc1ccc(Nc2ncnc3cc(Br)sc23)cc1
Fine tuning...
Mean value of predictions: 0.5779977
Proportion of valid SMILES: 0.5370428258830885
Sample trajectories:
BP(=O)(OCCCCCC)c1ccc(N)c(Br)c1
BrC=CC=CC=CC=CC=CCC=CCC=CC=NNc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c2c(NCCN3CCN(CCc4ccccc4)CC3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1

 16 Training on 18614 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.271216
Reward: 5.240570
Trajectories with max counts:
77	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5686436
Proportion of valid SMILES: 0.5323538605814317
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3sccc23)c1
Brc1cc(Nc2cnc3ccsc3c2)cc(Br)c1Br
Brc1cc2ccccc2nc1Nc1ccccc1Nc1ccc(Nc2cccc3ccccc23)cc1
Brc1ccc(Br)c(Nc2ccc(Nc3ncnc4ccc(Br)cc34)cc2)c1
Brc1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.5383529
Proportion of valid SMILES: 0.5315822388993121
Sample trajectories:
BP(=O)(OCC)C(=O)O
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(I)c(Br)c(Nc2cccc3nc(Br)ccc23)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1ccc(-c2ccccn2)c(Br)c1
Fine tuning...
Mean value of predictions: 0.5489162
Proportion of valid SMILES: 0.5337711069418386
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Br)cc1
Brc1cc(Br)c(Nc2ccc(Nc3ncnc4cc(Br)ccc34)cc2)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3ccsc23)o1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1

 17 Training on 20931 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.168986
Reward: 5.368397
Trajectories with max counts:
269	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.62202644
Proportion of valid SMILES: 0.42602439787300594
Sample trajectories:
BP(=O)(Oc1ccc(Nc2ncnc3c(Br)cc(Br)cc23)cc1)c1ccc(Br)cc1
Br
BrC(=NNc1ccc(Br)cc1)Nc1cc(Br)ccc1Br
BrC(=NNc1nc2ccc(Br)cc2s1)c1ccccc1
BrSc1cccc(Nc2cc3ccccc3s2)c1
Policy gradient replay...
Mean value of predictions: 0.56854945
Proportion of valid SMILES: 0.4870271959987496
Sample trajectories:
Bc1cc(Br)cc(Br)c1Nc1cc(Nc2ncn[nH]2)c2ccccc2n1
Bc1cc(Br)cc(Br)c1Nc1ccc(Nc2ncnc3sc(Br)cc23)cc1
BrC=CC=CBr
BrCN1CCCCCCCCC1CNc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc(Br)c(Br)c(Br)c1
Fine tuning...
Mean value of predictions: 0.58675677
Proportion of valid SMILES: 0.4625
Sample trajectories:
BP(=O)(N1CCC(F)(F)CC1)C(F)(F)F
BP(=O)(Nc1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(OCC)C(=O)Nc1cc(Br)cc(N)c1-c1ccc(Br)cc1
BP(=O)(OCC)C(F)(F)F
B[PH](=O)(=O)C(Br)Br

 18 Training on 23042 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.285092
Reward: 5.335863
Trajectories with max counts:
275	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6348778
Proportion of valid SMILES: 0.371285580231467
Sample trajectories:
BP(=O)(Nc1cc(Br)c(Br)c(Br)c1)P(=O)(Oc1ccc(F)cc1F)C(=O)Nc1c(F)cc(F)c(F)c1F
BP(=O)(OC(=O)c1ccc(Br)c(Br)c1)c1ccc(Br)cc1
BP(=O)(OCC)Oc1ccc(Nc2ncnc3c4ccccc4c23)cn1
BP(=O)(O[SH](=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O)N(O)P(=O)(O)OP(=O)(O)OP(=O)(O)O
Bc1c(Br)cc(Br)cc1Br
Policy gradient replay...
Mean value of predictions: 0.57030237
Proportion of valid SMILES: 0.5796557120500783
Sample trajectories:
BP(=O)(OCC)OC(=O)C(CCCCCCCC)NC(=O)C(N)CCCCCC#N
BrCc1nc2c(Nc3cc(Br)c4ccccc4n3)ncnc2s1
Brc1[nH]ccc1-c1c(-c2ccncc2)cccc2ncnc12
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Nc2ncnc3c(Br)ccc(Br)c23)c(Br)c1
Fine tuning...
Mean value of predictions: 0.5723358
Proportion of valid SMILES: 0.5809881175734835
Sample trajectories:
BP(=O)(OCCCC)OC(=O)CCCCCCn1cnc2c(N)ncnc21
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(-c4c(Br)cc(Br)c(Br)c4Br)cc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccccc4)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1

 19 Training on 25404 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.822047
Reward: 5.750754
Trajectories with max counts:
214	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6468056
Proportion of valid SMILES: 0.45
Sample trajectories:
BP(=O)(N=C(N)Nc1ccc(Br)cn1)OCC
BP(=O)(OCC)c1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Bc1ccc(Br)c(Br)c1Br
Bc1ccc(Nc2ncnc3cccc(Br)c23)cc1Cl
Bc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.57247823
Proportion of valid SMILES: 0.5023459493274945
Sample trajectories:
BP(=O)(O)c1cccc(Br)c1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1c(Nc2ncnc3ccsc23)sc(-c2ccccc2)c1Br
Brc1cc(Br)c(Nc2ncnc3cccnc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.5890192
Proportion of valid SMILES: 0.5068792995622264
Sample trajectories:
BP(=O)(Nc1ccc(Nc2ncnc3cc(Br)ccc23)cc1)C(=O)OCC
BrC=CBr
BrCC(=NNc1ccccc1)c1cccc(Nc2ccccc2Br)n1
BrCCNc1ccc(Nc2ncnc3ccsc23)cc1
BrCNC=Nc1cccc(Nc2ncnc3cccc(Br)c23)c1

 20 Training on 27758 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.234275
Reward: 5.927921
Trajectories with max counts:
221	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.61715734
Proportion of valid SMILES: 0.3715625
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1cccc(Nc2ncnc3ccsc23)c1
BrC=NNc1ccc(Nc2ncnc3sccc23)cc1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccsc12
Brc1cc(Br)c2ncnc(Nc3cccc(Nc4ccccc4Br)c3)c2c1
Policy gradient replay...
Mean value of predictions: 0.61439735
Proportion of valid SMILES: 0.5601750547045952
Sample trajectories:
BrCc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3cccc(-c4ccccc4Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Br)nc(Nc2ncnc3ccsc23)c1
Fine tuning...
Mean value of predictions: 0.6063509
Proportion of valid SMILES: 0.5609375
Sample trajectories:
B[PH](=O)(Nc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O
BrC(Br)=NNc1ccc(Br)cc1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2c(Br)scc12
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccccc4)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1

Trajectories with max counts:
512	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5634377
Proportion of valid SMILES: 0.4635107247826903
Mean Internal Similarity: 0.47825750138520046
Std Internal Similarity: 0.1129284008485821
Mean External Similarity: 0.4021034035296136
Std External Similarity: 0.06556680423232387
Mean MolWt: 402.2382987208428
Std MolWt: 99.02904468764923
Effect MolWt: -0.9252471224938784
Mean MolLogP: 4.801637296212693
Std MolLogP: 1.6421907786835208
Effect MolLogP: 0.055958626704130696
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.384937% (931 / 956)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 0, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5499.5077583789825, 'valid_fraction': 0.4635107247826903, 'active_fraction': 0.5379114948731786, 'max_counts': 512, 'mean_internal_similarity': 0.47825750138520046, 'std_internal_similarity': 0.1129284008485821, 'mean_external_similarity': 0.4021034035296136, 'std_external_similarity': 0.06556680423232387, 'mean_MolWt': 402.2382987208428, 'std_MolWt': 99.02904468764923, 'effect_MolWt': -0.9252471224938784, 'mean_MolLogP': 4.801637296212693, 'std_MolLogP': 1.6421907786835208, 'effect_MolLogP': 0.055958626704130696, 'generated_scaffolds': 956, 'novel_scaffolds': 931, 'novel_fraction': 0.9738493723849372, 'save_path': '../logs/replay_combo_s1-4.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.761316
Reward: 1.000000
Trajectories with max counts:
2	Cc1cccc(O)c1
Mean value of predictions: 0.00081103
Proportion of valid SMILES: 0.7766929133858268
Sample trajectories:
Brc1ccc(Nc2ncnc3nc(Br)nn23)cc1
C#CC(=O)N1CCN(CCOC(=O)c2ccccc2)C(C)C1
C#CC(CCC(=C)CCCCC#N)c1cn(O)c2c1C(C)(C)NC2(C)C
C#CCCCCCC1C(C)CCCN1CCC#N
C#CCCCCCCCC(=O)N1CCC(P(=O)(O)COC)CC1
Policy gradient replay...
Mean value of predictions: 0.012785388
Proportion of valid SMILES: 0.6169014084507042
Sample trajectories:
Brc1cc2c(c3c1CCNC3)OCCO2
Brc1ccc(NN=C2CCCCCC2)cc1
Brc1ccc(Nc2ccnc3ncnc(Nc4cccc(Br)c4)c23)cc1
Brc1ccc(Nc2ncnc3ncncc23)nc1
Brc1ccc(Nc2onc(-c3ccccc3)c2-c2ccc3[nH]ccc3n2)cc1
Fine tuning...
Mean value of predictions: 0.020717131
Proportion of valid SMILES: 0.6286787726988102
Sample trajectories:
B[PH](=O)(NO)(NC(=O)c1ccc(Br)c(Br)c1)Nc1ccc(Br)cc1
Brc1ccc(Nc2ncnc3cccnc23)nc1
Brc1cccc(-n2ncc3ccccc32)n1
Brc1cccc(Nc2ncnc3c(Br)cc(Br)cc23)c1
Brc1cccc2ccc(Nc3nc(Br)c4ccccc4n3)cc12

  2 Training on 323 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.489611
Reward: 1.043665
Trajectories with max counts:
5	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.028792571
Proportion of valid SMILES: 0.606762680025047
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3c(-c4ccncc4)ncnc23)c1
Brc1cc2onc(Br)c2cc1Br
Brc1ccc(-c2ccc3c(c2)OCO3)c2ccccc12
Brc1ccc(-c2nc3ccc(Br)nc3[nH]2)cc1
Brc1ccc(Cc2ncnc(N3CCN(c4ccncc4)CC3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.038625002
Proportion of valid SMILES: 0.5047318611987381
Sample trajectories:
Brc1ccc2c(c1)-c1ncnc(-c3csc(Br)c3)c1CCC2
Brc1ccc2c(c1)C(N1CCCCC1)=N2
Brc1ccc2c(c1-c1ccccc1)-c1[nH]c(-c3ccccc3)nc1-c1ccccc1-c1nn2[nH]1
Brc1ccc2oc(-c3ccc(-c4ccc(OCCN5CCCC5)cc4)s3)nc2c1
Brc1cnc2ncnn2c1
Fine tuning...
Mean value of predictions: 0.07135263
Proportion of valid SMILES: 0.5298904538341158
Sample trajectories:
Brc1cc(-c2ccc3c(n2)NCCC3)c2ccccc2n1
Brc1ccc(-c2ccc(Nc3ncnc4cc(Br)ccc34)s2)cc1
Brc1ccc(-c2ccccc2)c2ccc(Nc3ccncc3)cc12
Brc1ccc(C=Nc2ncnc(-c3ccc(Br)cc3)n2)cc1
Brc1ccc(N=Nc2ncnc3nc[nH]c23)cc1

  3 Training on 668 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 22.106880
Reward: 1.351598
Trajectories with max counts:
2	COc1cc2ncnc(Nc3ccc(F)c(Cl)c3F)c2cc1OC
2	COc1cc2ncnc(Nc3cccc(F)c3)c2cc1OC
2	Clc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
2	Clc1ccc(Nc2ncnc3ccc(Cl)cc23)cc1
2	Clc1ccc2c(Nc3ccncn3)ncnc2c1
2	Fc1ccc(Nc2ncnc3cc(Cl)ccc23)cc1
2	Fc1ccc(Nc2ncnc3ccc(F)cc23)cc1
2	Nc1ccc2ncnc(Nc3ccc(F)cc3)c2c1
2	Nc1ccc2ncnc2c1
2	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.07767804
Proportion of valid SMILES: 0.5233322893830253
Sample trajectories:
B[PH](=O)(Cl)(OCCCl)P(O)(F)(F)(F)(F)F
BrBr
BrCCN(CCN1CCCCCC1)C(Sc1ccncn1)c1ccccc1
BrCCNc1nc(NCc2cnn(-c3cccc4ccccc34)c2)c2ccc(Br)cc2n1
Brc1cc2c(s1)N2
Policy gradient replay...
Mean value of predictions: 0.14028952
Proportion of valid SMILES: 0.5195863365716077
Sample trajectories:
Brc1cc2ncnc(Nc3ccc(N4CCCCC4)cc3)c2nc1-c1cnc2ccccc2n1
Brc1ccc(-c2ccc(Nc3ncnc4ccsc34)cc2)cc1
Brc1ccc(Nc2cncnc2)cc1
Brc1ccc(Nc2nc(Nc3ccc(Br)c(Br)c3)nc3ncnc(Nc4cccc(Br)c4)c23)cc1
Brc1ccc(Nc2nc3cncnc3s2)cc1
Fine tuning...
Mean value of predictions: 0.11256656
Proportion of valid SMILES: 0.5872420262664165
Sample trajectories:
BP(=O)(OCC)OC(=O)CP(=O)(O)OP(=O)(O)O
Brc1cc(-c2cc(N3CCCC3)c3cccc(Br)c3n2)ccn1
Brc1cc2c(Nc3ccncc3)ncnc2cc1CN1CCCCCC1
Brc1ccc(-c2cc3cncnc3c3ccccc23)c2ccncc12
Brc1ccc(-c2nc3ccccc3[nH]2)c2ccccc12

  4 Training on 1442 replay instances...
Setting threshold to 0.150000
Policy gradient...
Loss: 22.239232
Reward: 1.539709
Trajectories with max counts:
35	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.13599999
Proportion of valid SMILES: 0.5393996247654784
Sample trajectories:
Brc1cc(Nc2nccs2)ccn1
Brc1cc2ncnc(N3CCOCC3)c2cc1Br
Brc1ccc(Nc2ccnc3ccnc(C4CCCC4)c23)cc1
Brc1ccc(Nc2ccnc3ccncc23)cc1
Brc1ccc(Nc2ccncc2)cc1-c1cnc2ncncc2c1
Policy gradient replay...
Mean value of predictions: 0.12446974
Proportion of valid SMILES: 0.6042513285401688
Sample trajectories:
BrC1=C2C=CC=CN2c2ncnn21
Brc1cc2c(Nc3ccccc3)ncnc2cc1NCCN1CCCC1CNc1cccc2ccccc12
Brc1cc2c(cc1-c1ccccc1)-c1ccccc1Cc1nccn1C=N2
Brc1ccc(-c2ncnc3ccccc23)cc1
Brc1ccc(N(c2ccccc2)c2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.1558212
Proportion of valid SMILES: 0.6020025031289111
Sample trajectories:
BrC=Cc1cnc2c(Nc3ccc(Br)cc3)ncnc2c1
BrCc1cc2cc(Br)ccc2[nH]1
Brc1cc2c(s1)c1c(Br)ncc(Br)c1ncnc(-c1ccccc1)N2
Brc1cc2c(sc3ncnn13)CCC2
Brc1ccc(-c2nc(Nc3ccccn3)cnc2-c2ccncc2)s1

  5 Training on 2467 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 22.159357
Reward: 1.680130
Trajectories with max counts:
73	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.14272863
Proportion of valid SMILES: 0.6255079712410128
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2cc1Nc1ncnc2ccccc12
Brc1ccc(-c2cccc3ccccc23)c2cnccc12
Brc1ccc(N=Nc2cccc(Br)c2)cc1
Brc1ccc(Nc2cc3c(ncn2)ncnc2ccc(Nc4ccccc4)cc23)nc1
Brc1ccc(Nc2ccc(Nc3ncnc4ccc(Br)cc34)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.10927644
Proportion of valid SMILES: 0.6739606126914661
Sample trajectories:
BrC1CCN(c2nc3cccnc3nc2Nc2ncnc3ccccc23)CC1
BrCCSc1cc2cc(Nc3ccccc3)ccc2cnc2cc(Br)ccc12
Brc1ccc(Nc2ccc(Br)cc2)cc1
Brc1ccc(Nc2ccc3ccccc3c2)cc1
Brc1ccc(Nc2cccc(Br)c2)cc1
Fine tuning...
Mean value of predictions: 0.19948348
Proportion of valid SMILES: 0.6053783614759225
Sample trajectories:
B[PH](=O)(Nc1ccccc1)(C(=O)NCC(P(=O)(O)O)P(=O)(O)O)c1ccc(Br)cc1
BrC=CC=Cc1ccc(Br)cc1
Brc1cc(Br)cc(Nc2ccc(Nc3cccc4ccc(I)cc34)cc2Br)c1
Brc1cc2c(Nc3cccnc3)ncnc2cc1Nc1ncnc2ccccc12
Brc1ccc(-c2ccc(Br)c3ccccc23)cc1

  6 Training on 3508 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 24.213381
Reward: 1.973468
Trajectories with max counts:
28	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.2535211
Proportion of valid SMILES: 0.5772357723577236
Sample trajectories:
Brc1cc(Br)c2ccc(NN=Cc3ccccc3)cc2n1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccccc23)ccn1
Brc1cc2c(-c3ccccc3)c[nH]c2c(-c2ccc3ccccc3c2)n1
Brc1cc2c(nc3ccc(Nc4cccc(-c5ccccc5)c4)nc3n1)-c1ccccc1-2
Policy gradient replay...
Mean value of predictions: 0.29322752
Proportion of valid SMILES: 0.5911792305286205
Sample trajectories:
BP(=O)(OCC1OC(n2cc(O)c3c(N)ncnc32)C(O)C1O)OP(=O)(O)Oc1ccccc1
Brc1cc(Br)c2c(Nc3cccs3)ncnc2c1
Brc1ccc(-c2n[nH]c3ccc(Br)cc23)cc1
Brc1ccc(I)cc1
Brc1ccc(Nc2cc(Nc3cccc4ccccc34)ncn2)cc1Br
Fine tuning...
Mean value of predictions: 0.23951142
Proportion of valid SMILES: 0.5889896778229591
Sample trajectories:
Brc1cc(Br)c2cccc(Br)c2c1
Brc1cc(Br)cc(Nc2ccc(Nc3ncnc4ccc(I)cc34)s2)c1
Brc1cc2ncn-2c1-c1ccccc1
Brc1cc2ncnc(Nc3cc[nH]n3)c2cc1Br
Brc1ccc(-c2ccnc3c2-c2cc(Br)ccc2N3)cc1

  7 Training on 5047 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 26.704162
Reward: 2.648799
Trajectories with max counts:
49	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.32705742
Proportion of valid SMILES: 0.5283077885517673
Sample trajectories:
BP(=O)(OC(C)C)c1ccc(Nc2ncnc(Nc3cccs3)n2)cc1
BrC(Br)=NNc1ccc(Br)o1
Brc1cc(Br)cc(Sc2ccc3ncncc3c2)c1
Brc1ccc(Br)c(Nc2ncnc3cc(Br)sc23)c1
Brc1ccc(Nc2nccc(NCc3c(Br)ccc4ncsc34)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.32651475
Proportion of valid SMILES: 0.562363238512035
Sample trajectories:
Brc1cc2ncnc(Nc3cc(Br)c(Br)cc3I)n2c1
Brc1cc2ncnc(Nc3ccc4c(c3)CCCC4)n2c1Br
Brc1cc2ncnc(Nc3ccnc4ccccc34)c2cc1OCCCN1CCCCC1
Brc1ccc(N(Cc2ccc(Br)cn2)Cc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4cccc(Br)c4)c3ncn2)cc1
Fine tuning...
Mean value of predictions: 0.30682647
Proportion of valid SMILES: 0.6010021922956468
Sample trajectories:
Brc1ccc(Br)c(Br)c1
Brc1ccc(CNc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2cc(-c3ccccc3)nc3ccccc23)cc1
Brc1ccc(Nc2cncc(Br)c2)cc1
Brc1ccc(Nc2nc3ccc(Br)cc3s2)c(Br)c1

  8 Training on 6791 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 27.127326
Reward: 3.154857
Trajectories with max counts:
193	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.33799598
Proportion of valid SMILES: 0.4646875
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)c1cc(Br)cc(Br)c1O
Brc1cc2ncncc2cc1Nc1ncnc2sc(Nc3ccccc3)cc12
Brc1ccc(-c2cc3ncnc(Nc4ccccc4)c3cc2-c2ccccc2)o1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1ccc(CNc2ccc(-c3ncnc4ccccc34)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.3332588
Proportion of valid SMILES: 0.5599374021909234
Sample trajectories:
Brc1ccc(COc2ccc(Br)cn2)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4cccc(I)c4)sc3c2)cc1
Brc1ccc(Nc2ccnc3cc(Br)c(Br)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.3217656
Proportion of valid SMILES: 0.6161300406376993
Sample trajectories:
Brc1ccc(-c2nc(-c3ccc4ccccc4c3)n(-c3ccccc3Br)n2)cc1
Brc1ccc(Br)c(Nc2ncnc3cc(Br)nn23)c1
Brc1ccc(NCc2c(Br)cc(Br)c(I)c2Br)s1
Brc1ccc(Nc2ccc(Nc3ncnc4ccc(Br)cc4N3)cc2)cc1
Brc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1

  9 Training on 8448 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 25.220006
Reward: 2.759231
Trajectories with max counts:
34	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4389246
Proportion of valid SMILES: 0.5353566958698373
Sample trajectories:
BP(=O)(OCC)c1cc(Br)c(NC(=O)Nc2ccc(Br)c(Br)c2F)c(Br)c1
Brc1cc(Nc2ncnc3cc(Br)sc23)c(Br)s1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Brc1cc2ncnc(Nc3ccc(Br)s3)n2n1
Brc1ccc(-c2ccsc2)c2scnc12
Policy gradient replay...
Mean value of predictions: 0.35112274
Proportion of valid SMILES: 0.5988117573483427
Sample trajectories:
BrCN1CCCCCC1c1nc(Nc2ncnc3ccccc23)cs1
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)cc(Nc2ccnc3ccc(Br)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3cnc(Nc4cccnc4)cc23)c1
Brc1ccc(-c2cnc3cnc(Nc4ccc5nnnn5c4)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.3589876
Proportion of valid SMILES: 0.6053783614759225
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3ccccc23)nc(Nc2cc3ccncc3c3ccccc23)c1
Brc1cc2ncnc(Nc3ccccc3)n2n1
Brc1cc2ncnc(Nc3cccnc3)c2cc1Br

 10 Training on 9792 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.193217
Reward: 2.677699
Trajectories with max counts:
49	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.3783814
Proportion of valid SMILES: 0.5653400188028831
Sample trajectories:
BP(=O)(CS)NC(CCCN(C(=O)NO)S(=O)(=O)O)S(=O)(=O)O
BP(=O)(Nc1cnc(Br)c(Br)c1)Nc1cccc(Br)n1
BP(=O)(c1ccc(NS(=O)(=O)Oc2cc(Cl)c(Cl)cc2F)cc1)N(C(=O)OCC)C(F)(F)F
Brc1cc(Br)c(Nc2ccc3ncnc(Nc4cc(Br)cnc4Br)c3c2)c(Br)c1
Brc1cc(Br)c2c(Br)c(Br)c(-c3cscc3Br)n2c1
Policy gradient replay...
Mean value of predictions: 0.35727495
Proportion of valid SMILES: 0.6421875
Sample trajectories:
B[PH](=O)(NO)(Nc1cc(Br)c(Br)cc1F)c1ccc(F)cc1
Brc1cc(Br)c2c(Br)cccc2c1Nc1ccccc1
Brc1cc2c(Nc3ccncc3)Nc3ccccc3Nc3ccc(ncnc2s1)c(Br)c3
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.385561
Proportion of valid SMILES: 0.6410256410256411
Sample trajectories:
BP(=O)(OCC)OC(=O)CSCI
Bc1cc(Br)c2ncnc(-c3cccc(Br)c3)c2n1
BrC(Cn1cncn1)Nc1ccnc2ccccc12
BrCBr
Brc1c2c(nc3cccnc13)CCO2

 11 Training on 11282 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.498650
Reward: 2.793227
Trajectories with max counts:
26	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.47879124
Proportion of valid SMILES: 0.5701754385964912
Sample trajectories:
BP(=O)(NCCCOC)c1cc(Br)c(Br)c(Br)c1
BrBr
BrCCN1c2ncnc(Br)c2Nc2ncnc(Nc3ccc(Br)c(Br)c3)c21
Brc1c[nH]c(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.45151016
Proportion of valid SMILES: 0.5690625
Sample trajectories:
BP(=O)(Br)OCCCCCBr
BP(=O)(Nc1ccc(Nc2nc3c(Br)cc(Br)c(Br)c3s2)cc1)N1CC1
BP(=O)(OCCCn1cnc2c(NCCCCCCO)ncnc21)OP(=O)(O)Oc1c(F)cc(F)c(F)c1F
BrCCNc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
BrSC(=Nc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1)c1cccc(Br)c1
Fine tuning...
Mean value of predictions: 0.41295803
Proportion of valid SMILES: 0.5888055034396498
Sample trajectories:
BP(=O)(NCCO)C(F)(F)F
BP(=O)(OCC(=O)N1C(=O)N(C(=O)C(N)Cc2ccc(Br)cc2)C1CCl)C(=O)C1CCCCC1
BrSc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Brc1cc(-c2nccnc2Br)ncn1
Brc1cc(Br)c(Br)c(Sc2ccccc2-c2ccccc2)c1

 12 Training on 13038 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.471565
Reward: 3.712239
Trajectories with max counts:
309	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4833971
Proportion of valid SMILES: 0.4088207694713794
Sample trajectories:
BBr
BP(=O)(Br)OP(=O)(OC)OC(=O)CBr
BP(=O)(NC(=O)CCC(C#CCCl)Nc1cc(Cl)c(Br)cc1F)OCC
BP(=O)(NC(C(=O)CCC)C(=O)N(C)C)S(=O)(=O)c1ccc2c(Nc3c(F)cc(F)c(F)c3F)ccnc2c1
BP(=O)(NCCO)Nc1cccc(Br)c1Cl
Policy gradient replay...
Mean value of predictions: 0.43723917
Proportion of valid SMILES: 0.5844277673545967
Sample trajectories:
B=C(Sc1nc2cccnc2s1)c1cc2ccccc2s1
BP(=O)(CCC=C(Br)Br)OCC
BP(=O)(OCC)OCCC=C(Br)Br
BP(=O)(OP(=O)(O)CCCl)C(=O)Nc1ccc(F)cc1F
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.44032788
Proportion of valid SMILES: 0.571875
Sample trajectories:
BP(=O)(Oc1cc2cc(Br)c(Br)cc2s1)P(Br)Br
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1

 13 Training on 14670 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.987592
Reward: 3.282719
Trajectories with max counts:
30	Nc1cc2ncnc(Nc3ccc(Br)cc3F)c2s1
Mean value of predictions: 0.5490798
Proportion of valid SMILES: 0.509375
Sample trajectories:
BP(=O)(N=C(NO)c1cc2cnc(Nc3cc(Br)cnc3O)cc2s1)OCC
Brc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)cnc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(CNc4nc5cncn5c5nnc(Nc6cccc(Br)c6)n45)sc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.36458853
Proportion of valid SMILES: 0.6267583619881213
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCOP(=O)(O)OP(=O)(O)OP(=O)(O)O
Br
BrC(Br)=C(CNc1ccnc2cc(Br)ccc12)c1ccccc1Br
BrCc1ccc(N2CCN(Cc3cc(Nc4ccc(Br)s4)ncn3)CC2)cc1
Brc1cc(Br)cc(Nc2ncnc3ccc4ccccc4c23)c1
Fine tuning...
Mean value of predictions: 0.4222668
Proportion of valid SMILES: 0.623125
Sample trajectories:
BP(=O)(CCl)NP(=O)(OC(C)=O)C(=O)NO
BP(=O)(N=C(C)CC(=O)Oc1ccc(Br)cc1)OCC
Bc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Oc1cccc2ncnc(Nc3ccc(Br)cc3)c12
Bc1ccc(Nc2cc(Br)cc(Br)c2)cc1-c1cc(Br)cc(Br)c1O
Bc1cccc(Nc2ncnc3ccccc23)c1

 14 Training on 16466 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.901995
Reward: 3.463344
Trajectories with max counts:
54	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5643545
Proportion of valid SMILES: 0.6177666562402252
Sample trajectories:
BP(=O)(OCC)OC(=O)CN(CCP(=O)(O)CCBr)OP(F)(F)(F)F
Bc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
BrCCBr
BrCc1ccc2ncnc(Nc3ccc(CN4CCCC4)nc3)c2c1
Brc1cc(Br)c(Nc2nc3c(s2)sc2cc(Br)c(Br)cc23)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.49427354
Proportion of valid SMILES: 0.66051891216005
Sample trajectories:
BP(=O)(OCC)C(=O)N1CCC(CCBr)=Nc2sc3c(c2C1)CCCCC3
BP(=O)(OCC)OCC=C
Brc1cc(Nc2ncnc3ccc(-c4ccc(Br)s4)cc23)cs1
Brc1cc2N(c3ccccc3)CCCN2c2sccc2c1Br
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2cn1
Fine tuning...
Mean value of predictions: 0.48008153
Proportion of valid SMILES: 0.6136292591434823
Sample trajectories:
Brc1cc(Br)c(NCCSc2nc3ncncc3s2)c(Br)c1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2ncnc(Nc3cc(Br)c(Br)s3)c2cc1Br
Brc1cc2ncnc(Nc3ccc(NC4CCCC4)cc3)c2s1

 15 Training on 18832 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 30.960639
Reward: 3.880085
Trajectories with max counts:
123	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.537092
Proportion of valid SMILES: 0.5268918073796123
Sample trajectories:
B[PH](=O)(Nc1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O
Bc1cc(Br)c(Br)cc1Br
Bc1cc(Br)cc(Br)c1NP(=O)(OCCCCCC)Oc1cccc(NC(=O)Nc2cc(Br)c(Br)c(Br)c2Br)c1
Bc1cc2ncnc(Nc3ccc(Br)s3)c2cc1Br
Brc1cc(Br)c(-c2ccc(Nc3ncnc4ccsc34)cc2)c(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.5532152
Proportion of valid SMILES: 0.6181533646322379
Sample trajectories:
BP(=O)(CC(F)(F)F)OCC
B[PH](=O)(NC(c1ccc(Br)cc1)P(Br)Br)=C(Br)Br
BrCCCCCCNc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1Br
Brc1cc(Br)c(Nc2ccsc2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.51203704
Proportion of valid SMILES: 0.6075
Sample trajectories:
BP(=O)(OCC)OC(=O)CSCC=C(Br)P(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCCS(=O)(=O)ON1CCOCC1)C(=O)NO
BP(=O)(c1cccc2ncnc(Nc3ccc(Cl)c(Cl)c3)c12)N(O)Cc1cccc2ccccc12
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c2ncnc(Nc3ccccc3)c2c1

 16 Training on 21090 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.159895
Reward: 3.854462
Trajectories with max counts:
19	CC(=O)Nc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5754596
Proportion of valid SMILES: 0.6802125664270084
Sample trajectories:
BP(=O)(OCC1OC(Nc2cc3cc(Br)ccc3nc2N)C(O)C1OP(=O)(O)O)c1ccncc1
BrCCNc1cc2ncnc(Nc3ccccc3Br)c2cc1Br
Brc1cc(Br)c2c(c1)C=Nc1sccc12
Brc1cc2ncnc(Nc3ccccc3)c2s1
Brc1cc2ncnc(Nc3cccs3)c2s1
Policy gradient replay...
Mean value of predictions: 0.5636271
Proportion of valid SMILES: 0.6138211382113821
Sample trajectories:
BC(=O)Nc1cc(Br)c(Br)cc1Cl
BP(=O)(Nc1ccc(Br)cc1)Oc1cc(Br)ccc1O
BP(=O)(Nc1ccc(Br)cc1F)Oc1cccc(F)c1
BP(=O)(OCC)C(=O)C(C)(Cl)Br
BP(=O)(OCC)OCCC(=O)Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Fine tuning...
Mean value of predictions: 0.5386869
Proportion of valid SMILES: 0.6197183098591549
Sample trajectories:
Br
BrC=CBr
BrC=CC1=Nc2sc3c(c21)CCCCC3
Brc1cc(Br)cc(Nc2ncnc3c(Br)cc(Br)cc23)c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1

 17 Training on 23837 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.051633
Reward: 4.026634
Trajectories with max counts:
42	Nc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Mean value of predictions: 0.6045249
Proportion of valid SMILES: 0.5528455284552846
Sample trajectories:
BP(=O)(Cn1cnc(Nc2ccc(Br)c(Br)c2F)n1)C(F)(F)P(=O)(O)O
BrCCNc1nc2c(Br)ncnc2s1
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.60711426
Proportion of valid SMILES: 0.62375
Sample trajectories:
BP(=O)(NC(Cc1cccc(Br)c1)P(=O)(O)O)C(N)=O
BP(=O)(OCCS)C(=O)OCO
BP(=O)(Oc1ccc2ncnc(Br)c2c1O)c1cccc2ccccc12
Bc1cc2ncnc(Nc3ccc(Br)c(Cl)c3)c2c(Br)c1Br
BrCc1cc2ncnc(Nc3cc(Br)cc(Br)c3)c2s1
Fine tuning...
Mean value of predictions: 0.56480557
Proportion of valid SMILES: 0.6274632467938692
Sample trajectories:
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BrNc1cnc2nc(Nc3cccnc3)cnc2c1
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(Br)c1
Brc1cc2c(Nc3cccs3)ncnc2s1
Brc1cc2sc3c(Br)ccc(Br)c3c2s1

 18 Training on 26620 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.437901
Reward: 4.434495
Trajectories with max counts:
50	Nc1cc2ncnc(Nc3ccc(Br)cc3F)c2s1
Mean value of predictions: 0.6775029
Proportion of valid SMILES: 0.555625
Sample trajectories:
Bc1cc2ncnc(Nc3ccc(Cl)cc3)c2s1
Bc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)c(-c2cc3ncnc(Nc4ccc(Br)c(Br)c4)c3s2)c(Br)c1
Brc1cc(Br)c(Br)[nH]1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2n1
Policy gradient replay...
Mean value of predictions: 0.63357824
Proportion of valid SMILES: 0.5959974984365228
Sample trajectories:
BP(=O)(C(=O)Oc1ccc(Br)cc1)N1CCN(CC(=O)Nc2cc(Br)c(Cl)c(Br)c2O)CC1
BP(=O)(OC(=O)CBr)C(O)C(N)CC=O
BP(=O)(c1ccc(Br)cc1)c1ccc(Br)cc1
BP(=O)(c1ccc(Cl)cc1)c1ccc(Br)cc1
Bc1cc2ncnc(Nc3ccc(Br)cc3F)n2n1
Fine tuning...
Mean value of predictions: 0.5470763
Proportion of valid SMILES: 0.6312167657178605
Sample trajectories:
BrC(=NNc1ccccc1)c1ccc(Br)cc1
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Brc1cc2c(Nc3ccccc3)ncnc2cc1NCCc1ccccc1

 19 Training on 29517 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.357644
Reward: 4.675105
Trajectories with max counts:
43	Nc1cc2ncnc(Nc3ccc(F)c(F)c3F)c2s1
Mean value of predictions: 0.6591928
Proportion of valid SMILES: 0.487964989059081
Sample trajectories:
BP(=O)(F)(F)(F)P(=O)(O)OP(=O)(O)OCCl
BP(=O)(Nc1ccc(Br)cc1)P(=O)(Oc1ccc(Br)cc1F)N1CCSS1
Bc1cc(Br)cc(Br)c1Nc1cc2ncnc(Nc3ccc(Br)cc3Br)c2s1
BrCC(Nc1nc2ncnc(Nc3cccc(Br)c3)c2nc1Nc1c(Nc2cc(Br)c(Br)cc2Br)c2cc(Br)ccc12)N1CCCC1
BrCCNc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Policy gradient replay...
Mean value of predictions: 0.5600976
Proportion of valid SMILES: 0.640625
Sample trajectories:
Bc1ccc(Br)c(Nc2ncnc3scnc23)c1
BrC1CCCCCN1
BrCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Oc2cc(Br)ccc2Br)c(Nc2ccc(Br)c(Br)c2)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.5851888
Proportion of valid SMILES: 0.62875
Sample trajectories:
BP(=O)(Nc1cccc(N)c1)P(=O)(OCC)OP(=O)(O)OP(=O)(O)O
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Nc2ncnc3ccccc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccccc3)ncnc2c1

 20 Training on 32187 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.544629
Reward: 4.587681
Trajectories with max counts:
89	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.6221203
Proportion of valid SMILES: 0.6434896687438505
Sample trajectories:
BP(=O)(CCCCC)NO
BP(=O)(CCCCCCCCCCCCCCCCCCCCCCCCCC(NC(=O)OC(C)(C)C)OCP(=O)(O)O)C(=O)NS(=O)(=O)CC(=O)O
BP(=O)(CCCN)Nc1cc2ncnc(Nc3cccc(Br)c3)c2s1
BP(=O)(OCC1OC(=N)C(Cl)=C(Br)C1Br)P(Br)Br
Bc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.67076
Proportion of valid SMILES: 0.6211316036261332
Sample trajectories:
BP(=O)(Oc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1)c1ccc(Br)cc1
BrCCNc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
BrCc1ccc(Nc2ncnc3cc(Br)sc23)cc1
BrCc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.59543073
Proportion of valid SMILES: 0.6569731081926203
Sample trajectories:
BrC=Cc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(-c2ccncc2)c2ncnc(Nc3ccc(Br)s3)c2c1
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)cc(Nc2ccc(Br)c(Nc3c4ccccc4nc4ccccc34)c2)c1

Trajectories with max counts:
191	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.55820125
Proportion of valid SMILES: 0.5563199699793608
Mean Internal Similarity: 0.48492796568545704
Std Internal Similarity: 0.0990081415673887
Mean External Similarity: 0.4209525410656187
Std External Similarity: 0.07516536171944184
Mean MolWt: 437.36867480530424
Std MolWt: 119.59716508117404
Effect MolWt: -0.5702106139908616
Mean MolLogP: 5.4687796484950555
Std MolLogP: 2.175768735889909
Effect MolLogP: 0.40147748690063073
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 96.854764% (1047 / 1081)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 20, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5586.217666149139, 'valid_fraction': 0.5563199699793608, 'active_fraction': 0.5341202922990445, 'max_counts': 191, 'mean_internal_similarity': 0.48492796568545704, 'std_internal_similarity': 0.0990081415673887, 'mean_external_similarity': 0.4209525410656187, 'std_external_similarity': 0.07516536171944184, 'mean_MolWt': 437.36867480530424, 'std_MolWt': 119.59716508117404, 'effect_MolWt': -0.5702106139908616, 'mean_MolLogP': 5.4687796484950555, 'std_MolLogP': 2.175768735889909, 'effect_MolLogP': 0.40147748690063073, 'generated_scaffolds': 1081, 'novel_scaffolds': 1047, 'novel_fraction': 0.9685476410730804, 'save_path': '../logs/replay_combo_s1-5.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.761316
Reward: 1.000000
Trajectories with max counts:
2	Cc1cccc(O)c1
Mean value of predictions: 0.00081103
Proportion of valid SMILES: 0.7766929133858268
Sample trajectories:
Brc1ccc(Nc2ncnc3nc(Br)nn23)cc1
C#CC(=O)N1CCN(CCOC(=O)c2ccccc2)C(C)C1
C#CC(CCC(=C)CCCCC#N)c1cn(O)c2c1C(C)(C)NC2(C)C
C#CCCCCCC1C(C)CCCN1CCC#N
C#CCCCCCCCC(=O)N1CCC(P(=O)(O)COC)CC1
Policy gradient replay...
Mean value of predictions: 0.012785388
Proportion of valid SMILES: 0.6169014084507042
Sample trajectories:
Brc1cc2c(c3c1CCNC3)OCCO2
Brc1ccc(NN=C2CCCCCC2)cc1
Brc1ccc(Nc2ccnc3ncnc(Nc4cccc(Br)c4)c23)cc1
Brc1ccc(Nc2ncnc3ncncc23)nc1
Brc1ccc(Nc2onc(-c3ccccc3)c2-c2ccc3[nH]ccc3n2)cc1
Fine tuning...
Mean value of predictions: 0.039248433
Proportion of valid SMILES: 0.5996870109546166
Sample trajectories:
Brc1cc(-c2nc(-c3ccccc3)cs2)sc1-c1nc2ccccc2nc1-c1ccncc1
Brc1cc2ncnc(Nc3ccc[nH]3)c2cc1Br
Brc1ccc(C2=COc3ccccc3O2)cc1
Brc1ccc(Nc2nc(Br)cs2)cc1
Brc1ccc(Nc2ncnc3cc(Br)cc(Nc4ccccc4)c23)cc1

  2 Training on 375 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.901193
Reward: 1.167319
Trajectories with max counts:
10	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.05042644
Proportion of valid SMILES: 0.5871674491392801
Sample trajectories:
BP(=O)(OCC)C(=O)OCC(c1nc(N)n(N)n1)N(CC(=O)OC(C)(C)C)c1ccc2c(c1)OCCO2
BP(=O)(OCC1CCC(N)C(OP(=O)(O)O)C1)n1cnc2c(N)ncnc21
BP(=O)(OCC1OC(OP(=O)(O)O)C(O)C1O)n1cnc2c(N=C(N)NO)cccc21
Brc1cc2c(cc1-c1cccnc1)C(c1ccncc1)=N2
Brc1ccc(-c2ccccn2)c2occc12
Policy gradient replay...
Mean value of predictions: 0.090051025
Proportion of valid SMILES: 0.49230769230769234
Sample trajectories:
BrCN1CCN(Cc2cncn2-c2ccnc3ccc(Br)cc23)CC1
Brc1cc(Br)nc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2ncnc(-c3ccncc3)c2cc1N1CCCCC1
Brc1cc2ncnc(N3CCC3)c2nc1-c1cnn(-n2nccn2)c1
Brc1cc2ncnc(Nc3cccnc3)c2cc1N1CCN(CC=Cc2cccnc2)CC1
Fine tuning...
Mean value of predictions: 0.12632795
Proportion of valid SMILES: 0.5437990580847724
Sample trajectories:
Brc1ccc(CN(SCc2cnc3ccccc3c2)C2CCCN2)cc1
Brc1ccc(Nc2ncnc3[nH]cnc23)cc1
Brc1ccc(Nc2ncnc3c4sccc4c23)cn1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)cnc23)cc1

  3 Training on 1028 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 20.665664
Reward: 1.380544
Trajectories with max counts:
17	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.15365185
Proportion of valid SMILES: 0.5695964967156709
Sample trajectories:
Brc1cc2c(Nc3nc4cncnc4s3)ncnc2cc1OCCCN1CCCCC1
Brc1cc2n(nc3ccccc13)-c1ccccc1N2
Brc1ccc(Br)c(Nc2ncc3ccccc3n2)c1
Brc1ccc(C2CCN(c3ncnc(Nc4ccc5ccccc5c4)n3)C2)cc1
Brc1ccc(Nc2nc(Nc3ccccc3)nc(-c3ccccc3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.21301568
Proportion of valid SMILES: 0.5393293638357881
Sample trajectories:
Brc1cc(Br)c2c(Nc3ncc(Br)c(-c4nn[nH]n4)n3)ncnc2c1
Brc1cc2[nH]c3ncnc(NCc4ccccc4)c3c2s1
Brc1cc2ncnc(Nc3ccc4c(c3)OCCCO4)c2cc1Br
Brc1cc2nnc(Nc3ccsc3)nc2cc1Br
Brc1ccc(Nc2c3cnccc3sc3ncnc23)cc1
Fine tuning...
Mean value of predictions: 0.20587619
Proportion of valid SMILES: 0.5976795233615554
Sample trajectories:
BP(=O)(OCOC(=O)c1cccc(F)c1)P(=O)(OC(=O)Oc1ccc(Cl)cc1)OC(C)C
BrSc1nccc(Nc2ccc(Br)s2)n1
Brc1cc(Br)c2c(Nc3ccncc3)ncc(Br)c2n1
Brc1cc2c(Nc3ccncc3)ncnc2ncnc1-c1ncn[nH]1
Brc1cc2ncnc(N3CC4CCC43)n2n1

  4 Training on 2418 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 23.175680
Reward: 1.925289
Trajectories with max counts:
39	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.22759724
Proportion of valid SMILES: 0.5867458580806502
Sample trajectories:
BP(=O)(OCCCn1cnc(-c2cc(Cl)ccn2)c1F)N1C=CC=CC1=O
Bc1ccc(Nc2ncnc3ccccc23)cc1
BrCCN1CCCC1CNc1cc2ncnc(Nc3ccccc3Br)n2n1
Brc1cc(-c2ccccc2)c2ncnn2c1Nc1ccccc1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)c4ccccc4c(-c4ccccc4)n23)c1
Policy gradient replay...
Mean value of predictions: 0.2426212
Proportion of valid SMILES: 0.5865625
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1ccc(Cl)c(Br)c1
Bc1cc(Nc2ncnc3ccc(Br)cc23)ccc1NCc1cccnc1
Brc1cc(Br)c2nc(-c3ccccc3)c(Nc3ncc(Br)c(-c4ccccc4)n3)cc2c1
Brc1ccc(-c2cc(Nc3ccccc3Nc3cncnc3)ccc2Br)cc1
Brc1ccc(-c2ncnc3ccccc23)c2ccccc12
Fine tuning...
Mean value of predictions: 0.27103278
Proportion of valid SMILES: 0.6208945886768846
Sample trajectories:
BrC1CCCCSCCCCN1
Brc1ccc(-c2nc3ncnc(Nc4ccc(Br)s4)c3s2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(CN2CCN(c3ccsc3)CC2)nc1
Brc1ccc(I)cc1Br

  5 Training on 4027 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 23.177758
Reward: 2.168209
Trajectories with max counts:
80	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.25390837
Proportion of valid SMILES: 0.5796875
Sample trajectories:
BrCCNc1ncccc1Nc1cccc2ccccc12
Brc1cc(Nc2ncnc3ccccc23)c2ccccc2n1
Brc1cc2c(Br)cccc2o1
Brc1cc2c(Nc3cccc4c(Nc5ccccc5Br)ncnc4cc3)ncnc2s1
Brc1ccc(CN2CCN(Cc3ccc(Br)cc3)CC2)cc1
Policy gradient replay...
Mean value of predictions: 0.27850056
Proportion of valid SMILES: 0.5672295184490307
Sample trajectories:
BrC=CC=Cc1cccc(Br)c1
Brc1cc(Br)c(-c2ncnc3cncnc23)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)c(Br)c23)c1
Brc1cc(Nc2ncnc3cc(Br)c(Br)cc23)c(Br)s1
Brc1cc2c(Nc3ccccc3)ncnc2cc1N1CCNCC1
Fine tuning...
Mean value of predictions: 0.33329684
Proportion of valid SMILES: 0.5718759787034137
Sample trajectories:
BP(=O)(NO)c1ccc(Nc2ccc(F)c(Cl)c2)cc1Cl
BrC1CCCN1c1cncnc1
BrCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(-c2ccc3c(c2)OCCO3)c(Br)o1
Brc1cc(Br)c(Br)c(Br)c1Br

  6 Training on 5718 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 24.725076
Reward: 2.541218
Trajectories with max counts:
26	Fc1ccc(Nc2ncnc3ccc(F)cc23)cc1
Mean value of predictions: 0.35473743
Proportion of valid SMILES: 0.5777291210509853
Sample trajectories:
BrCCCCCNc1cc(Br)ccc1Br
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(I)ccc1Nc1ncnc2cc(Br)c(Br)c(Br)c12
Brc1ccc(-c2nc3ccc(Br)cc3[nH]2)cc1
Brc1ccc(-c2ncnc3c(Br)noc23)cc1
Policy gradient replay...
Mean value of predictions: 0.33768517
Proportion of valid SMILES: 0.571831869510665
Sample trajectories:
BrCCNc1ncnc2ncnc(Nc3ccc(Br)c(Br)c3)c12
Brc1cc(Br)c(Br)c(Nc2ncnc3nnnn23)c1
Brc1cc(Br)c(Nc2ncnc3sc(Nc4ccc(Br)c(Br)c4)nc23)c(Br)c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)s3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.35559025
Proportion of valid SMILES: 0.6013133208255159
Sample trajectories:
Bc1cc(Br)cc(Br)c1Nc1ncnc2sc(C3CC3)nc12
Brc1cc(-c2ncnc3ccccc23)c2cccnc2n1
Brc1cc(Br)c(Nc2cc3ncnc(Nc4ccnc5c(Br)csc45)c3s2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(I)cc3)ncnc2c1

  7 Training on 7670 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 26.271602
Reward: 3.110544
Trajectories with max counts:
96	Fc1ccc(Nc2ncnc3ccc(F)cc23)cc1
Mean value of predictions: 0.36706665
Proportion of valid SMILES: 0.46904315196998125
Sample trajectories:
BP(=O)(OCCO)N(=O)=O
BrCc1nc(N(Nc2ccc(Br)cc2)c2ccccc2)cs1
Brc1cc(Br)c2ncnc(Nc3c(Br)cccc3Br)c2c1
Brc1ccc(Br)c(Br)c1
Brc1ccc(C2c3cc(n4cncn4)c(Nc4ccc(Br)cn4)ncnc32)cc1
Policy gradient replay...
Mean value of predictions: 0.43804464
Proportion of valid SMILES: 0.5894143438772315
Sample trajectories:
BrCCNc1nc(Nc2cncc3ccccc23)c2cnc(Nc3ccc(Br)cc3)ncc2o1
Brc1cc(Br)c2c(Nc3ncc(Br)c(Br)n3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Nc2ncnc3cc(Nc4ccccc4Br)sc23)c(Br)s1
Brc1cc2ncnc(Nc3ccc(Br)s3)c2cn1
Fine tuning...
Mean value of predictions: 0.40978202
Proportion of valid SMILES: 0.5885481852315394
Sample trajectories:
BP(=O)(OC(C)(C)S(=O)(=O)C(C)OC(C)(C)C)N(c1ccnc(Nc2ccc(F)cc2F)c1)P(=O)(OCC)OCC
BrC(=CC(Nc1ncnc2cc(Br)ccc12)c1ccc(Br)cc1)c1cc(Br)cc(Br)c1
Brc1cc(Br)c(Nc2ncnc3ccc(Br)cc23)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

  8 Training on 9751 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 26.063186
Reward: 3.072521
Trajectories with max counts:
30	Fc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.49376416
Proportion of valid SMILES: 0.5517672818267125
Sample trajectories:
BP(F)(F)(F)N1CCN(c2nc3ncc(Cl)c(Nc4ccc(F)cc4)c3cc2N)CC1
Bc1cnc(Nc2ncnc3cc(Br)cc(Br)c23)c2ncnc(Nc3cccc(Br)c3)nc2c1
BrC1=CN(Nc2ccc(Br)cc2)c2ccc(Br)nc2C=C1
BrCCNc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1Br
BrCCNc1ncnc2sccc12
Policy gradient replay...
Mean value of predictions: 0.45292243
Proportion of valid SMILES: 0.5883088465145357
Sample trajectories:
BP(=O)(NS(=O)(=O)Nc1ccc(Br)cc1)(ON[PH](=NS(=O)(=O)Oc1ccc(F)c(F)c1)Nc1ccc(Br)cc1)(N(=O)=O)S(N)(=O)=O
BrC1=CN2CCCC2C2CCCN1C2
BrCOc1ccc(Br)cc1-c1cc2ncnc(Nc3ccc(Br)cc3)c2cn1
Brc1cc(Br)c(C(Br)(Br)Br)cc1Nc1ncnc2c(Nc3ccccc3)cc12
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.42441747
Proportion of valid SMILES: 0.6170678336980306
Sample trajectories:
Brc1cc(Br)c2c(Nc3cc(Nc4ccccc4Br)ncn3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(-n4cncc4Br)cc3)ncnc2c1
Brc1cc(Br)c2c(c1)[S+]CCC2
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)c2ncnc(Nc3ccsc3)c2c1

  9 Training on 11458 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.615969
Reward: 3.115318
Trajectories with max counts:
21	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.53858185
Proportion of valid SMILES: 0.600125156445557
Sample trajectories:
BP1(=O)OCCS(=O)(=O)ON(Cc2[nH]c(CBr)nc2Nc2cc(F)c(F)c(F)c2)s1
Brc1c[nH]c2nc3c(ccc4ncc(Br)cc43)Nc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ncc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(-c4ccc5ncnc(Nc6cccc(Br)c6)c5n4)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.46002004
Proportion of valid SMILES: 0.62375
Sample trajectories:
B[PH](=O)(Nc1ccc(Br)cc1)=C(Br)Br
BrCCNc1nc2c(nc1-n1cnc3c(Nc4ccccc4)ncnc31)sc1c(-c3cccc(Br)c3)cccc12
BrN=CC=NNCC=CC=CCNc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.4365507
Proportion of valid SMILES: 0.6380118787120975
Sample trajectories:
BP(=O)(COc1ccc(Nc2ncnc3cc(Cl)cc(Cl)c23)cc1)Nc1cccc(Nc2ccccc2)n1
BP(=O)(OCC=C)c1ccc(Br)cc1
Bc1ccc(Nc2ncnc3ncnc(Br)c23)cc1Br
Brc1cc(Br)c2c(Nc3cc(Br)c(Br)cn3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1

 10 Training on 13452 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.653575
Reward: 3.342641
Trajectories with max counts:
24	Oc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Mean value of predictions: 0.57066816
Proportion of valid SMILES: 0.551875
Sample trajectories:
Bc1ccc(NS(=O)(=O)c2ccc(Nc3cc(Br)c(Br)s3)nc2)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc(Nc2ncnc3sc(Br)cc23)cc1
BrCc1cc(Nc2ncnc3cc(Br)sc23)ccc1Br
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.5369247
Proportion of valid SMILES: 0.5984350547730829
Sample trajectories:
BC(=O)Oc1cc(Br)cc(Br)c1O
BP(=O)(OCC)Oc1nc(Nc2c(Cl)ccc(Br)c2F)ccc1Br
BrC1=CN=C(c2ccc(Br)cc2)Nc2[nH]cnc2N1
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)c(Nc2cnc3ncnc(Nc4ccc(Br)c(Br)c4)c3c2)c(Br)c1
Fine tuning...
Mean value of predictions: 0.48391327
Proportion of valid SMILES: 0.6198812128790246
Sample trajectories:
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccsc23)c1
Brc1cc(Br)c(Nc2ncnc3cnccc23)cc1Br
Brc1cc(Br)cc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1

 11 Training on 15682 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 28.728668
Reward: 3.683449
Trajectories with max counts:
40	Fc1cc2ncnc(Nc3ccc(F)c(F)c3)c2s1
Mean value of predictions: 0.5648566
Proportion of valid SMILES: 0.5123476086276961
Sample trajectories:
BP(=O)(O)COP(=O)(O)Oc1ccc(F)c(F)c1
Bc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Bc1cc2ncnc(Nc3ccc(Br)c(Cl)c3)c2s1
BrCc1cc2ncnc(Nc3cc(Br)c(Br)s3)n2n1
Brc1cc(Br)c(Br)s1
Policy gradient replay...
Mean value of predictions: 0.5388803
Proportion of valid SMILES: 0.6084375
Sample trajectories:
BP(=O)(Nc1nc2ccc(Br)cc2s1)OCCCCC
BP(=O)(Nc1sc(Cl)cc1F)C(N)=O
B[PH](=O)(OCC)=C(Nc1ccnc(Nc2cccc(F)c2)n1)c1ccc(F)c(F)c1
BrCCCCCCCCBr
BrCN1N=CC=CC1=Nc1ccc(Nc2ncnc3cccc(Br)c23)cc1
Fine tuning...
Mean value of predictions: 0.51439923
Proportion of valid SMILES: 0.6297686053783614
Sample trajectories:
BP(=O)(OCC1OC(NC=O)OC(=O)N1C)c1ccc(Cl)cc1
BrC=CC=NNc1ncnc2ncnc(Nc3ccccc3)c12
BrCCCCOc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2cc1Br
BrCN1CCCN(Cc2ccc3nc(-c4cncnc4Br)ncnc23)CC1
Brc1c[nH]c2ncnc(Nc3ccncc3)c12

 12 Training on 17900 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 29.945385
Reward: 4.105515
Trajectories with max counts:
160	Oc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Mean value of predictions: 0.63176316
Proportion of valid SMILES: 0.46452016255079714
Sample trajectories:
BP(=O)(NO)c1cc(Br)c(Br)cc1Br
BrCCc1c(Br)cc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1Nc1ncnc2ccc(Br)cc12
BrCc1cc2nncn2c(Nc2ncnc3cc(Br)ccc23)n1
Brc1c[nH]c(Nc2ncnc3ccc(I)cc23)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.576564
Proportion of valid SMILES: 0.6297686053783614
Sample trajectories:
BP(=O)(NP(=O)(O)C(F)(F)F)N(O)Cc1ccc(Br)cc1
BP(=O)(OCC)C(Br)=P(O)(CC(=O)OP(=O)(O)CBr)NO
BP(=O)(OCC)OCCO
BrC(=NN=C1Nc2ncnc(Nc3ccc(Br)cc3)c21)N1CCOCC1
BrCc1cc2c(Nc3ccc(Br)cn3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.54327935
Proportion of valid SMILES: 0.655625
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Br)c(Cl)c1
Bc1cc(Br)c2ncnc(Nc3cc(Br)c(Br)c(Br)c3)c2c1S(=O)(=O)Nc1ccc(Br)cc1
Bc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Bc1ccc(Nc2ncnc3scnc23)cc1
BrCN1CC=C(Nc2cc3ncnc(Nc4ccc(Br)s4)c3cn2)CC1

 13 Training on 20408 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 31.389153
Reward: 4.142397
Trajectories with max counts:
39	Oc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Mean value of predictions: 0.63195264
Proportion of valid SMILES: 0.5284552845528455
Sample trajectories:
BP(=O)(OCC)C(=O)OCC=CC(=O)Oc1c(Br)cc(Br)cc1Br
Bc1cc2ncnc(Nc3cc(Br)c(Br)c(Br)c3Br)c2nc1Br
Bc1cc2ncnc(Nc3cc(Br)cs3)c2s1
BrC=NN1CCCC1
BrCCCCCCCCCCCCCn1c(Br)cc2ncnc(Nc3cc(Br)c(Br)s3)c21
Policy gradient replay...
Mean value of predictions: 0.54999995
Proportion of valid SMILES: 0.6326977180368866
Sample trajectories:
BP(=O)(OCCC)OC(=O)COC(=O)c1ccc(Br)cc1Br
BrC1=Nc2cc(Br)cnc2N(c2nc3ccccc3s2)CO1
BrCc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3c(Br)csc23)c1
Fine tuning...
Mean value of predictions: 0.56802356
Proportion of valid SMILES: 0.6371875
Sample trajectories:
BrC(=NNc1ccc(Br)cc1)c1cccnc1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Nc4c(Br)ccc(Br)c4Br)cc3)ncnc2c1
Brc1cc(Br)c2c(c1)c1c(Br)cc(Br)c3c(Br)cccc3cc1N2

 14 Training on 23012 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.241832
Reward: 4.482664
Trajectories with max counts:
32	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.63138807
Proportion of valid SMILES: 0.551734917161613
Sample trajectories:
BP(=O)(OCC)C(=O)Oc1cc2ncnc(Nc3ccc(Br)cc3F)c2s1
BrC(Br)=CCc1cc(Nc2ncnc3cc(Br)c(Br)cc23)cc(Br)c1Br
BrCc1c(Br)cc(Nc2ncnc3cc(Br)sc23)cc1Br
BrCc1cnc(Nc2ncnc3ccc(Br)cc23)cc1Br
Brc1c[nH]c(Nc2ncnc3c(Br)cc(Br)nc23)c1
Policy gradient replay...
Mean value of predictions: 0.57165724
Proportion of valid SMILES: 0.66375
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2n1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.57815206
Proportion of valid SMILES: 0.649375
Sample trajectories:
BP(=O)(O)Oc1cc(Br)cc(Br)c1Br
BrCCOc1ccc2ncnc(Nc3cc(Br)cnc3Br)c2c1
BrCc1ccc2ncnc(Nc3ccnc(Br)c3)c2c1
Brc1cc(Br)c(N2CCN(Cc3ccc(Br)s3)CC2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1

 15 Training on 25735 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.412717
Reward: 4.784415
Trajectories with max counts:
36	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.67027026
Proportion of valid SMILES: 0.6128125
Sample trajectories:
BP(=O)(N=O)NC(Cl)(Cl)P(=O)(OCOCOC(=O)C(Cl)(Cl)Cl)OC(Cl)(Cl)Cl
Brc1cc(Br)c2c(NCCN3CCN(c4nc5c(Nc6ccc(Br)c(Br)c6)ncnc5cc4Br)CC3)ncnc2c1
Brc1cc(Br)c2c(Nc3cc(Br)c4ncccc4c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3csc(Br)c3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.6470821
Proportion of valid SMILES: 0.631875
Sample trajectories:
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)sc23)c1
Brc1cc(Br)c2c(Nc3cc(Br)c(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.6032016
Proportion of valid SMILES: 0.6246875
Sample trajectories:
Bc1cc(Nc2ncnc3ccc(Br)cc23)ccc1Br
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(Br)cc23)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ncnc4ccc(Br)cc34)ccnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1

 16 Training on 28781 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.748338
Reward: 4.720734
Trajectories with max counts:
57	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.61999035
Proportion of valid SMILES: 0.6475922451532208
Sample trajectories:
BP(=O)(CCCCC(N)(P(=O)(O)O)P(=O)(O)O)NO
BP(=O)(N=C(N)O)OCC
BP(=O)(NCCO)c1ncnc2nc(Nc3ccc(Br)cc3Br)cnc12
Bc1cc2ncnc(Nc3ccc(Br)cc3)c2cc1-c1cccc(Br)c1Br
Bc1cc2ncnc(Nc3ccc(Br)cc3)c2nc1CCCCI
Policy gradient replay...
Mean value of predictions: 0.60912657
Proportion of valid SMILES: 0.636875
Sample trajectories:
BP(=O)(OCC)OCCBr
BP(=O)(OCCCCCC)OC(=O)N1Cc2c(Br)cc(Br)cc2C(C(=O)O)c2ccc(NS(=O)(=O)c3ccc(Br)cc3)cc2C1=O
BP1(=O)Oc2cccc(c(Cl)nc2)Nc2ncnc(c2F)N1CCS(=O)(=O)Oc1cccc(F)c1F
Bc1ccc(Nc2ncnc3cc(F)sc23)cc1F
Bc1cccc(Nc2ncnc3cc(F)ccc23)c1
Fine tuning...
Mean value of predictions: 0.62606984
Proportion of valid SMILES: 0.6353125
Sample trajectories:
BrBr
BrCCc1cnc(Nc2ccccc2)s1
Brc1cc(Br)c(Br)cc1Br
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1

 17 Training on 31815 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 37.506953
Reward: 5.162130
Trajectories with max counts:
223	Oc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Mean value of predictions: 0.72027457
Proportion of valid SMILES: 0.5012507817385866
Sample trajectories:
Brc1c[nH]c(Nc2ncnc3c(Br)cc(Br)c(Br)c23)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3c(Br)cc(Br)cc23)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(N4CCCC4)cc23)c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.6324988
Proportion of valid SMILES: 0.6617693029071585
Sample trajectories:
BP(=O)(OCCc1cc(Br)cs1)C(=O)c1cc(Br)cc(Br)c1
BrCCNc1ncnc2nc3c(Br)cccc3nc12
Brc1cc(Br)c(Nc2ncnc3cc(Br)sc23)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3scnc23)cc1Br
Brc1cc(Br)c2c(Nc3cc(Br)c(Br)s3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.6349282
Proportion of valid SMILES: 0.6533291653641763
Sample trajectories:
Brc1cc(-c2cccc3cccnc23)c2c(N3CCCC3)ncnc2c1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3ccsc23)c1
Brc1cc(Br)c(Nc2ncnc3ccsc23)cc1Br
Brc1cc(Br)c2c(Nc3cc(Br)ncn3)ncnc2c1

 18 Training on 35027 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 37.668023
Reward: 5.501565
Trajectories with max counts:
142	Nc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Mean value of predictions: 0.72678196
Proportion of valid SMILES: 0.4515625
Sample trajectories:
BP(=O)(OCCl)c1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BrCN1CCNCC1
Brc1cc(Br)c(Nc2cc3ncnc(Nc4ccc(Br)c(Br)c4)c3s2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccncc3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.6670414
Proportion of valid SMILES: 0.6117536730228196
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(Br)c(Br)c(Nc2cc(Nc3ncnc4ccc(Br)cc34)ccc2Br)c1
Brc1cc(Br)c(Nc2ccc(Br)c(Br)c2)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.61934257
Proportion of valid SMILES: 0.6559375
Sample trajectories:
BP(=O)(O)CNc1cc(Br)c2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
BP(=O)(OCC)OC(=O)Nc1ccc(Br)cc1
BS(=O)(=O)OCCCBr
Bc1cc2ncnc(Nc3ccccc3)c2s1
Brc1cc(Br)c(NCCN2CCCOCC2)c(I)c1

 19 Training on 38009 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 39.660638
Reward: 5.671770
Trajectories with max counts:
83	Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1
Mean value of predictions: 0.7289655
Proportion of valid SMILES: 0.5440900562851783
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(Nc4ccccc4)ncnc32)C(O)C1O)C(=O)O
B[PH](=O)(=NO)Nc1cc2cc(N3CCCC3)ccc2s1
B[PH](=O)(NO)=C(Br)Br
Bc1cc(Nc2ncnc3cc(Br)sc23)cc2sncn2c1
Bc1cc(Nc2ncnc3ccccc23)nc(Nc2ccccc2)n1
Policy gradient replay...
Mean value of predictions: 0.7257026
Proportion of valid SMILES: 0.6115625
Sample trajectories:
Bc1cc2ncnc(Nc3cc(Br)sc3Br)c2s1
BrSc1ccc(Nc2ncnc3cc(Br)c(Br)nc23)cc1
Brc1cc(-c2ncnc3ccsc23)c2scnc2n1
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(Nc2ncnc3c(Br)c(Br)cnc23)cc1Br
Fine tuning...
Mean value of predictions: 0.66125
Proportion of valid SMILES: 0.6506099468251486
Sample trajectories:
BrCCOc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
Brc1cc(Br)c(Nc2ncnc3ccc(Nc4ncc(Br)s4)cc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Brc1cc(Br)c2ncnc(Nc3cc(Br)c(Br)c(Br)c3)c2n1

 20 Training on 41481 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 43.991628
Reward: 6.105896
Trajectories with max counts:
41	Nc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Mean value of predictions: 0.7571584
Proportion of valid SMILES: 0.57625
Sample trajectories:
Bc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Brc1cc(Br)c(Nc2ncnc3cc(Br)ccc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)nc3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)nc23)c1
Policy gradient replay...
Mean value of predictions: 0.7138255
Proportion of valid SMILES: 0.6984375
Sample trajectories:
BP(=O)(OCOc1cc(Br)cc(Br)c1Br)Oc1ccc(Br)cc1
BrCc1cc2ncnc(Nc3cc(Br)c(Br)c(Br)c3)c2s1
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)c4ncncc4c23)c1
Brc1cc(Br)c(Nc2ncnc3cc(Br)c(N4CCCCN4)cc23)cc1Br
Fine tuning...
Mean value of predictions: 0.6845619
Proportion of valid SMILES: 0.6742732103782432
Sample trajectories:
B=S(=O)(Nc1cc2c(Nc3ccc(Br)cc3)ccnc2cn1)c1cccc(Br)c1
Brc1cc(Br)c(Nc2ncncc2Nc2ncnc3ccc(Br)nc23)c(Br)c1
Brc1cc(Br)c2c(Nc3cc(Br)c4cncnc4c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(-c4ccccc4)c3)ncnc2c1
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1

Trajectories with max counts:
79	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.61270696
Proportion of valid SMILES: 0.5588547852722385
Mean Internal Similarity: 0.46969532374777884
Std Internal Similarity: 0.09238679341622177
Mean External Similarity: 0.41833463136122623
Std External Similarity: 0.06977466740752518
Mean MolWt: 416.7530288097044
Std MolWt: 96.38733852699367
Effect MolWt: -0.8211201273298966
Mean MolLogP: 4.634970521228205
Std MolLogP: 1.347018530176408
Effect MolLogP: -0.06231312672924837
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.368421% (1110 / 1140)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 100, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 6137.140300035477, 'valid_fraction': 0.5588547852722385, 'active_fraction': 0.5901565995525727, 'max_counts': 79, 'mean_internal_similarity': 0.46969532374777884, 'std_internal_similarity': 0.09238679341622177, 'mean_external_similarity': 0.41833463136122623, 'std_external_similarity': 0.06977466740752518, 'mean_MolWt': 416.7530288097044, 'std_MolWt': 96.38733852699367, 'effect_MolWt': -0.8211201273298966, 'mean_MolLogP': 4.634970521228205, 'std_MolLogP': 1.347018530176408, 'effect_MolLogP': -0.06231312672924837, 'generated_scaffolds': 1140, 'novel_scaffolds': 1110, 'novel_fraction': 0.9736842105263158, 'save_path': '../logs/replay_combo_s1-6.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.060398
Reward: 1.000000
Mean value of predictions: 0.0012028869
Proportion of valid SMILES: 0.782063342740671
Sample trajectories:
BrCc1ccc[n+](CCCCCCNCCC2CCCC2)c1
Brc1ccc(I)cc1
Brc1ccc(OCCc2ccccn2)nc1
Brc1cccc(C2=NC3=C(C=Cc4cc(Br)ccc4NC=N2)CCCC3)c1
C#CCCN(C)C1Cc2ccc2OCc2cc(F)ccc21
Policy gradient replay...
Mean value of predictions: 0.0013654619
Proportion of valid SMILES: 0.7810539523212046
Sample trajectories:
Brc1cc(C=Cc2ccccc2)nc(-c2ccccc2)c1
Brc1ccc(Sc2[nH]c3cccnc3c2Br)cc1Br
Brc1cccc(-c2cnnc(Nc3ccccc3)n2)c1
Brc1cccc(-c2nnc3ccnc(NCCN4CCCCC4)c3n2)c1
C#CC(O)CCCCC
Fine tuning...
Mean value of predictions: 0.0008028904
Proportion of valid SMILES: 0.7823492462311558
Sample trajectories:
Brc1ccc(C#CCCCCNCc2c(-c3ccc(Br)cc3)[nH]c3ccccc23)cc1
Brc1ccc(CSc2nnc(C3CC3)c3nnnn23)cc1
Brc1ccc2c(c1)C2N1CCOCC1
C#CCC#CC(O)C(Oc1c(O)cc(-c2ccc(Cl)cc2)cc1Cl)C(=O)O
C#CCCC(NC(=O)C(N)CCCC)C(=O)OC1CC2CC=CC=CC(C(C)(C)C)OC(=O)CC(C)C2(C)CCC1C#Cc1ccc(-c2cnccn2)cc1

  2 Training on 227 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.371923
Reward: 1.000000
Trajectories with max counts:
2	Cc1ccc2c(c1)NC(=O)N2
2	O=C1c2ccccc2Oc2ccccc21
Mean value of predictions: 0.00088602497
Proportion of valid SMILES: 0.7783699059561129
Sample trajectories:
BrCC1C2CC3CC3C1CC2Cc1ccc2c(c1)OCO2
BrCCn1ccnc1NCCCn1nnc2c(-c3ccccc3)ncnc21
Brc1cc(C#Cc2cn(C=C(c3ccccc3)c3ccncc3)nc2Cc2ccc3c(c2)OCO3)cc(I)c1Br
Brc1ccc(-c2nc3ccc(-c4cc5ccccc5s4)cc3s2)o1
Brc1ccc(OCc2nnc(-n3ccc4ccccc43)n2Cc2ccccc2-c2ccccc2)cc1
Policy gradient replay...
Mean value of predictions: 0.0011433238
Proportion of valid SMILES: 0.766270337922403
Sample trajectories:
B[PH](=N)(N)(NC(=O)c1ccccc1)P(=O)(Oc1noc(C)n1)OC(C)C
Brc1cnc(NCCCc2ccccc2)nc1-c1ccccc1
C#CCCC(=C)C(C)(CC#N)SC
C#CCCCCCN(CCN)C(=O)CN(CC)C1CCC(C)CC1C
C#CCCCCNC(=S)NCCCC
Fine tuning...
Mean value of predictions: 0.0015599343
Proportion of valid SMILES: 0.7633970542149796
Sample trajectories:
BC(CCc1ccccc1)NCC1CCCC1
Brc1ccc2c(c1)[nH]c1cc(OCCCN3CCCC3)ccc12
Brc1ccc2nnc(N(C#Cc3ccccn3)CCOc3ccc4c(c3)OCO4)c(Br)c2c1
Brc1ccc2oc(-c3ccc(-c4nc5ccccc5[nH]4)cc3)nc2c1
C#CC(=O)NC(C)C1(N(C(=O)c2cccnc2)c2ccc(Cl)cc2)CC1

  3 Training on 242 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.552171
Reward: 1.019292
Trajectories with max counts:
2	Cc1cccc2ccccc12
2	NC(Cc1ccc(-c2ccccc2)cc1)C(=O)O
Mean value of predictions: 0.0015479876
Proportion of valid SMILES: 0.8087636932707355
Sample trajectories:
Brc1ccc(-c2nnc(NCc3ccccc3)o2)cc1
Brc1ccc(NCc2ccccc2)c(Br)c1
Brc1ccc2[nH]c(-c3cc(-c4ccccc4)on3)nc2c1
Brc1ccc2[nH]cc(C3=CCN(CC4CCCC4)CC3)c2c1
Brc1ccc2c(c1)N2CCCCCCN1CCCC1
Policy gradient replay...
Mean value of predictions: 0.0014878622
Proportion of valid SMILES: 0.7993740219092331
Sample trajectories:
Brc1cc(C=Nc2cccc(C=NN3CCCCC3)c2)no1
Brc1ccc(Br)c(Br)c1
Brc1ccc(CN2CCN(c3ncccn3)CC2)cc1
Brc1ccc(Cc2ccc3oc4ccc(Br)cc4c3c2)cc1
Brc1cccc(-c2nn3ncccc3c2-c2ccccc2)c1
Fine tuning...
Mean value of predictions: 0.00047114253
Proportion of valid SMILES: 0.7974326862867878
Sample trajectories:
Brc1ccc(-c2nnn(CCCCCNCc3cccc(-c4ncc[nH]4)c3)n2)cc1
Brc1ccc(C23CC4CC(CC(C4)C2)C3)cc1
Brc1ccc(OCCCCCCCCc2ccccc2)cc1
Brc1ccc(OCCCCCN=CNc2nccc3c2[nH]c2c(Br)cccc23)cc1
Brc1ccc2[nH]c(-c3ccccc3)nc2c1

  4 Training on 256 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.897532
Reward: 1.008640
Trajectories with max counts:
2	CCCN1C(=O)c2ccccc2C1=O
2	Cc1cccc2ccccc12
2	Cn1c2ccccc2c2ccccc21
2	O=C(O)Cc1cccc2ccccc12
Mean value of predictions: 0.0007092199
Proportion of valid SMILES: 0.7936210131332082
Sample trajectories:
Brc1ccc(OC(CCCOc2ccc(Br)s2)=NCCc2c[nH]c3ccccc23)cc1
Brc1ccc(Oc2ccccc2)cc1
Brc1ccc2c(c1)C1CNCCC1N2
Brc1ccc2nc(N3CC(c4cccc5ccccc45)C3)sc2c1
Brc1cccc(-n2ccc3ccccc32)c1
Policy gradient replay...
Mean value of predictions: 0.00070866145
Proportion of valid SMILES: 0.79375
Sample trajectories:
BP(=O)(CCC)NP(=O)(OCC1OC(O)C(O)C1O)n1cnc2c(N)ncnc21
Brc1ccc(-c2nccc3ccccc23)nc1
Brc1ccc(C2=NCCN2)cc1
Brc1cccc(-c2noc(Nc3ccccc3Br)n2)c1
Brc1cccc(CCN2CCNC2)c1
Fine tuning...
Mean value of predictions: 0.0011682243
Proportion of valid SMILES: 0.8035043804755945
Sample trajectories:
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)C(=O)O
BrBr
Brc1ccc(-c2ccc3c(NC4CCC4)n[nH]c3c2)cc1
Brc1ccc(-c2ccc3ncccc3c2)cc1-c1ncccn1
Brc1ccc(-c2cccc3[nH]ccc23)cc1

  5 Training on 269 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.623855
Reward: 1.005139
Trajectories with max counts:
4	Cc1cccc2ccccc12
Mean value of predictions: 0.00076481845
Proportion of valid SMILES: 0.8174429509221631
Sample trajectories:
BrC1=CN2C(=CC=C2Cc2c[nH]c3ccccc23)C=C1
Brc1ccc(-c2cc3ccccn3c2-c2ccc(-c3cnc4[nH]ncc4c3)cc2)cc1
Brc1ccc(Br)cc1
Brc1ccc(N=C2NCCc3ccccc32)o1
Brc1ccc2c(c1)C1(CCNCC1)CCO2
Policy gradient replay...
Mean value of predictions: 0.0013333333
Proportion of valid SMILES: 0.7978723404255319
Sample trajectories:
Brc1ccc(-c2ccc3ncccc3c2)cc1
Brc1ccc(C2=NNC3=C(c4ccccc43)C2c2ccccc2)cc1
Brc1ccc(NCC=Cc2cscn2)cc1
Brc1ccc2c(NCc3ccccc3)ncnc2c1
Brc1ccccc1COc1cccnc1C1=NN(c2ccncc2)CCCN1
Fine tuning...
Mean value of predictions: 0.0011764705
Proportion of valid SMILES: 0.797373358348968
Sample trajectories:
Brc1c[nH]c(-c2c3ccccc3cc3ccccc23)n1
Brc1ccc(-c2cc(Oc3ccccc3)c3ccccc3n2)s1
Brc1ccc(-c2ccc(C#Cc3ccccc3)cc2)cc1
Brc1ccc(-c2ccccc2Nc2ccnc3[nH]ccc23)cc1
Brc1ccc2[nH]c(C3CCN(Cc4ccccc4)CC3)nc2c1

  6 Training on 287 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.565742
Reward: 1.038872
Mean value of predictions: 0.0014996055
Proportion of valid SMILES: 0.7926180794494839
Sample trajectories:
BP1(=O)Oc2ccc(Br)cc2O1
BrCCc1ccc(C2=COc3cc(Br)cc(Br)c3O2)cc1
Brc1ccc(-c2cc(C3=CCCN4CCCC34)ccc2-c2nn[nH]n2)cc1
Brc1ccc(CN2CCN(Cc3ccccc3)CC2)cc1
Brc1ccc(N2CCc3ccccc3C2)nc1
Policy gradient replay...
Mean value of predictions: 0.0012480499
Proportion of valid SMILES: 0.8017510944340213
Sample trajectories:
Brc1cc2cc(Nc3ncc4[nH]nc(C#Cc5ccccc5)cc34)cccc2n1
Brc1ccc(-c2cccc(Nc3cnccn3)c2)cc1Oc1ccccc1
Brc1ccc(-c2nc(Nc3ccncc3)co2)cc1
Brc1ccc2c(c1)C(c1ccccc1)=Nc1ccccc1N2
Brc1ccc2c(c1)N=C(c1nc3ccccc3[nH]1)S2
Fine tuning...
Mean value of predictions: 0.0011538462
Proportion of valid SMILES: 0.8130081300813008
Sample trajectories:
Brc1ccc(Br)c(-c2ccc3ncn(c4ccc(Br)cc24)n3-c2ccccc2)c1
Brc1ccc(Br)c2n[nH]c(n1)N2c1ccc(-c2ccccc2)cc1
Brc1ccc(CN2C=NC3=C2C=NN3C2CC2)cc1
Brc1ccc2[nH]c(-c3ccccc3)nc2c1
Brc1ccccc1C1=NNC(c2ccccc2)=N1

  7 Training on 304 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.496599
Reward: 1.036985
Trajectories with max counts:
4	Cc1cccc2ccccc12
4	O=C1Nc2ccccc2C1=O
Mean value of predictions: 0.0017950636
Proportion of valid SMILES: 0.8358862144420132
Sample trajectories:
BP(=O)(NCCCCOCCOC)c1ccc(C(=O)c2ccccc2OCc2ccccc2)cc1
Brc1ccc(-c2noc3ccccc23)cc1
Brc1ccc(C2=CSC3=NC=CC23)cc1
Brc1ccc(N2CCN(CCCc3ccc4ccccc4c3)CC2)cc1
Brc1ccc2ccccc2c1
Policy gradient replay...
Mean value of predictions: 0.0016460905
Proportion of valid SMILES: 0.835573616755236
Sample trajectories:
Brc1ccc(-n2nc3ccccc3c2NC2=CCCCC2)cc1
Brc1ccc2cc(-c3cccc4c5cn[nH]c5c34)[nH]c2c1
Brc1cccc(Nc2cncn3cc(-c4ccccc4Br)cc23)c1
Brc1cccc2cccnc12
Brc1ccccc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.0018545995
Proportion of valid SMILES: 0.8432905849233656
Sample trajectories:
Brc1ccc(-c2ccc(-c3ccc4ccccc4c3)cc2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)cc1
Brc1ccc(N2C=CC=CC(c3cccs3)=C2)cc1
Brc1ccc(Nc2nccc(-c3ccccn3)n2)cc1

  8 Training on 328 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.079386
Reward: 1.041215
Trajectories with max counts:
2	O=C1c2ccccc2-c2ccccc21
2	Oc1ccc2ccccc2c1
Mean value of predictions: 0.0018896448
Proportion of valid SMILES: 0.826875
Sample trajectories:
Brc1ccc(-c2ccc3ccccc3c2)o1
Brc1ccc(N2CCN(Cc3cccc4c3CO4)CC2)o1
Brc1ccc(Nc2ncnc3ccc(-c4ccccc4)cc23)cc1
Brc1ccc2[nH]cc(CCCNc3cccc4ccccc34)c2c1
Brc1ccc2cc3nc[nH]c3cc2c1
Policy gradient replay...
Mean value of predictions: 0.0010558069
Proportion of valid SMILES: 0.8295276822020644
Sample trajectories:
Brc1[nH]c2ccc3ccccc3c2c1-c1cccs1
Brc1cc(Nc2nccc3ccccc23)cc(OCc2ccccc2)c1
Brc1ccc(-c2ccc3c(c2)OCO3)cc1
Brc1ccc(-c2nsc(C3CCCN(c4cccs4)C3)n2)cc1
Brc1ccc(OCc2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.0009863429
Proportion of valid SMILES: 0.8240075023444826
Sample trajectories:
BrCc1cnc(-c2ccncc2)cc1-c1cccc2ccccc12
Brc1c(C2=NCCO2)n[nH]c1-c1ccc(-c2ccccc2)cc1
Brc1ccc(-n2c3ccccc3c3ccccc32)cc1
Brc1ccc(-n2cc(-c3ccccc3)nn2)cc1
Brc1ccc(C#Cc2c[nH]c3ccc(N4CCOCC4)cc23)cc1

  9 Training on 344 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.645269
Reward: 1.054786
Trajectories with max counts:
3	Fc1ccccc1F
Mean value of predictions: 0.0014240505
Proportion of valid SMILES: 0.79
Sample trajectories:
BrC1=CNc2ccc(NC3C4CCCC43)cc2-c2ccc(cc2)N2CC3=CC=C(N=C3C2)c2c1oc1ccccc21
Brc1[nH]ccc1-c1ccc(C=C2c3ccccc3C(Br)(C=Cc3ccccc3)C3Nc4ccccc4OCC23)cc1
Brc1ccc(-c2nnc3ccccc3n2)cc1
Brc1ccc(-n2cc3ccccc3c2)cc1
Brc1ccc(Sc2ccccc2N2CCN(Cc3ccccc3)CC2)cc1
Policy gradient replay...
Mean value of predictions: 0.0029174665
Proportion of valid SMILES: 0.814571607254534
Sample trajectories:
Brc1c[nH]c2nc(NN=Cc3ccccc3)sc12
Brc1ccc(Cn2cncc2CSc2cncc(Br)[n+]2-c2ccccc2)cc1
Brc1ccc(Nc2nccc(Br)c2Oc2ccccc2)cc1
Brc1ccc(Oc2ccccc2)cc1OCc1ccccc1
Brc1ccc2cc[nH]c2c1
Fine tuning...
Mean value of predictions: 0.0032444957
Proportion of valid SMILES: 0.8100750938673341
Sample trajectories:
BrC1=CSC2=NC(C34CC5CC(CC(C5)C3)C4)=CC=C12
Brc1cc(Br)c2c(c1)C=CN2
Brc1ccc(CN2CCN(Cc3ccc4c(c3)OCO4)CC2)cc1
Brc1ccc(N=Cc2cn(-c3cccc4ccccc34)nn2)cc1
Brc1ccc(OCCn2ccc3c(N4CCCCC4)ncnc32)cc1

 10 Training on 374 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.798677
Reward: 1.048757
Trajectories with max counts:
4	Cc1cccc2ccccc12
Mean value of predictions: 0.002433281
Proportion of valid SMILES: 0.7964989059080962
Sample trajectories:
BrCCN=C1C=CN1
Brc1ccc(-c2ccc3cccnc3c2)cc1
Brc1ccc(Oc2nc3ncccc3s2)cc1
Brc1ccc2c(c1)C1=C(CCC1)N2
Brc1ccc2c(c1)cc1[nH]c3cccnc3CN(C3CCC(c4ccccc4)CC3)CCn12
Policy gradient replay...
Mean value of predictions: 0.0007656968
Proportion of valid SMILES: 0.8170159524554269
Sample trajectories:
Brc1ccc(-c2oc3ccccc3c2-c2c[nH]c3ccccc23)cc1
Brc1ccc(CN2CCCN(c3ccncc3)CC2)cc1
Brc1ccc(Cn2ccc3ccccc32)c(OCc2ccc(C#CC3CCCC3)cc2)n1
Brc1ccc2nc3c(nc2c1)CCCC3
Brc1ccc2oc(N3CCN(c4ccncc4)CC3)cc2c1
Fine tuning...
Mean value of predictions: 0.0015582392
Proportion of valid SMILES: 0.8029402564904599
Sample trajectories:
Brc1cccc(-c2[nH]c3ccccc3c2CN2CCCCC2)c1
Brc1cccc(C2=C3C=CC=CC3=Nc3ccccc3S2)c1
Brc1cccc2[nH]ccc12
Brc1cccs1
Brc1cncc(NC=C2COC3=C2NC=NN2CCN(CC2)C3)c1

 11 Training on 395 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.154685
Reward: 1.038882
Trajectories with max counts:
3	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.0021434461
Proportion of valid SMILES: 0.7590738423028786
Sample trajectories:
BrC1(c2ccc(C=CC3CCN(c4ccccc4)CC3)cc2)CC(c2ccccc2)=N1
Brc1cc(Br)c(C=Cc2ccncc2)[nH]1
Brc1cc(NC2CNCCN2CN2CCCC2)ccc1Nc1nccs1
Brc1ccc(C2CCN(c3cccs3)C2)c(Br)c1
Brc1ccc(CN2CCN(c3cnccn3)CC2)cc1
Policy gradient replay...
Mean value of predictions: 0.0029588165
Proportion of valid SMILES: 0.782540675844806
Sample trajectories:
BrC=CC=C(CNC1CCCC1)NCCCC=CC=CC=CC=CCCCCCCCCCCBr
Brc1ccc(CN2CCN(C3=Nc4cc(Br)ccc4O3)CC2)cc1
Brc1ccc(CN2CCN(Cc3ccccc3)CC2)cc1
Brc1ccc(N2CCN(c3ccccc3)CC2)c(OCc2ccccc2)c1
Brc1ccc2c(-c3nc4ccccc4[nH]3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.0028301885
Proportion of valid SMILES: 0.7957460118861432
Sample trajectories:
BrC1=CC2=NCCCC2C2(CCN(c3ccccc3)CC2)c2nccn2C1
Brc1c[nH]c2ccc(NC3CCNCC3)cc12
Brc1ccc(-n2ncc3ccc(Br)cc32)cc1
Brc1ccc(C=CC2=C(c3ccccc3)NCCN2)cc1
Brc1ccc(Nc2ncccc2Br)cc1

 12 Training on 424 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.405390
Reward: 1.027202
Mean value of predictions: 0.0021268215
Proportion of valid SMILES: 0.7944305381727159
Sample trajectories:
BP(=O)(NCCCCCN)c1cccc(F)c1
Brc1ccc(-c2ccccc2)s1
Brc1ccc2N=C3CC=CCC3N2C1=Nc1ccccc1
Brc1cccc(Nc2nccs2)c1
C#CCC=CCCCNC(=N)N
Policy gradient replay...
Mean value of predictions: 0.0014751554
Proportion of valid SMILES: 0.8055034396497811
Sample trajectories:
Brc1ccc(N2CCN(CCNc3cccc(C4=NCCCN4)c3)CC2)cc1
Brc1ccc2c(c1)C(c1ccccc1)=NN1CCNCC1=N2
Brc1ccc2c(c1)OC(c1ccc3ccccc3c1)C=N2
Brc1ccc2c(c1)SC(=NC1=NCCCN1)N2
Brc1cccc(-c2ccc3ccccc3c2)c1
Fine tuning...
Mean value of predictions: 0.0010280743
Proportion of valid SMILES: 0.7913016270337923
Sample trajectories:
BrC1=CCC2=C(NCN2)C12CCCN2
Brc1c(CN2CCCNC2=Nc2ccncc2)ccc2ccccc12
Brc1ccc(-c2cnc3ccc(Br)nc3c2NCc2ccco2)cc1
Brc1ccc(Br)c(CN2CCCNCC2)c1
Brc1ccc(N2CCN(c3ccccc3)CC2)c(CNc2ccnc3ccccc23)c1

 13 Training on 444 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.521707
Reward: 1.026095
Trajectories with max counts:
3	CCn1ccc2ccccc21
3	Cc1cccc2ccccc12
3	Nc1cccc2ccccc12
Mean value of predictions: 0.0026244693
Proportion of valid SMILES: 0.8099406064395124
Sample trajectories:
Brc1ccc2c(Oc3cccc4ccccc34)cccc2c1
Brc1ccc2c(Oc3ccccc3N=Cc3ccccc3)cccc2c1
Brc1ccccc1-c1cc2[nH]ccc2cc1Oc1ccc(-c2ccccc2)cc1
C#CCN(C(NCCC1(C)CCCC1)N1CCCC1)S(=O)(=O)C1CCC=CCC1
C#CCN(c1cccc(NS(C)(=O)=O)c1)c1cn(CCC)c(C)c1Nc1ccccc1
Policy gradient replay...
Mean value of predictions: 0.0014643546
Proportion of valid SMILES: 0.811698467313106
Sample trajectories:
BP(=O)(Oc1ccc(F)cc1)c1ccc2ccccc2n1
Brc1ccc(Br)c(Br)c1
Brc1ccc(I)c(Nc2cccc(-c3cnc4[nH]cnc4c3)c2)c1
Brc1ccc2c(c1)[nH]c1c(OCc3ccccc3)cccc12
Brc1cccc2c(N=CN3CCN(c4ccccc4)CC3)Nc3ncc(cc3N=Cc3ccco3)nc12
Fine tuning...
Mean value of predictions: 0.0019290124
Proportion of valid SMILES: 0.8110137672090113
Sample trajectories:
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)OP(=O)(O)O
BrCCBr
Brc1cc(NCc2ccccc2)c(-c2ccc3oc4ccccc4c3c2)cn1
Brc1ccc(-c2ccc3onc(N=Cc4ccccc4)c3c2)cc1
Brc1ccc(-c2nc3ccccc3o2)cc1

 14 Training on 465 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.131396
Reward: 1.032141
Trajectories with max counts:
3	Cc1cccc2ccccc12
Mean value of predictions: 0.0018065487
Proportion of valid SMILES: 0.8316118935837246
Sample trajectories:
Brc1ccc(-c2ccc3sccc3c2)s1
Brc1ccc(CN2CCN(C(c3ccccc3)c3ccccc3)CC2)cc1
Brc1ccc(NC2=NCC=CC2)cc1
Brc1ccc2c(c1)OC(c1ccccc1)=C(Nc1ccc(OCc3ccccc3)cc1)N2
Brc1ccc2nc(-c3ccco3)c(-c3ccccc3)nc2c1
Policy gradient replay...
Mean value of predictions: 0.0017617771
Proportion of valid SMILES: 0.8174702567313713
Sample trajectories:
BP(=O)(OCC)n1cc(Br)c(Br)c1
BrC1=C(c2ccc3cccnc3c2)CCCN1
Brc1ccc(C#Cc2cccnc2)cc1-c1noc(-c2ccncc2)n1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(OCCCCCNc2nc(-c3ccccc3Br)cs2)cc1
Fine tuning...
Mean value of predictions: 0.0006862372
Proportion of valid SMILES: 0.8207133917396746
Sample trajectories:
Brc1ccc(C#CC2=NCCCN2)cc1
Brc1ccc(C#Cc2c[nH]cn2)cc1
Brc1ccc(C#Cc2ccccc2)s1
Brc1ccc(C(Nc2ccccc2)c2ccccc2)cc1
Brc1ccc(Oc2ccccc2)cc1

 15 Training on 482 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.113152
Reward: 1.019955
Trajectories with max counts:
2	CCn1c2ccccc2c2ccccc21
2	Cc1ccc(S(=O)(=O)N2CCN(CC3CC3)CC2)cc1
2	Cn1ccc2ccccc21
Mean value of predictions: 0.002381867
Proportion of valid SMILES: 0.8147104851330204
Sample trajectories:
Brc1ccc(C#Cc2ccco2)cc1
Brc1ccc(C2Oc3ccccc3C2Nc2ccccc2)cc1
Brc1ccc(N2CCN(CCc3ccncc3)CC2)c2ccccc12
Brc1ccc(Sc2ccccc2)c(Br)c1
Brc1ccc2c(c1)CCCO2
Policy gradient replay...
Mean value of predictions: 0.00077790744
Proportion of valid SMILES: 0.8044430538172715
Sample trajectories:
Brc1ccc(-c2cccc3ccccc23)cc1
Brc1ccc(-c2nc3c(ccc4[nH]cc(Br)c43)c3ccccc3[nH]2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(C2CCNCC2NCc2cc3ccccc3o2)o1
Brc1ccc(C=NNC=Cc2ccccc2)cc1
Fine tuning...
Mean value of predictions: 0.0016304349
Proportion of valid SMILES: 0.8062597809076683
Sample trajectories:
Brc1cc2[nH]c(CCc3c[nH]c4ccccc34)cc2cn1
Brc1ccc(Br)c(Br)c1
Brc1ccc2[nH]c(-c3ccc4ccccc4c3)nc2c1
Brc1ccc2sc(NCCn3ccc4ccccc43)cc2c1
Brc1cccc(C2=CCc3ccccc3C2)c1

 16 Training on 506 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.083810
Reward: 1.024093
Mean value of predictions: 0.0043894653
Proportion of valid SMILES: 0.7841051314142679
Sample trajectories:
BP(=O)(OCC1OC(N2C=CC=CC2=O)C(O)C1O)C(=O)NCCc1ccccc1
Brc1cc(Nc2ccccn2)ncc1-c1nc2ccccc2o1
Brc1ccc(NC2=NCCN2)cc1Br
Brc1ccc(Nc2nc(-c3ccccn3)[nH]c2Br)o1
Brc1cccc(C2CC=CN(CCCN3CCCCC3)CC2)c1
Policy gradient replay...
Mean value of predictions: 0.0017849898
Proportion of valid SMILES: 0.7705532979055955
Sample trajectories:
B[PH](=O)(Cn1cnc2c(Br)cc(Br)cc21)=NP(N)(=O)OP(=O)(O)OCC1OC(n2cnc3c(Br)cc(Br)cc32)C(O)C1O
Brc1cc[nH]n1
Brc1ccc2[nH]c(C3CCN(Cc4ccncc4)CC3)cc2c1
Brc1ccc2oc(-c3ccc(CN4CCC5CNCC5C4)c4ccccc34)nc2c1
Brc1cccc(N2CCN(Cc3ccc4c(c3)OCO4)CC2)c1
Fine tuning...
Mean value of predictions: 0.0012929293
Proportion of valid SMILES: 0.7748904195366312
Sample trajectories:
BP(=O)(NCCCO)C(=O)N(CCO)CCCC
Brc1cc(NN=Cc2ccccc2Br)ccn1
Brc1ccc(C(c2ccccc2)(c2ccccc2)c2ccccc2)cc1
Brc1ccc2[nH]c(-c3ccccc3)nc2c1
Brc1cccc(Nc2ncnc3cccc(N4CCCC4)c23)c1

 17 Training on 532 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.060359
Reward: 1.015519
Mean value of predictions: 0.0024740624
Proportion of valid SMILES: 0.7858262778300408
Sample trajectories:
Brc1ccc(C2=NOC(c3ccccc3)c3ccccc32)cc1
Brc1ccc(CCNCc2ccccc2)cc1
Brc1ccc(CSC2=Nc3ccccc3-c3ccccc32)cc1
Brc1ccc2c(c1)CC(Nc1ccnc(NCc3ccccn3)n1)O2
Brc1cccc(Oc2ccccc2)c1
Policy gradient replay...
Mean value of predictions: 0.0003952569
Proportion of valid SMILES: 0.7918622848200313
Sample trajectories:
BrC1=NC(=Cc2c[nH]c3ccccc23)Cn2cncc2CN1
Brc1cc(NCCc2cccc3ccccc23)cnc1N1CCCNCC1
Brc1ccc(Nc2c(-c3ccc4nc[nH]c4c3)cnc3ccc(-c4ccc(Br)cc4)cc23)cc1
Brc1cccc(N2CCCNCC2)c1
Brc1cccc(Nc2nc(-c3ccccc3)cs2)n1
Fine tuning...
Mean value of predictions: 0.001607717
Proportion of valid SMILES: 0.7792045098653304
Sample trajectories:
BP(=O)(NC(Cc1ccccc1)C(=O)O)P(=O)(O)O
Brc1ccc(Br)c(Oc2ncc3ccc(NCc4ccccc4)nc3c2N2CCSCC2)c1
Brc1ccc(CN2CCN(c3nsnc3Br)CC2)cc1
Brc1ccc(NCc2cccnc2)c(-c2ccncc2)c1
Brc1ccc(NN=Cc2ccccc2)cc1

 18 Training on 550 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.076512
Reward: 1.009232
Trajectories with max counts:
2	Cc1ccc(S(N)(=O)=O)cc1
2	Fc1ccccc1F
2	Nc1ncnc2c1ncn2C1OC(COP(=O)(O)OP(=O)(O)OP(=O)(O)O)C(O)C1O
Mean value of predictions: 0.0013333333
Proportion of valid SMILES: 0.7981220657276995
Sample trajectories:
Bc1ccc(S(=O)(=O)O)cc1
BrC=CCN=C1CN2CCCC2C(N2CCN(Cc3c[nH]c4ccccc34)CC2)=N1
BrCCCc1ccc(NCc2ccccc2)nc1
Brc1cc2c3ccccc3c1c1cccc(c1)-c1[nH]c3ccccc3c1-2
Brc1ccc(-c2cc(-c3n[nH]cc3Br)cc(Nc3ccccc3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.0013343799
Proportion of valid SMILES: 0.7977457733249843
Sample trajectories:
Brc1ccc(Nc2c(Br)cccc2Br)s1
Brc1ccc2c(Br)cccc2c1
Brc1cnc2[nH]ncc2c1Br
C#CC(C)CCC(C)C
C#CCCC(=O)Nc1ccccc1C(C#N)c1ccc(C(=O)Nc2ccc(c3ccncc3)cc2F)cc1
Fine tuning...
Mean value of predictions: 0.0014251781
Proportion of valid SMILES: 0.7916013788780947
Sample trajectories:
Brc1ccc(CNC2CCCCC2)cc1
Brc1ccc(OCCSc2nc3ccccc3s2)c(Br)c1
Brc1cccc(Nc2ncnc3ccnnc23)c1
Brc1cccc2c(N3CCNCCCNCC3)cc(C#CCNCc3ccccc3)cc12
Brc1ccccc1Br

 19 Training on 566 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 16.966538
Reward: 1.005491
Mean value of predictions: 0.0008785943
Proportion of valid SMILES: 0.7856918732350172
Sample trajectories:
BrCCc1ccc(-c2ccccc2)c(N2CCOCC2)c1
Brc1ccc(N2CCN(C3=Nc4ccccc4Sc4ccccc43)CC2)cc1
Brc1ccc(NCc2ccco2)cc1
Brc1ccc2c(NCCCNCc3ccccc3)ncnc2c1
Brc1cccc(Nc2nc3ccccc3nc2Sc2ccccc2)c1
Policy gradient replay...
Mean value of predictions: 0.001910828
Proportion of valid SMILES: 0.7877077453747257
Sample trajectories:
BrC1=C[N+][N+]C2=CCCC2C1
Brc1ccc(C2=Nc3ccccc3C3=CCCN(Cc4ccccc4)CCN32)cc1
Brc1ccc(CCNCCc2ccc(C#Cc3c[nH]c4ccc(Br)cc34)cc2)cc1
Brc1ccc(OCC2CCCN2)cc1
Brc1ccc(OCCCN2CCN(c3ccccc3)CC2)cc1
Fine tuning...
Mean value of predictions: 0.0014948859
Proportion of valid SMILES: 0.7961165048543689
Sample trajectories:
BrCCCCCc1ccc(Br)cc1
Brc1cc(CN(Cc2ccco2)CN2C=CC=CN2)c2ccccc2n1
Brc1ccc(-c2cc(-c3csc(N4CCCCC4)n3)ncn2)cc1
Brc1ccc(-c2cc(CN3CCN4C(c5ccccc5)CC34)c3ccccc3n3cccc3n2)cn1
Brc1ccc(-c2nc3ccccc3[nH]2)cc1

 20 Training on 582 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.332183
Reward: 1.003266
Trajectories with max counts:
2	Cc1cccs1
Mean value of predictions: 0.0015625
Proportion of valid SMILES: 0.8017538365173817
Sample trajectories:
BP(=O)(CCCCCCCNC(=O)CCCSC)NC(=O)CN
BP(=O)(NCOCCN1C=C(O)N(O)C(=O)c2ccccc21)c1ccc(-c2ccccc2)cc1
Brc1ccc(-c2nc(-c3ccccc3)no2)cc1
Brc1ccccc1-n1ccc2cnccc21
C#CC1=Cc2ccc(cc2)C(=N)NC(=O)C2=NC=C(C)CN12
Policy gradient replay...
Mean value of predictions: 0.001178782
Proportion of valid SMILES: 0.7973057644110275
Sample trajectories:
Brc1ccc(CN2CCC3CCCCN=C3NC2c2ccccc2)cc1
Brc1ccc(CNC(CCCNC(c2ccccc2)N2CCCC2)c2cccc(Br)c2)cc1
Brc1ccc(NC2=CCCC3C=CCCCC3c3ccccc32)cc1
Brc1ccc(NC2=NCCN2)cc1OCC1CCCC1
Brc1cccc(Cc2cccc3ccccc23)c1
Fine tuning...
Mean value of predictions: 0.00094007055
Proportion of valid SMILES: 0.8003134796238245
Sample trajectories:
Brc1ccc(C2=NN(CCN3CCOCC3)N2)cc1
Brc1ccc(C=Cc2ccccc2Br)cc1
Brc1ccc(NCc2nc3ccccc3[nH]2)cc1
Brc1cccc(C=NNC2CNCCN2)c1
Brc1ccccc1Oc1ncc(CN2CCCCC2)c2sccc12

Trajectories with max counts:
3	Cc1ccc(NC(=O)CN2CCN(c3ccccc3)CC2)cc1
3	NCCCCCN
Mean value of predictions: 0.0012806497
Proportion of valid SMILES: 0.8022301572386143
Mean Internal Similarity: 0.4696270987917516
Std Internal Similarity: 0.18458501019162304
Mean External Similarity: 0.4193448339656822
Std External Similarity: 0.07194854933224125
Mean MolWt: 366.8497500000001
Std MolWt: 53.97418833803807
Effect MolWt: -1.4847556941434634
Mean MolLogP: 4.447790000000003
Std MolLogP: 1.1181964165240716
Effect MolLogP: -0.20461311125194898
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 81.818182% (9 / 11)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 0, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5670.143462896347, 'valid_fraction': 0.8022301572386143, 'active_fraction': 0.0009370607527721381, 'max_counts': 3, 'mean_internal_similarity': 0.4696270987917516, 'std_internal_similarity': 0.18458501019162304, 'mean_external_similarity': 0.4193448339656822, 'std_external_similarity': 0.07194854933224125, 'mean_MolWt': 366.8497500000001, 'std_MolWt': 53.97418833803807, 'effect_MolWt': -1.4847556941434634, 'mean_MolLogP': 4.447790000000003, 'std_MolLogP': 1.1181964165240716, 'effect_MolLogP': -0.20461311125194898, 'generated_scaffolds': 11, 'novel_scaffolds': 9, 'novel_fraction': 0.8181818181818182, 'save_path': '../logs/replay_combo_s1-7.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.060398
Reward: 1.000000
Mean value of predictions: 0.0012028869
Proportion of valid SMILES: 0.782063342740671
Sample trajectories:
BrCc1ccc[n+](CCCCCCNCCC2CCCC2)c1
Brc1ccc(I)cc1
Brc1ccc(OCCc2ccccn2)nc1
Brc1cccc(C2=NC3=C(C=Cc4cc(Br)ccc4NC=N2)CCCC3)c1
C#CCCN(C)C1Cc2ccc2OCc2cc(F)ccc21
Policy gradient replay...
Mean value of predictions: 0.0013654619
Proportion of valid SMILES: 0.7810539523212046
Sample trajectories:
Brc1cc(C=Cc2ccccc2)nc(-c2ccccc2)c1
Brc1ccc(Sc2[nH]c3cccnc3c2Br)cc1Br
Brc1cccc(-c2cnnc(Nc3ccccc3)n2)c1
Brc1cccc(-c2nnc3ccnc(NCCN4CCCCC4)c3n2)c1
C#CC(O)CCCCC
Fine tuning...
Mean value of predictions: 0.006451613
Proportion of valid SMILES: 0.6996865203761755
Sample trajectories:
Brc1ccc(-c2cc(Nc3ccc(Br)cn3)c3ncnnc3n2)cc1
Brc1ccc(CN2C=Nc3ccccc3SC2=Nc2ccccc2)cc1
Brc1ccc(Nc2nc3ccccc3n2-c2ccccc2)cc1
Brc1ccc(Nc2ncc[nH]2)c2ccccc12
Brc1ccc2c(c1)[nH]c1c(CCCN3CCN(c4ccccc4)CC3)CC12

  2 Training on 247 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.491705
Reward: 1.051394
Trajectories with max counts:
2	CC(C)Br
2	Fc1ccc(Nc2ncnc3ccccc23)cc1
2	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
2	O=C1Nc2ccccc2C1=O
Mean value of predictions: 0.012524655
Proportion of valid SMILES: 0.6355374490755249
Sample trajectories:
BrC1=CC2C=CCCCC2=Nc2ccccc21
Brc1ccc(-c2cc(-c3cccnc3)ncn2)s1
Brc1ccc2[nH]c(-c3ncnc4[nH]c5ccccc5c34)nc2c1
Brc1ccc2[nH]cc(CCNCc3ccnc4ccccc34)c2c1
Brc1ccccc1Nc1ncnc2cc3ncnn3-n3ccnc3nc12
Policy gradient replay...
Mean value of predictions: 0.011246944
Proportion of valid SMILES: 0.6410658307210031
Sample trajectories:
BrC12CCC1N(Cc1ccccc1)C2
Brc1ccc(C=NN=C2Nc3ccccc32)cc1
Brc1ccc(C=NNc2cc(Nc3ccc4[nH]cnc4c3)ncn2)cc1
Brc1ccc(CCN2C(Nc3ccccc3Br)=NC23CCC(Br)(c2ccccc2Nc2ncnc4ccccc24)CC3)cc1
Brc1ccc(Nc2ccncc2)nc1
Fine tuning...
Mean value of predictions: 0.018584907
Proportion of valid SMILES: 0.6643685365089314
Sample trajectories:
Brc1ccc(Nc2ncc3ncn(CC4CCC4)c3n2)nc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cccc(Nc2nnc(-c3ccoc3)n2-c2ccccc2)c1
Brc1cccc2ccccc12
Brc1cnc(Nc2ccc3ncccc3c2)nc1

  3 Training on 380 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.866967
Reward: 1.087372
Trajectories with max counts:
3	Fc1ccccc1F
Mean value of predictions: 0.023253676
Proportion of valid SMILES: 0.6823455628723738
Sample trajectories:
BP(=O)(N(c1ccc(Br)cc1)c1cc(Cl)cc(Cl)c1)P(=O)(c1ccccc1)c1cccc(Br)c1
Brc1ccc(C=NN2CCN(c3ccccn3)CC2)cc1
Brc1ccc(Nc2cc(Br)c(-c3nc4ccccc4o3)cn2)cc1
Brc1ccc(Nc2ccc3nc(-c4cscn4)oc3c2)cc1
Brc1ccc(Nc2nc(Nc3ccccc3)nc(Nc3ccc(CN4CCCCC4)cc3)n2)cc1
Policy gradient replay...
Mean value of predictions: 0.025230205
Proportion of valid SMILES: 0.67981220657277
Sample trajectories:
Brc1ccc(-c2nc(Nc3ccc(I)cc3)c3cc(Br)ccc3n2)cc1
Brc1ccc(Nc2c(Br)cnn2-c2ccccn2)cc1
Brc1ccc(Nc2nc(Nc3ccc(Nc4ccccc4)cc3)nc(Nc3cccs3)n2)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cccc(Nc2ncnc3cc(Br)c(Br)cc23)c1
Fine tuning...
Mean value of predictions: 0.029162562
Proportion of valid SMILES: 0.636563185951709
Sample trajectories:
BP(=O)(Nc1ccccc1)OC(=CSC(=S)N1CCOCC1)S(=O)(=O)c1cccc(Br)c1
BrC(=CCN1CCCCCC1)c1cccc2ccccc12
Brc1c(Br)c2sc(N3CCN(c4ccc5nccn5c4)CC3)nc12
Brc1ccc(-c2nc(-c3ccc(Br)o3)no2)cc1
Brc1ccc(NN=C2Nc3ccccc3S2)cc1

  4 Training on 631 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.485272
Reward: 1.226776
Trajectories with max counts:
11	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.040620588
Proportion of valid SMILES: 0.6653112292774476
Sample trajectories:
BP(=O)(N1CCN(C(=O)c2c(F)cccc2F)CC1)P(=O)(O)O
Brc1cc(Br)c2cccnc2c1
Brc1ccc(-c2sc3nccnc3c2-c2cccnc2)o1
Brc1ccc(Nc2ccnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2n[nH]c3ncnc(-c4ccccc4)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.037664782
Proportion of valid SMILES: 0.6641651031894934
Sample trajectories:
B[PH](=O)(=CN1C=C(F)C(=O)Nc2c(F)c(F)c(F)c(F)c2C1=O)OCC
Brc1cc(-c2cccc3ccccc3-c3ncccc23)on1
Brc1ccc(-c2ccccc2)c2c1-c1ccccc1S2
Brc1ccc(C(I)=NN=C2CCCCC2)cc1
Brc1ccc(Nc2cc(-c3cccnc3)ncn2)cc1
Fine tuning...
Mean value of predictions: 0.048621554
Proportion of valid SMILES: 0.6246086412022542
Sample trajectories:
Brc1cc(Nc2ccncc2)c2ccccc2n1
Brc1ccc(Br)c(Nc2ccncc2)c1
Brc1ccc(C=NNc2ccc3ccccc3n2)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccncc23)cc1

  5 Training on 1001 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.943720
Reward: 1.374911
Trajectories with max counts:
23	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.08266528
Proportion of valid SMILES: 0.6018170426065163
Sample trajectories:
BP(=O)(OCc1ccc(Br)cc1)Oc1ccc(F)c(F)c1
Brc1ccc(-n2cnc3c(N4CC4Br)ncnc32)cc1
Brc1ccc(C2Nc3cc(Br)ccc3Nc3ccccc32)cc1
Brc1ccc(CNc2ncnc3[nH]cnc23)cc1
Brc1ccc(NN=Cc2ccccc2)cc1
Policy gradient replay...
Mean value of predictions: 0.075650364
Proportion of valid SMILES: 0.6017532874139011
Sample trajectories:
Brc1ccc(I)cc1
Brc1ccc(Nc2cc3cc(Br)ccc3cn2)nc1
Brc1ccc(Nc2ccc(Nc3ccnc4cnc(Br)cc34)cc2)cc1
Brc1ccc(Nc2ccnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2cncnc2)cc1
Fine tuning...
Mean value of predictions: 0.089801975
Proportion of valid SMILES: 0.631447327289778
Sample trajectories:
Brc1ccc(-c2nc3ccccc3s2)c(C=Cc2ccco2)c1
Brc1ccc(Br)c(Nc2ccc3nncn3c2)c1
Brc1ccc(Br)c2c1Nc1ncccc1-2
Brc1ccc(N=Nc2ccc3c(c2)-c2cc(Br)sc2-3)cc1
Brc1ccc(NN=Nc2ccccc2)cc1

  6 Training on 1673 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.801517
Reward: 2.230560
Trajectories with max counts:
489	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.1219544
Proportion of valid SMILES: 0.4796875
Sample trajectories:
Brc1ccc(-c2nccnc2Oc2ccc(Br)s2)cc1
Brc1ccc(Nc2ncnc3c(Nc4ccccc4Br)ncnc23)cc1
Brc1ccc(Nc2ncnc3c4cc5ncccnc5Nc(ncnc23)n4)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cc4ccccc4nc23)cc1Nc1ccc2ncnc(Nc3cccc4ccccc34)c2c1
Policy gradient replay...
Mean value of predictions: 0.117914446
Proportion of valid SMILES: 0.4675
Sample trajectories:
Brc1cc2c(Nc3ccccc3Br)cccc2s1
Brc1ccc(-n2cnc3c(NCc4ccccc4)ncnc32)cc1
Brc1ccc(Nc2ccncc2N2CCN(Cc3ccccc3)CC2)cc1
Brc1ccc(Nc2ncnc3ccc(I)cc23)cc1
Brc1ccc(Nc2ncnc3cccc(Br)c3s2)cc1
Fine tuning...
Mean value of predictions: 0.14497109
Proportion of valid SMILES: 0.5411323115420706
Sample trajectories:
BrC1=CC2=CNC2=Nc2cc3c(cc2O1)OCO3
Brc1ccc(-c2cc(CNc3ccnc4ccc(-c5ccncc5)cc34)ncn2)nc1
Brc1ccc(C2CC(c3ccccn3)=NN2CC2CC2)cc1
Brc1ccc(N2CCCC2)nc1
Brc1ccc(NN=Cc2ccc3ccccc3c2)cc1

  7 Training on 2500 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 22.923143
Reward: 2.707955
Trajectories with max counts:
337	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.2090037
Proportion of valid SMILES: 0.4238348451673444
Sample trajectories:
BP(=O)(OCCC=C)Oc1ccc(Br)cc1
Brc1ccc(Nc2ncc3c4ncc(Br)cc4c3sc2Br)cc1
Brc1ccc(Nc2ncnc(Nc3ccc(Br)cc3)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.21729894
Proportion of valid SMILES: 0.4120037511722413
Sample trajectories:
Brc1cc2cn[nH]c2cc1OCCN1CCOCC1
Brc1ccc(-c2cc3sc4ncnc(Nc5cccc(Br)c5)c4c3[nH]2)cc1
Brc1ccc(Nc2ncnc(Nc3ccc(Br)cc3)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.18012197
Proportion of valid SMILES: 0.5125
Sample trajectories:
BP(=O)(OCC1OC(c2ccc(Br)cc2)C(O)C1O)c1ccc(Br)cc1
BP(=O)(OCCC(N)=O)N(=O)=O
B[PH](=O)(Nc1ccc(Br)cc1)(c1ccccc1)c1ccc(Br)cc1
Brc1ccc(Nc2cccnc2)cc1
Brc1ccc(Nc2ccncc2)nc1

  8 Training on 3543 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 23.108101
Reward: 3.851432
Trajectories with max counts:
869	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.23403361
Proportion of valid SMILES: 0.2975
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3cccc(Br)c23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cn1
Brc1ccc(Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.24244985
Proportion of valid SMILES: 0.2959375
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(Nc4ccccc4)ccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3sc(Cc4ccccc4)cc23)cc1
Brc1ccc2ncnc(Nc3ccncc3)Nc3ccccc3Sc2c1
Brc1cccc(Nc2ncnc3ccccc23)c1
Fine tuning...
Mean value of predictions: 0.22961877
Proportion of valid SMILES: 0.42638324476398876
Sample trajectories:
BP(=O)(OCC1CCCCC1)n1cnc2c(N)ncnc21
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1OP(=O)(O)O)C(N)=O
BP(=O)(OP(=O)(O)OP(=O)(O)OCC)N(CC(=O)N(O)Cc1cc(I)c(O)c(Br)c1Br)c1ccc(Br)cc1
Brc1ccc(Nc2ccncn2)nc1
Brc1ccc(Nc2ncccc2-c2ncnc3sccc23)cc1

  9 Training on 4409 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 24.786628
Reward: 4.123285
Trajectories with max counts:
98	Fc1ccc(Nc2ncnc3cc(F)cc(F)c23)cc1
Mean value of predictions: 0.33691275
Proportion of valid SMILES: 0.46606193306224586
Sample trajectories:
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Br)c2c(Nc3oc(Br)cc3Br)ncnc2c1
Brc1ccc(Nc2cc3c(CN4CCC(c5nc6cc(-c7ccc(Br)cc7)c(-c7cccs7)nc6s5)CC4)cc(Br)cc3s2)cc1
Brc1ccc(Nc2ncnc3c(Br)cc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Nc4cc(Br)c(Br)c(Br)c4)ncnc23)cc1
Policy gradient replay...
Mean value of predictions: 0.3247268
Proportion of valid SMILES: 0.45807259073842305
Sample trajectories:
BP(=O)(OC)OCC
Brc1cc(Br)c(Nc2ncnc3cc(Br)cc(Br)c23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccco3)ncnc2c1
Brc1cc(Br)nc(Nc2nc(N3CCOCC3)nc3cc(Br)cc(Br)c23)c1
Brc1cc(Nc2cc(Br)c(Br)cc2c2ccc(Br)c(Br)c2)c2c(Nc3ccc(I)cc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.26386914
Proportion of valid SMILES: 0.439375
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)C(=O)Oc1ccc(Br)cc1
BrC(Br)(Br)Br
BrCC1CCCC(N2CCc3c(NCc4ccccc4)ncnc32)C1
Brc1ccc(-c2ccc3ccccc3c2)nc1
Brc1ccc(-c2ccccc2)c(Nc2ncnc3ccccc23)c1

 10 Training on 5891 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 22.450285
Reward: 4.788151
Trajectories with max counts:
707	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.28975832
Proportion of valid SMILES: 0.2715625
Sample trajectories:
BrC1=CC2(CCCN2CCc2ccccc2)CCN1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2nncnc2Nc2ccc3ccccc3c2)cc1
Policy gradient replay...
Mean value of predictions: 0.28229886
Proportion of valid SMILES: 0.2719599874960925
Sample trajectories:
Brc1cc2c(N3CCN(Cc4ccc(Nc5ccccc5)cc4)CC3)c(Br)ccc2c(Br)c1Br
Brc1ccc(Nc2ncnc(Nc3ccccc3)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.30805153
Proportion of valid SMILES: 0.388125
Sample trajectories:
Brc1ccc(Br)c(Br)c1
Brc1ccc(Nc2cc(Br)nc3cc(Br)cc(Br)c23)cc1
Brc1ccc(Nc2ncnc3c(Br)cc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3c2COc2ccccc2-3)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1

 11 Training on 6786 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 22.939817
Reward: 5.407569
Trajectories with max counts:
809	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.173235
Proportion of valid SMILES: 0.2965625
Sample trajectories:
Bc1ccc(Br)cc1Br
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cccc(Nc2ncccc2-c2ncnc3ccccc23)c1
Brc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Policy gradient replay...
Mean value of predictions: 0.1891892
Proportion of valid SMILES: 0.300625
Sample trajectories:
Bc1cccc(Nc2ncnc3ccccc23)c1
Brc1cc(Nc2ncnc3ncncc23)ccc1Nc1cccc(Nc2ncccn2)c1
Brc1ccc(Nc2ncnc3cc(-c4ccccc4)nc(Br)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.31367293
Proportion of valid SMILES: 0.3496875
Sample trajectories:
BP(=O)(OC(=O)Cl)c1ccc(Nc2ncnc3c(Br)ccc(Cl)c23)cc1
Brc1ccc(Br)c(Nc2cccnc2)c1
Brc1ccc(C2=Nc3cc(Br)ccc3Nc3ccc(Br)cc32)cc1
Brc1ccc(Nc2ncnc3c2c2ccccc2S3)cc1
Brc1ccc(Nc2ncnc3cc(-c4nn[nH]n4)sc23)cc1

 12 Training on 7158 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 20.269648
Reward: 4.320008
Trajectories with max counts:
585	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.3635659
Proportion of valid SMILES: 0.3225
Sample trajectories:
Brc1cc(Nc2ncnc3ccsc23)cs1
Brc1ccc(-c2csc3c4ncncc4cc(Br)nn23)cc1
Brc1ccc(Nc2ccnc3ccsc23)cc1
Brc1ccc(Nc2ccncn2)cc1
Brc1ccc(Nc2nc3ccccc3s2)nc1
Policy gradient replay...
Mean value of predictions: 0.37629485
Proportion of valid SMILES: 0.31375
Sample trajectories:
BP(=O)(NO)c1ccc(Nc2nc(Cl)nc(Nc3cc(Br)cs3)c2Nc2sc(Br)cc2Br)cc1
Brc1[nH]nc2ncnc(Nc3ccccc3)c12
Brc1cc(Nc2ncnc3ccsc23)cs1
Brc1ccc(-c2ncnc3scc(Br)c23)cc1
Brc1ccc(Nc2cc(Nc3ncnc4ccsc34)ccc2Br)cc1
Fine tuning...
Mean value of predictions: 0.3304805
Proportion of valid SMILES: 0.41625
Sample trajectories:
Brc1cc(Br)c(Nc2ncnc3ccccc23)c(Br)c1
Brc1cc(Br)c2c(Nc3ccc(Br)s3)ncnc2c1
Brc1cc(Nc2ncnc3ccsc23)cc2c1sc1ccccc12
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ncnc3ccc(Br)cc23)c1

 13 Training on 7833 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.270005
Reward: 5.031523
Trajectories with max counts:
1050	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.43843135
Proportion of valid SMILES: 0.2390625
Sample trajectories:
BrC(=NN1Sc2cc(Br)ccc2OC1Cn1ccnc1)c1ccc(Br)cc1
Brc1cc2ncnc(Nc3ccsc3)c2cc1Br
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ccc3nncn3n2)c1
Brc1ccc(N=Nc2ncnc3ccsc23)cc1
Policy gradient replay...
Mean value of predictions: 0.4451613
Proportion of valid SMILES: 0.2325
Sample trajectories:
BrCc1nc2c(N=Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(Nc2ncnc3ccsc23)c2ccccc2n1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1
Brc1cc2ccccc2nc1Cc1ccncc1
Brc1ccc(Nc2nc3ccccc3s2)cc1
Fine tuning...
Mean value of predictions: 0.38754326
Proportion of valid SMILES: 0.36125
Sample trajectories:
BP(=O)(Nc1cccc(Nc2ncnc3cc(F)cc(F)c23)c1F)C(F)(F)F
BP(=O)(OCC)N(C(F)(F)F)P(=S)(Nc1ccc(Br)cc1)C(F)(F)F
BP(=O)(OCC1(Nc2ccc(F)cc2)CC(=O)Oc2ccccc21)C(F)(F)F
BP(=O)(OCC1OC(CO)C(O)C(O)C1O)Oc1cccc2c(Br)ccc(Br)c12
BP(=O)(OCC1OC(N2C=C(Br)C(=O)NC2=O)C(O)C(O)C1O)Oc1ccc(Br)cc1

 14 Training on 8606 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 18.900795
Reward: 5.320633
Trajectories with max counts:
1379	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5068853
Proportion of valid SMILES: 0.190625
Sample trajectories:
BP(=O)(Nc1cc(F)c(F)c(F)c1)OCC
BP(=O)(Nc1ccc(Cl)cc1)Nc1ccc(Br)cc1
BP(=O)(OCCO)C(=O)Nc1cc(Nc2ncnc3sccc23)cs1
Brc1cc(Nc2ncnc3ccsc23)cs1
Brc1ccc(Nc2ccc(Nc3ccnc4cccnc34)cc2)cc1
Policy gradient replay...
Mean value of predictions: 0.51137924
Proportion of valid SMILES: 0.18125
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1cccc(Nc2ncnc3ccsc23)c1
BrC(Br)c1ccc[nH]1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1ccc(Nc2cc(Br)cnc2Br)cc1
Fine tuning...
Mean value of predictions: 0.43183675
Proportion of valid SMILES: 0.30625
Sample trajectories:
BP(=O)(C(=O)OCC)N(CC(=O)Nc1cccc(Br)c1)P(=O)(O)O
BP(=O)(c1ccc(F)c(F)c1)N(O)C(F)(F)F
B[PH](=O)(=NC)OC(C)=O
Bc1cc(Br)cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)c1
Brc1cc2ncnc(Nc3ccc(Br)c(Br)c3)c2s1

 15 Training on 9314 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.771240
Reward: 5.880230
Trajectories with max counts:
1430	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5055644
Proportion of valid SMILES: 0.1965625
Sample trajectories:
BP(=O)(OC(C)CO)P(=O)(O)O
BrC=C(Br)Br
BrCc1cc2ncnc(Nc3ccc(Br)cc3)c2s1
Brc1cc(Nc2ncnc3ccsc23)cc2ccccc12
Brc1cc(Nc2ncnc3ccsc23)ccc1Nc1ncnc2sccc12
Policy gradient replay...
Mean value of predictions: 0.49965867
Proportion of valid SMILES: 0.183125
Sample trajectories:
BP(=O)(Nc1ccc(F)cc1)C(=O)Oc1ccc(Nc2ncnc3ccccc23)cc1
BP(=O)(c1ccc(Br)c(Br)c1)N1CCN(C(=O)c2ccc(N)c(I)c2)CC1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1ccc(NNc2ncnc3ccsc23)cc1
Brc1ccc(Nc2ccnc(Nc3ccsc3)c2)cc1
Fine tuning...
Mean value of predictions: 0.3971831
Proportion of valid SMILES: 0.310625
Sample trajectories:
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrCCN1Oc2cc(Br)cc(Br)c2C=Cc2c(Nc3ccc(Br)s3)ncnc21
Brc1cc(Br)c2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
Brc1cc(Br)c2ncc(Br)c(Br)c2c1
Brc1ccc(-c2nc3ccc(Br)cn3c2-c2cccs2)cc1

 16 Training on 10034 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 18.244940
Reward: 6.613193
Trajectories with max counts:
1639	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5289157
Proportion of valid SMILES: 0.155625
Sample trajectories:
Brc1cc(Br)c(Br)c(Sc2ccccc2Nc2ccccc2Br)n1
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c(Nc2ncnc3sc4c(Nc5ccccc5)cc4c23)cc1Br
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(Br)nc(Nc2ncnc3cc(Br)c(Br)nc23)c1
Policy gradient replay...
Mean value of predictions: 0.51382405
Proportion of valid SMILES: 0.1740625
Sample trajectories:
Brc1cc(Br)c(N2CCN(CCNc3ncncn3)CC2)c(Br)c1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(N(c2cnn(C3CC3)c2)c2ncnc3ccsc23)cnc1-c1cccs1
Brc1cc(Nc2ncnc3c(Nc4ccccc4Br)ncn23)ncn1
Brc1cc(Nc2ncnc3ccsc23)ccc1Nc1ncccn1
Fine tuning...
Mean value of predictions: 0.44948983
Proportion of valid SMILES: 0.245
Sample trajectories:
Bc1ccc(Nc2nc(Nc3ccccc3)sc2C#N)cc1
BrCCSc1ccc(Nc2ncnc3ccccc23)cc1
BrCc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(-c2ccccc2)n(-c2ccccc2)c1-c1ccccc1
Brc1cc(I)cc(Nc2ncnc3sccc23)c1

 17 Training on 10690 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 21.151334
Reward: 6.889933
Trajectories with max counts:
976	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.44771242
Proportion of valid SMILES: 0.19125
Sample trajectories:
BP(=O)(Nc1ccc(Nc2ncnc3ccsc23)cc1)Nc1ccc(Br)cc1F
BP(=O)(O)OP(=O)(O)OP(=O)(O)O
Bc1cc(Nc2ncnc3ccsc23)ccc1Br
Bc1ccc(Nc2ncnc3ccsc23)cc1
BrC=Nc1ccccc1-c1ccccc1
Policy gradient replay...
Mean value of predictions: 0.4858569
Proportion of valid SMILES: 0.1878125
Sample trajectories:
BP(=O)(CCCc1ccccc1)NO
BP(=O)(OCC1OC(=N)NC(c2ccsc2)=N[PH](c2cccnc2)(C(F)(F)F)C(O)C1O)C(O)CP(=O)(O)O
BP(=O)(OCC1OC(=NO)C(O)C1O)c1ccc(Br)cc1
B[PH](=O)(Nc1ccc(Br)cc1)=P(=NO)c1ccccc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Fine tuning...
Mean value of predictions: 0.42436883
Proportion of valid SMILES: 0.2846875
Sample trajectories:
BP(=O)(Oc1ccc(Br)c(Br)c1)OC(C)COC(=O)CCCCCCCCC
BP(F)(F)(F)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)CP(=O)(O)OP(=O)(O)C(F)(F)P(=O)(O)O
BP1(=O)OCC2OC(=N)N(O1)C2C(=O)Oc1cc(Br)cc(Br)c1
B[PH](=O)(CCNc1ccc(Br)cn1)=[PH](c1ccc(Br)cc1)P(=O)(O)Oc1cccc(Br)c1
Bc1ccc(Nc2ncnc3cccc(Br)c23)cc1

 18 Training on 11392 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.798561
Reward: 7.461629
Trajectories with max counts:
1447	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.502193
Proportion of valid SMILES: 0.1425
Sample trajectories:
B[PH](=O)(Nc1ccc(Nc2ccccc2Br)cc1)(P(=O)(O)O)[PH](O)(O)OP(=O)(O)O
Bc1cc(Nc2ncnc3ccsc23)ccc1F
BrBr
Brc1cc(Br)c(Nc2ccc(Br)c(Br)c2)c(I)c1
Brc1cc(Br)c2cncnc2c1Nc1ncnc2ccsc12
Policy gradient replay...
Mean value of predictions: 0.5134474
Proportion of valid SMILES: 0.1278125
Sample trajectories:
BP(=O)(Nc1ccc(F)c(F)c1)c1ccc(F)c(Br)c1
BP(=O)(Nc1ccc(NP(=O)(O)OCC(F)(F)F)cc1)c1ccc(F)cc1
BP(=O)(OCC)ON(C(=O)OP(=O)(O)OP(=O)(O)OP(=O)(O)C(F)(F)F)c1cnc(NP(=O)(OC)OC(F)(F)F)cc1F
Bc1c(I)cc(I)c(Nc2ncnc3ccsc23)c1I
Brc1cc(Br)c(Nc2ncnc3ccccc23)cc1Br
Fine tuning...
Mean value of predictions: 0.45927978
Proportion of valid SMILES: 0.225625
Sample trajectories:
BP(=O)(N(O)C(Cc1ccccc1)NCP(=O)(O)OP(=O)(O)O)P(=O)(O)O
BP(=O)(NO)N(O)C(=O)OC
BP(=O)(NO)c1ccc(Br)cc1
Bc1ccc(Nc2nc3cc(Br)ccc3s2)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1

 19 Training on 11956 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.328473
Reward: 7.696481
Trajectories with max counts:
1561	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49582505
Proportion of valid SMILES: 0.1571875
Sample trajectories:
BP(=O)(NO)c1ccc(Nc2cc(Br)cc(Br)c2)cc1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(Nc2cncnc2)cs1
Brc1ccc(-c2ccc(Nc3ncnc4ccsc34)cc2)s1
Brc1ccc(-c2ccc(Nc3ncnc4sccc34)cc2)s1
Policy gradient replay...
Mean value of predictions: 0.4568138
Proportion of valid SMILES: 0.1628125
Sample trajectories:
BP(=O)(NCc1ccc(Br)cc1)c1c(Br)c(Br)c(Br)c(Br)c1Br
BP(=O)(Nc1cc(Br)cc(Nc2ncnc3ccccc23)c1)OCCO
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)Oc1cccc2c(I)cc(Br)cc12
Bc1cc(Br)c(Br)cc1Br
Brc1c(Br)c(Br)c(Br)c(Br)c1Br
Fine tuning...
Mean value of predictions: 0.5041769
Proportion of valid SMILES: 0.254375
Sample trajectories:
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c(Br)c(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Brc1cc(Nc2ncnc3ccccc23)ccc1C=NNc1ccc(Nc2ncnc3ccsc23)cc1
Brc1ccc(-c2cc(Nc3ncnc4ccsc34)co2)cc1

 20 Training on 12608 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 20.147293
Reward: 7.970521
Trajectories with max counts:
791	Fc1ccc(Nc2ncnc3ccsc23)cc1F
Mean value of predictions: 0.56970954
Proportion of valid SMILES: 0.150625
Sample trajectories:
BP1(=O)OCC2OC(OC(=O)CCCCCCC(C(C)(Br)C(O)C(C)O)C(C)(C)C(=CCBr)CC1O)C(O)C2Br
BrCC1CCCN1Sc1ccc(Nc2cc3ccsc3cn2)o1
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(Nc2ncnc3ccsc23)ccn1
Brc1ccc(Br)c(Nc2ccc(Nc3nccs3)cc2)c1
Policy gradient replay...
Mean value of predictions: 0.6097473
Proportion of valid SMILES: 0.173125
Sample trajectories:
BP(=O)(O)C(F)(F)F
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)O
BP1(=O)OCC2OC(C(O)C2O)N(C=C(Br)Br)C(=O)NC1=O
Brc1cc(Br)cc(Nc2ncnc3ccsc23)c1
Brc1cc(c2ccccc2Br)sc1-c1ccc(Nc2ncnc3ccsc23)nc1
Fine tuning...
Mean value of predictions: 0.54013604
Proportion of valid SMILES: 0.2296875
Sample trajectories:
BP(=O)(c1ccc(Br)cc1)N(CC(=O)OC)C(F)(F)F
Br
BrCBr
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Nc2nc(Nc3ccsc3)nc3ccccc23)cc1Br

Trajectories with max counts:
4759	Fc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.46281743
Proportion of valid SMILES: 0.1796875
Mean Internal Similarity: 0.47822129479082975
Std Internal Similarity: 0.11629405820111426
Mean External Similarity: 0.39626110380215046
Std External Similarity: 0.06825734681377792
Mean MolWt: 359.07231389102077
Std MolWt: 83.12793875671748
Effect MolWt: -1.3219425962291051
Mean MolLogP: 4.671416078280892
Std MolLogP: 1.1441292095383229
Effect MolLogP: -0.03656472299061179
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 95.982143% (430 / 448)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 20, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5568.819264888763, 'valid_fraction': 0.1796875, 'active_fraction': 0.45321739130434785, 'max_counts': 4759, 'mean_internal_similarity': 0.47822129479082975, 'std_internal_similarity': 0.11629405820111426, 'mean_external_similarity': 0.39626110380215046, 'std_external_similarity': 0.06825734681377792, 'mean_MolWt': 359.07231389102077, 'std_MolWt': 83.12793875671748, 'effect_MolWt': -1.3219425962291051, 'mean_MolLogP': 4.671416078280892, 'std_MolLogP': 1.1441292095383229, 'effect_MolLogP': -0.03656472299061179, 'generated_scaffolds': 448, 'novel_scaffolds': 430, 'novel_fraction': 0.9598214285714286, 'save_path': '../logs/replay_combo_s1-8.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.060398
Reward: 1.000000
Mean value of predictions: 0.0012028869
Proportion of valid SMILES: 0.782063342740671
Sample trajectories:
BrCc1ccc[n+](CCCCCCNCCC2CCCC2)c1
Brc1ccc(I)cc1
Brc1ccc(OCCc2ccccn2)nc1
Brc1cccc(C2=NC3=C(C=Cc4cc(Br)ccc4NC=N2)CCCC3)c1
C#CCCN(C)C1Cc2ccc2OCc2cc(F)ccc21
Policy gradient replay...
Mean value of predictions: 0.0013654619
Proportion of valid SMILES: 0.7810539523212046
Sample trajectories:
Brc1cc(C=Cc2ccccc2)nc(-c2ccccc2)c1
Brc1ccc(Sc2[nH]c3cccnc3c2Br)cc1Br
Brc1cccc(-c2cnnc(Nc3ccccc3)n2)c1
Brc1cccc(-c2nnc3ccnc(NCCN4CCCCC4)c3n2)c1
C#CC(O)CCCCC
Fine tuning...
Mean value of predictions: 0.024125695
Proportion of valid SMILES: 0.6177207263619287
Sample trajectories:
Brc1ccc(Nc2nc3cc(Br)ccc3[nH]2)cc1
Brc1ccc(Nc2ncc3ccccc3n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)cnc23)cc1
Brc1ccc2[nH]cc(-c3ccccc3)c2c1
Brc1cccc(-c2nc(-c3cccs3)c3cnc4occc4c3n2)c1

  2 Training on 297 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 18.891616
Reward: 1.150085
Trajectories with max counts:
12	Nc1ncnc2c1ncn2C1OC(CO)C(O)C1O
Mean value of predictions: 0.044809863
Proportion of valid SMILES: 0.6088861076345432
Sample trajectories:
BP(=O)(OCC)OC(=O)CCC(CP(=O)(O)O)P(=O)(O)O
Brc1ccc(-c2ncnc3ccc(-c4ccccc4)cc23)s1
Brc1ccc(Nc2cc(Nc3ncnc4ccc(Br)cc34)ccn2)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc2c(c1)C1=CC(=N2)CS1
Policy gradient replay...
Mean value of predictions: 0.04783715
Proportion of valid SMILES: 0.6152160300563556
Sample trajectories:
BP(=O)(OCC(=O)NO)OC(=O)C=Cc1ccc(F)cc1
BP(=O)(c1cc2ccccc2cc(-c2ccc(Cl)cc2)c2ccc3c(O)cc(Br)cc3c12)N1CCOCC1
BrCC1CCCNCC1
Brc1ccc(-c2ncnn2C2CCCCN2)c2cccnc12
Brc1ccc(C=NN2CCCN(CCNc3ccccc3Br)CC2)cc1
Fine tuning...
Mean value of predictions: 0.085863195
Proportion of valid SMILES: 0.4836168872085696
Sample trajectories:
Brc1ccc(Nc2cncnc2)cc1
Brc1ccc(Nc2nc(Nc3ccccc3Br)sc2Br)cc1
Brc1ccc(Nc2ncnc3[nH]cnc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Br)cc23)cc1
Brc1ccc(Nc2ncnc3cccnc23)cc1

  3 Training on 757 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.693679
Reward: 1.317353
Trajectories with max counts:
9	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.06252028
Proportion of valid SMILES: 0.578716744913928
Sample trajectories:
BrCCNc1nc2ccccc2cc1-c1ccc2ccccc2c1
Brc1ccc(-c2cc3ccccc3c3ccccc23)o1
Brc1ccc(-c2ccccc2)c(-c2cccnc2)c1-c1ccncc1
Brc1ccc(Nc2ccccc2Sc2nc3ccccc3s2)cc1
Brc1ccc(Nc2cnc3cc(-c4cccnc4)cc3nc2Nc2cncnc2)cc1
Policy gradient replay...
Mean value of predictions: 0.065557405
Proportion of valid SMILES: 0.564319248826291
Sample trajectories:
Brc1ccc(Nc2ccc(Nc3ccc4ncccc4c3)nc2)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4cccc(I)c4)c3n2)cc1
Brc1ccc(Nc2ncnc3[nH]c4cnccc4c23)cc1
Brc1ccc(Nc2ncnc3c2N=CC(c2ccc4c(Nc5ccccc5Br)ncnc4c2)=N3)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.12290562
Proportion of valid SMILES: 0.589375
Sample trajectories:
Brc1cc(Br)c2c(Br)cc3ncnn3c2c1
Brc1ccc(-n2cncc2-c2nccnc2Nc2cccc(I)c2)cc1
Brc1ccc(Nc2ccc3ncnc(Nc4ccc(Br)cc4)c3c2)cc1
Brc1ccc(Nc2ccnc3ccccc23)cc1
Brc1ccc(Nc2ccncc2)cc1

  4 Training on 1403 replay instances...
Setting threshold to 0.100000
Policy gradient...
Loss: 20.560359
Reward: 1.595567
Trajectories with max counts:
25	Fc1ccc(Nc2ncnc3cc(F)ccc23)cc1
Mean value of predictions: 0.20305538
Proportion of valid SMILES: 0.49480314960629923
Sample trajectories:
Brc1cc(Br)c(Br)s1
Brc1ccc(-n2cc(Nc3ncnc4ccccc34)cn2)c(Br)c1
Brc1ccc(Br)c(Nc2cccnc2)c1
Brc1ccc(CSc2ncnc3nc(Nc4ccc(Br)s4)sc23)cc1
Brc1ccc(N2CCN(Cc3cccc(Nc4nc5ccccc5s4)c3)CC2)cc1
Policy gradient replay...
Mean value of predictions: 0.20523295
Proportion of valid SMILES: 0.49338790931989923
Sample trajectories:
B[PH](=O)(Nc1cc(Br)c(Br)c(Br)c1)=[PH](=O)(c1ccc(F)cc1)c1ccc(F)c(F)c1
Brc1cc2ccc(Nc3ncn[nH]3)cc2cn1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2cc(I)cc(Nc3cc(Br)cnc3I)n2)c1
Brc1ccc(N2CCN(c3cncnc3Nc3cnccn3)CC2)cc1Nc1c2ccccc2nc2cccnc12
Fine tuning...
Mean value of predictions: 0.18733081
Proportion of valid SMILES: 0.5777291210509853
Sample trajectories:
BP(=O)(OCC)OCCCCCCC
Brc1cc(Br)c2c(Nc3ccc(Br)o3)ncnc2c1
Brc1cc(Br)cc(Nc2ncnc3ccccc23)c1
Brc1cc(Nc2ccncc2)ncn1
Brc1cc2c(cc1Br)C1NCCCC1C2

  5 Training on 2734 replay instances...
Setting threshold to 0.250000
Policy gradient...
Loss: 21.640924
Reward: 2.705866
Trajectories with max counts:
460	Fc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.20469256
Proportion of valid SMILES: 0.38625
Sample trajectories:
Brc1ccc(-c2ncnc3ccccc23)c2ccccc12
Brc1ccc(N=Nc2ccc3ncnc(Nc4ccccc4)c3c2)cc1
Brc1ccc(Nc2cc(Nc3ccc(Br)cc3I)nc3ccccc23)cc1
Brc1ccc(Nc2cc(Nc3cccnc3Br)ccc2Br)nc1
Brc1ccc(Nc2nc3ccccc3s2)cc1Br
Policy gradient replay...
Mean value of predictions: 0.21008404
Proportion of valid SMILES: 0.371875
Sample trajectories:
Brc1cc2ncnc(Nc3ccccc3)c2cc1N1CCCC1
Brc1ccc(Br)c(Nc2ncnc3ccccc23)c1
Brc1ccc(C2CCN(c3ncnc4c(Nc5c(Br)ccc6ncncc56)ncnc34)N2c2ccccc2)cc1
Brc1ccc(Nc2ccnc3ccccc23)cc1
Brc1ccc(Nc2ccncc2Br)cc1Br
Fine tuning...
Mean value of predictions: 0.24758807
Proportion of valid SMILES: 0.5765625
Sample trajectories:
BrC1=CC(c2nc(N3CCN(Cc4ccccc4)CC3)nn2-c2ccccc2)=NC1=Nc1ccc(Br)cc1
Brc1cc(Br)c(Br)cn1
Brc1cc(Br)c2c(Nc3cccs3)ncnc2c1
Brc1ccc(-c2nnc(-c3ccc(Br)c4cc(Br)ccc34)n2-c2ccc(Br)cc2)cc1
Brc1ccc(Br)c(Br)c1

  6 Training on 3837 replay instances...
Setting threshold to 0.400000
Policy gradient...
Loss: 22.767592
Reward: 3.354539
Trajectories with max counts:
485	Fc1ccc(Nc2ncnc3ccccc23)cc1F
Mean value of predictions: 0.24025045
Proportion of valid SMILES: 0.349375
Sample trajectories:
Brc1ccc(Nc2nccc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccc4ccccc4c23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Nc2ncnc3ccccc23)cc1Br
Brc1ccc(Nc2ncnc3cccnc23)cc1
Policy gradient replay...
Mean value of predictions: 0.25753662
Proportion of valid SMILES: 0.36292591434823385
Sample trajectories:
BrBr
Brc1ccc(Nc2ncnc3ccc(Nc4ccccc4)cc23)cc1
Brc1ccc(Nc2ncnc3cccc(Br)c23)cc1
Brc1ccc(Nc2ncnc3ccccc23)c(I)c1
Brc1ccc(Nc2ncnc3ccccc23)cc1
Fine tuning...
Mean value of predictions: 0.26313996
Proportion of valid SMILES: 0.549718574108818
Sample trajectories:
BrCN1CCCN(Cc2ccccc2)CC1
BrCc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(Br)c(Nc2cc(Br)c(Br)cn2)c1
Brc1ccc(Br)c(Nc2ncnc3ccc(Nc4cnccc4Br)nc23)c1
Brc1ccc(Nc2ccc(Nc3cccc4cc(Br)ccc34)c(CC[n+]3ccccc3)c2)cc1

  7 Training on 4967 replay instances...
Setting threshold to 0.550000
Policy gradient...
Loss: 25.989153
Reward: 4.130722
Trajectories with max counts:
137	Fc1ccc(Nc2ncnc3c(F)c(F)c(F)c(F)c23)cc1
Mean value of predictions: 0.3432602
Proportion of valid SMILES: 0.3994990607388854
Sample trajectories:
Brc1cc(Br)c2ncnc(Nc3ccc(Nc4ncnc5c4ncn5C4CN5CCCC45)c(Br)c3)c2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc(CN2CCCCC2)cc(Nc2c(Br)cnc3[nH]c(Br)c(Br)c23)c1
Brc1cc(Nc2nc(-c3cncnc3)ncc2Br)ccn1
Brc1cc(Nc2ncnc3cc(Br)c(Br)c(Br)c23)nc(Nc2cc(Br)c(Br)c(Br)c2)n1
Policy gradient replay...
Mean value of predictions: 0.33363986
Proportion of valid SMILES: 0.4089627076151677
Sample trajectories:
Brc1cc(Br)c(Br)c(Br)c1Br
Brc1cc(Br)c(N(Cc2ccc(Nc3ncnc4c(Br)c(Br)c(Br)c(Br)c(Br)c(Br)c34)cc2Br)c2nc3ncc(Br)c(Br)c3cc2Br)cc1Br
Brc1cc(Br)c(Nc2ncnc3cc(Nc4cc(Br)c(Br)c(Br)c4)cnc23)c(Br)c1
Brc1cc(Nc2ncnc3c(Br)cc(Br)c(Br)c23)ncn1
Brc1ccc(Br)c(-c2cc(Br)c(Br)c(Br)c2)c1
Fine tuning...
Mean value of predictions: 0.31785324
Proportion of valid SMILES: 0.5709818636647905
Sample trajectories:
BP(=O)(O)OP(=O)(O)c1cc(F)c(F)c(F)c1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cc(Br)c2c(Nc3ncccn3)ncnc2n1
Brc1cc(Br)c2ncnc(Nc3cc(Br)c(Br)cc3Br)c2c1
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2s1

  8 Training on 6515 replay instances...
Setting threshold to 0.700000
Policy gradient...
Loss: 25.304513
Reward: 5.042418
Trajectories with max counts:
833	Fc1ccc(Nc2ncnc3ccccc23)cc1F
Mean value of predictions: 0.33601
Proportion of valid SMILES: 0.2490625
Sample trajectories:
Brc1ccc(Nc2ncnc3c(Br)cccc23)cc1
Brc1ccc(Nc2ncnc3c(Br)cccc23)cc1Br
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc(-c4ccccc4)cc23)cc1Br
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Policy gradient replay...
Mean value of predictions: 0.35654208
Proportion of valid SMILES: 0.2675
Sample trajectories:
BP(=O)(OCC)C(=O)Nc1ccccc1Br
Brc1ccc(Br)c(Nc2ccc(Nc3ncnc4ccccc34)cc2)c1
Brc1ccc(Nc2ccc3ncnc(Nc4ccccc4Br)c3c2)c(Br)c1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Fine tuning...
Mean value of predictions: 0.37351072
Proportion of valid SMILES: 0.5411580594679186
Sample trajectories:
Brc1cc(Br)c2c(Nc3ncc(Br)s3)ncnc2c1
Brc1cc(Br)c2ccccc2c1
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc2c(Nc3ccccc3Br)cccc2s1
Brc1cc2c(s1)Sc1ccccc1N2

  9 Training on 7742 replay instances...
Setting threshold to 0.850000
Policy gradient...
Loss: 25.314023
Reward: 3.753207
Trajectories with max counts:
18	Fc1ccc(Nc2ncnc3c(F)c(F)c(F)c(F)c23)cc1F
Mean value of predictions: 0.39486283
Proportion of valid SMILES: 0.5364860632633887
Sample trajectories:
BP(=O)(OCC1OC(OP(=O)(O)O)C(O)C1O)n1cnc2c(N)ncnc21
BrC1=CN(c2ccnc3c(Br)cc(Br)c(Br)c23)C1
BrC=CBr
BrC=Cc1nc(NN=C(Br)N2CCCOCC2)nc(-c2cncnc2)n1
BrCC(Br)Br
Policy gradient replay...
Mean value of predictions: 0.41177142
Proportion of valid SMILES: 0.5479023168440826
Sample trajectories:
BBr
BP(=O)(CCC(=O)Oc1cc(Cl)c(N)c(Br)c1)NO
BP(=O)(O)C(=O)Oc1cc(Br)c(Br)c(Br)c1Br
BP(=O)(OCCS)C(=O)O
BP(=O)(c1cc(F)c(F)c(F)c1)N(O)C(F)(F)F
Fine tuning...
Mean value of predictions: 0.4011508
Proportion of valid SMILES: 0.5432947796186308
Sample trajectories:
BP(=O)(OCC1OC(c2ccc(Br)cc2)NC1=O)c1ccccc1
Brc1cc(Br)c(Br)c(Br)c1
Brc1cc(Br)c(Br)s1
Brc1cc(Br)c(Nc2c(Br)ccc3ncncc23)c(Br)c1
Brc1cc(Br)c(Nc2ncnc3cc(Br)ccc23)c(Br)c1

 10 Training on 9000 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.125125
Reward: 3.515287
Trajectories with max counts:
64	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.50344026
Proportion of valid SMILES: 0.5364404128870817
Sample trajectories:
BP(=O)(OCC)C(=O)Oc1c(F)c(F)c(F)c(F)c1F
BP(=O)(OCC)C(O)C(F)(F)F
B[PH](=O)(NO)(Nc1cc(Br)c(Br)cc1Br)OCc1ccc(Br)cc1
B[PH](=O)(O)(O)C=CBr
Bc1cc(Br)c(Nc2ncnc3ccsc23)cn1
Policy gradient replay...
Mean value of predictions: 0.46725145
Proportion of valid SMILES: 0.534876446668752
Sample trajectories:
BP(=O)(O)Oc1cc(Br)c2c(c1Br)CCO2
BP(=O)(OC)OP(=O)(O)OP(=O)(O)OCCOP(=O)(O)O
Bc1cc(Br)cc(Br)c1Nc1ncnc2c(Br)c(Br)cc(Br)c12
BrC(=NNc1ccc(Br)cc1)c1cccnc1
BrCc1cc2c(Nc3ccc(Br)cc3)ncnc2s1
Fine tuning...
Mean value of predictions: 0.38160303
Proportion of valid SMILES: 0.5809375
Sample trajectories:
BP(=O)(O)NP(=O)(OCC)OCC
BP(=O)(OCC)C(=O)ONc1c(Cl)cc(Br)cc1Br
BP(=O)(OCC1OC(c2ccc(F)cc2)C(O)(O)C1O)c1ccc(F)cc1F
BrCCNc1ccc2c(Nc3cccnc3)ncnc2c1
Brc1cc(Br)c(Br)c(Nc2ccc(Br)c(Nc3ncnc4cc(Br)ccc34)c2)c1

 11 Training on 10509 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.910799
Reward: 3.946086
Trajectories with max counts:
192	Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Mean value of predictions: 0.5020628
Proportion of valid SMILES: 0.3485464207564864
Sample trajectories:
BC(=N)Nc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BP(=O)(CCC=C(Br)Br)OCC
BP(=O)(NO)c1ccc(Br)c(Nc2ccc(Br)cc2)c1
BP(=O)(O)Oc1ccc(Nc2ncnc3c(Br)c(Br)ccc4c(Br)cc(Br)c(Br)c4c23)cc1
BP(=O)(OC1OC(C#N)C(C(F)F)=NP1(F)(F)F)c1cccc(F)c1
Policy gradient replay...
Mean value of predictions: 0.5234799
Proportion of valid SMILES: 0.3340625
Sample trajectories:
BP(=O)(CBr)CC(Br)Br
BP(=O)(CCCC=CC=C)OCCCN
BP(=O)(CCCNCC=C)OCC
BP(=O)(CCOP(=O)(O)O)NO
BP(=O)(CCl)NP(=O)(Nc1ccc(F)c(F)c1)N1CCOCC1
Fine tuning...
Mean value of predictions: 0.43880928
Proportion of valid SMILES: 0.567584480600751
Sample trajectories:
BrCc1cc(Br)cc2ncnc(Nc3c(Br)sc4ccccc34)c12
BrCc1ccc2c(Nc3ccc(Br)cc3Br)ncnc2c1
BrCc1ncnc2c(Nc3ccc(Br)cc3)ncnc12
BrSc1ccc(Nc2ncnc3ccsc23)cc1
Brc1cc(Br)c(Br)c(Br)c1

 12 Training on 11883 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.076148
Reward: 4.394209
Trajectories with max counts:
443	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.476731
Proportion of valid SMILES: 0.2753125
Sample trajectories:
BBr
BP(=O)(NC(c1ccc(Br)cc1)P(=O)(O)O)N(=O)=O
BP(=O)(OCC)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(B)(=O)OP(=O)(O)O[PH](B)(Br)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP1(=O)OCC(C(=O)O)(C(=O)NS(=O)(=O)c2ccccc2)c2sc3ccccc3c21
B[PH](=O)(Nc1ccccc1)(OCC1CCCO1)P(=O)(NO)Oc1ccccc1
Policy gradient replay...
Mean value of predictions: 0.4737798
Proportion of valid SMILES: 0.27539856205064084
Sample trajectories:
BP(=O)(CCOc1ccc(Nc2cc(Br)cc(Br)c2)cc1)Nc1ccc(Br)cc1
BP(=O)(NC(c1ccccc1)c1cccnc1)c1ccc(F)c(F)c1
BP(=O)(Nc1cc(F)c(F)c(F)c1)N1C=CC(=O)NC1=O
BP(=O)(O)CNC(=O)C(Cl)P(=O)(O)O
BP(=O)(O)c1ccc2c(Nc3ccc(C4CC4)cc3)ncnc2c1
Fine tuning...
Mean value of predictions: 0.449033
Proportion of valid SMILES: 0.549718574108818
Sample trajectories:
BP(=O)(NO)(c1ccc(Br)cc1)(P(=O)(O)O)P(=O)(O)O
B[PH](=O)(Cl)(Br)OCCCl
BrCCc1c[nH]c2ccccc12
BrSc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Brc1c(Br)c(Br)c2c(Br)c(Br)c(Br)c(Br)c2c1Br

 13 Training on 13006 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 27.060836
Reward: 4.864759
Trajectories with max counts:
186	Brc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Mean value of predictions: 0.5568162
Proportion of valid SMILES: 0.3690625
Sample trajectories:
BP(=O)(OC(Cl)(Cl)Cl)c1cc(Nc2ncnc3cc(Br)ccc23)ccc1Br
BP(=O)(OCC)OC(=O)CCCCCCCCCCCS
BP(=O)(OCC)Oc1cc(Br)c(Br)c(Br)c1Br
BP(=O)(OCC)c1c(F)cc(F)c(N)c1-c1c(F)cc(F)cc1F
BP(=O)(OCC1OC(N2C=CC(N)=NC2=O)C(O)C1O)OP(=O)(O)OP(=O)(O)O
Policy gradient replay...
Mean value of predictions: 0.5744415
Proportion of valid SMILES: 0.3497968115035949
Sample trajectories:
BP(=O)(Br)OCCBr
BP(=O)(CCl)Nc1ccc2ncnc(Nc3cc(F)c(F)c(Cl)c3F)c2c1
BP(=O)(CO)OCCO
BP(=O)(F)(F)(F)Oc1cc(Nc2ncnc3c(F)c(F)c(F)c(F)c23)cc(F)c1F
BP(=O)(Nc1cc(F)c(Br)c(Br)c1F)OCOc1cc(F)c(F)c(F)c1
Fine tuning...
Mean value of predictions: 0.49129942
Proportion of valid SMILES: 0.5532979055954986
Sample trajectories:
Bc1cc(Br)c(Br)c(Br)c1Br
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
BrCCCCN1c2ccc1c1c(Nc3ccc(-c4ncnc5sccc45)c(Br)c3)ncnc1cc2
BrCCCNc1ccc(I)cc1Br
BrCCNc1cc(Nc2nc3c(Br)cccc23)ccc1Br

 14 Training on 14658 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.798352
Reward: 5.058713
Trajectories with max counts:
221	Brc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Mean value of predictions: 0.53943217
Proportion of valid SMILES: 0.2971875
Sample trajectories:
BP(=O)(NCc1ccc(Br)cc1Br)OCCl
BP(=O)(O)c1cccc(Nc2ncnc3ccccc23)c1
BP(=O)(Oc1ccc2ncnc(Br)c2c1Br)N1CCCC1
BP(=O)(c1ccc(Br)cc1)c1cc(Br)cc(Br)c1
Bc1cc(Br)cc(Br)c1Br
Policy gradient replay...
Mean value of predictions: 0.56835705
Proportion of valid SMILES: 0.308125
Sample trajectories:
BP(=O)(Br)Nc1ccc(Br)cc1Br
BP(=O)(C(=O)Nc1ccc(Br)cc1Br)N(Cc1ccccc1)c1ccc(Br)cc1
BP(=O)(NO)c1ccc(Nc2nc(Br)c(Br)c(Br)c2O)nc1
BP(=O)(Nc1ccc(Br)c(Br)c1)P(=O)(Oc1cccc(Br)c1)N(=O)=O
BP(=O)(Nc1ccc(Br)cc1)P(=O)(O)O
Fine tuning...
Mean value of predictions: 0.49140438
Proportion of valid SMILES: 0.5164113785557987
Sample trajectories:
BP(=O)(NC)OCC1OC(n2cnc3c(N)nc(Br)nc32)C(O)C1O
BP(=O)(OCC)OC(=O)CCCCCCCCCCCCCCCCCCCCC(Cl)P(=O)(O)c1ccc(O)c(Br)c1
BP(=O)(OCC1OC(O)C(N2C=CC(=O)NC2=O)O1)Oc1ccc(Cl)c(F)c1
BP(=O)(OCCCCBr)S(=O)(=O)Nc1cc(Br)cc(Br)c1Br
Bc1cc(Nc2ncnc3cc(Br)cc(Br)c23)ccc1Br

 15 Training on 16070 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.800299
Reward: 5.517673
Trajectories with max counts:
561	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.41006994
Proportion of valid SMILES: 0.22350734604563927
Sample trajectories:
BP(=O)(Nc1cccc(Br)c1)N1CCCCC1
BP(=O)(O)c1ccc(F)cc1Nc1c(F)c(F)c(F)c(F)c1F
BP(=O)(O)c1ccc(Nc2ccc(F)cc2)cc1Br
BP(=O)(OC(=O)OCCl)Oc1ccc(Nc2ccccc2Br)cc1
BP(=O)(OC(C)CBr)C(F)F
Policy gradient replay...
Mean value of predictions: 0.4297376
Proportion of valid SMILES: 0.214375
Sample trajectories:
BP(=O)(NS(=O)(=O)c1ccc(Br)cc1)OCC1CCCO1
BP(=O)(Nc1cc(Br)c(Br)cc1Br)c1ccc(Br)cc1Br
BP(=O)(OCC)OC(=O)N(c1cc(Br)c(Br)c(Br)c1Br)N1C(=O)Nc2ccccc21
BP(=O)(OCC1OC(=O)Oc2cc(Br)c(Br)cc21)C(=O)Oc1ccc(Br)cc1
BP(=O)(Oc1ccc(Br)cc1)Oc1c(F)c(F)c(F)c(F)c1F
Fine tuning...
Mean value of predictions: 0.4798423
Proportion of valid SMILES: 0.5553470919324578
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)N(O)Cc1ccc(Br)cc1
BP(=O)(Nc1ccc(F)c(F)c1F)C(F)(F)[PH](Br)(Br)Br
BP(=O)(OCC)OCCC=CCCC(=O)O
Bc1cc(Br)cc(Nc2ncnc3c(Br)cccc23)c1
BrCc1ccccc1Nc1ncnc2sc(Br)c(Br)c12

 16 Training on 17085 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.406956
Reward: 6.045689
Trajectories with max counts:
669	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.40294984
Proportion of valid SMILES: 0.211875
Sample trajectories:
BP(=O)(Cl)CCl
BP(=O)(Nc1ccc(F)cc1)C1CCN(CC(F)F)CCC(CBr)(c2ccc(Cl)c(F)c2)CC1
BP(=O)(OCC)OC(=O)C=CN(c1ccccc1)c1ccccc1
BP(=O)(OCC1OC(Oc2ccc(Br)cc2)C(O)C1O)N(=O)=O
BP1(=O)OCC(Oc2ccc(Br)cc2Br)C(O)C(N2CCCC2)C(=C(O)c2ccccc2)C1=O
Policy gradient replay...
Mean value of predictions: 0.41934523
Proportion of valid SMILES: 0.21
Sample trajectories:
BP(=O)(NC(c1ccccc1)c1ccc(Br)cc1)P(=C)(O)O
BP(=O)(NCCCO)c1cccc(Br)c1Br
BP(=O)(Nc1ccc(F)cc1F)c1nc(Nc2ccc(F)cc2F)c2ccccc2n1
BP(=O)(O)c1ccccc1-c1cccc(F)c1
BP(=O)(OC(=O)CBr)OC(C=CBr)C(Br)Br
Fine tuning...
Mean value of predictions: 0.5019814
Proportion of valid SMILES: 0.5364176305095343
Sample trajectories:
BP(=O)(NP(=O)(O)O[PH](=O)(O)(O)OP(=O)(O)OP(=O)(O)Oc1ccc(Br)cc1Br)OCC=C
BP(=O)(O)C(=O)Nc1ccc(F)c(F)c1
BP(=O)(O)Oc1ccc(Br)cc1
BP(=O)(OC(=O)c1ccc(Br)cc1)c1ccc(Br)c(Br)c1
BP(=O)(c1cc(Br)cc(Br)c1Br)N(CCCNC(=O)OC(C)(F)F)C(F)(F)F

 17 Training on 18063 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.350425
Reward: 6.248490
Trajectories with max counts:
861	Brc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.3138614
Proportion of valid SMILES: 0.189375
Sample trajectories:
BP(=O)(NCc1ccccc1)c1ccccc1-c1ccc(Br)cc1Br
BP(=O)(NO)c1ccc(Br)c(Nc2sc3ccccc3c2Cl)c1Br
BP(=O)(Nc1ccc(F)c(F)c1)c1ccc(F)c(Br)c1
BP(=O)(Nc1ccc(Nc2ccccc2F)cc1)C(=O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OC(=O)c1ccccc1)c1ccccc1
Policy gradient replay...
Mean value of predictions: 0.32217053
Proportion of valid SMILES: 0.201625507971241
Sample trajectories:
BP(=O)(Cl)Nc1ccc(Nc2ncnc3ccccc23)cc1
BP(=O)(O)C(=O)Nc1cccc(Nc2ccccc2)c1
BP(=O)(O)c1cccc(F)c1Nc1ccc(Nc2ncnc3ccc(F)cc23)cc1
BP(=O)(OCC)OCC
BP(=O)(Oc1ccccc1Cl)c1ccccc1
Fine tuning...
Mean value of predictions: 0.49411073
Proportion of valid SMILES: 0.530625
Sample trajectories:
B[PH](=O)(=NO)OC(=O)I
Br
Brc1cc(-c2cccc(Br)c2Br)cc(Br)c1Br
Brc1cc(-c2ccccc2)ccc1Nc1nccc(-c2ccc(Nc3ncnc4ccccc34)cc2Br)n1
Brc1cc(Br)c(-c2ncnc3c(Br)cccc23)cc1Br

 18 Training on 18877 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.277243
Reward: 6.258652
Trajectories with max counts:
772	Brc1ccc(Nc2ncnc3ccccc23)cc1Br
Mean value of predictions: 0.50406504
Proportion of valid SMILES: 0.1921875
Sample trajectories:
BP(=O)(CC(O)(c1ccc(F)c(F)c1)C1CC2CCC(C2)C1)OCC(=O)ON
BP(=O)(Nc1cc(Nc2cc(Br)cc(Br)c2)cc(Br)c1Br)N1CCCC1=O
BP(=O)(Nc1cccc(Nc2ncnc3c(Br)cccc23)c1)N(=O)=O
BP(=O)(O)Oc1ncnc2c(F)c(F)c(F)c(F)c12
BP(=O)(O)c1ccc(Nc2ccccc2Br)cc1
Policy gradient replay...
Mean value of predictions: 0.5009317
Proportion of valid SMILES: 0.20125
Sample trajectories:
BP(=O)(O)C=Cc1ccc(Br)cc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)O[PH](=S)OC(Cl)(Cl)Cl
Bc1cc(Nc2ncnc3ccccc23)ccc1Br
Bc1ccc(N(=O)=O)c(Nc2cccc(Br)c2)c1Br
Bc1ccc(Nc2ncnc3cc(Br)ccc23)cc1Br
Fine tuning...
Mean value of predictions: 0.53661615
Proportion of valid SMILES: 0.4951547358549547
Sample trajectories:
BP(=O)(OCC)OC(=O)CN(C1=NP(=O)(N(Nc2cc(Br)c(Br)cc2Cl)C(=O)OC(C)(C)O)C1=O)c1ccccc1
BP(=O)(OCC)Oc1ccc(Nc2ncnc3sc(Br)cc23)cc1
Bc1cc(Br)c2ncnc(Nc3cc(Br)cc(Br)c3)c2c1Br
Bc1cccc(Nc2ncnc3cc(Br)ccc23)c1
BrBr

 19 Training on 19959 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 24.892248
Reward: 6.243565
Trajectories with max counts:
369	Brc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.45992863
Proportion of valid SMILES: 0.2628125
Sample trajectories:
BBr
BP(=O)(CCCCl)NP(=O)(OCCO)c1nc2ccccc2s1
BP(=O)(NO)c1cccc(Br)c1
BP(=O)(O)c1ccc(NC(=O)C2=Nc3ccccc3-c3ccccc3N2)cc1
BP(=O)(O)c1ccccc1Br
Policy gradient replay...
Mean value of predictions: 0.4794686
Proportion of valid SMILES: 0.25875
Sample trajectories:
BP(=O)(NO)c1cccc2c(Br)cc(Br)cc12
BP(=O)(O)c1ccccc1Nc1cccc(Nc2ccc(Cl)cc2)c1
BP(=O)(OCC)N1CCC(Br)(Br)C1
BP(=O)(OCC)c1ccc(F)c(F)c1
BP(=O)(OCC1OC(N2C=CC(=O)NC2=O)C(O)C1O)Oc1ccccc1
Fine tuning...
Mean value of predictions: 0.54379565
Proportion of valid SMILES: 0.51375
Sample trajectories:
BP1(=O)OCC(Br)(C(=O)Oc2c(F)cc(F)c(F)c2F)c2c(Br)c(Br)c(F)c(F)c21
B[PH](=O)(Br)(OCCCCCCCBr)c1ccc(Br)cc1
Bc1cc(Br)c(Br)c(Br)c1Br
Bc1cc(Nc2ncnc3scnc23)cc(Cl)c1O
Bc1ccc(Nc2ncnc3ccsc23)cc1Br

 20 Training on 21263 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.725459
Reward: 6.747767
Trajectories with max counts:
242	Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Mean value of predictions: 0.616849
Proportion of valid SMILES: 0.285625
Sample trajectories:
BOc1ccc(Nc2ncnc3c(F)cc(F)cc23)cc1Br
BP(=O)(COc1cc(Br)c(Br)cc1Br)P(=O)(O)O
BP(=O)(N=C(F)F)OCC
BP(=O)(OCC)OC(=O)C=CC=CC=CC1OC(=O)C2CCCCCCN2C(=O)O1
BP(=O)(OCC)OC(=O)CN
Policy gradient replay...
Mean value of predictions: 0.6260215
Proportion of valid SMILES: 0.290625
Sample trajectories:
BOc1ccc(Nc2ncnc3cc(Br)cc(Br)c23)cc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)Oc1cc(Br)c2nc(Br)nc(N)c2n1
BP(=O)(Oc1cc(Br)c(Br)c(Br)c1)N(O)C=O
B[PH](=O)Oc1ccc(Br)cc1Br
Bc1cc(Nc2ncnc3ccccc23)cc(OCc2cc(Br)c(Br)cn2)c1Br
Fine tuning...
Mean value of predictions: 0.53690624
Proportion of valid SMILES: 0.5134459036898061
Sample trajectories:
B=[SH](F)(F)c1ccc(Nc2ncnc3sccc23)cc1
BP(=O)(Cc1c(F)c(F)c(F)c(F)c1F)OCC(=O)Nc1ccc(F)c(F)c1
BP(=O)(OCF)Oc1cc(F)c(Nc2ncnc3c(F)ccc(Br)c23)cc1F
Bc1cc(Nc2ccc(Br)cc2Br)nc(Nc2ccc3c(c2)OCO3)c1Br
Bc1ccc(Nc2ncnc3c(Br)cccc23)cc1Br

Trajectories with max counts:
287	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.4712221
Proportion of valid SMILES: 0.4281695423855964
Mean Internal Similarity: 0.48558673155723225
Std Internal Similarity: 0.1083971685745142
Mean External Similarity: 0.41152877960235584
Std External Similarity: 0.074913387818531
Mean MolWt: 396.902520952381
Std MolWt: 94.21039323053984
Effect MolWt: -0.9873470861050474
Mean MolLogP: 4.899700965079367
Std MolLogP: 1.284550248319006
Effect MolLogP: 0.13857938766534839
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 96.027397% (701 / 730)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 25, 'n_policy_replay': 0, 'n_fine_tune': 100, 'seed': 1, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 6101.126538276672, 'valid_fraction': 0.4281695423855964, 'active_fraction': 0.45992115637319314, 'max_counts': 287, 'mean_internal_similarity': 0.48558673155723225, 'std_internal_similarity': 0.1083971685745142, 'mean_external_similarity': 0.41152877960235584, 'std_external_similarity': 0.074913387818531, 'mean_MolWt': 396.902520952381, 'std_MolWt': 94.21039323053984, 'effect_MolWt': -0.9873470861050474, 'mean_MolLogP': 4.899700965079367, 'std_MolLogP': 1.284550248319006, 'effect_MolLogP': 0.13857938766534839, 'generated_scaffolds': 730, 'novel_scaffolds': 701, 'novel_fraction': 0.9602739726027397, 'save_path': '../logs/replay_combo_s1-9.smi'}
