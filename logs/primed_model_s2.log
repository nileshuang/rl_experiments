starting log


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.698529
Reward: 1.000000
Mean value of predictions: 0.0011881188
Proportion of valid SMILES: 0.7910401002506265
Sample trajectories:
BP(=O)(OC(C)C)C1CCCC=CC1OC(=O)CCC(N)=O
Brc1cc(Br)c2c(c1Br)C1CC=CC(CO2)O1
Brc1ccc(-c2ccc3oc(N4CCCC4)nc3n2)cc1
Brc1ccco1
C#CCC#Cc1ccn(C(=O)N2CCN(c3ccccc3)CC2)c1
Policy gradient replay...
Mean value of predictions: 0.0028673834
Proportion of valid SMILES: 0.7846875
Sample trajectories:
Brc1ccc2c(c1)Sc1ccccc1O2
Brc1ccc2ccccc2c1
Brc1ccccc1
Brc1ccccc1-c1cc2ccccc2cc1-c1ccccc1
Brc1ccccc1-c1cc2ccccc2nc1-c1ccccc1
Fine tuning...
Mean value of predictions: 0.018753069
Proportion of valid SMILES: 0.6377582968065122
Sample trajectories:
Brc1ccc(-c2[nH]ncc2-c2cccc(-c3cccc(Br)c3)c2)cc1
Brc1ccc(Nc2ncnc3cnccc23)nc1
C#CCCC12CC1(CO)CC1C2N1CCCc1nsc(CN2CCC(N3CCN(CC)CC3)O2)n1
C#CCNC(=O)C1=NN(Cc2cn(CC)cn2)Nc2ccccc21
C#Cc1c(-c2ccccc2)ccc(Nc2cc(-c3ccccc3)c(C)nn2)c1C#N

  2 Training on 287 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 17.377350
Reward: 1.040467
Trajectories with max counts:
6	Oc1ccc2ccccc2c1
Mean value of predictions: 0.022521008
Proportion of valid SMILES: 0.7439824945295405
Sample trajectories:
Brc1ccc(-c2ccccc2)o1
Brc1ccc(Nc2nc(Nc3ccccc3)nc3ccccc23)cc1
Brc1ccc(Nc2ncc3sc(-c4ccc(Br)o4)nc3n2)cc1
Brc1ccc(Nc2ncnc3cnc(Nc4cccc(Br)c4)nc3-2)cc1
Brc1ccc2ncc(Nc3ccccc3Br)nc2c1
Policy gradient replay...
Mean value of predictions: 0.005430211
Proportion of valid SMILES: 0.8176985616010006
Sample trajectories:
Brc1cc2ccccc2cc1-c1ccc2ccccc2c1
Brc1ccc(Nc2ccccc2Nc2ccccc2Br)cc1
Brc1ccc(Nc2cccnc2)cc1
Brc1cccc(Nc2ncccc2-c2ccccc2)c1
Brc1ccccc1-c1cccc2ccccc12
Fine tuning...
Mean value of predictions: 0.04758621
Proportion of valid SMILES: 0.6359649122807017
Sample trajectories:
BrCCOc1ccc(-c2ccccn2)c2ccccc12
Brc1ccc(-c2nc3cccnc3s2)o1
Brc1ccc(-c2ncc[nH]2)s1
Brc1ccc(Br)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1

  3 Training on 531 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 19.292418
Reward: 1.334564
Trajectories with max counts:
11	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.055350196
Proportion of valid SMILES: 0.6425
Sample trajectories:
BP(=O)(CCC)C(=O)Nc1ccc(Br)cc1
Brc1ccc(-c2ccncc2)o1
Brc1ccc(-c2nc(-c3cncnc3)c3ccccc3n2)cc1
Brc1ccc(Nc2ccncn2)cc1
Brc1ccc(Nc2nc3ccccc3[nH]2)cc1
Policy gradient replay...
Mean value of predictions: 0.064326465
Proportion of valid SMILES: 0.7126328955597249
Sample trajectories:
BP(=O)(c1ccccc1)N1CCOCC1
Brc1cc2c(Br)cccc2c2ccccc12
Brc1ccc(N2C=C(Nc3nccs3)c3ccccc3C23COC(N2CCN(c4ccccc4)CC2)C3)cc1
Brc1ccc(N=Nc2ccncc2)cc1
Brc1ccc(Nc2nc3ccccc3nc2c2ncnc3ccccc32)cc1
Fine tuning...
Mean value of predictions: 0.10121774
Proportion of valid SMILES: 0.641963727329581
Sample trajectories:
BP(=O)(CCN1CCCC(F)C(F)C1)OCC
BP(=O)(N(O)COP(=O)(O)OP(=O)(O)O)P(=O)(Nc1cccc(F)c1)OCOC(=O)N(O)C(CC(=O)O)NS(=O)(=O)CCl
BP(=O)(NO)c1cccc(Cl)c1
BrCc1ccc2c(Nc3ccccc3-c3cccnc3-c3cncnc3-c3ccccc3Br)ncnc2c1
Brc1ccc(-c2ccc3[nH]ccc3c2)c2ccccc12

  4 Training on 1203 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 20.076134
Reward: 1.582864
Trajectories with max counts:
90	Clc1ccc(Nc2ncnc3ccccc23)cc1
Mean value of predictions: 0.17416807
Proportion of valid SMILES: 0.554582421019706
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc2c(cc1CSc1nc3ccccc3nc1Nc1ncnc3sc4ccccc4c13)Sc1ccccc1N2
Brc1ccc(-c2cnc(Nc3ccccc3)nc2)cc1
Brc1ccc(Br)c(-c2ccc(Nc3ccc(Br)s3)cc2)c1
Brc1ccc(C=C2NC(c3c[nH]c4ccccc34)=NSS2)cc1
Policy gradient replay...
Mean value of predictions: 0.16084905
Proportion of valid SMILES: 0.5304973412574289
Sample trajectories:
Bc1cccc(Nc2ncnc3cc(Br)cc(Br)c23)c1
Brc1cc2ncnc(Nc3ccccc3)c2nc1CN1CCCCC1
Brc1ccc(-c2ccccc2Br)cc1
Brc1ccc(-c2ncnc3c(Nc4ccccc4Br)ncnc23)s1
Brc1ccc(Br)c(Nc2ncc3ncnc(-c4ccccc4)c3n2)c1
Fine tuning...
Mean value of predictions: 0.17446353
Proportion of valid SMILES: 0.5828642901813633
Sample trajectories:
BP(=O)(C=O)OCC
BrCCOc1ccc(Nc2ncnc3ccccc23)cc1
Brc1ccc(-c2ccccc2)cc1Nc1ncnc2ccccc12
Brc1ccc(-c2ccccc2)nc1Nc1ncc2ccccc2n1
Brc1ccc(-c2nc(-c3ccccc3)c3c(-c4ccccc4Br)ncnc3n2)s1

  5 Training on 2438 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 19.652863
Reward: 2.065749
Trajectories with max counts:
422	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.09520295
Proportion of valid SMILES: 0.508125
Sample trajectories:
Bc1cnccc1N=C(Nc1ccccc1N)C(N)=O
Brc1cc2ccccc2cc1-c1cccc(Nc2nccs2)c1
Brc1ccc(-c2ccc(Nc3cccs3)cc2)cc1
Brc1ccc(-c2ccccc2)c2ccccc12
Brc1ccc(NCCCN2CCN(c3ccccc3Br)CC2)s1
Policy gradient replay...
Mean value of predictions: 0.17282984
Proportion of valid SMILES: 0.6373866833385433
Sample trajectories:
BrCc1ccccc1-n1ccc2ccccc21
Brc1ccc(Br)c(Nc2cccc(Nc3ccccc3N=Cc3ccccc3Br)n2)c1
Brc1ccc(Br)c(Nc2ncnc3ncnc(Nc4ccccc4Br)nc3n2)c1
Brc1ccc(Nc2ncnc(Nc3cccc(Br)c3)n2)cc1
Brc1ccc(Nc2ncnc3cc(Br)ccc23)cc1
Fine tuning...
Mean value of predictions: 0.18297435
Proportion of valid SMILES: 0.609375
Sample trajectories:
BP(=O)(OCC)OC(=O)CCC(=O)OC(C)C=CC=CBr
Bc1ccccc1-c1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cc(-c2cccnc2)c2ncnc(Nc3ccccc3)c2c1Nc1ccccc1
Brc1cc(Br)c(Br)c(-c2ccccc2-c2ccccc2)c1
Brc1ccc(-c2[nH]ccc2Br)cc1

  6 Training on 3542 replay instances...
Setting threshold to 0.300000
Policy gradient...
Loss: 19.323853
Reward: 2.696473
Trajectories with max counts:
1084	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.18702596
Proportion of valid SMILES: 0.313125
Sample trajectories:
Brc1cccc(Nc2ncnc3ccccc23)c1
Brc1ccccc1-c1ccccc1-c1ccccc1Nc1ccccc1Nc1ncnc2ccccc12
Brc1ccccc1-c1ccccc1Nc1ncccc1Nc1ccccc1
Brc1ccccc1-c1ccccc1Nc1ncnc2ccccc12
Brc1ccccc1-c1cnccc1Nc1ncnc2ccccc12
Policy gradient replay...
Mean value of predictions: 0.26840797
Proportion of valid SMILES: 0.5028142589118199
Sample trajectories:
BP(=O)(NC(C)(COC(=O)CCCl)OC)OCCF
BP(=O)(OCC)OC(=O)CCCCCCC(=O)CI
BP(=O)(OCC1OC(OP(=O)(O)O)C(O)C1O)n1cnc2c(N)ncnc21
BrC1=NNc2ncc(Br)c3cccc1c23
BrC1=Nc2sc3c(c2C=C1)CCCCC3
Fine tuning...
Mean value of predictions: 0.26655114
Proportion of valid SMILES: 0.5417840375586854
Sample trajectories:
BP(=O)(C(=O)Oc1ccc(F)cc1)N1CCOCC1
Brc1cc(Nc2ncnc3ncnc(Nc4ccncc4Br)c23)ccn1
Brc1ccc(-c2ccccc2)cc1Nc1ncnc2ccccc12
Brc1ccc(-c2nc3ncccc3s2)cc1
Brc1ccc(Br)c(Nc2ncnc3cc(Br)ccc23)c1

  7 Training on 4760 replay instances...
Setting threshold to 0.450000
Policy gradient...
Loss: 21.623724
Reward: 2.868256
Trajectories with max counts:
439	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.22054055
Proportion of valid SMILES: 0.4625
Sample trajectories:
BP(=O)(OCCCC)OCOP(=O)(O)OP(=O)(O)O
BP(=O)(Oc1ccccc1P(=O)(Oc1ccccc1)c1ccccc1)C(O)CCC
BP1(=O)CCN1CCS(=O)(=O)Nc1ccc(Br)cc1
BrC1=Nc2ccccc2Nc2ccccc2S1
BrSc1ccccc1Nc1ncnc2ncnc(Nc3ccccc3)c12
Policy gradient replay...
Mean value of predictions: 0.3390625
Proportion of valid SMILES: 0.5209768315591734
Sample trajectories:
BP(=O)(N=Nc1ccc(I)cc1)N(O)CP(=O)(O)O
BrCCOc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cc(N2CCCC2CN2CCC(N3CCOCC3)CC2)c2c(Nc3cccc(-c4ccccc4Br)c3)ncnc2c1
Brc1ccc(-c2cnc3sc(Nc4ccccc4)nc3c2)cn1
Brc1ccc(Br)c(Nc2cc(Br)c(Br)cn2)c1
Fine tuning...
Mean value of predictions: 0.27404994
Proportion of valid SMILES: 0.5758049390434511
Sample trajectories:
BP(=O)(Nc1ccc(F)cc1)c1ccc(F)cc1F
Bc1cccc(Nc2ncnc3cc(Cl)ccc23)c1
BrCCc1ccncc1Nc1ncnc2ccccc12
Brc1cc2c(nc1Nc1ncnc3sc(-c4ccccc4)cc13)Sc1ccccc1N2
Brc1ccc(-c2ccccc2Nc2ccccc2Br)o1

  8 Training on 6190 replay instances...
Setting threshold to 0.600000
Policy gradient...
Loss: 22.429478
Reward: 3.021695
Trajectories with max counts:
82	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.33915246
Proportion of valid SMILES: 0.4945261182358461
Sample trajectories:
BP(=O)(OCC)OC(=O)C1OP(F)(=S)OC1(F)F
Brc1cc2ccc1Oc1cccc3nc[nH]c3c(ncn1)N2
Brc1ccc(-c2nc(Nc3cccs3)[nH]c2-c2ccccc2)cc1
Brc1ccc(Br)c(Br)c1
Brc1ccc(Br)c(Nc2ncnc3ccccc23)c1
Policy gradient replay...
Mean value of predictions: 0.25866947
Proportion of valid SMILES: 0.5965625
Sample trajectories:
BP(=O)(Nc1cccc(Nc2ccccc2Cl)c1)NS(=O)(=O)c1ccc(Br)cc1
BP(=O)(Oc1ccccc1Br)P(=O)(Oc1ccccc1)Oc1ccc(Br)cc1
Bc1ccc(Nc2ccccc2Br)c(Br)c1Nc1ccccc1Br
BrC1=Nc2ccccc2-c2ccc(Br)c(Br)c2S1
BrCCCCC1=Nc2sc3ccccc3c2OC1=Nc1ccccc1
Fine tuning...
Mean value of predictions: 0.2955665
Proportion of valid SMILES: 0.5712945590994372
Sample trajectories:
BP(=O)(NC(=O)OCC)c1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CSCCP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(c1ccccc1Nc1c(N)cc(Cl)cc1O)N(O)CC=O
Bc1[nH]c2ccccc2c1-c1nc2cc(Nc3ccc(Br)cc3)cc(Br)cn12
BrC=C1N=Nc2ccc(Br)cc2Nc2cc(Br)ccc21

  9 Training on 7707 replay instances...
Setting threshold to 0.750000
Policy gradient...
Loss: 23.830911
Reward: 3.064039
Trajectories with max counts:
62	Fc1ccccc1Nc1ncnc2ccccc12
Mean value of predictions: 0.2935501
Proportion of valid SMILES: 0.5961237886839638
Sample trajectories:
BP(=O)(OCC)C1Oc2sc(Br)cc2N1C(=O)c1cc(Br)ccc1Br
BrCBr
Brc1c(-c2ccccc2)cccc1-c1cnc2ccccn12
Brc1cc(-c2ccccc2)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2sccc12
Policy gradient replay...
Mean value of predictions: 0.39168197
Proportion of valid SMILES: 0.5112570356472795
Sample trajectories:
BP(=O)(N1CCN(C(=O)Oc2ccc(Cl)c(F)c2)CC1)C(F)(F)F
BP(=O)(OCC1OC(n2cnc3c(N)cc(Br)cc32)C(O)(O)C1O)C(=O)NS(=O)(=O)Oc1ccccc1
BP(=O)(OCCC)OCOP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BrC1(c2ccccc2)CC1=NNc1ncnc2sc3cccs3c2n1
Brc1cc(Br)c2ncnc(Nc3ccc(I)cc3)c2c1
Fine tuning...
Mean value of predictions: 0.38305473
Proportion of valid SMILES: 0.5423569865582994
Sample trajectories:
BP(=O)(NCc1ccccc1)P(=O)(O)O
BP(=O)(OCC)OP(=O)(O)OP(=O)(O)Nc1ccc(F)c(F)c1
BrC1=Nc2scnc2Nc2ccccc21
BrCc1nc2ncnc(Nc3cccc(-c4ccccc4Br)c3)c2s1
Brc1cc(-c2cccs2)c2c(Br)c(Br)cnc2c1-c1ccccc1

 10 Training on 9282 replay instances...
Setting threshold to 0.900000
Policy gradient...
Loss: 22.537999
Reward: 3.240157
Trajectories with max counts:
176	Fc1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.5040068
Proportion of valid SMILES: 0.36679174484052535
Sample trajectories:
BP(=O)(CC(F)(F)F)NO
BrBr
Brc1cc(Br)c2ncsc2c1
Brc1cc(Nc2nc3ccccc3s2)sc1Br
Brc1cc(Nc2ncnc3sccc23)sc1Nc1ncnc2sc3ccccc3c12
Policy gradient replay...
Mean value of predictions: 0.3842627
Proportion of valid SMILES: 0.504375
Sample trajectories:
BP(=O)(NC(C1CCC1)P(=O)(O)O)n1cnc2c(N)ncnc21
BP(=O)(NCc1ccc(F)cc1)OP(=O)(Nc1cc(Br)cc(Br)c1)Oc1ccccc1Br
BP(=O)(OCC)Oc1nc2c(Br)cc(Br)cc2s1
BP(=O)(OCC)c1nc(-c2ccccc2)c2nc(-c3ccccc3)c(-c3ccccc3)c(Br)c(Br)c2[nH]1
BP(=O)(OCCCC)Oc1ccc(Nc2ncnc3sc4scnc4c3s2)cc1
Fine tuning...
Mean value of predictions: 0.38331428
Proportion of valid SMILES: 0.546875
Sample trajectories:
BP(=O)(OCC(=O)Nc1ccc(F)cc1F)c1cccc(F)c1
BP(=O)(OCC)C(=O)NNc1ccc(Br)cc1
BP(=O)(OCC)OC(=O)CCCCOP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)CC(F)(F)F
B[PH](=O)(Br)(Br)C(Br)=C(Br)Br
Bc1ccc(Nc2ncnc3ccsc23)cc1

 11 Training on 10634 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.040359
Reward: 3.436797
Trajectories with max counts:
323	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4588517
Proportion of valid SMILES: 0.391875
Sample trajectories:
BP(=O)(CCC=C(Br)Br)OCC
BP(=O)(NCc1ccc(Br)cc1)[PH](=O)(Nc1cncs1)(Oc1ccccc1Nc1ccc(N)cc1)C(F)(F)F
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1O
Bc1cccc(Br)c1Nc1ncnc2ccccc12
Bc1cccc(Nc2ncnc3cc(Br)sc23)c1
Policy gradient replay...
Mean value of predictions: 0.440882
Proportion of valid SMILES: 0.524867062871442
Sample trajectories:
BP(=O)(COP(=O)(O)O)N(CCN)P(=O)(O)O
BP(=O)(OCBr)OC(=O)Cc1cnc(Br)[nH]1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1Br
Br
Fine tuning...
Mean value of predictions: 0.41147733
Proportion of valid SMILES: 0.5501719287277275
Sample trajectories:
BP1(=O)OCC2OC(NC(=O)OCC(COc3c(Br)cc(Br)cc3Br)O1)C(O)(n1cnc3c(Br)cc(Br)cc31)C(O)C2O
B[PH](=O)(=NO)Nc1ccc(Br)cc1
BrCc1cc2c(Nc3cccc(Br)c3)ncnc2s1
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2c1
Brc1cc2c(N=Cc3ccccc3Br)ncnc2s1

 12 Training on 12073 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.384958
Reward: 3.817497
Trajectories with max counts:
291	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.49982083
Proportion of valid SMILES: 0.3488590184432635
Sample trajectories:
BP(=O)(CCC=CBr)OCC
BP(=O)(Cl)OP(=O)(O)OP(=O)(O)OP(O)(F)(F)Br
BP(=O)(N(O)C=O)n1cnc2c(N)ncnc21
BP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC)OC(=O)Nc1ccc(Br)c(Br)c1
Policy gradient replay...
Mean value of predictions: 0.47692308
Proportion of valid SMILES: 0.463849765258216
Sample trajectories:
BP(=O)(OCC1OC(O)C(O)C1O)OP(=O)(O)OP(=O)(O)Oc1ccc(Nc2ncc(Br)c(Br)c2Br)nc1
Bc1ccc(Nc2ncnc3c2c(Br)sc3c2nc3ccc(Br)cc3s2)cc1
Bc1ccc(Nc2ncnc3sc(Nc4ccccc4)cc23)cc1
BrC1=Nc2ncnc(Br)[n+]2s1
BrCc1nc2c(Nc3nc(-c4ccc(Br)cc4)c(Br)cc3Br)ncnc2s1
Fine tuning...
Mean value of predictions: 0.4195923
Proportion of valid SMILES: 0.5520475148483901
Sample trajectories:
BP(=O)(CC(=O)ON=C(Nc1ccc(Br)nc1)Oc1ccccc1)NO
Bc1ccccc1Nc1ncnc2sc(Nc3ccc(Br)c(Br)c3)nc12
BrC(=NNc1ncnc(Nc2ccc(Br)cc2)n1)c1ncnc2sc(Br)cc12
BrCCI
BrCc1ccccc1-c1nc(N2CCOCC2)c2ncnc(Nc3ccccc3)c2n1

 13 Training on 13568 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.813995
Reward: 3.502899
Trajectories with max counts:
66	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4378855
Proportion of valid SMILES: 0.567677399187246
Sample trajectories:
BP(=O)(C(=O)O)N1CCC(=O)Nc2sc(C(=O)Oc3ccccc3)cc21
Bc1cccc(Nc2ncnc3ccsc23)c1
BrCCNc1ccc(Nc2ncnc3ncnc(Nc4ccc(Br)cc4)c23)cc1
BrCc1ccccc1-c1ccccc1Nc1ncnc2sc(Nc3ccccc3Br)nc12
Brc1cc(Br)c2ncnc(Nc3ccccc3Br)c2n1
Policy gradient replay...
Mean value of predictions: 0.40839326
Proportion of valid SMILES: 0.5214129415442326
Sample trajectories:
BP(=O)(NC(c1ccccc1)c1ccccc1)c1ccccc1
BP(=O)(OCCS)C(=O)N1CCC(Br)(CBr)CC1
BrC(Br)=Nc1ccccc1Nc1ncnc2nc(Br)ccc12
Brc1cc(Br)c2c(c1)Oc1cc(Br)sc1-2
Brc1cc(Br)cc(Nc2ncnc3cc(Br)ccc23)c1
Fine tuning...
Mean value of predictions: 0.39943945
Proportion of valid SMILES: 0.5575
Sample trajectories:
Brc1cc(Br)c2sc3ncnc(Nc4ccccc4Oc4ccccc4Br)c3c2c1
Brc1cc(Nc2ccsc2)ccc1I
Brc1cc(Nc2ncnc3ccccc23)nc2ccccc12
Brc1cc2c(Nc3cccc(-c4ccccc4Br)c3)ncnc2s1
Brc1cc2c(Nc3cccc(I)c3)ncnc2s1

 14 Training on 15102 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.593015
Reward: 3.773487
Trajectories with max counts:
317	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.5105079
Proportion of valid SMILES: 0.356875
Sample trajectories:
BP(=O)(O)Oc1ccccc1Nc1ncnc2ccccc12
BP(=O)(O)c1ccccc1Nc1ncnc2c(Br)c(Br)ccc12
BP(=O)(OC)OC(=O)CBr
BP(=O)(OCC)C1(Cc2ccc(Br)cc2)OC(=O)C(C)(C)Oc2c1ccc(Br)c2Br
BP(=O)(OCC)OCC1NC(O)=CC(=N)O1
Policy gradient replay...
Mean value of predictions: 0.27874362
Proportion of valid SMILES: 0.4278125
Sample trajectories:
BP(=O)(CCOCCOCCC(=O)N1CCCN(Cc2ccc(Br)s2)O1)OCC
Bc1ccccc1-c1ccccc1Nc1ncnc2ccccc12
Bc1ccccc1N1cc2c3cccnc3Nc3ccccc3Nc3ccccc3N=CC=C2N(c2ccccc2)CC1
BrC=CC=CC=CCNc1ncnc2sc3ccccc3c12
BrCCOc1ccccc1Nc1ncnc2ccccc12
Fine tuning...
Mean value of predictions: 0.4305263
Proportion of valid SMILES: 0.534375
Sample trajectories:
BP(=O)(OCC)c1cc(Br)cc(Br)c1Br
BrC1=C(c2ccccc2)c2nc(-c3ccccc3)sc2-c2cc(Br)ccc2N=C1CN1CCOCC1
BrCCBr
BrCCCCCCCCBr
Brc1c(Br)n(-c2ccccc2)c2ccc(Nc3ncnc4ccccc34)cc12

 15 Training on 16316 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 22.354739
Reward: 3.694452
Trajectories with max counts:
167	Fc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.4548632
Proportion of valid SMILES: 0.41125
Sample trajectories:
BP(=O)(CCCCCCCCCCCCCC=CCC=C)OCC
BP(=O)(OCC)OCCCCCC
BP(=O)(Oc1cc(Br)ccc1Br)OP(=O)(O)O
Bc1ccc(Nc2ccsc2)cc1Cl
BrBr
Policy gradient replay...
Mean value of predictions: 0.4954717
Proportion of valid SMILES: 0.49703032197561736
Sample trajectories:
BP(=O)(CCC(=O)Oc1ccccc1Nc1ncnc2sccc12)COCC
BP(=O)(CCC(=O)c1cc(Br)c(OP(=O)(O)O)c(Br)c1)OCC
BP(=O)(OCC)C(F)(F)F
BP(=O)(OCC)OC1CCC[P+](=O)([O-])Oc2ccc(Br)cc21
BP(=O)(OCC)Oc1ccc(Br)cc1S(=O)(=O)Nc1nccs1
Fine tuning...
Mean value of predictions: 0.45218405
Proportion of valid SMILES: 0.5367302281963113
Sample trajectories:
BP(=O)(N(O)C(F)F)P(=O)(Oc1ccccc1)Oc1ccc(Br)c(Br)c1
BP(=O)(OCC)OC(=O)Nc1ccc(Br)c(Cl)c1
BP(=O)(OCC)c1ccc(Br)cc1
BrC=Cc1ccccc1Nc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1Br
Brc1cc(Br)c2c(Br)n(-c3ccccc3Nc3ncnc4sccc34)c(-c3cccs3)c2c1

 16 Training on 17898 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.409496
Reward: 4.006271
Trajectories with max counts:
406	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.5596014
Proportion of valid SMILES: 0.34510784620193813
Sample trajectories:
BP(=O)(NC1CCCC(C(=O)N2CCCCC2)C1)OS(=O)(=O)O
BP(=O)(Nc1cc(Br)c(Br)cc1Br)C(P(=O)(O)O)P(=O)(O)P(=O)(O)O
BP(=O)(Nc1cc(Br)cc(Nc2ccc(Br)c(Br)c2)c1)C(F)(F)P(=O)(O)O
BP(=O)(Nc1ccc(Br)c(Br)c1)c1ccc(Br)c(Br)c1
BP(=O)(OCC)c1ccc(Nc2ncnc3c(Br)c(Br)c(Br)c(Br)c(Br)c(Br)c23)cc1
Policy gradient replay...
Mean value of predictions: 0.4288961
Proportion of valid SMILES: 0.5775
Sample trajectories:
Bc1ccccc1Nc1ncnc2cc(Br)ccc12
BrCCNc1ccc2ncnc(Nc3ccccc3-c3ccccc3Br)c2n1
BrCc1nc2c(-c3ccccc3Br)ncnc2s1
BrCc1sc2ncnc(Nc3ccc(Br)cc3)c2c1Br
BrSc1ccccc1-c1ccccc1Nc1ccccc1
Fine tuning...
Mean value of predictions: 0.43241727
Proportion of valid SMILES: 0.5571875
Sample trajectories:
BP(=O)(CCCO)c1cc2ncnc(Nc3ccc(Br)cc3F)c2s1
BP(=O)(OCCCC)Oc1ccc(Nc2ncnc3ccccc23)cc1
Bc1ccc(Nc2nc(Nc3cc4ccccc34)nc(-c3cccs3)c2-c2cncnc2)cc1
Bc1ccccc1Nc1ncnc2sc(Nc3ccccc3Br)nc12
BrC(=Nc1ccsc1)c1ccccc1Nc1cccc2c(Nc3ccccc3Br)ncnc12

 17 Training on 19549 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 23.230745
Reward: 3.590635
Trajectories with max counts:
40	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.49202454
Proportion of valid SMILES: 0.5095342294467021
Sample trajectories:
BP(=O)(CCCCCl)NO
BP(=O)(Nc1ccc(Nc2ncnc3ccccc23)cc1)c1ccc(Br)cc1
BP(=O)(OCC)C(=O)Nc1ccc(Br)cc1
BP(=O)(OCC1CCCN1C(=O)OP(=O)(O)CCC1CC1)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)OP(=O)(O)O
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)OP(=O)(O)OP(=O)(O)OP(=O)(OCC)OCCCCCC
Policy gradient replay...
Mean value of predictions: 0.49682155
Proportion of valid SMILES: 0.5114098155673648
Sample trajectories:
BP(=O)(OCC)Oc1ccc(Br)cc1
BP(=O)(OCC1OC(n2cnc3c(N)ncnc32)C(O)C1O)c1cccc(Br)c1
Bc1cnc(Nc2cc3ccc(Br)cc3nc3c(Br)c(Br)nc23)[nH]1
BrCc1nc2c(Nc3ccc(Br)cc3)ncnc2s1
Brc1cc(-c2ccccc2Br)ccc1Nc1ncnc2ccsc12
Fine tuning...
Mean value of predictions: 0.49479166
Proportion of valid SMILES: 0.5403377110694184
Sample trajectories:
BP(=O)(C=COc1ccc(Br)cc1)OC
BP(=O)(CCC=CC(C)=O)OCCC
BP(=O)(NOP(=O)(O)OP(=O)(O)O)P(=O)(O)OP(=O)(O)Oc1ccccc1Br
Bc1cccc(Nc2ncnc3ccsc23)c1
BrC=CBr

 18 Training on 21412 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.229218
Reward: 4.113505
Trajectories with max counts:
115	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.54767823
Proportion of valid SMILES: 0.4778125
Sample trajectories:
BIc1ccccc1Nc1ncnc2nc(-c3ccccc3)sc12
BP(=O)(OCC1CCCCC1)Oc1ccccc1Nc1ncnc(Nc2ccc(Br)c(Br)c2O)n1
Bc1ccccc1-c1cc(Nc2cccc(Br)c2)ncn1
Bc1ccccc1Nc1ncnc2sc(Br)cc12
Bc1ccccc1Nc1ncnc2sccc12
Policy gradient replay...
Mean value of predictions: 0.49370462
Proportion of valid SMILES: 0.51625
Sample trajectories:
Bc1ccc2c(Nc3ccccc3)ccnc2c1
Bc1ccccc1-c1ccccc1-c1ncnc2sc3ccccc3c12
Bc1ccccc1-c1ccccc1Nc1ncnc2sc(Nc3ccc(Br)s3)cc12
Bc1ccccc1Nc1ncnc2sccc12
BrCC=CCC=CC=NNc1ncnc2sc(Br)cc12
Fine tuning...
Mean value of predictions: 0.48607877
Proportion of valid SMILES: 0.5636136292591435
Sample trajectories:
BP(=O)(Nc1ccc(Br)cc1)OCC1OC(c2cnc(Br)cc2Br)C(O)(CC)O1
Brc1c(-c2ccc(Br)c3c(Nc4ccccc4)ncnc23)ccc2c1CCC2
Brc1cc(Br)c2sc3ncnc(Nc4ccccc4Br)c3c2c1
Brc1cc(CN2CCCC2)c2c(Nc3ccc(Nc4ccccc4Br)cc3)ncnc2c1
Brc1cc(Nc2ncnc3sc(-c4ccsc4)cc23)cc2ccccc12

 19 Training on 23340 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 26.129191
Reward: 4.499421
Trajectories with max counts:
326	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.6182163
Proportion of valid SMILES: 0.329375
Sample trajectories:
BP(=O)(NCc1ccccc1)c1ccc(Nc2ccc(Br)cc2)s1
BP(=O)(Nc1ccc(Br)cc1Br)OCC=C
BP(=O)(Nc1nc(N)cs1)Nc1ccc(Br)cc1F
BP(=O)(OCC)OC(=O)C(Br)Br
BP(=O)(OCC)OC(=O)CN(c1ccccc1)c1ccc(Br)cc1
Policy gradient replay...
Mean value of predictions: 0.44122905
Proportion of valid SMILES: 0.559375
Sample trajectories:
BP(=O)(CCCl)NO
BP(=O)(Nc1ccccc1)c1ccc(Br)cc1
Bc1ccccc1Nc1ncnc2sc(Nc3cccc(Br)c3)nc12
BrC12Cc3ccccc3C1CNc1ccncc12
BrCc1nc2c(-c3ccccc3Br)ncnc2s1
Fine tuning...
Mean value of predictions: 0.5047934
Proportion of valid SMILES: 0.5671875
Sample trajectories:
BP(=O)(Nc1cc(Br)cc(Br)c1Br)c1nc2c(Br)c(Br)c(Br)c(Br)c2s1
B[PH](=O)(Cl)(Cl)OCCCl
Bc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Bc1ccc(Nc2ncnc3ccsc23)cc1
Bc1ccc2ncnc(Nc3ccc(Br)cc3)c2c1

 20 Training on 25129 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 25.623426
Reward: 4.858248
Trajectories with max counts:
274	Brc1ccccc1Nc1ncnc2sccc12
Mean value of predictions: 0.5507274
Proportion of valid SMILES: 0.3221875
Sample trajectories:
BP(=O)(NCCCCCCl)c1ccccc1Nc1cccc(Nc2ncnc3sccc23)c1
BP(=O)(c1ccccc1)N1CCC(F)(F)C1
BP(=O)(c1ccccc1Nc1ccccc1)N1CCCCC1
BP1(=O)OCC(O)C(Oc2ccccc2)N1Cc1cncnc1Cl
B[PH](=O)(OCC)=C(Br)Br
Policy gradient replay...
Mean value of predictions: 0.4937616
Proportion of valid SMILES: 0.5072055137844611
Sample trajectories:
BP(=O)(OCC)Oc1cccc(Br)c1Nc1c2c(F)c(Br)c(Br)cc2nc2c(F)c(F)c(F)c(F)c12
BP(=O)(c1cc(Br)cc(Nc2cncnc2)c1)S(=O)(=O)Nc1cc(F)cc(Cl)c1
BP1(=O)OCC2OC(=O)OCC2c2cc(N)c(F)c(F)c21
Bc1ccc(Nc2cc(-c3ccc4ccc(Br)cc4n3)nc(Br)c2F)cc1
BrCCC=CC=CC=CC=CC=C=NNc1ncnc2sccc12
Fine tuning...
Mean value of predictions: 0.51342136
Proportion of valid SMILES: 0.5684803001876173
Sample trajectories:
BP(=O)(OCC)OC(=O)CCCl
Bc1ccccc1Nc1ncnc2sccc12
BrC(=Nc1ccc(Br)cc1)c1ccc(Nc2ncnc3sc(Br)cc23)cc1
BrC=CC(Br)Br
Brc1cc(-c2cccs2)sc1-c1ccsc1-c1cncc(Nc2ccc3ncnc(Br)c3c2)c1

Trajectories with max counts:
231	Brc1ccc(Nc2ncnc3ccsc23)cc1
Mean value of predictions: 0.46509284
Proportion of valid SMILES: 0.4713383759454898
Mean Internal Similarity: 0.4655441047686505
Std Internal Similarity: 0.10073750341490033
Mean External Similarity: 0.4167015223187768
Std External Similarity: 0.06696205085652614
Mean MolWt: 414.1099825878116
Std MolWt: 93.67718164568349
Effect MolWt: -0.8567850736331483
Mean MolLogP: 5.700259867907537
Std MolLogP: 1.60665689782518
Effect MolLogP: 0.6390134862536615
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 97.936371% (1139 / 1163)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/checkpoint_batch_training'}:
{'duration': 5551.369987726212, 'valid_fraction': 0.4713383759454898, 'active_fraction': 0.44177718832891244, 'max_counts': 231, 'mean_internal_similarity': 0.4655441047686505, 'std_internal_similarity': 0.10073750341490033, 'mean_external_similarity': 0.4167015223187768, 'std_external_similarity': 0.06696205085652614, 'mean_MolWt': 414.1099825878116, 'std_MolWt': 93.67718164568349, 'effect_MolWt': -0.8567850736331483, 'mean_MolLogP': 5.700259867907537, 'std_MolLogP': 1.60665689782518, 'effect_MolLogP': 0.6390134862536615, 'generated_scaffolds': 1163, 'novel_scaffolds': 1139, 'novel_fraction': 0.9793637145313844, 'save_path': '../logs/primed_model_s2-1.smi'}


  1 Training on 216 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 41.908343
Reward: 5.161882
Trajectories with max counts:
27	COc1cc2ncnc(Nc3cccc(Br)c3)c2cc1OC
Mean value of predictions: 0.6384169
Proportion of valid SMILES: 0.593671679197995
Sample trajectories:
Brc1ccc(Cc2c(Nc3cccc(Br)n3)ncnc3ccccn23)cc1
Brc1ccc(Nc2ncnc3[nH]cc(-c4ccc(CN5CCOCC5)cc4)c23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(Nc4ccc(Oc5ccccc5)cc4)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1cccc(-c2cc(-c3cc[nH]c3)[nH]n2)c1
Policy gradient replay...
Mean value of predictions: 0.6620935
Proportion of valid SMILES: 0.6171213546566322
Sample trajectories:
Brc1cccc(Nc2cnc(Nc3ccncc3)cn2)c1
Brc1cccc(Nc2ncc3ncnc(Nc4cccc(Br)c4)c3n2)c1
Brc1cccc(Nc2ncnc3cnc(NC4CCCCC4)cc23)c1
C#CCOc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCC
C#CCOc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCC1CNC2C(OS(C)(=O)=O)CCC12
Fine tuning...
Mean value of predictions: 0.68942595
Proportion of valid SMILES: 0.6214017521902377
Sample trajectories:
Brc1ccc(Nc2ncnc3[nH]c(Br)cc23)c(Br)c1
Brc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cccc(Nc2ncnc3c2[nH]c2ccccc23)c1
Brc1cccc(Nc2ncnc3ccc(NCCN4CCCCC4)cc23)n1
Brc1cccc(Nc2ncnc3ncnc(Nc4ccccc4Br)c23)c1

  2 Training on 4421 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 42.301377
Reward: 5.704741
Trajectories with max counts:
76	COc1cc2c(Nc3ccc(F)c(Cl)c3F)ncnc2cc1OC1COC2C(F)COC12
Mean value of predictions: 0.7145744
Proportion of valid SMILES: 0.5880512980919612
Sample trajectories:
Brc1ccc(Nc2ncnc3[nH]c(OCc4ccccc4Br)cc23)cc1Br
Brc1ccc(Nc2ncnc3c(Nc4cccc(Br)c4)ncnc3cn2)cc1
Brc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cccc(Nc2ncnc3ccc4[nH]ncc4c23)c1
Policy gradient replay...
Mean value of predictions: 0.6763092
Proportion of valid SMILES: 0.6267583619881213
Sample trajectories:
Brc1ccc(Nc2ncnc3ccccc23)cc1
Brc1cccc(Nc2ncnc3cc(Br)ncc23)c1
Brc1cccc(Nc2ncnc3cc(nc(N4CCCC4)n2)C3)c1
Brc1cccc(Nc2ncnc3nc4ccccc4cc23)c1
Brc1ccccc1Nc1ncnc2nc(Nc3ccccc3)ncc12
Fine tuning...
Mean value of predictions: 0.729907
Proportion of valid SMILES: 0.6396368190356919
Sample trajectories:
Brc1ccc(Nc2ncnc3ccccc23)c(Br)c1
Brc1cccc(Nc2ncnc3c(Nc4cccc(Br)c4)ncnc23)c1
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cccc(Nc2ncnc3ccccc23)c1
Brc1cccc(Nc2ncnc3cncnc23)c1

  3 Training on 8799 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 41.055633
Reward: 5.895612
Trajectories with max counts:
36	COc1cc2c(Nc3ccc(F)c(Cl)c3F)ncnc2cc1OC1COC2C(F)COC12
Mean value of predictions: 0.7922542
Proportion of valid SMILES: 0.6303599374021909
Sample trajectories:
Brc1ccc2ncnc(Nc3ccc(C4CCNCC4)nc3)c2c1
Brc1cccc(-c2cc3c(Nc4ccccc4)ncnc3s2)c1
Brc1cccc(Nc2ncnc3cnc(Br)cc23)c1
Brc1ccn[nH]1
C#CCCN1CCOC(COc2cc3ncnc(Nc4ccc(F)c(Cl)c4)c3cc2OC)C1
Policy gradient replay...
Mean value of predictions: 0.7755184
Proportion of valid SMILES: 0.6189730745147151
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(OCC4CCOC4)ccc23)cc1
Brc1cccc(Nc2cc(Nc3cccc(Br)c3)ncn2)c1
Brc1cccc(Nc2cc3ncnc(Nc4cccc(Br)c4)c3s2)c1
Brc1cccc(Nc2ncnc3cnc(Br)cc23)c1
C#CCCN(CCOC)CCc1cc2c(Nc3cccc(Cl)c3F)c(C#N)cnc2cc1OC
Fine tuning...
Mean value of predictions: 0.7519486
Proportion of valid SMILES: 0.6822020644354082
Sample trajectories:
Brc1cccc(Nc2ncnc3cc(OC4COC5CC(C4)Nc4ccc(Br)c(Br)c45)c(Br)cc23)c1
Brc1cccc(Nc2ncnc3cnc(COc4cc5ccccc5s4)cc23)c1
C#CCCN1CCC(Oc2cc3ncnc(Nc4ccc(F)c(Cl)c4F)c3cc2CC)OCCN1C
C#CCN(CCOc1cc2ncnc(Nc3ccc(Br)cc3F)c2cc1NC(=O)C=C)N1CCOCC1
C#CCN1CCN(C(C)(C)C#Cc2cc3ncnc(Nc4ccc(F)c(Cl)c4)c3cc2NC(=O)C=C)CC1

  4 Training on 13660 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 42.920916
Reward: 6.342559
Trajectories with max counts:
17	COc1cc2ncnc(Nc3ccc(Cl)c(Cl)c3)c2cc1OC
Mean value of predictions: 0.7842347
Proportion of valid SMILES: 0.6835839598997494
Sample trajectories:
Brc1ccc(Nc2ncnc3cnc(Br)cc23)cc1
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cccc(Nc2ncnc3nc(Nc4ccc(CNCCN5CCOCC5)cc4)sc23)c1
C#CC(=O)COc1cc2c(Nc3ccc(Br)c(Cl)c3)ncnc2cc1OCCC1CCC1
C#CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCC
Policy gradient replay...
Mean value of predictions: 0.7530641
Proportion of valid SMILES: 0.658018130665833
Sample trajectories:
Brc1ccc(Nc2ncnc3cnc4ccc(Br)cc4c3[nH]2)cc1
Brc1cccc(Nc2ncnc3cc(Br)ncc23)c1
Brc1cccc(Nc2ncnc3ccc(Br)nc23)c1
C#CC(=O)Nc1cc2c(Nc3cccc(Cl)c3P(C)(C)=O)n2c(N)n1
C#CC1CCCN(C(=O)C=C)Cc2nc(Nc3ccc(OCC(=O)N(C)C)cc3)nn21
Fine tuning...
Mean value of predictions: 0.81170416
Proportion of valid SMILES: 0.6675
Sample trajectories:
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2cn1
Brc1ccc2ncnc(Nc3ccc(Br)c(Br)c3)c2c1
Brc1cccc(Nc2ncnc3cc(Br)ncc23)c1
Brc1cccc(Nc2ncnc3cccc(Br)c23)c1
C#CC=CC(=O)Nc1cc2c(Nc3ccc(NCc4ccc(F)cc4)c(Cl)c3)c(C#N)cnc2cc1OC

  5 Training on 18723 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 38.931630
Reward: 6.092365
Trajectories with max counts:
42	COc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1NC(=O)C=CCN1CCOCC1
Mean value of predictions: 0.7850615
Proportion of valid SMILES: 0.637132122730119
Sample trajectories:
Brc1cccc(Nc2ncn[nH]2)c1
Brc1cccc(Nc2ncnc3cc(N4CCCC4)ncc23)c1
C#CCCCCCCCCCCCCCN(C)CCCOc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OC
C#CCCN(c1cc2c(N)ncnc2cn1)C(C)(C)C#Cc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OC
C#CN1CCC(Oc2cc3c(Nc4ccc(Cl)c(Cl)c4F)ncnc3cc2OC)CC1=O
Policy gradient replay...
Mean value of predictions: 0.805686
Proportion of valid SMILES: 0.6931498279637159
Sample trajectories:
Brc1cccc(NCc2ncnc3cc(OC4COC5CCCC54)ccc23)c1
Brc1cccc(Nc2ncnc3cc(OCCCN4CCOCC4)c(Br)cc23)c1
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
C#CC(=O)N1CCC(COc2cc3c(Nc4ccc(F)c(Cl)c4F)ncnc3cc2OC)CC1
C#CC(=O)N1CCOC(Oc2cc3c(Nc4ccc(F)c(Cl)c4)ncnc3cc2OC)CC1
Fine tuning...
Mean value of predictions: 0.8095067
Proportion of valid SMILES: 0.6973108192620388
Sample trajectories:
Brc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2cn1
Brc1ccc(Nc2ncnc3cc(OCC4COCCO4)c(Br)cc23)cc1Br
C#CCCN1CCOC(COc2cc3ncnc(Nc4ccc(F)c(Cl)c4F)c3cc2OC)C1
C#CCOc1cc2ncnc(Nc3ccc(Br)c(Cl)c3)c2cc1OC
C#CCOc1cc2ncnc(Nc3ccc(F)c(Cl)c3F)c2cc1NC(=O)C=C

  6 Training on 23835 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 41.709324
Reward: 6.602547
Trajectories with max counts:
21	COc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCC1COC2C(F)COC12
Mean value of predictions: 0.825
Proportion of valid SMILES: 0.6635367762128326
Sample trajectories:
Brc1ccc(Nc2ncc3c4cnc(Br)cc4c3n2)cc1
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
C#CCN(CC(=O)Nc1cc2ccc(Nc3ccc(F)c(Cl)c3)cc2s1)c1ccc2ncnc(Nc3ccc(F)c(Cl)c3)c2c1
C#CCOC(=O)N1CCOC(COc2cc3ncnc(Nc4ccc(Cl)c(Cl)c4)c3cc2OC)CC1
C#CCOc1cc2c(Nc3ccc(Br)c(Cl)c3)ncnc2cc1OCC
Policy gradient replay...
Mean value of predictions: 0.81259704
Proportion of valid SMILES: 0.725735754539762
Sample trajectories:
Brc1cccc(Nc2ncnc3cnc(Br)cc23)c1
C#CCN1CCC(COc2cc3ncnc(Nc4ccc(F)c(Cl)c4F)c3cc2OC)C2OCCC2(F)C1
C#CCN1CCN(CC=CC(=O)Nc2cc3c(Nc4ccc(F)c(Cl)c4F)ncnc3cc2OC)CC1
C=C(COC(=O)Nc1cc2c(Nc3ccc(Br)cc3F)ncnc2cc1OCCCCCC)N1CCOCC1
C=C=CCC1CC1COC(=O)c1cc2c(Nc3cccc(Cl)c3F)ncnc2cc1OCC1CCO1
Fine tuning...
Mean value of predictions: 0.81809485
Proportion of valid SMILES: 0.7191116671879887
Sample trajectories:
Brc1cccc(Nc2ncnc3cc(OC4CCCCC4)ccc23)c1
Brc1cccc(Nc2ncnc3ccnc(Nc4ccc(CN5CCCC5)cc4)sc23)c1
Brc1cccc(Nc2ncnc3nc(Nc4cccc(Br)c4)c3s2)c1
C#CC(C#Cc1cc2c(Nc3cccc(F)c3F)ncnc2cc1OCCOC)CC(=O)NO
C#CCCCCCCCCCCCCCCCCCCCCOc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncn2c2cccc(OCc3ccccc3)c2cc1OCCCN1CCOCC1

  7 Training on 29100 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 39.529437
Reward: 6.541060
Trajectories with max counts:
27	C=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCCCN1CCOCC1
Mean value of predictions: 0.8568441
Proportion of valid SMILES: 0.6579111944965603
Sample trajectories:
Brc1cccc(Nc2ccc(Nc3ncnc4ccccc34)nc2)c1
Brc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cccc(Nc2ncnc3ccc(OCC4CCCCC4)cc23)c1
C#CC(C#Cc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1NC(=O)C=C)C(=O)N1CCOCC1
C#CCC(C)(CC)C(=O)NCCOc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1NC(=O)C=C
Policy gradient replay...
Mean value of predictions: 0.84076214
Proportion of valid SMILES: 0.7064162754303599
Sample trajectories:
Brc1cccc(Nc2ncnc3cc(OCCC4CC5CC(N4)N5)c(Br)cc23)c1
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
Brc1cccc(Nc2ncnc3cnc(Br)cc23)c1
C#CCC(=O)N1CCC(COc2cc3ncnc(Nc4ccc(Br)cc4F)c3cc2NC(=O)C=C)CC1
C#Cc1ccccc1NC(=O)CCN1CCN(CC=C)CC1
Fine tuning...
Mean value of predictions: 0.84263873
Proportion of valid SMILES: 0.7153125
Sample trajectories:
Brc1ccc(Nc2ncnc3c(Br)cccc23)cc1
Brc1ccc(Nc2ncnc3cc(Br)c(OC4COC5CC54)cc23)cc1Br
Brc1cccc(Nc2ncnc3[nH]ncc23)c1
C#CCN1CCCOc2ccc3ccc(nc4ccn5c(ncnc45)NC3=O)COc(c2)CC1
C#CCOc1cc2ncnc(Nc3ccc(Br)c(Cl)c3)c2cc1NC(=O)C=C

  8 Training on 34429 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 38.623902
Reward: 6.826088
Trajectories with max counts:
38	COc1cc2c(Nc3cccc(Cl)c3F)ncnc2cc1OC1COC2C(F)COC12
Mean value of predictions: 0.8503325
Proportion of valid SMILES: 0.6109375
Sample trajectories:
Brc1cccc(Nc2ncnc3cc(Br)ccc23)c1
Brc1cccc(Nc2ncnc3cc(OC4COC4)c(Br)cc23)c1
C#CCCCN1CC(COc2cc3ncnc(Nc4ccc(Br)c(Cl)c4)c3cc2OC)Cc2ccccc21
C#CCN1CCC(COc2cc3ncnc(Nc4cccc(Br)c4)c3cc2OC)OC(C)OCOCCN(C)CC1
C#Cc1ccc2c(Nc3ccc(C4CCCN(C(C)C)CCC4)cc3)ncnc2c1
Policy gradient replay...
Mean value of predictions: 0.833665
Proportion of valid SMILES: 0.7224132541419194
Sample trajectories:
C#CC=CC(=O)Nc1cc2c(Nc3ccc(Br)cc3F)ncnc2cc1OCCNC(=O)C=CCN(C)C
C#CCC(CC)P(=O)(OC)n1cc2c(Nc3ccccc3F)c(C#N)cnc2c1Nc1ccc(OC)c(F)c1
C#CCCCN1CC(COc2cc3ncnc(Nc4ccc(F)c(Cl)c4)c3cc2OC)COCC1=O
C#CCCCN1COC(COc2cc3ncnc(Nc4cccc(Br)c4)c3cc2OCC)C1
C#CCN1CCOCN(CC(=O)NC(C)COc2cc3ncnc(Nc4ccc(F)c(Cl)c4)c3cc2NC(=O)C=C)CC1
Fine tuning...
Mean value of predictions: 0.85004324
Proportion of valid SMILES: 0.7247103037895396
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(OCC4CCOC4)c(Br)cc23)cc1Br
C#CC(C#CC=CC(C)(C)O)=C(C=COC)OCOC(=O)COC(=O)OCOC(=O)C=CC=CC(=O)OC
C#CCCCCCCC(=O)Nc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCC1CCN2CCCC2OC1
C#CCCOc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2cc1OCCCCCN(C(=O)C=CCN(C)C)C(C)C
C#Cc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCCO

  9 Training on 39688 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 37.673211
Reward: 7.057830
Trajectories with max counts:
71	C=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCCCN1CCOCC1
Mean value of predictions: 0.8796316
Proportion of valid SMILES: 0.6113892365456821
Sample trajectories:
Brc1ccc(Nc2ncnc3cc(OCC=CCN4CCOCC4)c(Br)cc23)cc1
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
C#CC(=CNC)C(=O)Nc1cc2nc(N)c(Nc3ccc(F)c(Cl)c3)nc2cc1OCOCOC
C#CCOc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1CN(C(=O)OCC)N1C(=O)OCC(C)(C)OC1=O
C#Cc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1NC(=O)C=C
Policy gradient replay...
Mean value of predictions: 0.8562447
Proportion of valid SMILES: 0.7463246793869253
Sample trajectories:
Brc1cccc(COc2cc3ncnc(Nc4ccc(Br)c(Br)c4)c3cc2Br)c1
Brc1cccc(Nc2ncnc3cnccc23)c1
Brc1cccc(Nc2ncnc3sccc23)c1
C#CCCCCN(CC)N1CCC(Oc2cc3c(Nc4ccc(F)c(Cl)c4F)ncnc3cc2OCCOC)NC(C)C1=O
C#CCCCCN1CCOC(COc2cc3ncnc(Nc4ccc(Br)c(Cl)c4)c3cc2OC)C1
Fine tuning...
Mean value of predictions: 0.8574624
Proportion of valid SMILES: 0.7283834586466166
Sample trajectories:
C#CC(=O)NCCCOc1cc2ncnc(Nc3ccc(F)c(F)c3F)c2cc1NC(=O)OCCOC
C#CC=CC=CCO
C#CCCN1CCOC(COc2cc3ncnc(Nc4ccc(Cl)c(Cl)c4)c3cc2OC)C1
C=C(C(=O)N(C)c1cc2c(Nc3cccc(Br)c3)ncnc2cn1)c1ccc2ncnc(Nc3cccc(Br)c3)c2c1
C=C(C)CC(=O)Nc1cc2c(Nc3ccc(Br)c(Cl)c3F)ncnc2cc1OCCOCCO

 10 Training on 45151 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.816673
Reward: 6.855337
Trajectories with max counts:
57	COc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1OC1CCOC1
Mean value of predictions: 0.8780952
Proportion of valid SMILES: 0.6576886940181648
Sample trajectories:
C#CC(=O)C=CC(=O)Nc1cc2c(Nc3ccc(F)c(Br)c3)ncnc2cc1OC1COCOC1
C#CC=CC(=O)N1CCC(Oc2cc3c(Nc4ccc(Br)c(Cl)c4F)ncnc3cc2OC)CC1
C#CCN1CCN(C(C)(C)C#Cc2cc3c(Nc4ccc(F)c(Cl)c4F)ncnc3cc2OCC)CC1
C#COC1CC(C)C(Oc2cc3ncnc(Nc4ccc(Cl)c(Cl)c4F)c3cc2OC)COC2COC12
C#Cc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OC1COC2CCOCC21
Policy gradient replay...
Mean value of predictions: 0.85875607
Proportion of valid SMILES: 0.7093241551939925
Sample trajectories:
C#CC(=O)NCCOc1cc2ncnc(Nc3ccc(Br)cc3F)c2cc1NC(=O)C=C
C#CC=CC=C1C(Nc2cccc(Br)c2)c2cccnc2n1C1COC2COCC21
C#CCCCCOc1cc2c(Nc3ccc(Br)cc3)ncnc2cc1OCC1CCN(CCO)CC1
C#CCN(CCO)C(=O)C#Cc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1NC(=O)C=C
C#CCN1CCC(COc2cc3ncnc(Nc4cc(Cl)c(Cl)c(Br)c4F)c3cc2OC)CC1
Fine tuning...
Mean value of predictions: 0.8546882
Proportion of valid SMILES: 0.7172349077259931
Sample trajectories:
Brc1cccc(Nc2cnc3c(Nc4cccc(Br)c4)ncnc3c2)c1
Brc1cccc(Nc2ncnc3ccc(Nc4cccc5ncn(-c6ccccc6)c45)cc23)c1
C#CC(=O)NCCCNc1ccnc2c(Nc3ccccc3C(=O)c3ccc(N)nc3)ncnc12
C#CCC=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCCCCCC
C#CCCCN1CCC(COc2c(OCCCOC)ccc3ncnc(Nc4ccc(F)c(Cl)c4F)c23)CC(Cl)C=C1C

 11 Training on 50630 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 39.119681
Reward: 7.029523
Trajectories with max counts:
33	COc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1OC1CCOCC1
Mean value of predictions: 0.8574011
Proportion of valid SMILES: 0.7267041901188243
Sample trajectories:
C#CC(=O)N1CCOCC(Oc2cc3c(Nc4ccc(F)c(Cl)c4F)ncnc3cc2OC)CC1
C#CC1C=CC2C(COc3cc4c(Nc5ccc(F)c(Cl)c5F)ncnc4cc3OC)CN12
C#CCN1CCC(Oc2cc3c(Nc4ccc(Cl)c(Cl)c4F)ncnc3cc2OC)OCCCCCCCCCC(CC)CCCCCC1=O
C#CCON=C(COc1cc2c(Nc3ccc(F)c(Cl)c3F)ncnc2cc1OC)C(C)C
C#CNCCOc1cc2ncnc(Nc3ccc(Cl)c(Cl)c3)c2cc1NC(=O)C=C
Policy gradient replay...
Mean value of predictions: 0.8502622
Proportion of valid SMILES: 0.7154471544715447
Sample trajectories:
Brc1ccc(Nc2ncnc3cnc(Br)cc23)cc1
C#CC(CCO)C(=O)NCCOc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1NC(=O)C=C
C#CCCCNCCCCOc1cc2ncnc(Nc3cc(F)c(Cl)c(Cl)c3F)c2cc1OC
C#CCN(CC=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3F)ncnc2cc1OCCOCOC)CCCN1CCCCC1
C#CCN1CCOC(Oc2cc3c(Nc4ccc(F)c(Cl)c4F)ncnc3cc2OC)CC1
Fine tuning...
Mean value of predictions: 0.8693001
Proportion of valid SMILES: 0.7280400125039075
Sample trajectories:
Brc1ccc2c(c1)c(Nc1nnc(N3CCCCC3)c(Br)c1Nc1ccccc1Br)cc1ccccc1O2
C#CC(=O)Nc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cn1
C#CCN1CCOC(COc2cc3ncnc(Nc4ccc(Cl)c(Cl)c4)c3cc2OCCCCCCCCOC)CC1
C#CCN1CCOC(Oc2cc3c(Nc4ccc(F)c(Cl)c4F)ncnc3cc2OCC)CC1
C#CN1CCC(Oc2cc3c(Nc4ccc(F)c(Cl)c4)ncnc3cc2OC)CCOC1=O

 12 Training on 56284 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 40.399577
Reward: 7.046311
Trajectories with max counts:
55	C=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCCCN1CCOCC1
Mean value of predictions: 0.8706746
Proportion of valid SMILES: 0.6820031298904539
Sample trajectories:
C#CC(=O)N1CCC(COc2cc3ncnc(Nc4cc(Br)ccc4F)c3cc2OC)CC1
C#CCC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCCCN1CCOCC1
C#CCCCCCCCCCCCCCC=CC=CCC(=O)Nc1ccc2ncnc(Nc3ccc(F)c(Br)c3)c2c1
C#CCCN1CCC(Oc2cc3ncnc(Nc4ccc(F)c(Cl)c4F)c3cc2OCC)CC2CN(C)CCC2C1
C#CCOc1cc2c(Nc3ccc(Cl)cc3)ncnc2cc1OC1COC2C1OCC2(O)CN1CCOCC1
Policy gradient replay...
Mean value of predictions: 0.8847559
Proportion of valid SMILES: 0.7366280888332812
Sample trajectories:
Brc1cccc(Nc2ncnc3[nH]cnc23)c1
C#CC(=NOCC1CCN(CCOc2ccc3ncnc(Nc4ccc(Cl)c(Cl)c4)c3c2)C1)C(=O)NO
C#CC1CCOC1CCOc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1OC
C#CCCCCCCOc1cc2c(Nc3cc(Cl)c(Cl)cc3Cl)ncnc2cc1OCCCN1CCOCC1
C#Cc1c(N)ncnc1Nc1cc2c(Nc3cccc(Cl)c3)ncnc2cc1OCC1CCO1
Fine tuning...
Mean value of predictions: 0.86348003
Proportion of valid SMILES: 0.7462453066332916
Sample trajectories:
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
C#CC(=O)Nc1cc2c(Nc3ccc(Br)cc3F)ncnc2cc1OCCN(C)C
C#CCCCC#CC=CC(=O)Nc1cc2c(cc1Nc1c(Cl)cccc1Cl)CCN2Cc1ccc(Cl)c(Cl)c1
C#CCN(CC)CCOc1cc2ncnc(Nc3ccc(Br)cc3F)c2cc1NC(=O)C=C
C#CCN1CCOCC(Oc2cc3c(Nc4ccc(F)c(Cl)c4)ncnc3cc2OCCOC)CC1

 13 Training on 62071 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 41.967522
Reward: 7.095435
Trajectories with max counts:
13	C=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCCCN1CCCCC1
Mean value of predictions: 0.7906727
Proportion of valid SMILES: 0.7638713745271122
Sample trajectories:
C#CC(=O)N1CCOCC(COc2cc3ncnc(Nc4ccc(F)c(Cl)c4F)c3cc2NC(=O)C=C)CC1
C#CC(=O)NCCCCOc1cc2ncnc(Nc3ccccc3C(=O)Nc3ccc(F)c(Cl)c3F)c2cc1NCCCCC
C#CC(C=CN1CCC(Oc2cc3ncnc(Nc4ccc(Br)cc4F)c3cc2NC(=O)C=C)NC1=O)=NO
C#CC=CC=CC(=O)Nc1cc2c(Nc3ccc(Br)cc3F)ncnc2cc1OCC(C)NC(C)=O
C#CC=CCCCCCCCCCCCCCCC=CCCCCCCCCCCCCNC(=O)OCCCNC(=O)OCCS(=O)(=O)NCCCO
Policy gradient replay...
Mean value of predictions: 0.88681835
Proportion of valid SMILES: 0.7282676672920575
Sample trajectories:
C#CCCCN1CCC(COc2cc3ncnc(Nc4cc(Cl)c(Br)c(Cl)c4F)c3cc2OC)C2OCCN21
C#CCN1CCOC(COc2cc3ncnc(Nc4cc(Cl)c(Br)cc4OC)c3cc2OC)C1
C#CCN1CCOC(COc2cc3ncnc(Nc4cc(Cl)c(F)c(OCCCCN(C)CC)c4)c3cc2OC)CC1
C#CN1OCC(O)C2OCC(Oc3cc4ncnc(Nc5ccc(Cl)c(Cl)c5)c4cc3OC)C3OCOCC=C3CCC21
C#Cc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCC1COCOCO1
Fine tuning...
Mean value of predictions: 0.88158333
Proportion of valid SMILES: 0.7507037847982484
Sample trajectories:
Brc1ccc2c(Nc3ccc(Br)c(Br)c3)ncnc2c1
C#CC(=O)N1CCC(Oc2cc3c(Nc4cc(F)c(C(F)(F)F)cc4F)ncnc3cc2OC)N(CC)CC1
C#CC(=O)Nc1cc2c(Nc3ccc(Cl)cc3Cl)ncnc2cc1OCC1CCCC1
C#CC(C=C)=CC=C(C)C#CC(=O)Nc1cc2c(Nc3cccc(Cl)c3OC)c(Cl)cnc2cc1OC1CCOC1
C#CC1OC2(F)COC3(CCCN4CC=CC43)C1OCC2Oc1cc2ncnc(Nc3ccc(F)c(Cl)c3F)c2cc1OC

 14 Training on 67889 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 41.689679
Reward: 6.948383
Trajectories with max counts:
35	COc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1OC1CCOCC1
Mean value of predictions: 0.84536564
Proportion of valid SMILES: 0.6717772215269087
Sample trajectories:
C#CCCCCCCOc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCCCCCCCCCC(=O)OC1OCOCO1
C#CCCN1CCC(Oc2cc3c(Nc4ccc(F)c(Cl)c4F)ncnc3cc2OC)c2nc(Nc3ccc(F)c(Cl)c3)ncc21
C#CCN1CCOC(COc2cc3ncnc(Nc4cc(F)c(F)c(Cl)c4F)c3cc2OC)C1
C#CCOc1cc2ncnc(Nc3ccc(Cl)c(Cl)c3)c2cc1NC(=O)C=CCOC1COCCOCCO1
C#Cc1cc(NNC(=O)c2sc3ncnc(Nc4ccc(OCc5cccnc5)c(Cl)c4)nc23)cc(Cl)c1OCC1COC1
Policy gradient replay...
Mean value of predictions: 0.87744457
Proportion of valid SMILES: 0.7192872772741482
Sample trajectories:
C#CC(=NOCC1COCOC1)C1CCOCC1
C#CC(C)(C#Cc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OC)C1(N)CCOCCOCCO1
C#CC=CC(=O)Nc1cc2c(Nc3cccc(Br)c3)ncnc2cc1OCCC1CCCCN1
C#CCC#CC(=O)Nc1cc2c(Nc3ccc(Br)cc3F)ncnc2cc1OC1COCCO1
C#CCCN1CCOCOC(COc2cc3ncnc(Nc4ccc(Br)c(Cl)c4F)c3cc2OC)C1
Fine tuning...
Mean value of predictions: 0.8642609
Proportion of valid SMILES: 0.7194244604316546
Sample trajectories:
Brc1ccc(Nc2n[nH]c3ccc(Br)cc23)cc1
Brc1ccc(Nc2ncnc3ccc(Br)cc23)cc1
Brc1ccnc2ccc(Nc3ncnc4ccccc34)cc12
C#CC(=O)Nc1cc2c(Nc3cccc(Br)c3)ncnc2cc1OC1COCO1
C#CC(C#CC=C(Cl)C(C)C#CC=CC(=O)Nc1cc2c(Nc3cccc(Br)c3F)ncnc2cc1OC)=CCOC

 15 Training on 73410 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 45.994566
Reward: 7.302647
Trajectories with max counts:
21	C=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1C#CC(C)(C)N1CCOCC1
Mean value of predictions: 0.85173315
Proportion of valid SMILES: 0.7219268063809822
Sample trajectories:
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
C#CC(=O)N1CCOC(Oc2cc3c(Nc4cccc(Cl)c4F)ncnc3cc2OCC)CC1
C#CC(=O)Nc1cc2c(Nc3cccc(Br)c3)ncnc2cn1
C#CC(C#CC(=O)Nc1cc2c(Nc3ccccc3)ncnc2cc1OCCOC(=O)c1ccc2cnc(Nc3ccccc3)nc2c1)NO
C#CC(C)CCCN1CCOCC1COc1cc2c(Nc3cccc(Cl)c3F)ncnc2cc1OCC1CCOCC1
Policy gradient replay...
Mean value of predictions: 0.85271496
Proportion of valid SMILES: 0.7889305816135085
Sample trajectories:
C#CC(=O)NCCCC(=O)Nc1cc2c(Nc3ccccc3F)ncnc2cc1OC1COCCO1
C#CC(CC=CC=CCN1CCOCC1)COc1cc2c(Nc3cccc(Cl)c3)ncnc2cc1OC1CCOC1
C#CCC(=O)Nc1cc2c(Nc3ccc(Br)c(Cl)c3F)ncnc2cc1OCCCCCCCN1CCOCC1
C#CCCCCCCCCCOc1cc2c(Nc3ccc(Br)c(Cl)c3)ncnc2cc1OCC1COCC1CO
C#CCCCCCCOC(=O)COCOc1cc2c(Nc3cccc(Cl)c3)ncnc2cc1OC
Fine tuning...
Mean value of predictions: 0.86841214
Proportion of valid SMILES: 0.740462789243277
Sample trajectories:
C#CC(NC(=O)OC(C)(C)C)C(=O)NCCOc1cc2ncnc(Nc3ccc(F)c(Br)c3)c2cc1NC(=O)C=C
C#CCCCCCCCCOc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCC1CCOCC1
C#CCNC(=O)CCCCCCCN(CC)CCCCCCCCOc1cc2c(Nc3cc(Cl)cc(Cl)c3)ncnc2cc1OC
C#CCOc1cc2c(Nc3ccc(Br)c(Cl)c3)ncnc2cc1OC1COC2CC1C2
C#CCOc1cc2ncnc(Nc3cccc(Cl)c3)c2cc1C#CC(=O)Nc1cc2c(Nc3ccc(Br)c(Br)c3)ncnc2cc1OCOC

 16 Training on 79286 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 43.909089
Reward: 7.179977
Trajectories with max counts:
16	COc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCC1CCOCC1
16	COc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCC1COCCO1
Mean value of predictions: 0.89243835
Proportion of valid SMILES: 0.8122845502977123
Sample trajectories:
C#CC(C#Cc1cc2ncnc(Nc3cc(Cl)c(Cl)c(Cl)c3F)c2cc1NC(=O)C=C)N1CCOCC1
C#CC(C#N)(C=O)C(=O)NCCOc1cc2c(Nc3cccc(C(F)(F)F)c3)ncnc2cc1OC
C#CC(CO)=NOCCOC(=O)c1cc2c(Nc3cccc(Cl)c3)ncnc2cc1OCCCN(C)C
C#CCCCCCN(CCOc1cc2ncnc(Nc3cc(Cl)c(Cl)c(Cl)c3)c2cc1NC(=O)C=C)NC(=O)COC1OCCOC1OC
C#CCCN1CCOCOCC(COc2cc3ncnc(Nc4ccc(Cl)c(Cl)c4)c3cc2OC)C1
Policy gradient replay...
Mean value of predictions: 0.87909883
Proportion of valid SMILES: 0.7502347417840376
Sample trajectories:
Brc1cccc(Nc2ncnc3ccc(Br)cc23)c1
C#CC(=O)COC(=O)c1cc2c(Nc3ccccc3)ncnc2cc1OCCCN1CCCCC1
C#CC1NC(N2CCOCC2)C2OCC(COc3cc4ncnc(Nc5ccc(Br)c(Cl)c5F)c4cc3OC)C(=O)N12
C#CC1c2c(NC(=O)c3c(NC(=O)C=C)cc4c(c3Nc3cccs3)c3ccccc3-4)cccc2C(=O)N1C
C#CCCCN1CCOC(COc2cc3ncnc(Nc4cccc(Cl)c4F)c3cc2OC)C1
Fine tuning...
Mean value of predictions: 0.8767692
Proportion of valid SMILES: 0.782051282051282
Sample trajectories:
Brc1ccc(Nc2ncc3c(n2)N2CCCC2=Cc2ccc(Br)c3c2)c(Br)c1
C#CC(=NOCCNCCOc1ccc2ncnc(Nc3ccc(Oc4ccccc4)c(Cl)c3)c2c1)C(=O)OCC
C#CC=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCCCN1CCCCC1
C#CCCCCCCCCCCCCCCCCCCCOc1cc2c(Nc3cccc(Cl)c3)ncnc2cc1OCC1CCCCC1
C#CCCCCCCOc1cc2c(Nc3ccc(Br)cc3F)ncnc2cc1OCCO

 17 Training on 85661 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 43.942216
Reward: 7.475944
Trajectories with max counts:
17	COc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCCCN1CCOCC1
Mean value of predictions: 0.9141139
Proportion of valid SMILES: 0.7837837837837838
Sample trajectories:
Brc1cccc(Nc2ccnc3cncnc23)c1
C#CC(NCCO)C(=O)NCCCOc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1NC(=O)C=C
C#CCCCCCCCCCCCCCCCC=CCN(CC#CC(=O)NCC(=O)O)CCOc1cc2c(Nc3cccc(Br)c3)ncnc2cc1OCC1COCCO1
C#CCCCCCCN1CCCC(COc2cc3ncnc(Nc4ccc(F)c(Cl)c4)c3cc2NC(=O)C=C)CC1
C#CCCCOCCOc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
Policy gradient replay...
Mean value of predictions: 0.8937499
Proportion of valid SMILES: 0.7311424100156495
Sample trajectories:
C#CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCC1COCO1
C#CCC(C)(C)N1CCC(Oc2cc3ncnc(Nc4ccc(F)c(Cl)c4F)c3cc2OCCOC)CC1
C#CCC1C(C)N(C)CCC(Oc2cc3c(Nc4ccc(Cl)c(Cl)c4F)ncnc3cc2OC)C2CC(N)CCN21
C#CCCCCCCCCCCCCCCOc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCC1COC2COCC2CN1O
C#Cc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1NC(=O)C=C
Fine tuning...
Mean value of predictions: 0.86364365
Proportion of valid SMILES: 0.7866082603254068
Sample trajectories:
C#CC(=O)Nc1cc2c(Nc3ccccc3C)ncnc2cc1OC
C#CC(COCc1cc2c(Nc3ccc(OCCCCC(=O)N(C)C)cc3)ncnc2cc1OCCCCCCCC(C)C)CC(=O)OCC
C#CCC(=O)N1CCC(Oc2cc3c(Nc4cccc(Cl)c4F)ncnc3cc2OCC)c([PH](F)(F)F)c2OC(C=CC(=O)NC3CCC(O)CC3)CCN21
C#CCCCCCCCCOc1cc2ncnc(Nc3ccc(Cl)c(Cl)c3F)c2cc1OC
C#CCCCN1CCOC(COc2cc3ncnc(Nc4ccc(Cl)c(Cl)c4)c3cc2OC)C1

 18 Training on 91963 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 45.439463
Reward: 7.755480
Trajectories with max counts:
29	C=CC(=O)Nc1cc2c(Nc3cccc(Br)c3)ncnc2cn1
Mean value of predictions: 0.9222175
Proportion of valid SMILES: 0.7461610780319649
Sample trajectories:
BrCCCCCCCCCCCOc1cc2c(Nc3ccccc3)ncnc2cc1OCC1COCCO1
Brc1cccc(Nc2ncnc3ccccc23)c1
Brc1cccc(Nc2ncnc3cnc(Nc4ccccc4Br)cc23)c1
C#CC(NO)C(=O)NCCOc1cc2ncnc(Nc3ccc(Br)cc3F)c2cc1NC(=O)C=C
C#CC=C(NCCCOc1cc2ncnc(Nc3cccc(Br)c3)c2cc1NC(=O)C=C)N(C)C
Policy gradient replay...
Mean value of predictions: 0.8878513
Proportion of valid SMILES: 0.7574334898278561
Sample trajectories:
C#CC(=O)N1CCC(COc2cc3ncnc(Nc4ccc(Br)c(Br)c4F)c3cc2N)CC1
C#CC(C#CC(=O)Nc1cc2c(F)c(Cl)ccc2nc1NC(=O)CO)C#CC(C)C(=O)N(C)C
C#CC(CCCO)=NOCCCN1CC2CCCCC2CO1
C#CCCCCCCCCCCC#CCCCCCCC1CCN(CCOc2cc3ncnc(Nc4cccc(Cl)c4)c3nc2OC)CC1
C#CCCCCCCCCCCCCCCCOc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCCCN1CCCCC1
Fine tuning...
Mean value of predictions: 0.88923836
Proportion of valid SMILES: 0.7557084766969033
Sample trajectories:
Brc1ccsc1Nc1nc2ccccn2c2c1nc1ccccc12
C#CC(C#N)(CC=CC(=O)Nc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OC)CCO
C#CCCC(=O)NCCOc1cc2ncnc(Nc3cccc(Cl)c3)c2cc1NC(=O)C=C
C#CCCCCCCCCCCCCOc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCCCCCCCCCCCC(=O)NO
C#CCCCCCCCCCCOc1cc2c(Nc3cccc(Cl)c3Cl)ncnc2cc1OCC1CCCO1

 19 Training on 98239 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 44.785802
Reward: 7.524108
Trajectories with max counts:
33	COc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1OC
Mean value of predictions: 0.85360277
Proportion of valid SMILES: 0.7130325814536341
Sample trajectories:
C#CC(=O)OCC#CCN1CCN(C(C)(C)C#Cc2cc3c(Nc4cccc(Cl)c4F)ncnc3cc2OCCOCOCOC)CC1
C#CCN(CC#Cc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OC)CCOC
C#Cc1c(C#N)cc(Nc2ncnc(OC)n2)c(Cl)cc(Nc2ccc(Cl)cc2F)c1C#C
C=CC(=O)Nc1cc(Nc2ncc(Cl)c(Nc3ccccc3P(C)(C)=O)n2)c(OC)cc1CN1CCOCC1
C=CC(=O)Nc1cc2c(Nc3cc(Cl)c(Cl)c(Cl)c3)c(Cl)c3c(N)ncnc3cnc2cc1OC
Policy gradient replay...
Mean value of predictions: 0.85548276
Proportion of valid SMILES: 0.7649452269170579
Sample trajectories:
C#CCCCCOc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OC1COC2CCC(C1)N2CCCC1CCCCCC1
C#CCCCOc1cc2ncnc(Nc3ccc(Cl)c(Cl)c3)c2cc1Nc1ccc2ncnc(Nc3ccc(Cl)cc3)c2c1
C#CCOc1cc2ncnc(Nc3ccc(Cl)c(Cl)c3)c2cc1NC(=O)C=CCN1CCOCC1
C#COC1COC2C(Oc3cc4ncnc(Nc5ccc(Br)c(Cl)c5)c4cc3OC)COC12
C#Cc1cc(Br)ccc1N1C(=O)N(c2cccc(NC(=O)C=C)c2)C1=O
Fine tuning...
Mean value of predictions: 0.89071
Proportion of valid SMILES: 0.7724851143842055
Sample trajectories:
Brc1ccc(CC#CCCN2CCCCCC2c2cc3c(Nc4cccc(Br)c4)ncnc3cn2)cc1
C#CC1(C)CC2C(COc3cc4ncnc(Nc5ccc(Br)c(Cl)c5)c4cc3OCC=C)CC3CC32C1
C#CC1C(=O)NCC(COc2cc3ncnc(Nc4cc(F)c(Cl)c(Br)c4F)c3cc2OC)CN1C
C#CCC=Cc1ccc2ncnc(Nc3ccc(Cl)c(Cl)c3)c2c1
C#CCCCCCCCCCCCCCCCCCCCCCOc1cc2c(Nc3cccc(Cl)c3)ncnc2cc1OCOc1ccc(CO)cc1

 20 Training on 104183 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 45.438374
Reward: 7.464371
Trajectories with max counts:
16	COc1cc2ncnc(Nc3cc(F)c(Cl)c(Cl)c3)c2cn1
Mean value of predictions: 0.840785
Proportion of valid SMILES: 0.7364121897580899
Sample trajectories:
Brc1cc(Br)cc(Nc2ncnc3cnc(Br)cc23)c1
C#CC=CC(=O)Nc1cc2c(Nc3cc(Br)c(Cl)c(Br)c3F)ncnc2cc1OCC1CCOCC1
C#CCCCCC=CCCCCCCCC(=O)Nc1cc2c(Nc3cc(Cl)c(F)c(Cl)c3F)ncnc2cc1OCCCN1CCOCC1
C#CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCOc1cc2ncnc(Nc3cc(Cl)c(Cl)c(Cl)c3)c2cc1NC(=O)C=CC(=O)N1CCN(C(=O)OCCCCCCC)CC1
C#CCCCCCCCN1COc2cc3ncnc(Nc4cc(Cl)c(Cl)c(Cl)c4)c3cc2O1
Policy gradient replay...
Mean value of predictions: 0.8674465
Proportion of valid SMILES: 0.7761427676894177
Sample trajectories:
C#CC1(C)CC(F)C(N2CCOC(COc3cc4ncnc(Nc5cccc(Br)c5F)c4cc3OC)CC2)O1
C#CC1CCN(CCC(=O)Nc2cc3ncnc(Nc4cccc(Br)c4)c3cc2OC)CC1
C#CC1N(C)CC(OC2COC3C(Oc4cc5ncnc(Nc6ccc(Br)c(Cl)c6F)c5cc4OC)COC23)C2CCON21
C#CCCCCCOc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCCCCCCCCCCCCCCCCCC(C)NCC(=O)OC1COC2COCCOC2C1O
C#CCCOc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCCCO
Fine tuning...
Mean value of predictions: 0.890625
Proportion of valid SMILES: 0.8017538365173817
Sample trajectories:
C#CC(=O)N1CCOC(Oc2cc3c(Nc4ccc(Cl)c(Cl)c4F)ncnc3cc2OCCOC)CC1
C#CC(COc1cc2c(Nc3ccccc3)ncnc2cc1OCCCCCCN(C)C)C(CO)CCCN(C)C
C#CC1NC(c2nc(COc3cc4ncnc(Nc5ccc(F)c(Cl)c5)c4cc3OC)cs2)C1O
C#CCCCCCCOc1cc2c(Nc3cccc(Cl)c3)ncnc2cc1OCCOC
C#CCCCCOc1cc2c(Nc3ccc(Cl)c(Cl)c3)ncnc2cc1OCC1CN(C)CCO1

Trajectories with max counts:
53	C=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCCCN1CCOCC1
Mean value of predictions: 0.88856
Proportion of valid SMILES: 0.7018147684605757
Mean Internal Similarity: 0.6543524283525606
Std Internal Similarity: 0.09420970628492194
Mean External Similarity: 0.5035420755233856
Std External Similarity: 0.1083184780588715
Mean MolWt: 536.6191401944897
Std MolWt: 106.36726574271414
Effect MolWt: 0.34519337313323556
Mean MolLogP: 5.38064660960292
Std MolLogP: 2.0216687476786364
Effect MolLogP: 0.3543558970869079
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 96.266369% (3455 / 3589)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/gen_actives.smi', 'primed_path': '../checkpoints/generator/egfr_clf_rnn_primed'}:
{'duration': 5967.337291717529, 'valid_fraction': 0.7018147684605757, 'active_fraction': 0.8802496656263932, 'max_counts': 53, 'mean_internal_similarity': 0.6543524283525606, 'std_internal_similarity': 0.09420970628492194, 'mean_external_similarity': 0.5035420755233856, 'std_external_similarity': 0.1083184780588715, 'mean_MolWt': 536.6191401944897, 'std_MolWt': 106.36726574271414, 'effect_MolWt': 0.34519337313323556, 'mean_MolLogP': 5.38064660960292, 'std_MolLogP': 2.0216687476786364, 'effect_MolLogP': 0.3543558970869079, 'generated_scaffolds': 3589, 'novel_scaffolds': 3455, 'novel_fraction': 0.9626636946224575, 'save_path': '../logs/primed_model_s2-2.smi'}


  1 Training on 219 replay instances...
Setting threshold to 0.050000
Policy gradient...
Loss: 39.260029
Reward: 6.945594
Trajectories with max counts:
125	O=C(CSc1nc(Nc2ccccc2)c2ccccc2n1)N1CCOCC1
Mean value of predictions: 0.40948838
Proportion of valid SMILES: 0.3359375
Sample trajectories:
C(=NNc1ncnc2sc3c(c12)CCC3)c1ccc2c(c1)OCCCO2
CC(=O)N1CCNc2ccc(Nc3ncnc4ccccc34)cc2NC1=O
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(ccc34)CCC5)ccc21
Policy gradient replay...
Mean value of predictions: 0.42339557
Proportion of valid SMILES: 0.3846875
Sample trajectories:
C=Cn1cnc(Nc2ncnc3scc(-c4ccccc4)c23)c1
CC(=O)N1CCCC1CC1CCN(CCc2ccc(Nc3ncnc4sccc34)cn2)CC1
CC(=O)N1CCCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
Fine tuning...
Mean value of predictions: 0.40961987
Proportion of valid SMILES: 0.4029384182557049
Sample trajectories:
C=CCOc1csc(NC(C)C(=O)c2sc3nccc(C)c3c2C)n1
CC(=O)N1CCN(Cc2ccccc2Nc2ccc(Nc3ncnc4ccccc34)cc2)CC1
CC(=O)N1CCc2c(Cn3ccnc3)cnc2c1C
CC(=O)N1CCc2cc(C1)Nc1nc(C(=O)N(C)C)ccc1n2
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21

  2 Training on 2321 replay instances...
Setting threshold to 0.200000
Policy gradient...
Loss: 39.622150
Reward: 7.123210
Trajectories with max counts:
482	O=S1(=O)CCCN1c1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.48967814
Proportion of valid SMILES: 0.2815625
Sample trajectories:
C#CCn1cc(Nc2ncnc3sc4c(c23)CCC4)cn1
CC(C)(C)S(=O)(=O)c1cccc(Nc2ncccn2)c1
CC(C)(NC1CCOCC1)c1nc(Nc2cccc(S(=O)(=O)N3CCOCC3)c2)c2ccc(-c3ccccc3)csc2n1
CC(Nc1ccc(Nc2ccccc2Nc2ncccc2OC(CO)Nc2ccc(NS(=O)(=O)c3ccccn3)cc2)cc1)c1ccccc1
CC(Nc1ncnc2sccc12)C(N)=O
Policy gradient replay...
Mean value of predictions: 0.50874126
Proportion of valid SMILES: 0.3575
Sample trajectories:
Brc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)N1CCCCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCCCc2nc(Nc3cccc(CO)c3)ccc21
CC(=O)N1CCCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc5c(c34)CCC5)ccc21
Fine tuning...
Mean value of predictions: 0.49721983
Proportion of valid SMILES: 0.3709375
Sample trajectories:
CC(=O)N(C)c1ccc(Nc2ncnc3sccc23)cn1
CC(=O)N(Nc1ncnc2sccc12)c1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)N1CC(c2ccc(Nc3ncnc4sccc34)c(C)c2)c2ccc(C)nc2Nc2ccc(Nc3ncnc4sccc34)cc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCOC4)nc4sc5c(c34)CCC5)ccc21

  3 Training on 4227 replay instances...
Setting threshold to 0.350000
Policy gradient...
Loss: 37.157115
Reward: 6.837348
Trajectories with max counts:
566	O=S1(=O)CCCN1c1ccc(Nc2ncnc3sccc23)cc1
Mean value of predictions: 0.5324853
Proportion of valid SMILES: 0.319375
Sample trajectories:
C=C(Sc1ccc(Nc2ncnc3sccc23)cc1)c1ccccc1
CC(=O)N1CCc2[nH]c3ccccc3c2CC1C(=O)Nc1cccc(Nc2ncnc3ncsc23)c1
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
Policy gradient replay...
Mean value of predictions: 0.52289534
Proportion of valid SMILES: 0.3971875
Sample trajectories:
CC#CCn1ccc(Nc2ncnc3c(F)cccc23)c1
CC(=Nc1ccc(Nc2ncnc3sccc23)cc1)N1CCOCC1
CC(=O)N1CCC(Nc2ncnc3sc(COc4ccccc4)cc23)C1
CC(=O)N1CCCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCN(c2ccc(Nc3ncnc4sccc34)cc2)CC1
Fine tuning...
Mean value of predictions: 0.5230153
Proportion of valid SMILES: 0.3896875
Sample trajectories:
C(=Nc1ncnc2scc(-c3cccs3)c12)c1cccnc1
C=C(Nc1ncnc2sccc12)c1cccc(Nc2ncnc3c2sc2ccccc23)c1
C=CCS(=O)(=O)N(CCn1ccnc1)c1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
CC(=O)N1CCc2cc(C3CCC3)Nc3ncnc4sc5c(ccc21)c(Cl)ccc5c34
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21

  4 Training on 6265 replay instances...
Setting threshold to 0.500000
Policy gradient...
Loss: 38.496401
Reward: 6.710067
Trajectories with max counts:
74	Cc1sc2nc(CN3CCOCC3)nc(Nc3ccc4c(c3)OCO4)c2c1C
Mean value of predictions: 0.4967901
Proportion of valid SMILES: 0.3796875
Sample trajectories:
Brc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=Nc1c(Nc2ccccc2)c2ccccc2S1=O)N1CCCC1
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
Policy gradient replay...
Mean value of predictions: 0.53081864
Proportion of valid SMILES: 0.389375
Sample trajectories:
Brc1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
CC(=O)N1CCc2c(Nc3ccc(N4CCO4)cc3)ncnc21
CC(=O)N1CCc2cc(N3CCOCC3)Nc3nc(CN4CCOCC4)nc4sc5c(ccc2O1)ccc5c(CCO)Nc34
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
Fine tuning...
Mean value of predictions: 0.5352853
Proportion of valid SMILES: 0.41625
Sample trajectories:
C=C1CCCC(Nc2ccc(Nc3ncnc4sccc34)cc2)C1C(N)=O
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)NC(=O)c1ccc(Nc2nc(CN3CCCO3)cs2)cc1
CC(=O)Nc1ccc(Nc2ncnc3onc(C)c23)cc1

  5 Training on 8277 replay instances...
Setting threshold to 0.650000
Policy gradient...
Loss: 35.069832
Reward: 6.172460
Trajectories with max counts:
182	Cc1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
Mean value of predictions: 0.495972
Proportion of valid SMILES: 0.356875
Sample trajectories:
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)Nc1cccc(Nc2cnc(Nc3ccccc3C)c(C)c2)c1
CC(=O)c1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC(=O)c1ccccc1Nc1ncnc2sccc12
CC(CNc1cc(Nc2ncnc3ccccc23)ccc1F)c1ccccc1
Policy gradient replay...
Mean value of predictions: 0.576514
Proportion of valid SMILES: 0.423125
Sample trajectories:
CC(=O)N1CCN(c2ccc(Nc3ncnc4ccccc34)cc2)CC1
CC(=O)N1CCNc2cc(Nc3cnn(CC4CCCCO4)c3)ccc2N1C
CC(=O)N1CCNc2cc(Nc3ncnc4sc5c(ccc34)CCC5)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc5c(c34)CCC5)ccc21
Fine tuning...
Mean value of predictions: 0.5811605
Proportion of valid SMILES: 0.4146875
Sample trajectories:
C=CC(Nc1cc2c3sc(ncn1)C23CCO)C1CCCCC1=CC
CC(=O)C1CCCN(CCSc2nc(Nc3ccccc3)c3ccccc3n2)CC1
CC(=O)N1CCCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CC5)ccc21

  6 Training on 10201 replay instances...
Setting threshold to 0.800000
Policy gradient...
Loss: 33.644736
Reward: 5.796914
Trajectories with max counts:
88	Cc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
Mean value of predictions: 0.6147287
Proportion of valid SMILES: 0.403125
Sample trajectories:
C#CC(c1ccc(C)nc1)c1scc(C)c1F
C#CC(c1ccc(Nc2cnn(C)n2)cc1)c1cc(C)cs1
C=CC(=O)Oc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=N)c1ccc(Nc2ncnc3scc(-c4cccs4)c23)cc1
CC(=O)N1CCCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
Policy gradient replay...
Mean value of predictions: 0.6191679
Proportion of valid SMILES: 0.420625
Sample trajectories:
CC(=O)N(C)c1cc(Nc2ncnc3sccc23)ccc1F
CC(=O)N(C)c1cccc(Nc2ncnc3sc4c(c23)CCCC4)c1
CC(=O)N1CCC2CCCCNc3ccccc3Nc3ccccc3Nc3ccccc3C2CCc2ccccc21
CC(=O)N1CCc2cc(C(=O)Nc3ccc(Nc4nc(C)nc5sc6c(c45)CCC6)cc3)sc2NC1=O
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
Fine tuning...
Mean value of predictions: 0.59799284
Proportion of valid SMILES: 0.4359375
Sample trajectories:
C#CC(C#N)c1scc(C)c1-c1ccc(Nc2ncnc3ccsc23)cc1
C=CCn1cc(Nc2ncnc3sc(-c4ccccc4)cc23)cn1
CC(=NNc1nc(CN2CCOCC2)nc2sccc12)c1ccco1
CC(=O)N(CCC1CC1)c1ccc(Nc2ncnc3sccc23)cc1
CC(=O)N1CCC(Cc2ccccc2)CC1CNc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1

  7 Training on 12195 replay instances...
Setting threshold to 0.950000
Policy gradient...
Loss: 34.254815
Reward: 5.796673
Trajectories with max counts:
59	COc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1S(N)(=O)=O
Mean value of predictions: 0.63100165
Proportion of valid SMILES: 0.393125
Sample trajectories:
C#CCCOc1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
C(=Cc1nc(CN2CCOCC2)nc2sccc12)c1cncs1
C(=Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1)c1ccoc1
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
Policy gradient replay...
Mean value of predictions: 0.61812824
Proportion of valid SMILES: 0.4240625
Sample trajectories:
C#Cc1scc(C)c1Cl
CC(=Nc1cccc(Nc2ncnc3ccccc23)c1)C1CCCO1
CC(=O)N(CCc1ccccc1)c1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
CC(=O)N1CCC(Cn2cncn2)O1
CC(=O)N1CCc2cc(C1)Nc1nc(CN3CCCO3)ccc1n2
Fine tuning...
Mean value of predictions: 0.6262898
Proportion of valid SMILES: 0.4421875
Sample trajectories:
C#CCC(=O)C(C#N)c1scc(C)c1C(=O)Nc1nc(Nc2ccccc2)c2ccccc2n1
C=CCn1c(C(CCSC)Nc2ncnc3sc(-c4ccccc4)cc23)nc2ccsc21
CC(=O)C1CN(c2ncnc3sc(-c4ccccc4)cc23)c2ccccc2O1
CC(=O)N1CCN(C(=O)CCO)C1
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21

  8 Training on 14139 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.424869
Reward: 5.544582
Trajectories with max counts:
84	CCc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1S(N)(=O)=O
Mean value of predictions: 0.7119449
Proportion of valid SMILES: 0.408125
Sample trajectories:
C=CCn1cc(Nc2ncnc3ccccc23)cc1CC
CC(=O)N1CCCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCCc2ccsc2N1CCCCCC1=NCC1
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
Policy gradient replay...
Mean value of predictions: 0.59890634
Proportion of valid SMILES: 0.4571875
Sample trajectories:
CC(=Nc1ncnc2sccc12)c1ccccc1NC(=O)c1cccc(Nc2ncnc3ccccc23)c1
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sccc34)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4ccsc34)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
Fine tuning...
Mean value of predictions: 0.6101623
Proportion of valid SMILES: 0.4428125
Sample trajectories:
C=CCOc1ccc(Nc2ncnc3scc(-c4ccc(Cl)c(Cl)c4)c23)cc1
CC(=O)N1CCc2c(c3cccnc3C)sc3ncnc4cccc(C1=O)-n4c23
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)NCc1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
CC(=O)Nc1cc(Nc2ncnc3c2oc2ccccc23)cc(C)c1Cl

  9 Training on 16221 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 32.564030
Reward: 5.535431
Trajectories with max counts:
350	COc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1S(N)(=O)=O
Mean value of predictions: 0.6927189
Proportion of valid SMILES: 0.3390625
Sample trajectories:
C=C(Nc1nnc(C(F)F)nc1C(F)(F)F)c1nccn1C
CC#CCn1ccc(Nc2ncnc3ccsc23)c1
CC(=O)C(C)Sc1nc(Nc2ccccc2)c2ccccc2n1
CC(=O)N(C)c1ccc(Nc2ncnc3sc4c(c23)CCCCC4)cc1
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
Policy gradient replay...
Mean value of predictions: 0.6646176
Proportion of valid SMILES: 0.4371875
Sample trajectories:
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4scnc34)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
CC(=O)Nc1cc(Nc2cnc(CN3CCOCC3)cn2)nc(-c2ccncc2)n1
CC(=O)Nc1cc(Nc2ncnc3scc(-c4ccccc4)c23)ccc1F
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
Fine tuning...
Mean value of predictions: 0.6632467
Proportion of valid SMILES: 0.4446875
Sample trajectories:
CC(=O)N1CCCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCCc2cc(Nc3ncnc4sccc34)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2cc(Nc3nccs3)ccc21

 10 Training on 18316 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.322992
Reward: 5.921538
Trajectories with max counts:
65	Cc1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
Mean value of predictions: 0.5598789
Proportion of valid SMILES: 0.4128125
Sample trajectories:
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCCCC4)cc1
CC(=O)Nc1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC(=O)Nc1cccc(Nc2ncnc3sc4c(c23)CCCC4)c1
CC(=O)Oc1cccc2c(Nc3ccccc3)nc(C)nc12
Policy gradient replay...
Mean value of predictions: 0.7072634
Proportion of valid SMILES: 0.4259375
Sample trajectories:
CC(=Cc1ccoc1C)c1ccccc1
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2nc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)NS(=O)(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)Nc1cc(Nc2ncnc3sc4c(c23)CCC4)ccc1S(=O)(=O)N(C)C
Fine tuning...
Mean value of predictions: 0.68416667
Proportion of valid SMILES: 0.45
Sample trajectories:
CC(=O)C(C)Nc1cc(Nc2ccc(F)cc2)ncn1
CC(=O)N1CCc2c(sc(C(=O)Oc3ccc(Nc4ncnc5sccc45)cc3)c2C2CC2)C1C
CC(=O)N1CCc2cc(Nc3nccs3)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sccc34)ccc21
CC(=O)NC(=O)CSc1nc(Nc2ccccc2)c2ccccc2n1

 11 Training on 20488 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 34.583391
Reward: 5.852355
Trajectories with max counts:
127	CCNC(=O)c1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
Mean value of predictions: 0.6367168
Proportion of valid SMILES: 0.5042654028436019
Sample trajectories:
CC#CCOCCn1cccc1Nc1ncnc2ccsc12
CC(=NOC(=O)c1ccccc1Nc1nc(CN2CCCCC2)nc2sc(-c3ccccc3)cc12)c1ccccc1F
CC(=Nc1ccccc1Nc1ccccc1)C(=O)c1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2cc(Nc3nccs3)ccc21
Policy gradient replay...
Mean value of predictions: 0.7207061
Proportion of valid SMILES: 0.4621901474741136
Sample trajectories:
C=CCOCCOc1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
C=CCn1cc(Nc2ncnc3sc4c(c23)CCCC4)cn1
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)Nc1cc(Nc2ncnc3scc(-c4ccccc4)c23)ccc1Cl
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
Fine tuning...
Mean value of predictions: 0.7208451
Proportion of valid SMILES: 0.44375
Sample trajectories:
C1=C(COc2cccc(Nc3ncnc4ccsc34)c2)OCC1
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)NNc1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC(=O)Nc1ccc(C(C)=O)Nc2ncnc3scc(cc1)-c1ccccc1c23
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1

 12 Training on 23124 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 35.672711
Reward: 6.071584
Trajectories with max counts:
93	COc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1S(N)(=O)=O
Mean value of predictions: 0.70041704
Proportion of valid SMILES: 0.3746875
Sample trajectories:
C1#CCOc2cc(Nc3ncnc4ccsc34)ccc2OC1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)c1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1Cl
CC1CCc2c(sc3ncnc(Nc4ccc5c(c4)OCCCO5)c23)C1
CC1CCc2ncnc3oncc3c3cc(ccc3c3cc(ncn3)N1)N2
Policy gradient replay...
Mean value of predictions: 0.7458963
Proportion of valid SMILES: 0.47713032581453635
Sample trajectories:
C=CCn1cc(Nc2ncnc3ccsc23)cc1C
CC#CCNc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1S(=O)(=O)N(C)C
CC#CCOc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1S(N)(=O)=O
CC(=O)N1CCCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2cc(N3CCCCCC3)nc(Nc3cccc(N)c3)c2C1
Fine tuning...
Mean value of predictions: 0.7091405
Proportion of valid SMILES: 0.45826820881525476
Sample trajectories:
C#Cc1cccc(Nc2ncnc3sc4c(c23)CC4)c1
CC(=O)N1CCc2cc(Nc3nccs3)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)N1c(c2nc3ccccc3o2)cc2c(Nc3ncnc4ccsc34)cccc21
CC(=O)NC(=O)c1cccc(Nc2ncnc3sc4c(c23)CCC4)c1

 13 Training on 25658 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 37.788055
Reward: 6.377093
Trajectories with max counts:
166	CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
Mean value of predictions: 0.7449296
Proportion of valid SMILES: 0.44569993722536094
Sample trajectories:
CC#CCn1cc(Nc2ncnc3sc4c(c23)CCC4)cc1Cl
CC#CCn1ccc(Nc2ncnc3ccsc23)n1
CC(=N)c1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
CC(=NNc1ccc(Nc2ncnc3ccsc23)cc1)NCC=O
CC(=NOCCn1ccc(Nc2ncnc3ccsc23)c1)N1CCOCC1
Policy gradient replay...
Mean value of predictions: 0.6961413
Proportion of valid SMILES: 0.47796186308221317
Sample trajectories:
C=CCCc1c(NCc2coc(C)c2)nc(CN2CCOCC2)nc1Nc1ccccc1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1S(=O)(=O)N(C)C
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
CC(=O)Nc1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
Fine tuning...
Mean value of predictions: 0.7252129
Proportion of valid SMILES: 0.47733666770865896
Sample trajectories:
CC(=Nc1ccc(Nc2ncnc3ccsc23)cc1)C1CCC1
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
CC(=O)Nc1ccc(Nc2ncnc3cc4nc2c2ccc4Nc2s3)cc1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cn1

 14 Training on 28400 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 36.860513
Reward: 6.401361
Trajectories with max counts:
143	CCc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1S(=O)(=O)N(C)C
Mean value of predictions: 0.7859197
Proportion of valid SMILES: 0.41540880503144656
Sample trajectories:
CC(=O)N1CCCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCCC4)cn1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCCCC4)cc1
Policy gradient replay...
Mean value of predictions: 0.71161205
Proportion of valid SMILES: 0.45778611632270166
Sample trajectories:
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cn1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCCC4)nc1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCCCC4)cc1
Fine tuning...
Mean value of predictions: 0.7586756
Proportion of valid SMILES: 0.471875
Sample trajectories:
C=CCOc1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC#Cc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)Nc1cc(C)cc(Nc2ncnc3scc(-c4cccs4)c23)c1

 15 Training on 31184 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 38.547080
Reward: 6.876608
Trajectories with max counts:
595	CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
Mean value of predictions: 0.8178848
Proportion of valid SMILES: 0.3634375
Sample trajectories:
C#CC(CCCCCCCCCCCCCCCCCCCCCCCCC)CCC(C)C(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
C=CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
C=CCn1cc(Nc2ncnc3ccsc23)cc1C
C=Cc1c2sc3ncnc(c13)Nc1cc(C)c(c(OC)c1)CC2C
CC(=N)Nc1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
Policy gradient replay...
Mean value of predictions: 0.7579086
Proportion of valid SMILES: 0.478125
Sample trajectories:
C=CCOCCOc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)N(C(=O)CO)c1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)NC(=O)c1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
CC(=O)Nc1cc(Nc2ncnc3sc4c(c23)C4)ccc1Cl
Fine tuning...
Mean value of predictions: 0.74270964
Proportion of valid SMILES: 0.48498122653316644
Sample trajectories:
C#CCC(Nc1nc(COC)nc2sc(C)c(C)c12)c1cc(C)c(C)c(C)c1
C#Cc1ccc(Nc2nc(C)nc3sc4c(c23)CCC4)cc1
C=CCn1ccc(Nc2ncnc3sccc23)c1
C=Cn1ccc(Nc2ncnc3ccsc23)c1
CC#CCn1ccc(Nc2ncnc3ccsc23)n1

 16 Training on 34045 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 38.336218
Reward: 6.766605
Trajectories with max counts:
196	Cc1nc(Nc2cnn(C)c2)c2c3c(sc2n1)CCC3
Mean value of predictions: 0.75684303
Proportion of valid SMILES: 0.3721875
Sample trajectories:
C(=Nc1nc(Nc2ccncc2)c2ccsc2n1)C1CCCCC1
C=CC(=O)c1cccc(Nc2ncnc3sc4c(c23)CC4)c1
CC#CC1CCC2c3ccc(Nc4ncnc5ccsc45)cc3CCN1CC2C
CC(=O)C1CN(c2ncnc3sc(-c4ccccc4)cc23)c2ccccc2O1
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
Policy gradient replay...
Mean value of predictions: 0.7068257
Proportion of valid SMILES: 0.47170990934667084
Sample trajectories:
CC#Cc1ccc(Nc2ncnc3ccc(F)cc23)cc1
CC(=O)Nc1cc(Nc2ncnc3ccccc23)ccc1F
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1S(N)(=O)=O
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
Fine tuning...
Mean value of predictions: 0.75006264
Proportion of valid SMILES: 0.499843603378167
Sample trajectories:
C#Cc1ccc(Nc2nc(C)c(CCCCSC3CCCO3)c3c2c2sc4c2c43)cc1Nc1cccc(CO)c1
CC#CCn1ccc(Nc2ncnc3ccsc23)n1
CC(=N)c1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC(=O)N1C(=O)c2cc(Nc3ncnc4ccsc34)ccc2Nc2ccccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21

 17 Training on 36759 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 40.724368
Reward: 6.941086
Trajectories with max counts:
49	CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
Mean value of predictions: 0.7902069
Proportion of valid SMILES: 0.4544030084612974
Sample trajectories:
CC#Cc1ccc(Nc2ncnc3sc4c(c23)CC4)cc1
CC(=O)N1CCc2c(Nc3ccc(NC(=O)C4CC4)cc3)ccnc2s1
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)NC(C)Cc1csc2ncnc(Nc3ccc4c(c3)OCO4)c12
Policy gradient replay...
Mean value of predictions: 0.76122314
Proportion of valid SMILES: 0.4804626445764301
Sample trajectories:
C=CCCCc1csc2c(Nc3ccc4nc(C)oc4c3)ncnc12
CC(=O)N1CCCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1C
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cn1
Fine tuning...
Mean value of predictions: 0.7837596
Proportion of valid SMILES: 0.4890556597873671
Sample trajectories:
C=CCNc1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC(=NO)c1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21
CC(=O)Nc1cc(Nc2ncnc3sc4c(c23)CCC4)ccc1Nc1nccc(CN2CCOCC2)n1
CC(=O)Nc1cc(Nc2ncnc3sccc23)c2c3c(sc2n1)CCCC3

 18 Training on 39896 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 41.288166
Reward: 7.274158
Trajectories with max counts:
284	Cn1cc(Nc2ncnc3sc4c(c23)CCC4)cn1
Mean value of predictions: 0.78742135
Proportion of valid SMILES: 0.298125
Sample trajectories:
CC#CCn1cc(Nc2ncnc3sc4c(c23)CCC4)cn1
CC(=N)c1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC(=Nc1ccc(Nc2ncnc3ccccc23)cc1)C1CCO1
CC(=Nc1ccc(Nc2ncnc3ccsc23)cc1)C1CC1
CC(=Nc1ccc(Nc2ncnc3ccsc23)cc1)c1ccc(F)cc1
Policy gradient replay...
Mean value of predictions: 0.7980535
Proportion of valid SMILES: 0.5177952755905512
Sample trajectories:
CC#CC1CCCCCCCNC(=O)C1
CC#CCn1ccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC(=Nc1ccc(Nc2nc(C)nc3scc(-c4cccs4)c23)cc1)C1CCNC1
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sc(C)c(C)c34)ccc21
CC(=O)Nc1cc(NC(=O)Cn2ccc(Nc3ncnc4ccccc34)c2)ccc1F
Fine tuning...
Mean value of predictions: 0.77287716
Proportion of valid SMILES: 0.5115625
Sample trajectories:
C=C(C)Sc1cc2c(ncnc-2Nc2ccc(OCCCCCCCCCCC)c3ncccc23)s1
C=CCn1ccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC(=Cc1ccco1)Nc1nc(CN2CCOCC2)nc2scc(-c3cccs3)c12
CC(=O)N1CCCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)N1CCc2cc(Nc3nc(CN4CCOCC4)nc4sc5c(c34)CCC5)ccc21

 19 Training on 42832 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 39.334103
Reward: 7.062909
Trajectories with max counts:
332	Cc1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
Mean value of predictions: 0.71163243
Proportion of valid SMILES: 0.3196875
Sample trajectories:
CC(=O)Nc1cc(Nc2ncnc3sc4c(c23)CCC4)ccc1F
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CC4)cc1
CC(=O)Nc1cccc(Nc2ncnc3sc4c(c23)CC4)c1
CC(=O)Nc1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC(=O)Nc1ccccc1Nc1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
Policy gradient replay...
Mean value of predictions: 0.78822815
Proportion of valid SMILES: 0.5198738170347004
Sample trajectories:
Brc1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
C=C1C(CCc2ccc(Nc3ncnc4ccsc34)cc2)=C1Nc1cccc(Nc2cncnc2)c1
C=CC(=O)c1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC#CCn1ccc(Nc2ncnc3ccsc23)c1
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
Fine tuning...
Mean value of predictions: 0.7782288
Proportion of valid SMILES: 0.5101976780671478
Sample trajectories:
C=CCOc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1
CC(=O)N(c1cc(Nc2ncnc3sccc23)ccc1F)C1CC1
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)Nc1cc(Nc2ncnc3sc4c(F)cccc4c23)ccc1F
CC(=O)Nc1cc(Nc2ncnc3sc4c(c23)CCCC4)ccc1F

 20 Training on 45733 replay instances...
Setting threshold to 1.000000
Policy gradient...
Loss: 39.396814
Reward: 7.063999
Trajectories with max counts:
127	CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCCC4)cc1
Mean value of predictions: 0.82681566
Proportion of valid SMILES: 0.5034375
Sample trajectories:
C(=CC1CC1)c1ccc(Nc2ncnc3sccc23)cc1
CC#CC1CCC(C)Nc2cc(Nc3ncnc4ccsc34)ccc21
CC(=O)C=Cc1ccccc1Nc1ncnc2sc3c(c12)CCC3
CC(=O)N1CCC(=O)Nc2cccc(Nc3ncnc4ccsc34)c2C1
CC(=O)N1CCCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
Policy gradient replay...
Mean value of predictions: 0.7997492
Proportion of valid SMILES: 0.4984375
Sample trajectories:
CC(=NOC(F)(F)F)c1cccc(Nc2ncnc3sc4c(c23)CCC4)c1
CC(=O)N1CCN(Cc2cccc(Nc3ncnc4ccccc34)c2)CC1
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)Nc1cc(Nc2ncnc3sc4c(c23)CCC4)ccc1Nc1cccc(Cl)c1
CC(=O)Nc1cc(Nc2ncnc3sc4c(c23)CCCC4)ccn1
Fine tuning...
Mean value of predictions: 0.7873433
Proportion of valid SMILES: 0.5234375
Sample trajectories:
CC(=Nc1ccc(Nc2ncnc3ccsc23)cc1)c1cccc(F)c1
CC(=O)N1CCN(Cc2ccc(Nc3ncnc4ccccc34)cc2)CC1
CC(=O)N1CCc2cc(Nc3nc(CN4CCCC4)nc4sccc34)ccc21
CC(=O)N1CCc2cc(Nc3ncnc4sc5c(c34)CCC5)ccc21
CC(=O)Nc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1

Trajectories with max counts:
214	COc1ccc(Nc2ncnc3sc4c(c23)CCC4)cc1S(N)(=O)=O
Mean value of predictions: 0.75044304
Proportion of valid SMILES: 0.3667729233077067
Mean Internal Similarity: 0.6177453541523228
Std Internal Similarity: 0.15969171502856538
Mean External Similarity: 0.4417925084268319
Std External Similarity: 0.06247691085265868
Mean MolWt: 402.55366902237944
Std MolWt: 113.09424435072519
Effect MolWt: -0.8659461732136385
Mean MolLogP: 5.373699684334513
Std MolLogP: 2.8579209683668054
Effect MolLogP: 0.28941885914110804
<IPython.core.display.HTML object>
Percentage of novel scaffolds: 99.128686% (1479 / 1492)
<IPython.core.display.HTML object>
Metrics for {'n_iterations': 20, 'n_policy': 15, 'n_policy_replay': 10, 'n_fine_tune': 20, 'seed': 2, 'replay_data_path': '../data/egfr_enamine.smi', 'primed_path': '../checkpoints/generator/egfr_clf_rnn_enamine_primed'}:
{'duration': 5701.042859315872, 'valid_fraction': 0.3667729233077067, 'active_fraction': 0.7234151329243353, 'max_counts': 214, 'mean_internal_similarity': 0.6177453541523228, 'std_internal_similarity': 0.15969171502856538, 'mean_external_similarity': 0.4417925084268319, 'std_external_similarity': 0.06247691085265868, 'mean_MolWt': 402.55366902237944, 'std_MolWt': 113.09424435072519, 'effect_MolWt': -0.8659461732136385, 'mean_MolLogP': 5.373699684334513, 'std_MolLogP': 2.8579209683668054, 'effect_MolLogP': 0.28941885914110804, 'generated_scaffolds': 1492, 'novel_scaffolds': 1479, 'novel_fraction': 0.9912868632707775, 'save_path': '../logs/primed_model_s2-3.smi'}
